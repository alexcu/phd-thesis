Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Huang:2005tc,
abstract = {The area under the ROC (Receiver Operating Characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. In this paper, we establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications. {\textcopyright}2005 IEEE.},
author = {Huang, Jin and Ling, Charles X},
doi = {10.1109/TKDE.2005.50},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {AUC of ROC,Accuracy,Evaluation of learning algorithms,ROC},
number = {3},
pages = {299--310},
title = {{Using AUC and accuracy in evaluating learning algorithms}},
volume = {17},
year = {2005}
}
@inproceedings{guo2016ms,
address = {Amsterdam, The Netherlands},
author = {Guo, Yandong and Zhang, Lei and Hu, Yuxiao and He, Xiaodong and Gao, Jianfeng},
booktitle = {Proceedings of the 16th European Conference on Computer Vision},
doi = {10.1007/978-3-319-46487-9_6},
pages = {87--102},
publisher = {Springer},
title = {{MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition}},
year = {2016}
}
@article{Kitchenham:2010wy,
abstract = {Systematic literature reviews (SLRs) are a major tool for supporting evidence-based software engineering. Adapting the procedures involved in such a review to meet the needs of software engineering and its literature remains an ongoing process. As part of this process of refinement, we undertook two case studies which aimed 1) to compare the use of targeted manual searches with broad automated searches and 2) to compare different methods of reaching a consensus on quality. For Case 1, we compared a tertiary study of systematic literature reviews published between January 1, 2004 and June 30, 2007 which used a manual search of selected journals and conferences and a replication of that study based on a broad automated search. We found that broad automated searches find more studies than manual restricted searches, but they may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers, or they are assessing research trends in research methodologies. For Case 2, we analyzed the process used to evaluate the quality of SLRs. We conclude that if quality evaluation of primary studies is a critical component of a specific SLR, assessments should be based on three independent evaluators incorporating at least two rounds of discussion. {\textcopyright}2010 Springer Science+Business Media, LLC.},
author = {Kitchenham, Barbara A and Brereton, Pearl and Turner, Mark and Niazi, Mahmood K and Linkman, Stephen and Pretorius, Rialette and Budgen, David},
doi = {10.1007/s10664-010-9134-8},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Automated search,Broad search,Case study,Manual search,Mapping studies,Quality evaluation process,Systematic literature review,Targeted search},
number = {6},
pages = {618--653},
title = {{Refining the systematic literature review process-two participant-observer case studies}},
volume = {15},
year = {2010}
}
@article{Geiger:2018fv,
abstract = {Computational research and data analytics increasingly relies on complex ecosystems of open source software (OSS) “libraries” – curated collections of reusable code that programmers import to perform a specific task. Software documentation for these libraries is crucial in helping programmers/analysts know what libraries are available and how to use them. Yet documentation for open source software libraries is widely considered low-quality. This article is a collaboration between CSCW researchers and contributors to data analytics OSS libraries, based on ethnographic fieldwork and qualitative interviews. We examine several issues around the formats, practices, and challenges around documentation in these largely volunteer-based projects. There are many different kinds and formats of documentation that exist around such libraries, which play a variety of educational, promotional, and organizational roles. The work behind documentation is similarly multifaceted, including writing, reviewing, maintaining, and organizing documentation. Different aspects of documentation work require contributors to have different sets of skills and overcome various social and technical barriers. Finally, most of our interviewees do not report high levels of intrinsic enjoyment for doing documentation work (compared to writing code). Their motivation is affected by personal and project-specific factors, such as the perceived level of credit for doing documentation work versus more ‘technical' tasks like adding new features or fixing bugs. In studying documentation work for data analytics OSS libraries, we gain a new window into the changing practices of data-intensive research, as well as help practitioners better understand how to support this often invisible and infrastructural work in their projects.},
author = {Geiger, R. Stuart and Varoquaux, Nelle and Mazel-Cabasse, Charlotte and Holdgraf, Chris},
doi = {10.1007/s10606-018-9333-1},
issn = {1573-7551},
journal = {Computer Supported Cooperative Work: CSCW: An International Journal},
keywords = {Collaboration,Documentation,Ethnography,Infrastructure,Invisible work,Motivations,Open source,Peer production,Standards},
month = {may},
number = {3-6},
pages = {767--802},
title = {{The Types, Roles, and Practices of Documentation in Data Analytics Open Source Software Libraries: A Collaborative Ethnography of Documentation Work}},
volume = {27},
year = {2018}
}
@inproceedings{Jeong:2009tu,
abstract = {All software today is written using libraries, toolkits, frameworks and other application programming interfaces (APIs). We performed a user study of the online documentation a large and complex API for Enterprise Service-Oriented Architecture (eSOA), which identified many issues and recommendations for making API documentation easier to use. eSOA is an appropriate testbed because the target user groups range from high-level business experts who do not have significant programming expertise (and thus are end-participant developers), to professional programmers. Our study showed that the participants' background influenced how they navigated the documentation. Lack of familiarity with business terminology was a barrier we observed for developers without business application experience. Participants with business software experience had difficulty differentiating similarly named services. Both groups avoided areas of the documentation that had an inconsistent visual design. A new design for the documentation that supports flexible navigation strategies seem to be required to support the wide range of users for eSOA. This paper summarizes our study and provides recommendations for future documentation for developers. {\textcopyright} 2009 Springer Berlin Heidelberg.},
address = {Siegen, Germany},
author = {Jeong, Sae Young and Xie, Yingyu and Beaton, Jack and Myers, Brad A. and Stylos, Jeff and Ehret, Ralf and Karstens, Jan and Efeoglu, Arkin and Busse, Daniela K.},
booktitle = {Proceedings of the First International Symposium on End User Development},
doi = {10.1007/978-3-642-00427-8\_6},
issn = {0302-9743},
keywords = {API Design,Business Solution Architects,Documentation,Service-Oriented Architecture,Usability,Web Services},
month = {mar},
pages = {86--105},
publisher = {Springer},
title = {{Improving documentation for eSOA APIs through user studies}},
volume = {5435 LNCS},
year = {2009}
}
@article{Shannon:1963ti,
address = {Urbana, IL, USA},
author = {Shannon, Claude E and Weaver, Warren},
doi = {10.1002/j.1538-7305.1948.tb01338.x},
journal = {The Bell System Technical Journal},
number = {3},
pages = {379--423},
publisher = {The University of Illinois Press},
title = {{The mathematical theory of communication}},
volume = {27},
year = {1948}
}
@article{996017,
abstract = {Multiobjective evolutionary algorithms (EAs) that use nondominated sorting and sharing have been criticized mainly for their: 1) O(MN3) computational complexity (where M is the number of objectives and N is the population size); 2) nonelitism approach; and 3) the need for specifying a sharing parameter. In this paper, we suggest a nondominated sorting-based multiobjective EA (MOEA), called nondominated sorting genetic algorithm II (NSGA-II), which alleviates all the above three difficulties. Specifically, a fast nondominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to Pareto-archived evolution strategy and strength-Pareto EA - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multiobjective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective seven-constraint nonlinear problem, are compared with another constrained multiobjective optimizer and much better performance of NSGA-II is observed.},
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T},
doi = {10.1109/4235.996017},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Constraint handling,Elitism,Genetic algorithms,Multicriterion decision making,Multiobjective optimization,Pareto-optimal solutions},
month = {apr},
number = {2},
pages = {182--197},
title = {{A fast and elitist multiobjective genetic algorithm: NSGA-II}},
volume = {6},
year = {2002}
}
@misc{Howard:2018tz,
annote = {Accessed: 28 August 2018},
author = {Howard, Christian},
month = {may},
title = {{Introducing Google AI}},
url = {http://bit.ly/2uI6vAr},
year = {2018}
}
@article{Barua:2012gz,
abstract = {Programming question and answer (Q{\&}A) websites, such as Stack Overflow, leverage the knowledge and expertise of users to provide answers to technical questions. Over time, these websites turn into repositories of software engineering knowledge. Such knowledge repositories can be invaluable for gaining insight into the use of specific technologies and the trends of developer discussions. Previous work has focused on analyzing the user activities or the social interactions in Q{\&}A websites. However, analyzing the actual textual content of these websites can help the software engineering community to better understand the thoughts and needs of developers. In the article, we present a methodology to analyze the textual content of Stack Overflow discussions. We use latent Dirichlet allocation (LDA), a statistical topic modeling technique, to automatically discover the main topics present in developer discussions. We analyze these discovered topics, as well as their relationships and trends over time, to gain insights into the development community. Our analysis allows us to make a number of interesting observations, including: the topics of interest to developers range widely from jobs to version control systems to C{\#} syntax; questions in some topics lead to discussions in other topics; and the topics gaining the most popularity over time are web development (especially jQuery), mobile applications (especially Android), Git, and MySQL. {\textcopyright}2012 Springer Science+Business Media New York.},
author = {Barua, Anton and Thomas, Stephen W and Hassan, Ahmed E},
doi = {10.1007/s10664-012-9231-y},
issn = {1573-7616},
journal = {Empirical Software Engineering},
keywords = {Knowledge repository,Latent Dirichlet allocation,Mining software repositories,Q{\&}A websites,Topic models,Trend analysis},
number = {3},
pages = {619--654},
title = {{What are developers talking about? An analysis of topics and trends in Stack Overflow}},
volume = {19},
year = {2014}
}
@inproceedings{Barnett:2015ec,
abstract = {Quality attributes are essential in software architecture and they are determined by identifying the concerns of the stakeholders of a system. The concerns of constructing mobile applications (apps) are quite specific due to the characteristics of mobile devices. These concerns have not been adequately addressed in industry standards and practices. In this paper, we present a mobile app development conceptual model comprising six key concepts that impact quality. Using two case studies, we show that these interrelated concepts influence the architectural decisions of mobile apps and their tradeoffs need to be well considered. As such, we suggest that these concepts should be first class entities when designing mobile app architecture to ensure that the quality attributes are satisfied.},
address = {Montreal, QC, Canada},
author = {Barnett, Scott and Vasa, Rajesh and Tang, Antony},
booktitle = {Proceedings of the 12th Working IEEE/IFIP Conference on Software Architecture},
doi = {10.1109/WICSA.2015.28},
isbn = {978-1-47-991922-2},
keywords = {Mobile Software Architecture,Quality Model},
month = {may},
pages = {105--114},
publisher = {IEEE},
title = {{A Conceptual Model for Architecting Mobile Applications}},
year = {2015}
}
@article{Radford2019,
abstract = {Our model, called GPT-2 (a successor to GPT), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper.},
author = {Radford, Alec and Wu, Jeffrey and Amodei, Dario and Amodei, Daniela and Clark, Jack and Brundage, Miles and Sutskever, Ilya},
journal = {OpenAI},
title = {{GPT2: Better Language Models and Their Implications}},
year = {2019}
}
@book{Meltzoff:1998wg,
abstract = {Could the research you read be fundamentally flawed? Could crucial effects in methodology slip by you undetected? To become an informed, interactive consumer of research, you may need an attitude adjustment: from acceptance to inquiry, from reverence to skepticism. Critical Thinking About Research: Psychology and Related Fields equips you with those tools needed to identify errors in others' research and to reduce them to a minimum in your own work. (PsycINFO Database Record (c) 2019 APA, all rights reserved) (Source: publicity materials)},
author = {Meltzoff, Julian and Cooper, Harris},
doi = {10.1037/0000052-000},
edition = {2nd},
publisher = {American Psychological Association},
title = {{Critical thinking about research: Psychology and related fields}},
year = {2018}
}
@incollection{Jick:1979el,
abstract = {This article describes and discusses issues related to research design and data analysis in the mixing of qualitative and quantitative methods. It is increasingly desirable to use multiple methods in research, but questions arise as to how best to design and analyze the data generated by mixed methods projects. I offer a conceptualization for such design, discuss issues of sampling, and describe a strategy for processing qualitative data in ways that allow for more sophisticated and dynamic integration with quantita- tive data. Finally, drawing on data from previous research, I describe tools and strategies for this dynamic data integration and illustrate how effective strategy and use of tools allow for more efficient and sophisticated analysis, interpretation, and presentation.},
author = {Mayring, Philipp},
booktitle = {Mixed Methodology in Psychological Research},
chapter = {6},
doi = {10.1163/9789087903503_007},
isbn = {978-9-07-787473-8},
pages = {27--36},
publisher = {Sense Publishers},
title = {{Mixing Qualitative and Quantitative Methods}},
year = {2007}
}
@article{Polyzotis2018a,
abstract = {Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.},
author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
doi = {10.1145/3299887.3299891},
issn = {01635808},
journal = {SIGMOD Record},
title = {{Data lifecycle challenges in production machine learning: A survey}},
year = {2018}
}
@book{Pressman:2005vf,
author = {Pressman, Roger S},
edition = {8th},
isbn = {978-0-07-802212-8},
publisher = {McGraw-Hill},
title = {{Software Engineering: A Practitioner's Approach}},
year = {2005}
}
@article{Abdalkareem2017WhatOverflow,
author = {Abdalkareem, R and Shihab, E and Rilling, J},
doi = {10.1109/MS.2017.31},
journal = {IEEE Software},
number = {2},
pages = {53--60},
title = {{What Do Developers Use the Crowd For? A Study Using Stack Overflow}},
volume = {34},
year = {2017}
}
@article{Seaman:1999vc,
abstract = {While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behavior. This paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout. Index Terms - Qualitative methods, data collection, data analysis, experimental design, empirical software engineering, participant observation, interviewing. {\textcopyright}1999 IEEE.},
author = {Seaman, Carolyn B},
doi = {10.1109/32.799955},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {4},
pages = {557--572},
title = {{Qualitative methods in empirical studies of software engineering}},
volume = {25},
year = {1999}
}
@inproceedings{Goodman:2016wf,
abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {{Wachter, Mitterlstadt}, Floridi},
booktitle = {Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning},
eprint = {1606.08813},
keywords = {machine learning},
month = {jun},
pages = {26--30},
title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
year = {2016}
}
@article{Henning:2009hz,
author = {Henning, Michi},
doi = {10.1145/1506409.1506424},
issn = {0001-0782},
journal = {Communications of the ACM},
number = {5},
pages = {46--56},
title = {{API design matters}},
volume = {52},
year = {2009}
}
@article{Borsci2009,
abstract = {The System Usability Scale (SUS), developed by Brooke (Usability evaluation in industry, Taylor {\&} Francis, London, pp 189-194, 1996), had a great success among usability practitioners since it is a quick and easy to use measure for collecting users' usability evaluation of a system. Recently, Lewis and Sauro (Proceedings of the human computer interaction international conference (HCII 2009), San Diego CA, USA, 2009) have proposed a two-factor structure-Usability (8 items) and Learnability (2 items)-suggesting that practitioners might take advantage of these new factors to extract additional information from SUS data. In order to verify the dimensionality in the SUS' two-component structure, we estimated the parameters and tested with a structural equation model the SUS structure on a sample of 196 university users. Our data indicated that both the unidimensional model and the two-factor model with uncorrelated factors proposed by Lewis and Sauro (Proceedings of the human computer interaction international conference (HCII 2009), San Diego CA, USA, 2009) had a not satisfactory fit to the data. We thus released the hypothesis that Usability and Learnability are independent components of SUS ratings and tested a less restrictive model with correlated factors. This model not only yielded a good fit to the data, but it was also significantly more appropriate to represent the structure of SUS ratings. {\textcopyright} 2009 Marta Olivetti Belardinelli and Springer-Verlag.},
author = {Borsci, Simone and Federici, Stefano and Lauriola, Marco},
doi = {10.1007/s10339-009-0268-9},
issn = {16124782},
journal = {Cognitive Processing},
keywords = {Questionnaire,System Usability Scale,Usability evaluation},
title = {{On the dimensionality of the System Usability Scale: A test of alternative measurement models}},
year = {2009}
}
@article{Niemeyer2008240,
author = {Niemeyer, Horst F and Niemeyer, Alice C},
issn = {0165-4896},
journal = {Mathematical Social Sciences},
keywords = {Alabama Paradox,Apportionment method,D'Hondt method,Hare/Hamilton method,Sainte-Lagu{\"{e}} method},
number = {2},
pages = {240--253},
title = {{Apportionment methods}},
volume = {56},
year = {2008}
}
@article{Dhar:2000vo,
abstract = {Prediction in financial domains is notoriously difficult for a number of reasons. First, theories tend to be weak or non-existent, which makes problem formulation open ended by forcing us to consider a large number of independent variables and thereby increasing the dimensionality of the search space. Second, the weak relationships among variables tend to be nonlinear, and may hold only in limited areas of the search space. Third, in financial practice, where analysts conduct extensive manual analysis of historically well performing indicators, a key is to find the hidden interactions among variables that perform well in combination. Unfortunately, these are exactly the patterns that the greedy search biases incorporated by many standard rule learning algorithms will miss. In this paper, we describe and evaluate several variations of a new genetic learning algorithm (GLOWER) on a variety of data sets. The design of GLOWER has been motivated by financial prediction problems, but incorporates successful ideas from tree induction and rule learning. We examine the performance of several GLOWER variants on two UCI data sets as well as on a standard financial prediction problem (S{\&}P500 stock returns), using the results to identify one of the better variants for further comparisons. We introduce a new (to KDD) financial prediction problem (predicting positive and negative earnings surprises), and experiment with GLOWER, contrasting it with tree- and rule-induction approaches. Our results are encouraging, showing that GLOWER has the ability to uncover effective patterns for difficult problems that have weak structure and significant nonlinearities. {\textcopyright}2000 Kluwer Academic Publishers.},
author = {Dhar, Vasant and Chou, Dashin and Provost, Foster},
doi = {10.1023/A:1009848126475},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Data mining,Financial prediction,Genetic algorithms,Investment decision making,Knowledge discovery,Machine learning,Rule learning,Systematic trading},
number = {4},
pages = {69--80},
title = {{Discovering interesting patterns for investment decision making with GLOWER - A genetic learner overlaid with entropy reduction}},
volume = {4},
year = {2000}
}
@book{Hwang:2017tr,
address = {Cambridge, MA, USA},
author = {Wang, Kai},
isbn = {978-0-26-203641-2},
pages = {624},
publisher = {MIT Press},
title = {{Cloud Computing for Machine Learning and Cognitive Applications: A Machine Learning Approach}},
year = {2017}
}
@book{Rosenberry:1992up,
author = {Rosenberry, Ward and Kenney, David and Fisher, Gerry},
isbn = {978-1-56-592005-7},
publisher = {O'Reilly {\&} Associates, Inc.},
title = {{Understanding DCE}},
year = {1992}
}
@article{Wettschereck:1997vw,
abstract = {Many lazy learning algorithms are derivatives of the k-nearest neighbor (k-NN) classifier, which uses a distance function to generate predictions from stored instances. Several studies have shown that k-NN's performance is highly sensitive to the definition of its distance function. Many k-NN variants have been proposed to reduce this sensitivity by parameterizing the distance function with feature weights. However, these variants have not been categorized nor empirically compared. This paper reviews a class of weight-setting methods for lazy learning algorithms. We introduce a framework for distinguishing these methods and empirically compare them. We observed four trends from our experiments and conducted further studies to highlight them. Our results suggest that methods which use performance feedback to assign weight settings demonstrated three advantages over other methods: they require less pre-processing, perform better in the presence of interacting features, and generally require less training data to learn good settings. We also found that continuous weighting methods tend to outperform feature selection algorithms for tasks where some features are useful but less important than others.},
author = {Wettschereck, Dietrich and Aha, David W and Mohri, Takao},
doi = {10.1007/978-94-017-2053-3_11},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Comparison,Feature weights,Lazy learning,k-nearest neighbor},
number = {1-5},
pages = {273--314},
title = {{A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms}},
volume = {11},
year = {1997}
}
@book{Litwin:1995wt,
abstract = {This Second Edition Has Been Thoroughly Revised And Updated And Efforts Have Been Made To Enhance The Usefulness Of The Book. In This Edition A New Chapter The Computer : Its Role In Research Have Been Added Keeping In View Of The Fact That Computers By Now Become A Indispensable Part Of Research Equipment. The Other Salient Feature Of This Revised Edition, Subject Contents Have Been Developed And Restructured At Several Places. New Problems Have Also Been Added In Various Chapters.Adoption Of Appropriate Methodology Is An Essential Characteristic Of Quality Research Studies Irrespective Of The Discipline With Which They Are Related. The Present Book Provides The Basic Tenets Of Methodological Research So That Researchers May Become Familiar With The Art Of Using Research Methods And Techniques.The Book Contains Introductory Explanations Of Several Quantitative Methods Enjoying Wide Use In Social Sciences. It Covers A Fairly Wide Range, Related To Research Methodology. The Presentations Are Uniformly Economical And Cogent. Illustrations Given Are Meaningful And Relevant. The Book Can Be Taken As A Well-Organised Guide For Researchers Whose Methodological Background Is Not Extensive.The Book Is Primarily Intended To Serve As A Textbook For Social Science Students Of All Indian Universities. It Will Also Serve As A Text For The Students Of M.Phil, Management, And Students Of Various Institutes. It Will Serve All Practitioners Doing Research Of One Form Or Other In A General Way.},
address = {Thousand Oaks, CA, USA},
author = {Litwin, Mark},
doi = {10.4135/9781483348957},
isbn = {978-0-80-395704-6},
publisher = {SAGE},
title = {{How to Measure Survey Reliability and Validity}},
volume = {7},
year = {1995}
}
@book{Judd:1991ug,
author = {Judd, C M and Smith, E R and Kidder, L H},
editor = {Kidder, L H},
isbn = {978-0-03-031149-9},
publisher = {Holt, Rinehart, and Winston},
title = {{Research Methods in Social Relations}},
year = {1991}
}
@book{Shull:2007vh,
abstract = {Empirical studies have become an integral element of software engineering research and practice. This unique text/reference includes chapters from some of the top international empirical software engineering researchers and focuses on the practical knowledge necessary for conducting, reporting and using empirical methods in software engineering. Part 1, 'Research Methods and Techniques', examines the proper use of various strategies for collecting and analysing data, and the uses for which those strategies are most appropriate. Part 2, 'Practical Foundations', provides a discussion of several important global issues that need to be considered from the very beginning of research planning. Finally, 'Knowledge Creation' offers insight on using a set of disparate studies to provide useful decision support. Topics and features: Offers information across a range of techniques, methods, and qualitative and quantitative issues, providing a toolkit for the reader that is applicable across the diversity of software development contexts Presents reference material with concrete software engineering examples Provides guidance on how to design, conduct, analyse, interpret and report empirical studies, taking into account the common difficulties and challenges encountered in the field Arms researchers with the information necessary to avoid fundamental risks Tackles appropriate techniques for addressing disparate studies - ensuring the relevance of empirical software engineering, and showing its practical impact Describes methods that are less often used in the field, providing less conventional but still rigorous and useful ways of collecting data Supplies detailed information on topics (such as surveys) that often contain methodological errors This broad-ranging, practical guide will prove an invaluable and useful reference for practising software engineers and researchers. In addition, it will be suitable for graduate students studying empirical methods in software development. {\textcopyright}Springer-Verlag London Limited 2008.},
author = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I K},
doi = {10.1007/978-1-84800-044-5},
editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I K},
isbn = {978-1-84-800043-8},
month = {nov},
publisher = {Springer},
title = {{Guide to Advanced Empirical Software Engineering}},
year = {2008}
}
@inproceedings{Bai:2007tl,
abstract = {A key issue with Web Services (WS) is the verification and validation (V{\&}V) of services to build trust between service providers and service users. This paper proposed a test-broker architecture so that all stakeholder within WS can contribute to improve the testing of the services. The test broker supports the submission, indexing, and querying of test artifacts such as test cases, defect reports and evaluations. It can also provide the services for the test generation, test coordination, and distributed testing services. The DCV{\&}V (Decentralized, Collaborative, Verification and Validation) framework is proposed with a set of distributed and collaborated test brokers dedicated to different V{\&}V tasks to enable scalable and flexible test collaborations. The paper explores the concept of design-by-contract and applies the principle to DCV{\&}V. It identifies two categories of testing contracts including TSC (Testing Service Contracts) and TCC (Test Collaboration Contracts). It illustrates the application of TSC with contract-based test generation based on WS OWL-S specification. It elaborates TCC with the analysis of the test artifacts definitions. {\textcopyright}Springer-Verlag Berlin Heidelberg 2007.},
address = {Medford, MA, USA},
author = {Bai, Xiaoying and Wang, Yongbo and Dai, Guilan and Tsai, Wei Tek and Chen, Yinong},
booktitle = {Proceedings of the 10th International Symposium of Component-Based Software Engineering},
doi = {10.1007/978-3-540-73551-9_18},
isbn = {978-3-54-073550-2},
issn = {0302-9743},
keywords = {Contract-based,Verification and validation,Web services},
month = {jul},
pages = {258--273},
publisher = {Springer},
title = {{A framework for contract-based collaborative verification and validation of Web services}},
year = {2007}
}
@inproceedings{Cummaudo:2020fse-demo,
address = {Virtual Event, USA},
author = {Cummaudo, Alex and Barnett, Scott and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3417919},
month = {nov},
pages = {1645--1649},
publisher = {ACM},
title = {{Threshy: Supporting Safe Usage of Intelligent Web Services}},
year = {2020}
}
@article{Rosen:2016uk,
abstract = {The popularity of mobile devices has been steadily growing in recent years. These devices heavily depend on software from the underlying operating systems to the applications they run. Prior research showed that mobile software is different than traditional, large software systems. However, to date most of our research has been conducted on traditional software systems. Very little work has focused on the issues that mobile developers face. Therefore, in this paper, we use data from the popular online Q{\&}A site, Stack Overflow, and analyze 13,232,821 posts to examine what mobile developers ask about. We employ Latent Dirichlet allocation-based topic models to help us summarize the mobile-related questions. Our findings show that developers are asking about app distribution, mobile APIs, data management, sensors and context, mobile tools, and user interface development. We also determine what popular mobile-related issues are the most difficult, explore platform specific issues, and investigate the types (e.g., what, how, or why) of questions mobile developers ask. Our findings help highlight the challenges facing mobile developers that require more attention from the software engineering research and development communities in the future and establish a novel approach for analyzing questions asked on Q{\&}A forums.},
author = {Rosen, Christoffer and Shihab, Emad},
doi = {10.1007/s10664-015-9379-3},
issn = {1573-7616},
journal = {Empirical Software Engineering},
keywords = {Mobile issues,Mobile software development,Stack overflow},
number = {3},
pages = {1192--1223},
title = {{What are mobile developers asking about? A large scale study using stack overflow}},
volume = {21},
year = {2016}
}
@article{Michie:1994wi,
abstract = {Statistical, machine learning and neural network approaches to classification are all covered in this volume. Contributions have been integrated to provide an objective assessment of the potential for machine learning algorithms in solving significant commercial and industrial problems, widening the foundation for exploitation of these and related algorithms.},
author = {{F. Elder} and Michie, Donald and Spiegelhalter, David J and Taylor, Charles C},
doi = {10.2307/2291432},
isbn = {978-0-13-106360-0},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {433},
pages = {436--438},
title = {{Machine Learning, Neural, and Statistical Classification.}},
volume = {91},
year = {1996}
}
@inproceedings{10.1007/978-3-319-08976-8_16,
abstract = {In the information era, enormous amounts of data have become available on hand to decision makers. Big data refers to datasets that are not only big, but also high in variety and velocity, which makes them difficult to handle using traditional tools and techniques. Due to the rapid growth of such data, solutions need to be studied and provided in order to handle and extract value and knowledge from these datasets. Furthermore, decision makers need to be able to gain valuable insights from such varied and rapidly changing data, ranging from daily transactions to customer interactions and social network data. Such value can be provided using big data analytics, which is the application of advanced analytics techniques on big data. This paper aims to analyze some of the different analytics methods and tools which can be applied to big data, as well as the opportunities provided by the application of big data analytics in various decision domains. {\textcopyright} 2014 Springer International Publishing Switzerland.},
address = {St. Petersburg, Russia},
author = {Elgendy, Nada and Elragal, Ahmed},
booktitle = {Proceedings of the 10th Industrial Conference on Data Mining},
doi = {10.1007/978-3-319-08976-8_16},
issn = {1611-3349},
keywords = {analytics,big data,data mining,decision making},
month = {jul},
pages = {214--227},
publisher = {Springer},
title = {{Big data analytics: A literature review paper}},
year = {2014}
}
@misc{ISO8402:1986,
author = {{International Organization for Standardization}},
title = {{ISO 8402:1986 Information Technology - Software Product Evaluation - Quality Characteristics and Guidelines for Their Use}},
url = {http://bit.ly/37SK4HP},
year = {1986}
}
@article{Moody:2009vo,
abstract = {Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields. {\textcopyright}2009 IEEE.},
author = {Moody, Daniel},
doi = {10.1109/TSE.2009.67},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Analysis,Communication,Concrete syntax,Diagrams,Modeling,Visual syntax,Visualization},
number = {6},
pages = {756--779},
title = {{The physics of notations: Toward a scientific basis for constructing visual notations in software engineering}},
volume = {35},
year = {2009}
}
@techreport{LoGiudice:2016wf,
author = {{Lo Giudice}, Diego and Mines, Christopher and LeClair, Amanda and Curran, Rowan and Homan, Amy},
institution = {Forrester Research, Inc.},
month = {nov},
title = {{How AI Will Change Software Development And Applications}},
url = {http://bit.ly/38RiAlN},
year = {2016}
}
@article{braiek2018testing,
abstract = {Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.},
archivePrefix = {arXiv},
arxivId = {1812.02257},
author = {Braiek, Houssem Ben and Khomh, Foutse},
eprint = {1812.02257},
month = {dec},
title = {{On Testing Machine Learning Programs}},
year = {2018}
}
@inproceedings{Ribeiro:2015dz,
abstract = {The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.},
address = {Miami, FL, USA},
author = {Ribeiro, Mauro and Grolinger, Katarina and Capretz, Miriam A M},
booktitle = {Proceedings of the 14th International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2015.152},
isbn = {978-1-50-900287-0},
keywords = {Machine learning as a service,Platform as a service,Prediction,Regression,Service component architecture,Service oriented architecture,Supervised learning},
month = {dec},
pages = {896--902},
publisher = {IEEE},
title = {{MLaaS: Machine learning as a service}},
year = {2015}
}
@article{THOMAS2014457,
abstract = {Topic models are generative probabilistic models which have been applied to information retrieval to automatically organize and provide structure to a text corpus. Topic models discover topics in the corpus, which represent real world concepts by frequently co-occurring words. Recently, researchers found topics to be effective tools for structuring various software artifacts, such as source code, requirements documents, and bug reports. This research also hypothesized that using topics to describe the evolution of software repositories could be useful for maintenance and understanding tasks. However, research has yet to determine whether these automatically discovered topic evolutions describe the evolution of source code in a way that is relevant or meaningful to project stakeholders, and thus it is not clear whether topic models are a suitable tool for this task. In this paper, we take a first step towards evaluating topic models in the analysis of software evolution by performing a detailed manual analysis on the source code histories of two well-known and well-documented systems, JHotDraw and jEdit. We define and compute various metrics on the discovered topic evolutions and manually investigate how and why the metrics evolve over time. We find that the large majority (87{\%}-89{\%}) of topic evolutions correspond well with actual code change activities by developers. We are thus encouraged to use topic models as tools for studying the evolution of a software system. {\textcopyright}2012 Elsevier B.V. All rights reserved.},
author = {Thomas, Stephen W and Adams, Bram and Hassan, Ahmed E and Blostein, Dorothea},
doi = {10.1016/j.scico.2012.08.003},
issn = {0167-6423},
journal = {Science of Computer Programming},
keywords = {Latent Dirichlet allocation,Mining software repositories,Software evolution,Topic model},
pages = {457--479},
title = {{Studying software evolution using topic models}},
volume = {80},
year = {2014}
}
@article{Robillard:2009uk,
abstract = {Most software projects reuse components exposed through APIs, which provide developers access to implemented functionality. APIs have grown large and diverse, which raises questions regarding their usability. This article reports on a study of the obstacles professional developers at Microsoft faced when learning how to use APIs. The study was grounded in developers' experience, collected through a survey and interviews. The resulting data showed that learning resources for APIs are critically important and shed light on three issues: the need to discover the design and rationale of the API when needed, the challenge of finding credible usage API examples at the right level of complexity, and the challenge of understanding inexplicable API behavior. The article describes each of these challenges in detail and discusses associated implications for API users and designers. {\textcopyright}2009 IEEE.},
author = {Robillard, Martin P},
doi = {10.1109/MS.2009.193},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {API design,API usability,Application interfaces,Code examples,Context,Data mining,Documentation,Empirical study,Software documentation,Usability},
number = {6},
pages = {27--34},
title = {{What makes APIs hard to learn? Answers from developers}},
volume = {26},
year = {2009}
}
@article{Chambers:1991uh,
abstract = {Less than 20 percent of elderly and other high-risk persons targeted for annual influenza vaccination are immunized each year. In most busy practice settings, it is difficult for primary care physicians to identify every patient in need of preventive health interventions. The purpose of this study was to assess the effect of microcomputer-generated reminders on influenza vaccination rates in a university-based family practice center. The practice uses an interactive encounter form system from which updated clinical information is routinely entered into a cumulative database. During a 2-month period, 686 patients were identified in the database as eligible to receive influenza vaccine according to accepted criteria. Practice physicians (n = 32) were stratified by level of training and randomized to one of three groups, thereby receiving printed reminders on the encounter forms of all, none, or half of their eligible patients. Patients of physicians who always received reminders were more likely to receive influenza vaccine during the study period than patients of the never-reminded physicians (51 percent versus 30 percent, P less than 0.001). Patients whose physicians received reminders for only half their patients had an intermediate likelihood of receiving a vaccination if a reminder was printed (38 percent) but were less likely than the patients of never-reminded physicians to receive the vaccine if no reminder was printed (20 percent, P less than 0.001). This study suggests that physicians learn to depend on reminders for preventive health activities and that reminders are most effective when they are provided at every patient encounter.},
author = {Chambers, C V and Balaban, D J and Carlson, B L and Grasberger, D M},
doi = {10.3122/jabfm.4.1.19},
issn = {0893-8652},
journal = {The Journal of the American Board of Family Practice / American Board of Family Practice},
number = {1},
pages = {19--26},
title = {{The effect of microcomputer-generated reminders on influenza vaccination rates in a university-based family practice center.}},
volume = {4},
year = {1991}
}
@article{Glass:2002wa,
abstract = {This article is a background report describing a comprehensive study of research in the three computing disciplines Computer Science, Software Engineering, and Information Systems. Findings relate to research topics, approaches, methods, reference disciplines, and levels of analysis. The article informally describes the process used and the research products produced. {\textcopyright}2008 Elsevier B.V. All rights reserved.},
author = {Glass, Robert L and Vessey, Iris and Ramesh, V},
doi = {10.1016/j.infsof.2008.09.015},
issn = {0950-5849},
journal = {Information and Software Technology},
keywords = {Computing research,Literature search,Research taxonomy},
number = {1},
pages = {68--70},
title = {{RESRES: The story behind the paper "Research in software engineering: An analysis of the literature"}},
volume = {51},
year = {2009}
}
@inproceedings{breck2016s,
abstract = {Using machine learning in real-world production systems is complicated by a host of issues not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for assessing the production-readiness of an ML system. But how much testing and monitoring is enough? We present an ML Test Score rubric based on a set of actionable tests to help quantify these issues.},
address = {Barcelona, Spain},
author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D},
booktitle = {Proceedings of the 30th Annual Conference on Neural Information Processing Systems},
month = {dec},
publisher = {Curran Associates Inc.},
title = {{What's your ML Test Score? A rubric for ML production systems}},
year = {2016}
}
@inproceedings{Foster:2003ur,
abstract = {In this paper, we discuss a model-based approach to verifying Web service compositions for Web service implementations. The approach supports verification against specification models and assigns semantics to the behavior of implementation model so as to confirm expected results for both the designer and implementer. Specifications of the design are modeled in UML (Unified Modeling Language), in the form of message sequence charts (MSC), and mechanically compiled into the finite state process notation (FSP) to concisely describe and reason about the concurrent programs. Implementations are mechanically translated to FSP to allow a trace equivalence verification process to be performed. By providing early design verification, the implementation, testing, and deployment of Web service compositions can be eased through the understanding of the differences, limitations and undesirable traces allowed by the composition. The approach is supported by a suite of cooperating tools for specification, formal modeling and trace animation of the composition workflow.},
address = {Linz, Austria},
author = {Foster, H and Uchitel, S and Magee, J and Kramer, J},
booktitle = {Proceedings of the 18th International Conference on Automated Software Engineering},
doi = {10.1109/ase.2003.1240303},
month = {sep},
pages = {152--161},
publisher = {IEEE},
title = {{Model-based verification of Web service compositions}},
year = {2004}
}
@inproceedings{Zhang:2017,
address = {Denver, CO, USA},
author = {Zhang, Xiaoyi and Ross, Anne Spencer and Caspi, Anat and Fogarty, James and Wobbrock, Jacob O},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3025453.3025846},
isbn = {978-1-4503-4655-9},
keywords = {accessibility,interaction proxies,runtime modification},
month = {may},
pages = {6024--6037},
publisher = {ACM},
series = {CHI '17},
title = {{Interaction Proxies for Runtime Repair and Enhancement of Mobile Application Accessibility}},
url = {https://doi.org/10.1145/3025453.3025846},
year = {2017}
}
@inproceedings{Alm2005EmotionsText,
address = {Vancouver, BC, Canada},
author = {Alm, Cecilia Ovesdotter and Roth, Dan and Sproat, Richard},
booktitle = {Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing},
doi = {10.3115/1220575.1220648},
month = {oct},
pages = {579--586},
publisher = {Association for Computational Linguistics},
title = {{Emotions from Text: Machine Learning for Text-Based Emotion Prediction}},
year = {2005}
}
@book{Yin:2017tf,
address = {Los Angeles, CA, USA},
author = {Yin, Robert K},
edition = {6th},
isbn = {978-1-50-633616-9},
publisher = {SAGE},
title = {{Case study research and applications: Design and methods}},
year = {2017}
}
@incollection{OpenSoftwareFoundation:1991vp,
author = {{Open Software Foundation}},
booktitle = {OSF DCE application development guide: revision 1.0},
month = {dec},
publisher = {Prentice Hall},
title = {{Part 3: DCE Remote Procedure Call (RPC)}},
year = {1991}
}
@inproceedings{Choi:2015wo,
abstract = {We present a preliminary investigation of Stack Overflow to reveal practitioner's interests about code clones. We then discuss possible future directions of research on code clones.},
address = {Montreal, QC, Canada},
author = {Choi, Eunjong and Yoshida, Norihiro and Kula, Raula Gaikovina and Inoue, Katsuro},
booktitle = {Proceedings of the 9th International Workshop on Software Clones},
doi = {10.1109/IWSC.2015.7069890},
isbn = {978-1-46-736914-5},
month = {mar},
pages = {49--50},
title = {{What do practitioners ask about code clone? a preliminary investigation of stack overflow}},
year = {2015}
}
@article{Robinson:2007tp,
abstract = {Over the past decade we have performed a sustained series of qualitative studies of software development practice, focusing on social factors. Using an ethnographically-informed approach, we have addressed four areas of software practice: software quality management systems, the emergence of object technology, professional end user development and agile development. Several issues have arisen from this experience, including the nature of research questions that such studies can address, the advantages and challenges associated with being a member of the community under study, and how to maintain rigour in data collection. In this paper, we will draw on our studies to illustrate our approach and to discuss these and other issues. {\textcopyright}2007 Elsevier B.V. All rights reserved.},
author = {Robinson, Hugh and Segal, Judith and Sharp, Helen},
doi = {10.1016/j.infsof.2007.02.007},
issn = {0950-5849},
journal = {Information and Software Technology},
keywords = {Field studies,Qualitative analysis,Software practice},
number = {6},
pages = {540--551},
title = {{Ethnographically-informed empirical studies of software practice}},
volume = {49},
year = {2007}
}
@article{Su:2017uw,
abstract = {Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97{\%} of the natural images in Kaggle CIFAR-10 test dataset and 16.04{\%} of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03{\%} and 22.91{\%} confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
archivePrefix = {arXiv},
arxivId = {1710.08864},
author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
doi = {10.1109/TEVC.2019.2890858},
eprint = {1710.08864},
issn = {1941-0026},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Convolutional neural network,differential evolution (DE),image recognition,information security},
number = {5},
pages = {828--841},
title = {{One Pixel Attack for Fooling Deep Neural Networks}},
volume = {23},
year = {2019}
}
@inproceedings{Schwabacher:2001wc,
abstract = {Pages: 489 - 496. Year of Publication: . ISBN:1-55860-778-1. Authors, Mark , Pat , Publisher, Morgan Kaufmann Publishers Inc.},
address = {Williamstown, MA, USA},
author = {Schwabacher, Mark and Langley, Pat},
booktitle = {Proceedings of the 18th International Conference on Machine Learning},
isbn = {978-1-55-860778-1},
month = {jun},
pages = {489--496},
publisher = {Morgan Kaufmann},
title = {{Discovering communicable scientific knowledge from spatio-temporal data}},
year = {2001}
}
@book{Grunwald:2007vg,
abstract = {A recent study of ovariectomized monkeys, treated with recombinant human parathyroid hormone (rhPTH)(1-34) at 1 or 5 mg/kg/day for 18 months or for 12 months followed by 6 months withdrawal from treatment, showed significant differences in the geometry and histomorphometry of cortical bone of the midshaft humerus. To determine the extent to which the rapid bone turnover and cortical porosity induced by rhPTH(1-34) in ovariectomized monkeys modified mineral content, mineral crystal maturity and collagen maturity (cross-link distribution) in the cortical periosteal and endosteal regions, cross-sections of the cortical bone of the mid-humerus, were examined using Fourier transform infrared imaging (FTIRI). FTIRI analyses demonstrated that rhPTH(1-34) altered bone mineral and collagen properties in a dose-dependent manner. Mineral crystal maturity and collagen cross-link ratio (pyridinoline/dehydro-dihydroxylysinonorleucine) on both endosteal and periosteal surfaces decreased relative to ovariectomized animals, consistent with new bone formation. These changes were partially sustained after withdrawal of the higher dose of rhPTH(1-34), suggesting a prolonged after-effect on bone properties for at least two bone remodeling cycles. In conclusion, treatment of ovariectomized monkeys with rhPTH(1-34) had significant effects on cortical bone mineral-to-matrix ratio, mineral crystal maturity, and collagen cross-link ratio. These were fully reversible when the 1-microg rhPTH(1-34) treatment was withdrawn, but only partially reversed when the 5-microg rhPTH(1-34) dose was withdrawn.},
author = {Gr{\"{u}}nwald, Peter D},
doi = {10.7551/mitpress/4643.001.0001},
publisher = {MIT press},
title = {{The Minimum Description Length Principle}},
year = {2019}
}
@article{Davison:2004wo,
abstract = {Despite the growing prominence of canonical action research (CAR) in the information systems discipline, a paucity of methodological guidance continues to hamper those conducting and evaluating such studies. This article elicits a set of five principles and associated criteria to help assure both the rigor and the relevance of CAR in information systems. The first principle relates to the development of an agreement that facilitates collaboration between the action researcher and the client. The second principle is based upon a cyclical process model for action research that consists of five stages: diagnosis, planning, intervention, evaluation and reflection. Additional principles highlight the critical roles of theory, change through action, and the specification of learning in terms of implications for both research and practice. The five principles are illustrated through the analysis of one recently published CAR study.},
author = {Davison, Robert M and Martinsons, Maris G and Kock, Ned},
doi = {10.1111/j.1365-2575.2004.00162.x},
issn = {1350-1917},
journal = {Information Systems Journal},
keywords = {Canonical action research,Interpretivism,Meta-analysis,Organizational change,Organizational learning,Research frameworks},
number = {1},
pages = {65--86},
title = {{Principles of canonical action research}},
volume = {14},
year = {2004}
}
@inproceedings{Aman2007IdentifyingText,
abstract = {Finding emotions in text is an area of research with wide-ranging applications. We describe an emotion annotation task of identifying emotion category, emotion intensity and the words/phrases that indicate emotion in text. We introduce the annotation scheme and present results of an annotation agreement study on a corpus of blog posts. The average inter-annotator agreement on labeling a sentence as emotion or non-emotion was 0.76. The agreement on emotion categories was in the range 0.6 to 0.79; for emotion indicators, it was 0.66. Preliminary results of emotion classification experiments show the accuracy of 73.89{\%}, significantly above the baseline.},
address = {Pilsen, Czech Republic},
author = {Aman, Saima and Szpakowicz, Stan},
booktitle = {Proceedings of the 10th International Conference on Text, Speech and Dialogue},
doi = {10.1007/978-3-540-74628-7_27},
month = {sep},
pages = {196--205},
publisher = {Springer},
title = {{Identifying Expressions of Emotion in Text}},
year = {2007}
}
@misc{ISO9126:1999,
author = {{International Organization for Standardization}},
month = {nov},
title = {{ISO/IEC 9126. Information technology -- Software product quality}},
year = {1999}
}
@article{Brooke:2013vt,
author = {Brooke, John},
issn = {1931-3357},
journal = {Journal of Usability Studies},
number = {2},
pages = {29--40},
title = {{SUS: a retrospective}},
volume = {8},
year = {2013}
}
@book{Breiman:1984tu,
abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
address = {New York, NY, USA},
author = {Breiman, Leo and Friedman, Jerome H and Olshen, Richard A and Stone, Charles J},
booktitle = {Classification and Regression Trees},
doi = {10.1201/9781315139470},
isbn = {978-1-35-146049-1},
issn = {1661-8564},
pages = {1--358},
publisher = {CRC press},
title = {{Classification and regression trees}},
year = {1984}
}
@inproceedings{amershi2015modeltracker,
abstract = {Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present Model Tracker, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with Model Tracker over six months shows Model Tracker is used often and throughout model building. A controlled experiment focusing on Model Tracker's debugging capabilities shows participants prefer Model Tracker over traditional tools without a loss in model performance.},
address = {Seoul, Republic of Korea},
author = {Amershi, Saleema and Chickering, Max and Drucker, Steven M and Lee, Bongshin and Simard, Patrice and Suh, Jina},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
doi = {10.1145/2702123.2702509},
isbn = {978-1-45-033145-6},
keywords = {Debugging,Interactive visualization,Machine learning,Performance analysis},
month = {apr},
pages = {337--346},
publisher = {ACM},
title = {{Modeltracker: Redesigning performance analysis tools for machine learning}},
year = {2015}
}
@inproceedings{Takagi2000,
abstract = {These days, the web has been coming to play various types of roles, so each site has been designed in a complex way to integrate as many roles as possible. Web authors tend to cram various functions and many links into one page to improve usability for sighted users. This authoring trend makes nonvisual Web access harder. To solve this problem, we decided to develop a system to transcode already-existing Web pages to be accessible, which works as an intermediary (proxy) between a Web server and a user. Our transcoding proxy consists of 5 modules using 3 kinds of annotations. The user interface of the system is characterized by three transcoding modes: simplification, full-text and original page. In this paper, we will describe an overview of our transcoding proxy as well as the user interface of the system.},
address = {Arlington, VA, USA},
author = {Takagi, H. and Asakawa, C.},
booktitle = {Proceedings of the 2000 ACM Conference on Assistive Technologies},
doi = {10.1145/354324.354371},
keywords = {Annotations,Blind,Differential,Portal,Transcoding,Web Accessibility},
month = {nov},
pages = {164--171},
publisher = {ACM},
title = {{Transcoding proxy for nonvisual Web access}},
year = {2000}
}
@misc{Hadley:2006vv,
abstract = {This article describes the Web Application Description Language (WADL). An increasing number of Web-based enterprises (Google, Yahoo, Amazon, Flickr - to name but a few) are developing HTTP-based applications that provide access to their internal data using XML. Typically these applications are described using a combination of textual protocol descriptions combined with XML schema-based data format descriptions; WADL is designed to provide a machine processable protocol description format for use with such HTTP-based Web applications, especially those using XML.},
author = {Hadley, Marc J and Marc, Hadley},
booktitle = {Search},
month = {aug},
publisher = {W3C},
title = {{Web Application Description Language}},
url = {http://bit.ly/2RXRhQ1},
year = {2009}
}
@book{Ingeno:2018,
address = {Birmingham, England, UK},
author = {Ingeno, Joseph},
isbn = {978-1-78862-406-0},
publisher = {Packt Publishing, Ltd.},
title = {{Software Architect's Handbook: Become a Successful Software Architect by Implementing Effective Architecture Concepts}},
year = {2018}
}
@inproceedings{Inzunza:2018dn,
address = {Naples, Italy},
author = {Inzunza, Sergio and Ju{\'{a}}rez-Ram{\'{i}}rez, Reyes and Jim{\'{e}}nez, Samantha},
booktitle = {Proceedings of the 6th World Conference on Information Systems and Technologies},
doi = {10.1007/978-3-319-77712-2\_22},
month = {mar},
pages = {229--239},
publisher = {Springer},
title = {{API Documentation}},
year = {2018}
}
@inproceedings{Cavano:1978gz,
abstract = {Research in software metrics incorporated in a framework established for software quality measurement can potentially provide significant benefits to software quality assurance programs. The research described has been conducted by General Electric Company for the Air Force Systems Command Rome Air Development Center. The problems encountered defining software quality and the approach taken to establish a framework for the measurement of software quality are described in this paper.},
author = {Cavano, Joseph P and McCall, James A},
booktitle = {Proceedings of the Software Quality Assurance Workshop on Functional and Performance Issues},
doi = {10.1145/800283.811113},
month = {nov},
number = {5},
pages = {133--139},
title = {{A framework for the measurement of software quality}},
volume = {3},
year = {1978}
}
@article{zurMuehlen:2005ci,
abstract = {This paper presents a case study of the development of standards in the area of cross-organizational workflows based on web services. We discuss two opposing types of standards: those based on SOAP, with tightly coupled designs similar to remote procedure calls, and those based on REST, with loosely coupled designs similar to the navigating of web links. We illustrate the standardization process, clarify the technical underpinnings of the conflict, and analyze the interests of stakeholders. The decision criteria for each group of stakeholders are discussed. Finally, we present implications for both the workflow and the wider Internet communities. {\textcopyright}2004 Elsevier B.V. All rights reserved.},
author = {{Zur Muehlen}, Michael and Nickerson, Jeffrey V and Swenson, Keith D},
doi = {10.1016/j.dss.2004.04.008},
issn = {0167-9236},
journal = {Decision Support Systems},
keywords = {Choreography,Integration,Interoperability,Process,REST,SOAP,Standards,Web services,Workflow},
month = {jul},
number = {1},
pages = {9--29},
title = {{Developing web services choreography standards - The case of REST vs. SOAP}},
volume = {40},
year = {2005}
}
@book{Parrott2001,
address = {Philadelphia},
editor = {Parrott, W. Gerrod},
isbn = {978-0-86-377682-3},
publisher = {Psychology Press},
title = {{Emotions in Social Psychology: Essential Readings}},
type = {Book},
year = {2001}
}
@article{Lipton:2016if,
archivePrefix = {arXiv},
arxivId = {1606.03490},
author = {Lipton, Zachary C},
doi = {10.1145/3233231},
eprint = {1606.03490},
issn = {1557-7317},
journal = {Communications of the ACM},
number = {10},
pages = {35--43},
title = {{The mythos of model interpretability}},
volume = {61},
year = {2018}
}
@article{Canfora:2006vk,
author = {Canfora, Gerardo and {Di Penta}, Massimiliano},
doi = {10.1109/MITP.2006.51},
issn = {1520-9202},
journal = {IT Professional},
number = {2},
pages = {10--17},
title = {{Testing services and service-centric systems: Challenges and opportunities}},
volume = {8},
year = {2006}
}
@inproceedings{7180082,
abstract = {API design is known to be a challenging craft, as API designers must balance their elegant ideals against 'real-world' concerns, such as utility, performance, backwards compatibility, and unforeseen emergent uses. However, to date, there is no principled method to collect or analyze API usability information that incorporates input from typical developers. In practice, developers often turn to Q{\&}A websites such as stackoverflow.com (SO) when seeking expert advice on API use, the popularity of such sites has thus led to a very large volume of unstructured information that can be searched with diligence for answers to specific questions. The collected wisdom within such sites could, in principle, be of great help to API designers to better support developer needs, if only it could be collected, analyzed, and distilled for practical use. In this paper, we present a methodology that combines several techniques, including social network analysis and topic mining, to recommend SO posts that are likely to concern API design-related issues. To establish a comparison baseline, we introduce two more recommendation approaches: a reputation-based recommender and a random recommender. We have found that when applied to Q{\&}A discussion of two popular mobile platforms, Android and iOS, our methodology achieves up to 93{\%} accuracy and is more stable with its recommendations when compared to the two baseline techniques.},
address = {Florence, Italy},
author = {Wang, Wei and Malik, Haroon and Godfrey, Michael W},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2015.28},
isbn = {978-0-7695-5594-2},
issn = {2160-1860},
keywords = {API usability,Application program interfaces,Online Q{\&}A,Recommendation systems,Software ecosystems,Stackoverflow},
month = {may},
pages = {224--234},
publisher = {IEEE},
title = {{Recommending Posts concerning API Issues in Developer Q{\&}A Sites}},
year = {2015}
}
@misc{Mapillar97:online,
annote = {Accessed: 25 January 2019},
author = {{GeoSpatial World}},
month = {sep},
title = {{Mapillary and Amazon Rekognition collaborate to build a parking solution for US cities through computer vision}},
url = {http://bit.ly/36AdRmS},
year = {2018}
}
@inproceedings{Sinha:2013tt,
abstract = {Success of a Q{\&}A forum depends on volume of content (questions and answers) and quality of content (are the questions asked relevant, answers provided correct etc). Community participation is essential to create and curate content. Since their inception in 2008, stack exchange based forums have been able to engage a large number of users to create a rich repository of good quality questions and answers. In this paper, we wish to investigate the "activeness" of users in the stackexchange network particularly from a perspective of content creation. We also attempt to measure how the forums' incentive mechanism has enabled user's activeness. Further, we investigate how user's have diffused to other parts of the stack exchange network over time, hence bootstrapping new forums. {\textcopyright}2013 IEEE.},
address = {San Francisco, CA, USA},
author = {Sinha, Vibha Singhal and Mani, Senthil and Gupta, Monika},
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2013.6624010},
isbn = {978-1-46-732936-1},
issn = {2160-1852},
month = {may},
pages = {77--80},
publisher = {IEEE},
title = {{Exploring activeness of users in QA forums}},
year = {2013}
}
@inproceedings{Kavaler:2013uh,
abstract = {Programming is knowledge intensive. While it is well understood that programmers spend lots of time looking for information, with few exceptions, there is a significant lack of data on what information they seek, and why. Modern platforms, like Android, comprise complex APIs that often perplex programmers. We ask: which elements are confusing, and why? Increasingly, when programmers need answers, they turn to StackOverflow. This provides a novel opportunity. There are a vast number of applications for Android devices, which can be readily analyzed, and many traces of interactions on StackOverflow. These provide a complementary perspective on using and asking, and allow the two phenomena to be studied together. How does the market demand for the USE of an API drive the market for knowledge about it? Here, we analyze data from Android applications and StackOverflow together, to find out what it is that programmers want to know and why. {\textcopyright}2013 Springer International Publishing.},
address = {Kyoto, Japan},
author = {Kavaler, David and Posnett, Daryl and Gibler, Clint and Chen, Hao and Devanbu, Premkumar and Filkov, Vladimir},
booktitle = {Proceedings of the 5th International Conference on Social Infomatics},
doi = {10.1007/978-3-319-03260-3_35},
isbn = {978-3-31-903259-7},
issn = {0302-9743},
month = {nov},
pages = {405--418},
publisher = {Springer},
title = {{Using and asking: APIs used in the Android market and asked about in StackOverflow}},
year = {2013}
}
@book{Sugiyama:2017ud,
address = {Cambridge, MA, USA},
author = {Schwaighofer, Anton and Lawrence, Neil D},
editor = {Qui{\~{n}}onero-Candela, Joaquin and Sugiyama, Masashi},
isbn = {978-0-26-217005-5},
publisher = {The MIT Press},
title = {{Dataset shift in machine learning}},
year = {2008}
}
@article{Klein:1999uv,
abstract = {This article discusses the conduct and evaluation of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflection and debate on the important subject of grounding interpretive research methodology.},
author = {Klein, Heinz K and Myers, Michael D},
doi = {10.2307/249410},
issn = {0276-7783},
journal = {MIS Quarterly: Management Information Systems},
keywords = {Case study,Critical perspective,Ethnography,Field study,Hermeneutics,IS research methodologies,Interpretivist perspective},
number = {1},
pages = {67--94},
title = {{A set of principles for conducting and evaluating interpretive field studies in information systems}},
volume = {23},
year = {1999}
}
@article{Cohen:1960tf,
abstract = {A coefficient of interjudge agreement for nominal scales, formula-omitted, is presented. It is directly interpretable as the pro-portion of joint judgments in which there is agreement, after chance agreement is excluded. Its upper limit is +1.00, and its lower limit falls between zero and -1.00, depending on the distribution of judgments by the two judges. The maximum value which x can take for any given problem is given, and the implications of this value to the question of agreement discussed. An interesting characteristic of x is its identity with 0 in the dichotomous case when the judges give the same marginal distributions. Finally, its standard error and techniques for estimation and hypothesis testing are presented. {\textcopyright}1960, Sage Publications. All rights reserved.},
author = {Cohen, Jacob},
doi = {10.1177/001316446002000104},
issn = {1552-3888},
journal = {Educational and Psychological Measurement},
number = {1},
pages = {37--46},
title = {{A Coefficient of Agreement for Nominal Scales}},
volume = {20},
year = {1960}
}
@book{Robbins:2014tr,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\backslashbackslashalpha{\{}\backslash{\{}{\}}\backslashbackslash{\{}\backslash{\$}{\}}{\{}$\backslash${\}}{\}}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA}for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Robbins, Stephen P},
edition = {14th},
isbn = {978-0-13-452470-2},
publisher = {Pearson},
title = {{Essentials of organizational behavior}},
year = {2017}
}
@article{Sun:2010ut,
abstract = {Null hypothesis significance testing has dominated quantitative research in education and psychology. However, the statistical significance of a test as indicated by a p-value does not speak to the practical significance of the study. Thus, reporting effect size to supplement p-value is highly recommended by scholars, journal editors, and academic associations. As a measure of practical significance, effect size quantifies the size of mean differences or strength of associations and directly answers the research questions. Furthermore, a comparison of effect sizes across studies facilitates meta-analytic assessment of the effect size and accumulation of knowledge. In the current comprehensive review, we investigated the most recent effect size reporting and interpreting practices in 1,243 articles published in 14 academic journals from 2005 to 2007. Overall, 49{\%} of the articles reported effect size-57{\%} of which interpreted effect size. As an empirical study for the sake of good research methodology in education and psychology, in the present study we provide an illustrative example of reporting and interpreting effect size in a published study. Furthermore, a 7-step guideline for quantitative researchers is also summarized along with some recommended resources on how to understand and interpret effect size. {\textcopyright}2010 American Psychological Association.},
author = {Sun, Shuyan and Pan, Wei and Wang, Lihshing Leigh},
doi = {10.1037/a0019507},
issn = {0022-0663},
journal = {Journal of Educational Psychology},
keywords = {Confidence intervals,Effect size,NHST,Practical significance,Statistical significance},
number = {4},
pages = {989--1004},
title = {{A Comprehensive Review of Effect Size Reporting and Interpreting Practices in Academic Journals in Education and Psychology}},
volume = {102},
year = {2010}
}
@inproceedings{Martins2015,
abstract = {The System Usability Scale (SUS) is a widely used self-administered instrument for the evaluation of usability of a wide range of products and user interfaces. The principal value of the SUS is that it provides a single reference score for participants' view of the usability of a product or service. This paper presents the translation, cultural adaptation and a contribution to the validation of the European Portuguese version of SUS. The conducted work comprised two phases, the scale translation, and the scale validation. The first phase resulted in a European Portuguese version equivalent to the original in terms of semantic and content. The second phase involved the assessment of the validity and reliability of the scale. The instrument has construct validity as it presents a high and significant correlation with other two usability metrics, the Post-Study System Usability Questionnaire (PSSUQ) (r = 0.70) and a general usability question (r = 0.48). The reliability results show less than satisfactory ICC values (ICC = 0.36), however the percentage of agreement is satisfactory (76.67{\%}). Further studies are needed to investigate the reliability of the Portuguese version.},
author = {Martins, Ana Isabel and Rosa, Ana Filipa and Queir{\'{o}}s, Alexandra and Silva, Anabela and Rocha, Nelson Pacheco},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2015.09.273},
issn = {18770509},
keywords = {European Portuguese validation,System Usability Scale,usability evaluation,user tests},
title = {{European Portuguese Validation of the System Usability Scale (SUS)}},
year = {2015}
}
@book{Boehm:1981ua,
address = {Englewood Cliffs, NJ, USA},
author = {Boehm, Barry W},
isbn = {0-13-822122-7},
publisher = {Prentice-Hall},
title = {{Software engineering economics}},
year = {1981}
}
@misc{Draper:vb,
address = {Glasgow, Scotland, UK},
author = {Draper, Stephen W},
booktitle = {University of Glasgow},
publisher = {University of Glasgow},
title = {{The Hawthorne, Pygmalion, Placebo and other effects of expectation: some notes}},
url = {http://bit.ly/2uO2Kth},
year = {2006}
}
@misc{Tabor:1997tw,
address = {New York, NY, USA},
author = {Tabor, Mary B W},
booktitle = {New York Times},
month = {feb},
title = {{Student Proves That S.A.T. Can Be: (D) Wrong}},
url = {https://nyti.ms/2UiKrrd},
year = {1997}
}
@inproceedings{Yi:2004ve,
abstract = {Web services aim to support efficient integration of applications over Web. Most Web services are stateful, such as services for business processes, and they converse with each other via properly ordered interactions, instead of individual unrelated invocations. In order to address efficient integration of conversational Web services, we create a unified specification model for both conversation protocol and composition; we propose methods to integrate a partner service with complex conversation protocol into a composition of Web services; assure the correctness of composition by formal verification. The mapping between our model and BPEL4WS is also discussed.},
address = {San Diego, CA, USA},
author = {Yi, Xiaochuan and Kochut, Krys J},
booktitle = {Proceedings of the 2004 IEEE International Conference on Web Services},
doi = {10.1109/icws.2004.1314810},
isbn = {0-76-952167-3},
month = {jul},
pages = {756--760},
publisher = {IEEE},
title = {{A CP-nets-based design and verification framework for web services composition}},
year = {2004}
}
@article{Drummond2006,
abstract = {This paper introduces cost curves, a graphical technique for visualizing the performance (error rate or expected cost) of 2-class classifiers over the full range of possible class distributions and misclassification costs. Cost curves are shown to be superior to ROC curves for visualizing classifier performance for most purposes. This is because they visually support several crucial types of performance assessment that cannot be done easily with ROC curves, such as showing confidence intervals on a classifier's performance, and visualizing the statistical significance of the difference in performance of two classifiers. A software tool supporting all the cost curve analysis described in this paper is available from the authors. {\textcopyright}Springer Science + Business Media, LLC 2006.},
author = {Drummond, Chris and Holte, Robert C},
doi = {10.1007/s10994-006-8199-5},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Classifiers,Machine learning,Performance evaluation,ROC curves},
month = {oct},
number = {1},
pages = {95--130},
title = {{Cost curves: An improved method for visualizing classifier performance}},
volume = {65},
year = {2006}
}
@inproceedings{1572302,
abstract = {Today's information technology society increasingly relies on software at all levels. Nevertheless, software quality generally continues to fall short of expectations, and software systems continue to suffer from symptoms of aging as they are adapted to changing requirements and environments. The only way to overcome or avoid the negative effects of software aging is by placing change and evolution in the center of the software development process. In this article we describe what we believe to be some of the most important research challenges in software evolution. The goal of this document is to provide novel research directions in the software evolution domain. {\textcopyright}2005 IEEE.},
address = {Lisbon, Portugal},
author = {Mens, Tom and Demeyer, Serge and Wermelinger, Michel and Hirschfeld, Robert and Ducasse, St{\'{e}}phane and Jazayeri, Mehdi},
booktitle = {Proceedings of the 8th International Workshop on Principles of Software Evolution},
doi = {10.1109/IWPSE.2005.7},
isbn = {0-76-952349-8},
issn = {1550-4077},
keywords = {Aging,Business,Collaborative software,Computer industry,Conferences,Information technology,Programming,Software quality,Software systems,Software tools,formal specification,formal verification,information technology society,requirements analysis,software aging,software development process,software evolution,software maintenance,software quality,software system},
month = {sep},
pages = {13--22},
publisher = {IEEE},
title = {{Challenges in software evolution}},
volume = {2005},
year = {2005}
}
@inproceedings{Cummaudo:2019icsme,
abstract = {Recent advances in artificial intelligence (AI) and machine learning (ML), such as computer vision, are now available as intelligent services and their accessibility and simplicity is compelling. Multiple vendors now offer this technology as cloud services and developers want to leverage these advances to provide value to end-users. However, there is no firm investigation into the maintenance and evolution risks arising from use of these intelligent services; in particular, their behavioural consistency and transparency of their functionality. We evaluated the responses of three different intelligent services (specifically computer vision) over 11 months using 3 different data sets, verifying responses against the respective documentation and assessing evolution risk. We found that there are: (1) inconsistencies in how these services behave; (2) evolution risk in the responses; and (3) a lack of clear communication that documents these risks and inconsistencies. We propose a set of recommendations to both developers and intelligent service providers to inform risk and assist maintainability.},
address = {Cleveland, OH, USA},
author = {Cummaudo, Alex and Vasa, Rajesh and Grundy, John and Abdelrazek, Mohamed and Cain, Andrew},
booktitle = {Proceedings of the 35th IEEE International Conference on Software Maintenance and Evolution},
doi = {10.1109/ICSME.2019.00051},
isbn = {978-1-72-813094-1},
month = {dec},
pages = {333--342},
publisher = {IEEE},
title = {{Losing Confidence in Quality: Unspoken Evolution of Computer Vision Services}},
year = {2019}
}
@misc{IEEE:1990wp,
abstract = {Describes the IEEE Std 610.12-1990, IEEE standard glossary of software engineering terminology, which identifies terms currently in use in the field of software engineering. Standard definitions for those terms are established.},
author = {IEEE},
doi = {10.1109/IEEESTD.1990.101064},
keywords = {definitions,dictionary,glossary,software engineering,terminology},
title = {{IEEE Standard Glossary of Software Engineering Terminology}},
year = {1990}
}
@article{Brereton:2007by,
abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone. {\textcopyright}2006 Elsevier Inc. All rights reserved.},
author = {Brereton, Pearl and Kitchenham, Barbara A and Budgen, David and Turner, Mark and Khalil, Mohamed},
doi = {10.1016/j.jss.2006.07.009},
issn = {0164-1212},
journal = {Journal of Systems and Software},
keywords = {Empirical software engineering,Systematic literature review},
month = {apr},
number = {4},
pages = {571--583},
title = {{Lessons from applying the systematic literature review process within the software engineering domain}},
volume = {80},
year = {2007}
}
@article{Light:1971vz,
abstract = {Notes that various procedures are available for measuring agreement among 2 or more os who classify responses among nominal categories, but that different problem situations require different measures. The general model of a contingency table with fixed margins is used to suggest (a) a measure of level of agreement among several os when compared internally, (b) a conditional measurement of agreement for several os compared internally, (c) a test for the joint agreement of several os when compared with a standard, and (d) a statistic for evaluating the pattern of agreement between 2 os. Illustrations are presented for each situation, and results of a monte carlo study of the behavior of the pattern agreement statistic are discussed. (19 ref.) (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright}1971 American Psychological Association.},
author = {Light, Richard J},
doi = {10.1037/h0031643},
issn = {0033-2909},
journal = {Psychological Bulletin},
keywords = {response agreement measurement among 2 or more Os},
number = {5},
pages = {365--377},
title = {{Measures of response agreement for qualitative data: Some generalizations and alternatives}},
volume = {76},
year = {1971}
}
@inproceedings{Cummaudo:2019esem,
abstract = {Background: Good API documentation facilitates the development process, improving productivity and quality. While the topic of API documentation quality has been of interest for the last two decades, there have been few studies to map the specific constructs needed to create a good document. In effect, we still need a structured taxonomy that captures such knowledge systematically.Aims: This study reports emerging results of a systematic mapping study. We capture key conclusions from previous studies that assess API documentation quality, and synthesise the results into a single framework.Method: By conducting a systematic review of 21 key works, we have developed a five dimensional taxonomy based on 34 categorised weighted recommendations.Results: All studies utilise field study techniques to arrive at their recommendations, with seven studies employing some form of interview and questionnaire, and four conducting documentation analysis. The taxonomy we synthesise reinforces that usage description details (code snippets, tutorials, and reference documents) are generally highly weighted as helpful in API documentation, in addition to design rationale and presentation.Conclusions: We propose extensions to this study aligned to developer utility for each of the taxonomy's categories.},
address = {Porto de Galinhas, Recife, Brazil},
author = {Cummaudo, Alex and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 13th International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2019.8870148},
isbn = {978-1-72-812968-6},
issn = {1949-3789},
keywords = {API,DevX,documentation,systematic mapping study,taxonomy},
month = {oct},
pages = {1--6},
publisher = {IEEE},
title = {{What should I document? A preliminary systematic mapping study into API documentation knowledge}},
year = {2019}
}
@book{Krippendorff:2018tda,
author = {Krippendorff, Klaus},
isbn = {978-1-50-639566-1},
publisher = {SAGE},
series = {An Introduction to Its Methodology},
title = {{Content Analysis}},
year = {1980}
}
@inproceedings{Wang:2013ue,
address = {Coimbra, Portugal},
author = {Wang, Shaowei and Lo, David and Jiang, Lingxiao},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
doi = {10.1145/2480362.2480557},
month = {mar},
pages = {1019--1024},
publisher = {ACM},
title = {{An empirical study on developer interactions in StackOverflow}},
year = {2013}
}
@article{Bratthall2002,
abstract = {As the demand for empirical evidence for claims of improvements in software development and evolution has increased, the use of empirical methods such as case studies has grown. In case study methodology various types of triangulation is a commonly recommended technique for increasing validity. This study investigates a multiple data source case study with the objective of identifying whether more findings, trustworthier findings and other findings are made using multiple data source triangulation, than had a single data source been used. The case study investigated analyses key lead-time success factors for a software evolution project in a large organization developing eBusiness systems with high-availability high throughput transaction characteristics. By tracing each finding in that study to the individual evidences motivating the finding, it is suggested that a multiple data source explorative case study can have a higher validity than a single data source study. It is concluded that a careful case study design with multiple sources of evidence can result in not only better justified findings than a single data source study, but also other findings. Thus this study provides empirically derived evidence that a multiple data source case study is more trustworthy than a comparable single data source case study.},
author = {Bratthall, Lars and J{\o}rgensen, Magne},
doi = {10.1023/A:1014866909191},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Case study,Evidence,Research method,Triangulation,Trustworthiness,Validity},
title = {{Can you trust a single data source exploratory software engineering case study?}},
year = {2002}
}
@inproceedings{Renzella2019,
author = {Renzella, J and Cummaudo, A and Cain, A and Grundy, J and Meyers, J},
booktitle = {Proceedings of 2018 IEEE International Conference on Teaching, Assessment, and Learning for Engineering, TALE 2018},
doi = {10.1109/TALE.2018.8615203},
title = {{SplashKit: A Development Framework for Motivating and Engaging Students in Introductory Programming}},
year = {2019}
}
@article{Marshall:2018uj,
abstract = {This paper presents a cognitive computing model, based on artificial intelligence (AI) technologies, supporting task automation in the accounting industry. Drivers and consequences of task automation, globally and in accounting, are reviewed. A framework supporting cognitive task automation is discussed. The paper recognizes essential differences between cognitive computing and data analytics. Cognitive computing technologies that support task automation are incorporated into a model delivering federated knowledge. The impact of task automation on accounting job roles and the resulting creation of new accounting job roles supporting innovation are presented. The paper develops a hypothetical use case of building a cloud-based intelligent accounting application design, defined as cognitive services, using machine learning based on AI. The paper concludes by recognizing the significance of future research into task automation in accounting and suggests the federated knowledge model as a framework for future research into the process of digital transformation based on cognitive computing.},
author = {Marshall, Thomas Edward and Lambert, Sherwood Lane},
doi = {10.2308/jeta-52095},
issn = {1558-7940},
journal = {Journal of Emerging Technologies in Accounting},
keywords = {Artificial intelligence,Augmented intelligence,Cognitive computing,Task automation,Workforce},
number = {1},
pages = {199--215},
title = {{Cloud-based intelligent accounting applications: Accounting task automation using IBM watson cognitive computing}},
volume = {15},
year = {2018}
}
@inproceedings{Brandt:2009tm,
abstract = {This paper investigates the role of online resources in problem solving. We look specifically at how programmers-an exemplar form of knowledge workers-opportunistically interleave Web foraging, learning, and writing code. We describe two studies of how programmers use online resources. The first, conductcd in the lab. observed participants' Web u*e while building an online chat room. We found that programmers leverage online resources with a range of intentions: They engage in just-in-time learning of new skills and approaches, clarify and extend their existing knowledge, and remind themselves of details deemed not worth remembering. The results also suggest that queries for different purposes have different styles and durations. Do programmers' queries "in the wild" have the same range of intentions, or is this result an artifact of the particular lab setting? We analyzed a month of queries to an online programming portal, examining the lexical structure, refinements made, and result pages visited. Here we also saw traits that suggest the Web is being used for learning and reminding. These results contribute to a theory of online resource usage in programming, and suggest opportunities for tools to facilitate online knowledge work. Copyright 2009 ACM.},
address = {Boston, MA, USA},
author = {Brandt, Joel and Guo, Philip J and Lewenstein, Joel and Dontcheva, Mira and Klemmer, Scott R},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing System},
doi = {10.1145/1518701.1518944},
isbn = {978-1-60-558247-4},
keywords = {Copy-and-paste,Opportunistic programming,Prototyping},
month = {apr},
pages = {1589--1598},
publisher = {ACM},
title = {{Two studies of opportunistic programming: Interleaving web foraging, learning, and writing code}},
year = {2009}
}
@inproceedings{Thrun:1996wh,
abstract = {This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks.},
address = {Denver, CO, USA},
author = {Thrun, Sebastian},
booktitle = {Proceedings of the 8th International Conference on Neural Information Processing Systems},
issn = {1049-5258},
month = {nov},
pages = {7},
publisher = {MIT Press},
title = {{Is Learning The n-th Thing Any Easier Than Learning The First?}},
year = {1996}
}
@inproceedings{Pautasso2008,
abstract = {Recent technology trends in the Web Services (WS) domain indicate that a solution eliminating the presumed complexity of the WS-* standards may be in sight: advocates of REpresentational State Transfer (REST) have come to believe that their ideas explaining why the World Wide Web works are just as applicable to solve enterprise application integration problems and to simplify the plumbing required to build service-oriented architectures. In this paper we objectify the WS-* vs. REST debate by giving a quantitative technical comparison based on architectural principles and decisions. We show that the two approaches differ in the number of architectural decisions that must be made and in the number of available alternatives. This discrepancy between freedom-from-choice and freedom-of-choice explains the complexity difference perceived. However, we also show that there are significant differences in the consequences of certain decisions in terms of resulting development and maintenance costs. Our comparison helps technical decision makers to assess the two integration styles and technologies more objectively and select the one that best fits their needs: REST is well suited for basic, ad hoc integration scenarios, WS-* is more flexible and addresses advanced quality of service requirements commonly occurring in enterprise computing.},
address = {Beijing, China},
author = {Pautasso, Cesare and Zimmermann, Olaf and Leymann, Frank},
booktitle = {Proceedings of the 17th International Conference on World Wide Web},
doi = {10.1145/1367497.1367606},
isbn = {978-1-60-558085-2},
keywords = {Architectural decision modeling,HTTP,REST,Resource oriented architecture,SOAP,Service oriented architecture,Technology comparison,WS-* vs. REST,WSDL,Web services},
month = {apr},
publisher = {ACM},
title = {{RESTful web services vs. "Big" web services: Making the right architectural decision}},
year = {2008}
}
@article{Meng:2017cx,
abstract = {The success of an application programming interface (API) crucially depends on how well its documentation meets the information needs of software developers. Previous research suggests that these information needs have not been sufficiently understood. This article presents the results of a series of semistructured interviews and a follow-up questionnaire conducted to explore the learning goals and learning strategies of software developers, the information resources they turn to and the quality criteria they apply to API documentation. Our results show that developers initially try to form a global understanding regarding the overall purpose and main features of an API, but then adopt either a concepts-oriented or a code-oriented learning strategy that API documentation both needs to address. Our results also show that general quality criteria such as completeness and clarity are relevant to API documentation as well. Developing and maintaining API documentation therefore need to involve the expertise of communication professionals.},
author = {Meng, Michael and Steinhardt, Stephanie and Schubert, Andreas},
doi = {10.1177/0047281617721853},
issn = {1541-3780},
journal = {Journal of Technical Writing and Communication},
keywords = {Application programming interface documentation,Audience analysis,Information design,Technical documentation,Usability},
month = {aug},
number = {3},
pages = {295--330},
title = {{Application programming interface documentation: What do software developers want?}},
volume = {48},
year = {2018}
}
@inproceedings{Patel:2008:ISM:1357054.1357160,
abstract = {As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distil] three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning. Copyright 2008 ACM.},
address = {Florence, Italy},
author = {Patel, Kayur and Fogarty, James and Landay, James A and Harrison, Beverly},
booktitle = {Proceedings of the 26th SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/1357054.1357160},
isbn = {978-1-60-558011-1},
keywords = {Software development,Statistical machine learning},
month = {apr},
pages = {667--676},
publisher = {ACM},
series = {CHI '08},
title = {{Investigating statistical machine learning as a tool for software development}},
year = {2008}
}
@article{Gilmore:1974um,
abstract = {"Quality is the degree to which a specific product conforms to a design or specification"},
author = {Gilmore, Harold L},
journal = {Quality progress},
number = {5},
pages = {16--19},
title = {{Product conformance cost}},
volume = {7},
year = {1974}
}
@article{Hallgren:2012kt,
abstract = {Many research designs require the assessment of inter-rater reliability (IRR) to demonstrate consistency among observational ratings provided by multiple coders. However, many studies use incorrect statistical procedures, fail to fully report the information necessary to interpret their results, or do not address how IRR affects the power of their subsequent analyses for hypothesis testing. This paper provides an overview of methodological issues related to the assessment of IRR with a focus on study design, selection of appropriate statistics, and the computation, interpretation, and reporting of some commonly-used IRR statistics. Computational examples include SPSS and R syntax for computing Cohen's kappa and intra-class correlations to assess IRR.},
author = {Hallgren, Kevin A},
doi = {10.20982/tqmp.08.1.p023},
issn = {1913-4126},
journal = {Tutorials in Quantitative Methods for Psychology},
month = {feb},
number = {1},
pages = {23--34},
pmid = {22833776},
title = {{Computing Inter-Rater Reliability for Observational Data: An Overview and Tutorial}},
volume = {8},
year = {2012}
}
@incollection{Seaman:2007wa,
author = {Seaman, Carolyn B},
booktitle = {Guide to Advanced Empirical Software Engineering},
chapter = {2},
doi = {10.1007/978-1-84800-044-5},
editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K.},
isbn = {978-1-84-800043-8},
month = {nov},
pages = {35--62},
publisher = {Springer},
title = {{Qualitative methods}},
year = {2007}
}
@inproceedings{Cheng:2001vw,
abstract = {This paper investigates the methods for learning predictive classifiers based on Bayesian belief networks (BN)-primarily unrestricted Bayesian networks and Bayesian multi-nets. We present our algorithms for learning these classifiers, and discuss how these methods address the overfitting problem and provide a natural method for feature subset selection. Using a set of standard classification problems, we empirically evaluate the performance of various BN-based classifiers. The results show that the proposed BN and Bayes multinet classifiers are competitive with (or superior to) the best known classifiers, based on both BN and other formalisms; and that the computational time for learning and using these classifiers is relatively small. These results argue that BN-based classifiers deserve more attention in the data mining community.},
address = {Ottawa, ON, Canada},
author = {Cheng, Jie and Greiner, Russell},
booktitle = {Proceedings of the 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence},
doi = {10.1007/3-540-45153-6_14},
isbn = {3-54-042144-0},
issn = {1611-3349},
month = {jun},
pages = {141--151},
publisher = {Springer},
title = {{Learning bayesian belief network classifiers: Algorithms and system}},
volume = {2056},
year = {2001}
}
@inproceedings{Santos2014,
abstract = {Handheld augmented reality (HAR) applications must be carefully designed and improved based on user feedback to sustain commercial use. However, no standard questionnaire considers perceptual and ergonomic issues found in HAR. We address this issue by creating a HAR Usability Scale (HARUS). To create HARUS, we performed a systematic literature review to enumerate user-reported issues in HAR applications. Based on these issues, we created a questionnaire measuring manipulability - the ease of handling the HAR system, and comprehensibility - the ease of understanding the information presented by HAR. We then provide evidences of validity and reliability of the HARUS questionnaire by applying it to three experiments. The results show that HARUS consistently correlates with other subjective and objective measures of usability, thereby supporting its concurrent validity. Moreover, HARUS obtained a good Cronbach's alpha in all three experiments, thereby demonstrating internally consistency. HARUS, as well as its decomposition into individual manipulability and comprehensibility scores, are evaluation tools that researchers and professionals can use to analyze their HAR applications. By providing such a tool, they can gain quality feedback from users to improve their HAR applications towards commercial success.},
author = {Santos, Marc Ericson C. and Polvi, Jarkko and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
doi = {10.1145/2671015.2671019},
isbn = {9781450332538},
keywords = {Augmented reality,Evaluation method,Handheld devices,Usability,User studies},
title = {{Usability scale for handheld augmented reality}},
year = {2014}
}
@article{Bunge:1963jm,
abstract = {A mathematical theory is proposed and exemplified, which covers an extended class of black boxes. Every kind of stimulus and response is pictured by a channel connecting the box with its environment. The input-output relation is given by a postulate schema according to which the response is, in general, a nonlinear functional of the input. Several examples are worked out: the perfectly transmitting box, the damping box, and the amplifying box. The theory is shown to be (a) an extension of the S-matrix theory and the accompanying channel picture as developed in microphysics; (b) abstract and applicable to any problem involving the transactions of a system (physical, biological, social, etc.) with its milieu; (c) superficial, because unconcerned with either the structure of the box or the nature of the stimuli and responses. The motive for building the theory was to show the capabilities and limitations of the phenomenological approach.},
author = {Bunge, Mario},
doi = {10.1086/287954},
issn = {0031-8248},
journal = {Philosophy of Science},
month = {oct},
number = {4},
pages = {346--358},
title = {{A General Black Box Theory}},
volume = {30},
year = {1963}
}
@inproceedings{Clark:1991vi,
abstract = {The CN2 algorithm induces an ordered list of classification rules from examples using entropy as its search heuristic. In this short paper, we describe two improvements to this algorithm. Firstly, we present the use of the Laplacian error estimate as an alternative evaluation function and secondly, we show how unordered as well as ordered rules can be generated. We experimentally demonstrate significantly improved performances resulting from these changes, thus enhancing the usefulness of CN2 as an inductive tool. Comparisons with Quinlan's C4.5 are also made.},
address = {Porto, Portugal},
author = {Clark, Peter and Boswell, Robin},
booktitle = {Proceedings of the 1991 European Working Session on Learning},
doi = {10.1007/BFb0017011},
isbn = {978-3-54-053816-5},
issn = {1611-3349},
keywords = {CN2,Laplace,Learning,Noise,Rule induction},
month = {mar},
pages = {151--163},
publisher = {Springer},
title = {{Rule induction with CN2: Some recent improvements}},
year = {1991}
}
@inproceedings{Garousi:2017:EGE:3084226.3084238,
abstract = {To systematically collect evidence and to structure a given area in software engineering (SE), Systematic Literature Reviews (SLR) and Systematic Mapping (SM) studies have become common. Data extraction is one of the main phases (activities) when conducting an SM or an SLR, whose objective is to extract required data from the primary studies and to accurately record the information researchers need to answer the questions of the SM/SLR study. Based on experience in a large number of SM/SLR studies, we and many other researchers have found the data extraction in SLRs to be time consuming and error-prone, thus raising the real need for heuristics and guidelines for effective and efficient data extraction in these studies, especially to be learnt by junior and young researchers. As a 'guideline' paper, this paper contributes a synthesized list of challenges usually faced during SLRs' data extraction phase and the corresponding solutions (guidelines). For our synthesis, we consider two data sources: (1) the pool of 16 SLR studies in which the authors have been involved in, as well as (2) a review of challenges and guidelines in the existing literature. Our experience in utilizing the presented guidelines in the near past have helped our junior colleagues to conduct data extractions more effectively and efficiently.},
address = {Karlskrona, Sweden},
author = {Garousi, Vahid and Felderer, Michael},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
doi = {10.1145/3084226.3084238},
isbn = {978-1-45-034804-1},
keywords = {Data extraction,Empirical software engineering,Research methodology,SLR,SM,Systematic literature reviews,Systematic mapping studies},
month = {jun},
pages = {170--179},
publisher = {ACM},
title = {{Experience-based guidelines for effective and efficient data extraction in systematic reviews in software engineering}},
volume = {Part F1286},
year = {2017}
}
@inproceedings{Ghazi2010HierarchicalTexts,
address = {Ottawa, ON, Canada},
author = {Ghazi, Diman and Inkpen, Diana and Szpakowicz, Stan},
booktitle = {Proceedings of the 23rd Canadian Conference on Artificial Intelligence},
doi = {10.1007/978-3-642-13059-5_7},
keywords = {Emotion in text,Emotion recognition,Hierarchical classification,Sentiment analysis,Text classification},
month = {may},
pages = {40--50},
publisher = {Springer},
title = {{Hierarchical approach to emotion recognition and classification in texts}},
volume = {6085 LNAI},
year = {2010}
}
@misc{ISO9126:1999,
author = {{International Organization for Standardization}},
month = {nov},
title = {{ISO/IEC 9126 Information Technology - Software Product Evaluation - Quality Characteristics and Guidelines for Their Use}},
url = {http://bit.ly/2tgMHUE},
year = {1999}
}
@inproceedings{Durieux2018,
abstract = {Over the last few years, the complexity of web applications has increased to provide more dynamic web applications to users. The drawback of this complexity is the growing number of errors in the front-end applications. In this paper, we present BikiniProxy, a novel technique to provide self-healing for the web. BikiniProxy is designed as an HTTP proxy that uses five self-healing strategies to rewrite the buggy HTML and Javascript code. We evaluate BikiniProxy with a new benchmark of 555 reproducible Javascript errors, DeadClick. We create DeadClick by randomly crawling the Internet and collect all web pages that contain Javascript errors. Then, we observe how BikiniProxy heals those errors by collecting and comparing the traces of the original and healed pages. To sum up, BikiniProxy is a novel fully-automated self-healing approach that is specific to the web, evaluated on 555 real Javascript errors, and based on original self-healing rewriting strategies for HTML and Javascript.},
address = {Memphis, TN, USA},
author = {Durieux, Thomas and Hamadi, Youssef and Monperrus, Martin},
booktitle = {Proceedings of the 29th International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.2018.00012},
issn = {1071-9458},
keywords = {Failure oblivious computing,Javascript,Repair proxy,Self healing},
month = {oct},
pages = {1--12},
publisher = {IEEE},
title = {{Fully Automated HTML and Javascript Rewriting for Constructing a Self-Healing Web Proxy}},
year = {2018}
}
@article{Selvaraju:2017bk,
abstract = {We propose a technique for producing ‘visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach—Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog' in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger' deep network from a ‘weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265–290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.},
author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
doi = {10.1007/s11263-019-01228-7},
issn = {1573-1405},
journal = {International Journal of Computer Vision},
keywords = {Explanations,Grad-CAM,Interpretability,Transparency,Visual explanations,Visualizations},
pages = {618--626},
publisher = {IEEE},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}},
year = {2019}
}
@book{Juristo:2013vj,
abstract = {Basics of Software Engineering Experimentation is a practical guide to experimentation in a field which has long been underpinned by suppositions, assumptions, speculations and beliefs. It demonstrates to software engineers how Experimental Design and Analysis can be used to validate their beliefs and ideas. The book does not assume its readers have an in-depth knowledge of mathematics, specifying the conceptual essence of the techniques to use in the design and analysis of experiments and keeping the mathematical calculations clear and simple. Basics of Software Engineering Experimentation is practically oriented and is specially written for software engineers, all the examples being based on real and fictitious software engineering experiments.},
address = {Boston, MA, USA},
author = {Juristo, Natalia and Moreno, Ana M},
doi = {10.1007/978-1-4757-3304-4},
month = {mar},
publisher = {Springer},
title = {{Basics of Software Engineering Experimentation}},
year = {2001}
}
@book{Hastie:2001wp,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H},
edition = {2nd},
month = {jan},
publisher = {Springer},
series = {Data Mining, Inference, and Prediction},
title = {{The Elements of Statistical Learning}},
year = {2001}
}
@inproceedings{Watson:2013fx,
abstract = {Studies of what software developers need from API documentation have reported consistent findings over the years; however, these studies all used similar methods - usually a form of observation or survey. Our study looks at API documentation as artifacts of the open-source software communities who produce them to study how documentation produced by the communities who use the software compares to past studies of what software developers want and need from API documentation. We reviewed API documentation from 33 of the most popular open-source software projects, assessed their documentation elements, and evaluated the quality of their visual design and writing. We found that the documentation we studied included most or all the documentation elements reported as desirable in earlier studies and in the process, we found that the design and writing quality of many documentation sets received considerable attention. Our findings reinforce the API requirements identified in the literature and suggest that the design and writing quality of the documentation are also critical API documentation requirements that warrant further study. {\textcopyright} 2013 ACM.},
address = {Greenville, SC, USA},
author = {Watson, Robert and {Mark Stamnes}, Mark and Jeannot-Schroeder, Jacob and Spyridakis, Jan H.},
booktitle = {Proceedings of the 31st ACM International Conference on Design of Communication},
doi = {10.1145/2507065.2507076},
keywords = {api,api reference documentation,application programming interface,software documentation,software libraries},
month = {sep},
pages = {165--174},
publisher = {ACM},
title = {{API documentation and software community values: A survey of open-source API documentation}},
year = {2013}
}
@inproceedings{myers2018patterns,
abstract = {Voice User Interfaces (VUIs) are growing in popularity. However, even the most current VUIs regularly cause frustration for their users. Very few studies exist on what people do to overcome VUI problems they encounter, or how VUIs can be designed to aid people when these problems occur. In this paper, we analyze empirical data on how users (n=12) interact with our VUI calendar system, DiscoverCal, over three sessions. In particular, we identify the main obstacle categories and types of tactics our participants employ to overcome them. We analyzed the patterns of how different tactics are used in each obstacle category. We found that while NLP Error obstacles occurred the most, other obstacles are more likely to frustrate or confuse the user. We also found patterns that suggest participants were more likely to employ a "guessing" approach rather than rely on visual AIDS or knowledge recall.},
address = {Montreal, QC, Canada},
author = {Myers, Chelsea and Furqan, Anushay and Nebolsky, Jessica and Caro, Karina and Zhu, Jichen},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3173574.3173580},
isbn = {978-1-45-035620-6},
keywords = {User experience,Voice User Interfaces,Voice control},
month = {apr},
pages = {6},
publisher = {ACM},
title = {{Patterns for how users overcome obstacles in Voice User Interfaces}},
volume = {2018-April},
year = {2018}
}
@article{Doderer:2006vt,
abstract = {In silico prediction of protein subcellular localization based on amino acid sequence can reveal valuable information about the protein's innate roles in the cell. Unfortunately, such prediction is made difficult because of complex protein sorting signals. Some prediction methods are based on searching for similar proteins with known localization, assuming that known homologs exist. However, it may not perform well on proteins with no known homolog. In contrast, machine learning-based approaches attempt to infer a predictive model that describes the protein sorting signals. Alas, in doing so, it does not take advantage of known homologs (if they exist) by doing a simple "table lookup". Here, we capture the best of both worlds by combining both approaches. On a dataset with 12 locations, similarity-based and machine learning independently achieve an accuracy of 83.8{\%} and 72.6{\%}, respectively. Our hybrid approach yields an improved accuracy of 85.9{\%}. We compared our method with three other methods' published results. For two of the methods, we used their published datasets for comparison. For the third we used the 12 location dataset. The Error Correcting Output Code algorithm was used to construct our predictive model. This algorithm gives attention to all the classes regardless of number of instances and led to high accuracy among each of the classes and a high prediction rate overall. We also illustrated how the machine learning classifier we use, built over a meaningful set of features can produce interpretable rules that may provide valuable insights into complex protein sorting mechanisms. {\textcopyright}2006 IOS Press. All rights reserved.},
author = {Doderer, Mark and Yoon, Kihoon and Salinas, John and Kwek, Stephen},
issn = {1386-6338},
journal = {In Silico Biology},
keywords = {Blast,Decision tree,Error correcting output code,Similarity search,Subcellular localization prediction},
number = {5},
pages = {419--433},
title = {{Protein subcellular localization prediction using a hybrid of similarity search and Error-Correcting Output Code techniques that produces interpretable results}},
volume = {6},
year = {2006}
}
@inproceedings{Pezzementi:2018tq,
abstract = {We introduce a method to evaluate the robustness of perception systems to the wide variety of conditions that a deployed system will encounter. Using person detection as a sample safety-critical application, we evaluate the robustness of several state-of-the-art perception systems to a variety of common image perturbations and degradations. We introduce two novel image perturbations that use 'contextual information' (in the form of stereo image data) to perform more physically-realistic simulation of haze and defocus effects. For both standard and contextual mutations, we show cases where performance drops catastrophically in response to barely-perceptible changes. We also show how robustness to contextual mutators can be predicted without the associated contextual information in some cases.},
address = {Philadelphia, PA, USA},
author = {Pezzementi, Zachary and Tabor, Trenton and Yim, Samuel and Chang, Jonathan K and Drozd, Bill and Guttendorf, David and Wagner, Michael and Koopman, Philip},
booktitle = {Proceedings of the 15th IEEE International Symposium on Safety, Security, and Rescue Robotics},
doi = {10.1109/SSRR.2018.8468619},
isbn = {978-1-53-865572-6},
month = {aug},
pages = {1--8},
publisher = {IEEE},
title = {{Putting Image Manipulations in Context: Robustness Testing for Safe Perception}},
year = {2018}
}
@phdthesis{Fielding:2000vh,
author = {Fielding, Roy Thomas},
doi = {978-0-599-87118-2},
school = {University of California, Irvine},
title = {{Architectural Styles and the Design of Network-based Software Architectures}},
year = {2000}
}
@article{Cummaudo:2020tse,
annote = {Unpublished},
author = {Cummaudo, Alex and Vasa, Rajesh and Grundy, John},
journal = {IEEE Transactions on Software Engineering},
keywords = {InReview},
mendeley-tags = {InReview},
title = {{Requirements of API Documentation: A Case Study into Computer Vision Services}},
year = {2020}
}
@inproceedings{Kitchenham:2004vj,
address = {Edinburgh, Scotland, UK},
author = {Kitchenham, Barbara A and Dyb{\aa}, Tore and Jorgensen, Magne},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
isbn = {978-0-76-952163-3},
month = {may},
pages = {273--281},
publisher = {IEEE},
title = {{Evidence-Based Software Engineering}},
year = {2004}
}
@book{Quinlan:1993vi,
address = {San Francisco, CA, USA},
author = {Quinlan, J Ross},
isbn = {978-1-55-860238-0},
publisher = {Morgan Kauffmann},
title = {{C4.5: Programs for machine learning}},
year = {1993}
}
@article{Venners:2003vw,
author = {Venners, B},
journal = {Artima Developer},
title = {{Design by Contract: A Conversation with Bertrand Meyer}},
url = {http://bit.ly/31bzGZ1},
year = {2003}
}
@article{Dejaeger:2012up,
abstract = {As a consequence of the heightened competition on the education market, the management of educational institutions often attempts to collect information on what drives student satisfaction by e.g. organizing large scale surveys amongst the student population. Until now, this source of potentially very valuable information remains largely untapped. In this study, we address this issue by investigating the applicability of different data mining techniques to identify the main drivers of student satisfaction in two business education institutions. In the end, the resulting models are to be used by the management to support the strategic decision making process. Hence, the aspect of model comprehensibility is considered to be at least equally important as model performance. It is found that data mining techniques are able to select a surprisingly small number of constructs that require attention in order to manage student satisfaction. {\textcopyright}2011 Elsevier B.V. All rights reserved.},
author = {Dejaeger, Karel and Goethals, Frank and Giangreco, Antonio and Mola, Lapo and Baesens, Bart},
doi = {10.1016/j.ejor.2011.11.022},
issn = {0377-2217},
journal = {European Journal of Operational Research},
keywords = {Comprehensibility,Data mining,Education evaluation,Multi class classification},
number = {2},
pages = {548--562},
title = {{Gaining insight into student satisfaction using comprehensible data mining techniques}},
volume = {218},
year = {2012}
}
@inproceedings{Narayanan:2002ti,
abstract = {Web services - Web-accessible programs and devices - are a key application area for the Semantic Web. With the proliferation of Web services and the evolution towards the Semantic Web comes the opportunity to automate various Web services tasks. Our objective is to enable markup and automated reasoning technology to describe, simulate, compose, test, and verify compositions of Web services. We take as our starting point the DAML-S DAML+OIL ontology for describing the capabilities of Web services. We define the semantics for a relevant subset of DAML-S in terms of a first-order logical language. With the semantics in hand, we encode our service descriptions in a Petri Net formalism and provide decision procedures for Web service simulation, verification and composition. We also provide an analysis of the complexity of these tasks under different restrictions to the DAML-S composite services we can describe. Finally, we present an implementation of our analysis techniques. This implementation takes as input a DAML-S description of a Web service, automatically generates a Petri Net and performs the desired analysis. Such a tool has broad applicability both as a back end to existing manual Web service composition tools, and as a stand-alone tool for Web service developers.},
address = {Honolulu, HI, USA},
author = {Narayanan, Srini and McIlraith, Sheila A},
booktitle = {Proceedings of the 11th International Conference on World Wide Web},
doi = {10.1145/511446.511457},
isbn = {1-58-113449-5},
keywords = {Automated reasoning,DAML,Distributed systems,Ontologies,Semantic web,Web service composition,Web services},
month = {may},
pages = {77--88},
publisher = {ACM},
title = {{Simulation, verification and automated composition of web services}},
year = {2002}
}
@article{Uddin:2019cz,
abstract = {With the proliferation of online developer forums, developers share their opinions about the APIs they use. The plethora of such information can present challenges to the developers to get quick but informed insights about the APIs. To understand the potential benefits of such API reviews, we conducted a case study of opinions in Stack Overflow using a benchmark dataset of 4522 sentences. We observed that opinions about diverse API aspects (e.g., usability) are prevalent and offer insights that can shape developers{\&}{\#}x0027; perception and decisions related to software development. Motivated by the finding, we built a suite of techniques to automatically mine and categorize opinions about APIs from forum posts. First, we detect opinionated sentences in the forum posts. Second, we associate the opinionated sentences to the API mentions. Third, we detect API aspects (e.g., performance, usability) in the reviews. We developed and deployed a tool called Opiner, supporting the above techniques. Opiner is available online as a search engine, where developers can search for APIs by their names to see all the aggregated opinions about the APIs that are automatically mined and summarized from developer forums.},
annote = {In Press},
author = {Uddin, Gias and Khomh, Foutse},
doi = {10.1109/TSE.2019.2900245},
issn = {1939-3520},
journal = {IEEE Transactions on Software Engineering},
keywords = {API,API Aspect,API Review Mining,Benchmark testing,Categorization,Data mining,InPress,Java,Opinion,Review,Search engines,Tools,Usability},
mendeley-tags = {InPress},
month = {feb},
title = {{Automatic Mining of Opinions Expressed About APIs in Stack Overflow}},
year = {2019}
}
@book{Casati:2003vi,
author = {Casati, Fabio and Kuno, Harumi and Alonso, G and Machiraju, V},
isbn = {978-3-64-207888-0},
title = {{Web Services-Concepts, Architectures and Applications}},
year = {2004}
}
@inproceedings{Pal:2012te,
abstract = {Community Question Answering (CQA) services thrive as a result of a small number of highly active users, typically called experts, who provide a large number of high quality useful answers. Understanding the temporal dynamics and interactions between experts can present key insights into how community members evolve over time. In this paper, we present a temporal study of experts in CQA and analyze the changes in their behavioral patterns over time. Further, using unsupervised machine learning methods, we show the interesting evolution patterns that can help us distinguish experts from one another. Using supervised classification methods, we show that the models based on evolutionary data of users can be more effective at expert identification than the models that ignore evolution. We run our experiments on two large online CQA to show the generality of our proposed approach. Copyright {\textcopyright}2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
address = {Dublin, Ireland},
author = {Pal, Aditya and Chang, Shuo and Konstan, Joseph A},
booktitle = {Proceedings of the 6th International AAAI Conference on Weblogs and Social Media},
isbn = {978-1-57-735556-4},
month = {jun},
pages = {274--281},
publisher = {AAAI},
title = {{Evolution of experts in question answering communities}},
year = {2012}
}
@inproceedings{Lopez2014,
address = {Hong Kong, China},
author = {Lopez-Lorca, Antonio A and Miller, Tim and Pedell, Sonja and Mendoza, Antonette and Keirnan, Alen and Sterling, Leon},
booktitle = {Proceedings of the 6th International Workshop on Social Software Engineering},
doi = {10.1145/2661685.2661691},
month = {nov},
pages = {25--32},
publisher = {ACM},
title = {{One size doesn't fit all: diversifying the user using personas and emotional scenarios}},
year = {2014}
}
@article{Subramanian:1992ue,
author = {Subramanian, Girish H and Nosek, John and Raghunathan, Sankaran P and Kanitkar, Santosh S},
doi = {10.1145/129617.129621},
issn = {1557-7317},
journal = {Communications of the ACM},
keywords = {computer games,decision aids effectiveness,effectiveness of structured tools,human aspects of computing,human factors of experimentation},
number = {1},
pages = {89--94},
title = {{A comparison of the decision table and tree}},
volume = {35},
year = {1992}
}
@article{Pazzani:2001tw,
abstract = {Objectives: The aim was to evaluate the potential for monotonicity constraints to bias machine learning systems to learn rules that were both accurate and meaningful. Methods: Two data sets, taken from problems as diverse as screening for dementia and assessing the risk of mental retardation, were collected and a rule learning system, with and without monotonicity constraints, was run on each. The rules were shown to experts, who were asked how willing they would be to use such rules in practice. The accuracy of the rules was also evaluated. Results: Rules learned with monotonicity constraints were at least as accurate as rules learned without such constraints. Experts were, on average, more willing to use the rules learned with the monotonicity constraints. Conclusions: The analysis of medical databases has the potential of improving patient outcomes and/or lowering the cost of health care delivery. Various techniques, from statistics, pattern recognition, machine learning, and neural networks, have been proposed to "mine" this data by uncovering patterns that may be used to guide decision making. This study suggests cognitive factors make learned models coherent and, therefore, credible to experts. One factor that influences the acceptance of learned models is consistency with existing medical knowledge.},
author = {Pazzani, M J and Mani, S and Shankle, W R},
doi = {10.1055/s-0038-1634196},
issn = {0026-1270},
journal = {Methods of Information in Medicine},
keywords = {Alzheimer Disease,Artificial Intelligence,Mental Retardation},
number = {5},
pages = {380--385},
pmid = {11776735},
title = {{Acceptance of rules generated by machine learning among medical experts}},
volume = {40},
year = {2001}
}
@inproceedings{Novielli:2015vda,
abstract = {A recent research trend has emerged to study the role of affect in in the social programmer ecosystem, by applying sentiment analysis to the content available in sites such as GitHub and Stack Overflow. In this paper, we aim at assessing the suitability of a state-of-the-art sentiment analysis tool, already applied in social computing, for detecting affective expressions in Stack Overflow. We also aim at verifying the construct validity of choosing sentiment polarity and strength as an appropriate way to operationalize affective states in empirical studies on Stack Overflow. Finally, we underline the need to overcome the limitations induced by domain-dependent use of lexicon that may produce unreliable results.},
address = {Bergamo, Italy},
author = {Novielli, Nicole and Calefato, Fabio and Lanubile, Filippo},
booktitle = {Proceedings of the 7th International Workshop on Social Software Engineering},
doi = {10.1145/2804381.2804387},
isbn = {978-1-45-033818-9},
keywords = {Online Q and A,Overflow,Sentiment Analysis,Social Programmer,Social Software Engineering,Stack,Technical Forum},
month = {aug},
pages = {33--40},
publisher = {ACM},
title = {{The challenges of sentiment detection in the social programmer ecosystem}},
year = {2015}
}
@inproceedings{Ko:2004td,
abstract = {As programming skills increase in demand and utility, the learnability of end-user programming systems is of utmost importance. However, research on learning barriers in programming systems has primarily focused on languages, overlooking potential barriers in the environment and accompanying libraries. To address this, a study of beginning programmers learning Visual Basic. MET was performed. This identified six types of barriers: design, selection, coordination, use, understanding, and information. These barriers inspire a new metaphor of computation, which provides a more learner-centric view of programming system design. {\textcopyright}2004 IEEE.},
address = {Rome, Italy},
author = {Ko, Andrew J and Myers, Brad A and Aung, Htet Htet},
booktitle = {Proceedings of the 2004 IEEE Symposium on Visual Languages and Human Centric Computing},
doi = {10.1109/vlhcc.2004.47},
isbn = {0-78-038696-5},
month = {sep},
pages = {199--206},
publisher = {IEEE},
title = {{Six learning barriers in end-user programming systems}},
year = {2004}
}
@misc{Ballinger:2014aa,
annote = {Accessed: 28 August 2018},
author = {Ballinger, Keith},
month = {dec},
title = {{Simplicity and Utility, or, Why SOAP Lost}},
url = {http://bit.ly/37vLms0},
year = {2014}
}
@article{Hohman2018VisualAI,
abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
doi = {10.1109/TVCG.2018.2843369},
issn = {1941-0506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Deep learning,information visualization,neural networks,visual analytics},
number = {8},
pages = {2674--2693},
title = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
volume = {25},
year = {2019}
}
@article{Graziontin:2017,
abstract = {The growing literature on affect among software developers mostly reports on the linkage between happiness, software quality, and developer productivity. Understanding happiness and unhappiness in all its components – positive and negative emotions and moods – is an attractive and important endeavor. Scholars in industrial and organizational psychology have suggested that understanding happiness and unhappiness could lead to cost-effective ways of enhancing working conditions, job performance, and to limiting the occurrence of psychological disorders. Our comprehension of the consequences of (un)happiness among developers is still too shallow, being mainly expressed in terms of development productivity and software quality. In this paper, we study what happens when developers are happy and unhappy while developing software. Qualitative data analysis of responses given by 317 questionnaire participants identified 42 consequences of unhappiness and 32 of happiness. We found consequences of happiness and unhappiness that are beneficial and detrimental for developers' mental well-being, the software development process, and the produced artifacts. Our classification scheme, available as open data enables new happiness research opportunities of cause-effect type, and it can act as a guideline for practitioners for identifying damaging effects of unhappiness and for fostering happiness on the job.},
author = {Graziotin, Daniel and Fagerholm, Fabian and Wang, Xiaofeng and Abrahamsson, Pekka},
doi = {10.1016/j.jss.2018.02.041},
issn = {0164-1212},
journal = {Journal of Systems and Software},
keywords = {Affect,Behavioral software engineering,Developer experience,Emotion,Happiness,Human aspects},
title = {{What happens when software developers are (un)happy}},
year = {2018}
}
@book{Creswell:2017vn,
author = {Creswell, John W},
edition = {4th},
isbn = {860-1-40-429618-5},
publisher = {SAGE},
title = {{Research design: Qualitative, quantitative, and mixed methods approaches}},
year = {2017}
}
@inproceedings{Canfora:2005vd,
abstract = {Service Oriented Architectures, and particularly Web Services, are receiving a growing attention from research and industry. With Web Services, software is used and not owned and operation happens on machines that are out of the user control. Therefore, providing users with means to build confidence that a service delivers the desired function with the expected QoS becomes a key issue.},
address = {Manchester, England, UK},
author = {Canfora, Gerardo},
booktitle = {Proceedings of the 9th European Conference on Software Maintenance and Reengineering},
doi = {10.1109/csmr.2005.57},
issn = {1534-5351},
month = {mar},
pages = {301},
publisher = {IEEE},
title = {{User-side testing of Web Services}},
year = {2005}
}
@inproceedings{gachechiladze2017,
address = {Buenos Aires, Argentina},
author = {Gachechiladze, Daviti and Lanubile, Filippo and Novielli, Nicole and Serebrenik, Alexander},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track},
doi = {10.1109/ICSE-NIER.2017.18},
month = {may},
organization = {IEEE},
pages = {11--14},
publisher = {IEEE},
title = {{Anger and its direction in collaborative software development}},
year = {2017}
}
@inproceedings{Juristo:2012bp,
abstract = {Experimentation has played a major role in scientific advancement. Replication is one of the essentials of the experimental methods. In replications, experiments are repeated aiming to check their results. Successful replication increases the validity and reliability of the outcomes observed in an experiment. There is debate about the best way of running replications of Software Engineering (SE) experiments. Some of the questions that have cropped up in this debate are, "Should replicators reuse the baseline experiment materials? Which is the adequate sort of communication among experimenters and replicators if any? What elements of the experimental structure can be changed and still be considered a replication instead of a new experiment?". A deeper understanding of the concept of replication should help to clarify these issues as well as increase and improve replications in SE experimental practices. In this chapter, we study the concept of replication in order to gain insight. The chapter starts with an introduction to the importance of replication and the state of replication in ESE. Then we discuss replication from both the statistical and scientific viewpoint. Based on a review of the diverse types of replication used in other scientific disciplines, we identify the different types of replication that are feasible to be run in our discipline. Finally, we present the different purposes that replication can serve in Experimental Software Engineering (ESE). {\textcopyright}2012 Springer-Verlag Berlin Heidelberg.},
address = {Elba Island, Italy},
author = {Juristo, Natalia and G{\'{o}}mez, Omar S},
booktitle = {Proceedings of the LASER Summer School on Software Engineering},
doi = {10.1007/978-3-642-25231-0_2},
isbn = {978-3-64-225230-3},
issn = {0302-9743},
keywords = {Empirical Software Engineering,Experimental Replicaction,Experimental Software Engineering,Types of Replication},
pages = {60--88},
publisher = {Springer},
title = {{Replication of software engineering experiments}},
year = {2011}
}
@inproceedings{Lin:2014vma,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright}2014 Springer International Publishing.},
address = {Zurich, Germany},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
booktitle = {Proceedings of the 13th European Conference on Computer Vision},
doi = {10.1007/978-3-319-10602-1_48},
editor = {Fleet, David and Pajdla, Tom{\'{a}}s and Schiele, Bernt and Tuytelaars, Tinne},
eprint = {1405.0312},
issn = {1611-3349},
month = {sep},
number = {PART 5},
pages = {740--755},
publisher = {Springer},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@inproceedings{StrapparavaWordNet-Affect:WordNet,
address = {Lisbon, Portugal},
author = {Strapparava, Carlo and Valitutti, Alessandro},
booktitle = {Proceedings of the 4th International Conference on Language Resources and Evaluation},
month = {may},
pages = {1083--1086},
publisher = {European Language Resources Association (ELRA)},
title = {{WordNet-Affect: an Affective Extension of WordNet}},
year = {2004}
}
@inproceedings{Hayete:2005tn,
abstract = {The Gene Ontology (GO) offers a comprehensive and standardized way to describe a protein's biological role. Proteins are annotated with GO terms based on direct or indirect experimental evidence. Term assignments are also inferred from homology and literature mining. Regardless of the type of evidence used, GO assignments are manually curated or electronic. Unfortunately, manual curation cannot keep pace with the data, available from publications and various large experimental datasets. Automated literature-based annotation methods have been developed in order to speed up the annotation. However, they only apply to proteins that have been experimentally investigated or have close homologs with sufficient and consistent annotation. One of the homology-based electronic methods for GO annotation is provided by the InterPro database. The InterPro2GO/PFAM2GO associates individual protein domains with GO terms and thus can be used to annotate the less studied proteins. However, protein classification via a single functional domain demands stringency to avoid large number of false positives. This work broadens the basic approach. We model proteins via their entire functional domain content and train individual decision tree classifiers for each GO term using known protein assignments. We demonstrate that our approach is sensitive, specific and precise, as well as fairly robust to sparse data. We have found that our method is more sensitive when compared to the InterPro2GO performance and suffers only some precision decrease. In comparison to the InterPro2GO we have improved the sensitivity by 22{\%}, 27{\%} and 50{\%} for Molecular Function, Biological Process and Cellular GO terms respectively.},
address = {Hawaii, USA},
author = {Hayete, Boris and Bienkowska, Jadwiga R},
booktitle = {Proceedings of the Pacific Symposium on Biocomputing 2005, PSB 2005},
doi = {10.1142/9789812702456_0013},
isbn = {9-81-256046-7},
month = {jan},
pages = {127--138},
publisher = {World Scientific Publishing Company},
title = {{Gotrees: Predicting go associations from protein domain composition using decision trees}},
year = {2005}
}
@book{kahneman2011thinking,
author = {Kahneman, Daniel},
isbn = {978-0-37-453355-7},
publisher = {Macmillan},
title = {{Thinking, Fast and Slow}},
year = {2011}
}
@article{AlQutaish:2010vua,
abstract = {The quality of the software is critical and essential in different types of organizations. In some types of software, poor quality of the software product in sensitive systems (such as: real-time systems, control systems, etc.) may lead to loss of human life, permanent injury, mission failure, or financial loss. In software engineering literature, there are a number of quality models in which they contain a number of quality characteristics (or factors, as called in some models). These quality characteristics could be used to reflect the quality of the software product from the view of that characteristic. Selecting which one of the quality models to use is a real challenge. In this paper, we will discuss the contents of the following quality models: McCall's quality model, Boehm's quality model, Dromey's quality model, FURPS quality model and ISO 9126 quality model. In addition, we will focus on a comparison between these quality models, and find the key differences between them. [Journal},
author = {Al-Qutaish, Rafa E},
journal = {Journal of American Science},
keywords = {Boehm's Quality Model,Dromey's Quality Model,FURPS Quality Model,ISO 9126,McCall's Quality Model,Quality Engineering,Quality Models,Software Quality},
number = {3},
pages = {166--175},
title = {{Quality Models in Software Engineering Literature: An Analytical and Comparative Study}},
volume = {6},
year = {2010}
}
@book{Dey:2003ty,
abstract = {This book comprehensively covers all major topics of Vygotskian educa- tional theory and its classroom applications. Particular attention is paid to the Vygotskian idea of child development as a consequence rather than premise of learning experiences. Such a reversal allows for new interpretations of the relationships between cognitive development and education at different junc- tions of the human life span. It also opens new perspectives on atypical de- velopment, learning disabilities, and assessment of children's learning poten- tial. Classroomapplications ofVygotskian theory, teacher preparation, and the changing role of a teacher in a sociocultural classroom are discussed in addi- tion to the issues of learning activities and peer interaction. Relevant research findings fromthe United States,Western Europe, and Russia are considered to- gether to clarify the possible new applications of Vygotskian ideas in different disciplinary areas. The sociocultural orientation of Vygotskian theory helps to reveal learning patterns that become obscured in more traditional research.},
address = {New York, NY},
author = {Dey, I},
doi = {10.4324/9780203412497},
isbn = {978-0-41-505852-0},
issn = {1367-6539},
pmid = {2539002},
publisher = {Routledge},
title = {{Qualitative Data Analysis: A User-Friendly Guide for Social Scientists}},
year = {1993}
}
@inproceedings{hohman2019gamut,
abstract = {Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability. Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools. Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, Gamut, to explore how interactive interfaces could better support model interpretation. Using Gamut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability. Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness. Participants also asked to use Gamut in their work, highlighting its potential to help data scientists understand their own data.},
address = {Glasgow, Scotland, UK},
author = {Hohman, Fred and Head, Andrew and Caruana, Rich and DeLine, Robert and Drucker, Steven M},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3290605.3300809},
isbn = {978-1-45-035970-2},
keywords = {Data visualization,Design probe,Interactive interfaces,Machine learning interpretability,Visual analytics},
month = {may},
publisher = {ACM},
title = {{Gamut: A design probe to understand how data scientists understand machine learning models}},
year = {2019}
}
@article{Reis:2018cp,
abstract = {The visually impaired must face several well-known difficulties on their daily life. The use of technology in assistive systems can greatly improve their lives by helping with navigation and orientation, for which several approaches and technologies have been proposed. Lately, it has been introduced powerful online image processing services, based on machine learning and deep learning, promising truly cognitive assessment capacities. Google and Microsoft are two of these main players. In this work we built a device to be used by the blind in order to test the usage of the Google and Microsoft services to assist the blind. The online services were tested by researchers in a laboratory environment and by blind users on a large meeting room, familiar to them. This work reports on our findings regarding the online services effectiveness, the user interface and system latency.},
author = {Reis, Ars{\'{e}}nio and Paulino, Dennis and Filipe, Vitor and Barroso, Jo{\~{a}}o},
doi = {10.1007/978-3-319-77712-2_17},
isbn = {978-3-31-977711-5},
issn = {2194-5357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Assistive systems,Cognitive services,Human-computer interaction,Visually impaired},
number = {12},
pages = {174--184},
title = {{Using online artificial vision services to assist the blind - An assessment of Microsoft Cognitive Services and Google Cloud Vision}},
volume = {746},
year = {2018}
}
@article{cicchetti1994guidelines,
abstract = {In the context of the development of prototypic assessment instruments in the areas of cognition, personality, and adaptive functioning, the issues of standardization, norming procedures, and the important psychometrics of test reliability and validity are evaluated critically. Criteria, guidelines, and simple rules of thumb are provided to assist the clinician faced with the challenge of choosing an appropriate test instrument for a given psychological assessment.},
author = {Cicchetti, Domenic V.},
doi = {10.1037/1040-3590.6.4.284},
issn = {10403590},
journal = {Psychological Assessment},
number = {4},
pages = {284--290},
publisher = {American Psychological Association},
title = {{Guidelines, Criteria, and Rules of Thumb for Evaluating Normed and Standardized Assessment Instruments in Psychology}},
volume = {6},
year = {1994}
}
@inproceedings{Szegedy:2016ws,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
address = {Las Vegas, NV, USA},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle = {Proceedings of the 2016 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
isbn = {978-1-46-738850-4},
issn = {1063-6919},
month = {jun},
pages = {2818--2826},
publisher = {IEEE},
title = {{Rethinking the Inception Architecture for Computer Vision}},
year = {2016}
}
@inproceedings{Lakkaraju:2016ka,
abstract = {One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a nonmonotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.},
address = {San Francisco, CA, USA},
author = {Lakkaraju, Himabindu and Bach, Stephen H and Leskovec, Jure},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2939672.2939874},
isbn = {978-1-45-034232-2},
month = {aug},
pages = {1675--1684},
publisher = {ACM},
title = {{Interpretable decision sets: A joint framework for description and prediction}},
year = {2016}
}
@inproceedings{Aghajani:2018et,
abstract = {The concept of monolithic stand-Alone software systems developed completely from scratch has become obsolete, as modern systems nowadays leverage the abundant presence of Application Programming Interfaces (APIs) developed by third parties, which leads on the one hand to accelerated development, but on the other hand introduces potentially fragile dependencies on external resources. In this context, the design of any API strongly influences how developers write code utilizing it. A wrong design decision like a poorly chosen method name can lead to a steeper learning curve, due to misunderstandings, misuse and eventually bug-prone code in the client projects using the API. It is not unfrequent to find APIs with poorly expressive or misleading names, possibly lacking appropriate documentation. Such issues can manifest in what have been defined in the literature as Linguistic Antipatterns (LAs), i.e., inconsistencies among the naming, documentation, and implementation of a code entity. While previous studies showed the relevance of LAs for software developers, their impact on (developers of) client projects using APIs affected by LAs has not been investigated. This paper fills this gap by presenting a large-scale study conducted on 1.6k releases of popular Maven libraries, 14k open-source Java projects using these libraries, and 4.4k questions related to the investigated APIs asked on Stack Overflow. In particular, we investigate whether developers of client projects have higher chances of introducing bugs when using APIs affected by LAs and if these trigger more questions on Stack Overflow as compared to non-Affected APIs.},
address = {Madrid, Spain},
author = {Aghajani, Emad and Nagy, Csaba and Bavota, Gabriele and Lanza, Michele},
booktitle = {Proceedings of the 34th International Conference on Software Maintenance and Evolution},
doi = {10.1109/ICSME.2018.00012},
isbn = {978-1-53-867870-1},
keywords = {Application Programming Interfaces (APIs),Empirical Study,Linguistic Antipatterns},
month = {sep},
pages = {25--35},
publisher = {IEEE},
title = {{A Large-scale empirical study on linguistic antipatterns affecting apis}},
year = {2018}
}
@inproceedings{Parekh:2017hx,
address = {Halifax, NS, Canada},
author = {Parekh, Rajesh},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3097983.3105815},
month = {aug},
pages = {27},
publisher = {ACM},
title = {{Designing AI at Scale to Power Everyday Life}},
year = {2017}
}
@article{WordNetMiller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonyms that are in turn linked through semantic relations that determine word definitions. {\textcopyright}1995, ACM. All rights reserved.},
address = {New York, NY, USA},
author = {Miller, George A},
doi = {10.1145/219717.219748},
issn = {1557-7317},
journal = {Communications of the ACM},
month = {nov},
number = {11},
pages = {39--41},
publisher = {ACM},
title = {{WordNet: A Lexical Database for English}},
volume = {38},
year = {1995}
}
@inproceedings{Head:2018baa,
abstract = {Without usable and accurate documentation of how to use an API, developers can find themselves deterred from reusing relevant code. In C++, one place developers can find documentation is in a header file. When information is missing, they may look at the corresponding implementation code. To understand what's missing from C++ API documentation and the factors influencing whether it will be fixed, we conducted a mixed-methods study involving two experience sampling surveys with hundreds of developers at the moment they visited implementation code, interviews with 18 of those developers, and interviews with 8 API maintainers. In many cases, updating documentation may provide only limited value for developers, while requiring effort maintainers don't want to invest. We identify a set of questions maintainers and tool developers should consider when improving API-level documentation.},
address = {Gothenburg, Sweden},
author = {Head, Andrew and Sadowski, Caitlin and Murphy-Hill, Emerson and Knight, Andrea},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
doi = {10.1145/3180155.3180176},
issn = {0270-5257},
month = {may},
pages = {643--653},
publisher = {ACM},
series = {questions and tradeoffs with API documentation for C++ projects},
title = {{When not to comment: Questions and tradeoffs with API documentation for C++ projects}},
year = {2018}
}
@book{Ekman1978,
address = {Palo Alto, CA, USA},
author = {Ekman, P and Friesen, W V and Hager, J},
publisher = {Consulting Pyschologists Press},
title = {{Facial Action Coding System: A Technique for the Measurement of Facial Movement}},
year = {1978}
}
@inproceedings{Pazzani:1997vp,
address = {Washington, DC, USA},
author = {Pazzani, M},
booktitle = {Proceedings of the First Federal Data Mining Conference and Exposition},
pages = {73--82},
title = {{Comprehensible knowledge discovery: gaining insight from data}},
year = {1997}
}
@inproceedings{Nakajima:2002ut,
abstract = {Model-checking is a promising technique for the verification and validation of software systems. Web service, an emerging technology in the Internet, is an autonomous server that may offer an individual service. It sometimes requires to combine more than one to meet our requirements. WSFL(Web Services Flow Language) is proposed as a language to provide means to describe Web service aggregation. We are interested in how much the software modelchecking technique can be used as a basis for raising reliability of Web service, Web service flow descriptions in particular. Our experience shows that faulty flow descriptions can be identified with the proposed method. The method is also very helpful in studying an alternative semantics of the WSFL in regard to the handling of dataflows.},
address = {Montreal, QC, Canada},
author = {Nakajima, Shin},
booktitle = {Proceedings of the First International Symposium on Cyber World},
isbn = {978-0-76-951862-6},
month = {nov},
pages = {378--385},
publisher = {IEEE},
title = {{Model-Checking Verification for Reliable Web Service}},
year = {2002}
}
@article{EhteshamiBejnordi:2017kq,
abstract = {IMPORTANCE: Application of deep learning algorithms to whole-slide pathology imagescan potentially improve diagnostic accuracy and efficiency. OBJECTIVE: Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin-stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists' diagnoses in a diagnostic setting. DESIGN, SETTING, AND PARTICIPANTS: Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n = 110) and without (n = 160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC). EXPOSURES: Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation. MAIN OUTCOMES AND MEASURES: The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor. RESULTS: The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4{\%} [95{\%} CI, 64.3{\%}-80.4{\%}]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95{\%} CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P {\textless}.001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95{\%} CI, 0.927-0.998] for the pathologist WOTC). CONCLUSIONS AND RELEVANCE: In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.},
author = {Bejnordi, Babak Ehteshami and Veta, Mitko and {Van Diest}, Paul Johannes and {Van Ginneken}, Bram and Karssemeijer, Nico and Litjens, Geert and {Van Der Laak}, Jeroen A W M and Hermsen, Meyke and Manson, Quirine F and Balkenhol, Maschenka and Geessink, Oscar and Stathonikos, Nikolaos and {Van Dijk}, Marcory C R F and Bult, Peter and Beca, Francisco and Beck, Andrew H and Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Zhong, Aoxiao and Dou, Qi and Li, Quanzheng and Chen, Hao and Lin, Huang Jing and Heng, Pheng Ann and Ha{\ss}, Christian and Bruni, Elia and Wong, Quincy and Halici, Ugur and {\"{O}}ner, Mustafa {\"{U}}mit and Cetin-Atalay, Rengul and Berseth, Matt and Khvatkov, Vitali and Vylegzhanin, Alexei and Kraus, Oren and Shaban, Muhammad and Rajpoot, Nasir and Awan, Ruqayya and Sirinukunwattana, Korsuk and Qaiser, Talha and Tsang, Yee Wah and Tellez, David and Annuscheit, Jonas and Hufnagl, Peter and Valkonen, Mira and Kartasalo, Kimmo and Latonen, Leena and Ruusuvuori, Pekka and Liimatainen, Kaisa and Albarqouni, Shadi and Mungal, Bharti and George, Ami and Demirci, Stefanie and Navab, Nassir and Watanabe, Seiryo and Seno, Shigeto and Takenaka, Yoichi and Matsuda, Hideo and Phoulady, Hady Ahmady and Kovalev, Vassili and Kalinovsky, Alexander and Liauchuk, Vitali and Bueno, Gloria and Fernandez-Carrobles, M Milagro and Serrano, Ismael and Deniz, Oscar and Racoceanu, Daniel and Ven{\^{a}}ncio, Rui},
doi = {10.1001/jama.2017.14585},
issn = {1538-3598},
journal = {Journal of the American Medical Association},
month = {dec},
number = {22},
pages = {2199--2210},
pmid = {29234806},
title = {{Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer}},
volume = {318},
year = {2017}
}
@article{Freitas:2004vv,
abstract = {This paper addresses the problem of how to evaluate the quality of a model built from the data in a multi-objective optimization scenario, where two or more quality criteria must be simultaneously optimized. A typical example is a scenario where one wants to maximize both the accuracy and the simplicity of a classification model or a candidate attribute subset in attribute selection. One reviews three very different approaches to cope with this problem, namely: (a) transforming the original multi-objective problem into a single-objective problem by using a weighted formula; (b) the lexicographical approach, where the objectives are ranked in order of priority; and (c) the Pareto approach, which consists of finding as many non-dominated solutions as possible and returning the set of non-dominated solutions to the user. One also presents a critical review of the case for and against each of these approaches. The general conclusions are that the weighted formula approach -- which is by far the most used in the data mining literature -- is to a large extent an ad-hoc approach for multi-objective optimization, whereas the lexicographic and the Pareto approach are more principled approaches, and therefore deserve more attention from the data mining community.},
author = {Freitas, Alex A},
doi = {10.1145/1046456.1046467},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {2},
pages = {77},
title = {{A critical review of multi-objective optimization in data mining}},
volume = {6},
year = {2004}
}
@article{Zahalka:2011ux,
abstract = {A widely persisting interpretation of Occam's razor is that given two classifiers with the same training error, the simpler classifier is more likely to generalize better. Within a long-lasting debate in the machine learning community over Occam's razor, Domingos (Data Min. Knowl. Discov. 3:409-425, 1999) rejects this interpretation and proposes that model complexity is only a confounding factor usually correlated with the number of models from which the learner selects. It is thus hypothesized that the risk of overfitting (poor generalization) follows only from the number of model tests rather than the complexity of the selected model. We test this hypothesis on 30 UCI data sets using polynomial classification models. The results confirm Domingos' hypothesis on the 0.05 significance level and thus refutes the above interpretation of Occam's razor. Our experiments however also illustrate that decoupling the two factors (model complexity and number of model tests) is problematic. {\textcopyright}The Author(s) 2010.},
author = {Zah{\'{a}}lka, Jan and {\v{Z}}elezn{\'{y}}, Filip},
doi = {10.1007/s10994-010-5227-2},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Empirical evaluation,Generalization,Model complexity},
number = {3},
pages = {475--481},
title = {{An experimental test of Occam's razor in classification}},
volume = {82},
year = {2011}
}
@article{Haenssle:2018bz,
abstract = {Background: Deep learning convolutional neural networks (CNN) May facilitate melanoma detection, but data comparing a CNN's diagnostic performance to larger groups of dermatologists are lacking. Methods: Google's Inception v4 CNN architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-II: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve (AUC) of receiver operating characteristics (ROC) for diagnostic classification (dichotomous) of lesions by the CNN versus an international group of 58 dermatologists during level-I or -II of the reader study. Secondary end points included the dermatologists' diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -II of the reader study. Additionally, the CNN's performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging (ISBI) challenge. Results: In level-I dermatologists achieved a mean (6standard deviation) sensitivity and specificity for lesion classification of 86.6{\%} (69.3{\%}) and 71.3{\%} (611.2{\%}), respectively. More clinical information (level-II) improved the sensitivity to 88.9{\%} (69.6{\%}, P ¼ 0.19) and specificity to 75.7{\%} (611.7{\%}, P {\textless}0.05). The CNN ROC curve revealed a higher specificity of 82.5{\%} when compared with dermatologists in level-I (71.3{\%}, P {\textless}0.01) and level-II (75.7{\%}, P {\textless}0.01) at their sensitivities of 86.6{\%} and 88.9{\%}, respectively. The CNN ROC AUC was greater than the mean ROC area of dermatologists (0.86 versus 0.79, P {\textless}0.01). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge. Conclusions: For the first time we compared a CNN's diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the CNN. Irrespective of any physicians' experience, they May benefit from assistance by a CNN's image classification.},
author = {Haenssle, H A and Fink, C and Schneiderbauer, R and Toberer, F and Buhl, T and Blum, A and Kalloo, A and {Ben Hadj Hassen}, A and Thomas, L and Enk, A and Uhlmann, L and Alt, Christina and Arenbergerova, Monika and Bakos, Renato and Baltzer, Anne and Bertlich, Ines and Blum, Andreas and Bokor-Billmann, Therezia and Bowling, Jonathan and Braghiroli, Naira and Braun, Ralph and Buder-Bakhaya, Kristina and Buhl, Timo and Cabo, Horacio and Cabrijan, Leo and Cevic, Naciye and Classen, Anna and Deltgen, David and Fink, Christine and Georgieva, Ivelina and Hakim-Meibodi, Lara Elena and Hanner, Susanne and Hartmann, Franziska and Hartmann, Julia and Haus, Georg and Hoxha, Elti and Karls, Raimonds and Koga, Hiroshi and Kreusch, Ju¨rgen and Lallas, Aimilios and Majenka, Pawel and Marghoob, Ash and Massone, Cesare and Mekokishvili, Lali and Mestel, Dominik and Meyer, Volker and Neuberger, Anna and Nielsen, Kari and Oliviero, Margaret and Pampena, Riccardo and Paoli, John and Pawlik, Erika and Rao, Barbar and Rendon, Adriana and Russo, Teresa and Sadek, Ahmed and Samhaber, Kinga and Schneiderbauer, Roland and Schweizer, Anissa and Toberer, Ferdinand and Trennheuser, Lukas and Vlahova, Lyobomira and Wald, Alexander and Winkler, Julia and Wo¨lbing, Priscila and Zalaudek, Iris},
doi = {10.1093/annonc/mdy166},
issn = {1569-8041},
journal = {Annals of Oncology},
keywords = {Automated melanoma detection,Computer algorithm,Deep learning convolutional neural network,Dermoscopy,Melanocytic nevi,Melanoma},
month = {may},
number = {8},
pages = {1836--1842},
title = {{Man against Machine: Diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists}},
volume = {29},
year = {2018}
}
@article{Augasta:2012wx,
abstract = {Artificial neural networks often achieve high classification accuracy rates, but they are considered as black boxes due to their lack of explanation capability. This paper proposes the new rule extraction algorithm RxREN to overcome this drawback. In pedagogical approach the proposed algorithm extracts the rules from trained neural networks for datasets with mixed mode attributes. The algorithm relies on reverse engineering technique to prune the insignificant input neurons and to discover the technological principles of each significant input neuron of neural network in classification. The novelty of this algorithm lies in the simplicity of the extracted rules and conditions in rule are involving both discrete and continuous mode of attributes. Experimentation using six different real datasets namely iris, wbc, hepatitis, pid, ionosphere and creditg show that the proposed algorithm is quite efficient in extracting smallest set of rules with high classification accuracy than those generated by other neural network rule extraction methods. {\textcopyright}Springer Science+Business Media, LLC. 2011.},
author = {{Gethsiyal Augasta}, M and Kathirvalavakumar, T},
doi = {10.1007/s11063-011-9207-8},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
number = {2},
pages = {131--150},
title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
volume = {35},
year = {2012}
}
@article{Wieringa:2006vd,
abstract = {This paper was triggered by concerns about the methodological soundness of many RE papers. We present a conceptual framework that distinguishes design papers from research papers, and show that in this framework, what is called a research paper in RE is often a design paper. We then present and motivate two lists of evaluation criteria, one for research papers and one for design papers. We apply both of these lists to two samples drawn from the set of all submissions to the RE'3 conference. Analysis of these two samples shows that most submissions of the RE'3 conference are design papers, not research papers, and that most design papers present a solution to a problem but neither validate this solution nor investigate the problems that can be solved by this solution. We conclude with a discussion of the soundness of our results and of the possible impact on RE research and practice. {\textcopyright}Springer-Verlag London Limited 2006.},
author = {Wieringa, Roel J and Heerkens, J M G},
doi = {10.1007/s00766-006-0037-6},
issn = {0947-3602},
journal = {Requirements Engineering},
number = {4},
pages = {295--307},
title = {{The methodological soundness of requirements engineering papers: A conceptual framework and two case studies}},
volume = {11},
year = {2006}
}
@book{Witten:2016ut,
abstract = {Data Mining: Practical Machine Learning Tools and Techniques, Fourth Edition, offers a thorough grounding in machine learning concepts, along with practical advice on applying these tools and techniques in real-world data mining situations. This highly anticipated fourth edition of the most acclaimed work on data mining and machine learning teaches readers everything they need to know to get going, from preparing inputs, interpreting outputs, evaluating results, to the algorithmic methods at the heart of successful data mining approaches. Extensive updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including substantial new chapters on probabilistic methods and on deep learning. Accompanying the book is a new version of the popular WEKA machine learning software from the University of Waikato. Authors Witten, Frank, Hall, and Pal include today's techniques coupled with the methods at the leading edge of contemporary research. Please visit the book companion website at http://www.cs.waikato.ac.nz/ml/weka/book.html It contains Powerpoint slides for Chapters 1-12. This is a very comprehensive teaching resource, with many PPT slides covering each chapter of the book Online Appendix on the Weka workbench; again a very comprehensive learning aid for the open source software that goes with the book Table of contents, highlighting the many new sections in the 4th edition, along with reviews of the 1st edition, errata, etc. Provides a thorough grounding in machine learning concepts, as well as practical advice on applying the tools and techniques to data mining projects Presents concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods Includes a downloadable WEKA software toolkit, a comprehensive collection of machine learning algorithms for data mining tasks-in an easy-to-use interactive interface Includes open-access online courses that introduce practical applications of the material in the book.},
author = {Witten, Ian H and Frank, Eibe and Hall, Mark A and Pal, Christopher J},
booktitle = {Data Mining: Practical Machine Learning Tools and Techniques},
doi = {10.1016/c2009-0-19715-5},
isbn = {978-0-12-804291-5},
pages = {1--621},
publisher = {Morgan Kaufmann},
title = {{Data Mining: Practical Machine Learning Tools and Techniques}},
year = {2016}
}
@article{shaver1987,
author = {Shaver, Phillip and Schwartz, Judith and Kirson, Donald and O'Connor, Cary},
doi = {10.1037/0022-3514.52.6.1061},
journal = {Journal of Personality and Social Psychology},
number = {6},
pages = {1061--1086},
title = {{Emotion knowledge: Further exploration of a prototype approach}},
type = {Journal Article},
volume = {52},
year = {1987}
}
@phdthesis{vasa2010growth,
abstract = {In this thesis we address the problem of identifying where, in successful software systems, maintenance effort tends to be devoted. By examin-ing a larger data set of open source systems we show that maintenance effort is, in general, spent on addition new classes and interestingly, efforts to base new code on stable classes will make those classes less stable as they need to be modified to meet the needs of the new clients. This thesis advances the state of the art in terms of our understanding of how evolving software systems grow and change. We propose an innovative method to better understand growth dynamics in evolving software systems. Rather than relying on the commonly used method of analysing aggregate system size growth over time, we analyze how the probability distribution of a range of software metrics change over time. Using this approach we find that the process of evolution typically drives the popular classes within a software system to gain additional users over time and the increase in popularity makes these classes change-prone.},
address = {Hawthorn, VIC, Australia},
author = {Vasa, Rajesh},
school = {Swinburne University of Technology},
title = {{Growth and Change Dynamics in Open Source Software Systems}},
year = {2010}
}
@article{Heckel:2005uk,
author = {Heckel, Reiko and Lohmann, Marc},
doi = {10.1016/j.entcs.2004.02.073},
issn = {1571-0661},
journal = {Electronic Notes in Theoretical Computer Science},
month = {jan},
pages = {145--156},
title = {{Towards Contract-based Testing of Web Services}},
volume = {116},
year = {2005}
}
@misc{Finalyson:2018aa,
address = {Fredericksburg, VA, USA},
author = {Finalyson, Ian},
publisher = {University of Mary Washington},
title = {{Nondeterministic Finite Automata}},
url = {http://bit.ly/319GOF9},
year = {2018}
}
@inproceedings{Wang:2013ub,
abstract = {Software frameworks provide sets of generic functionalities that can be later customized for a specific task. When developers invoke API methods in a framework, they often encounter obstacles in finding the correct usage of the API, let alone to employ best practices. Previous research addresses this line of questions by mining API usage patterns to induce API usage templates, by conducting and compiling interviews of developers, and by inferring correlations among APIs. In this paper, we analyze API-related posts regarding iOS and Android development from a Q{\&}A website, stackoverflow.com. Assuming that API-related posts are primarily about API usage obstacles, we find several iOS and Android API classes that appear to be particularly likely to challenge developers, even after we factor out API usage hotspots, inferred by modelling API usage of open source iOS and Android applications. For each API with usage obstacles, we further apply a topic mining tool to posts that are tagged with the API, and we discover several repetitive scenarios in which API usage obstacles occur. We consider our work as a stepping stone towards understanding API usage challenges based on forum-based input from a multitude of developers, input that is prohibitively expensive to collect through interviews. Our method helps to motivate future research in API usage, and can allow designers of platforms - such as iOS and Android - to better understand the problems developers have in using their platforms, and to make corresponding improvements. {\textcopyright}2013 IEEE.},
address = {San Francisco, CA, USA},
author = {Wang, Wei and Godfrey, Michael W},
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2013.6624006},
isbn = {978-1-46-732936-1},
issn = {2160-1852},
month = {may},
pages = {61--64},
publisher = {IEEE},
title = {{Detecting API usage obstacles: A study of iOS and android developer questions}},
year = {2013}
}
@article{Jiarpakdee2020,
author = {Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Dam, Hoa Khanh and Grundy, John},
doi = {10.1109/tse.2020.2982385},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {c},
pages = {1--1},
publisher = {IEEE},
title = {{An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models}},
volume = {5589},
year = {2020}
}
@inproceedings{Ohtake:2019vi,
abstract = {Intelligent APIs, such as Google Cloud Vision or Amazon Rekognition, are becoming evermore pervasive and easily accessible to developers to build applications. Because of the stochastic nature that machine learning entails and disparate datasets used in their training, the output from different APIs varies over time, with low reliability in some cases when compared against each other. Merging multiple unreliable API responses from multiple vendors may increase the reliability of the overall response, and thus the reliability of the intelligent end-product. We introduce a novel methodology – inspired by the proportional representation used in electoral systems – to merge outputs of different intelligent computer vision APIs provided by multiple vendors. Experiments show that our method outperforms both naive merge methods and traditional proportional representation methods by 0.015 F-measure.},
address = {Daejeon, Republic of Korea},
author = {Ohtake, Tomohiro and Cummaudo, Alex and Abdelrazek, Mohamed and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 19th International Conference on Web Engineering},
doi = {10.1007/978-3-030-19274-7\_28},
isbn = {978-3-03-019273-0},
issn = {1611-3349},
keywords = {Application programming interfaces,Artificial intelligence,Data integration,Supervised learning,Web services},
month = {jun},
pages = {391--406},
publisher = {Springer},
title = {{Merging intelligent API responses using a proportional representation approach}},
year = {2019}
}
@inproceedings{Lei:2016wi,
abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications - rationales - that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
address = {Austin, TX, USA},
archivePrefix = {arXiv},
arxivId = {1606.04155},
author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
booktitle = {Proceedings of the 9th International Joint Conference on Natural Language Processing and Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/d16-1011},
eprint = {1606.04155},
isbn = {978-1-94-562625-8},
month = {nov},
pages = {107--117},
publisher = {Association for Computational Linguistics},
title = {{Rationalizing neural predictions}},
year = {2016}
}
@techreport{Kazman2000,
address = {Pittsburgh, PA, USA},
author = {Kazman, Rick and Klein, Mark and Clements, Paul},
institution = {Software Engineering Institute},
publisher = {Carnegie-Mellon University},
title = {{ATAM: Method for architecture evaluation}},
year = {2000}
}
@article{Arnold:2005vc,
author = {Arnold, Ken},
doi = {10.1145/1071713.1071731},
issn = {1542-7749},
journal = {ACM Queue},
number = {5},
pages = {54--59},
title = {{Programmers are People, Too}},
volume = {3},
year = {2005}
}
@book{Sheskin:2003tx,
abstract = {Called the "bible of applied statistics," the first two editions of the Handbook of Parametric and Nonparametric Statistical Procedures were unsurpassed in accessibility, practicality, and scope. Now author David Sheskin has gone several steps further and added even more tests, more examples, and more background information-more than 200 pages of new material.The Third Edition provides unparalleled, up-to-date coverage of over 130 parametric and nonparametric statistical procedures as well as many practical and theoretical issues relevant to statistical analysis. If you need toDecide what method of analysis to useUse a particular test for the first timeDistinguish acceptable from unacceptable researchInterpret and better understand the results of pubished studiesthe Handbook of Parametric and Nonparametric Statistical Procedures will help you get the job done.},
address = {New York, NY, USA},
author = {Sheskin, David J},
doi = {10.4324/9780203489536},
publisher = {Chapman and Hall/CRC},
title = {{Handbook of Parametric and Nonparametric Statistical Procedures}},
year = {2004}
}
@inproceedings{Bottomley:2005fs,
abstract = {There are few resources geared to technical writers working on documentation for software developers. This paper presents the results of online surveys and telephone interviews that cover the experience, technical knowledge, and practices of technical writers in this area, with a large percentage of respondents who are Microsoft employees. Respondents value strong writing skills and the ability to learn quickly and continuously, with the amount and type of knowledge needed being specific to the subject area and audience for their work. {\textcopyright} 2005 IEEE.},
address = {Limerick, Ireland},
author = {Bottomley, Christina},
booktitle = {Proceedings of the 2005 IEEE International Professional Communication Conference},
doi = {10.1109/IPCC.2005.1494255},
keywords = {API Documentation,Developer documentation,Documentation,Programmer Writer,SDK Documentation,Technical Writer},
month = {jul},
pages = {802--812},
publisher = {IEEE},
title = {{What part writer? What part programmer? A survey of practices and knowledge used in programmer writing}},
year = {2005}
}
@article{Ondrej:2016,
author = {Bruna, O and Avetisyan, H and Holub, J},
doi = {10.1088/1742-6596/772/1/012063},
journal = {Journal of Physics: Conference Series},
month = {nov},
pages = {12063},
publisher = {IOP Publishing},
title = {{Emotion models for textual emotion classification}},
volume = {772},
year = {2016}
}
@inproceedings{Bigham2008,
abstract = {People often use computers other than their own to access web content, but blind users are restricted to using only computers equipped with expensive, special-purpose screen reading programs that they use to access the web. Web-Anywhere is a web-based, self-voicing web browser that enables blind web users to access the web from almost any computer that can produce sound without installing new software. The system could serve as a convenient, low-cost solution for blind users on-the-go. for blind users unable to afford a full screen reader and for web developers targeting accessible design. This paper overviews existing solutions for mobile web access for blind users and presents the design of the WebAnywhere system. WebAnywhere generates speech remotely and uses prefetching strategies designed to reduce perceived latency. A user evaluation of the system is presented showing that blind users can use Web-Anywhere to complete tasks representative of what users might want to complete on computers that are not their own. A survey of public computer terminals shows that WebAnywhere can run on most. Copyright 2008 ACM.},
address = {Beijing, China},
author = {Bigham, Jeffrey P. and Prince, Craig M. and Ladner, Richard E.},
booktitle = {Proceedings of the 2008 International Cross-Disciplinary Conference on Web Accessibility},
doi = {10.1145/1368044.1368060},
keywords = {Blind users,Screen reader,Web accessibility,Webanywhere},
month = {apr},
pages = {73--82},
publisher = {ACM},
title = {{WebAnywhere}},
year = {2008}
}
@inproceedings{Graetsch:2021caise,
address = {Melbourne, VIC, Australia},
annote = {In Review},
author = {Graetsch, Ulrike Maria and Cummaudo, Alex and Curumsing, Maheswaree Kissoon and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 33rd International Conference on Advanced Information Systems Engineering},
keywords = {InReview},
publisher = {Springer},
title = {{Using Pre-Trained Emotion Classification Models against Stack Overflow Questions}},
year = {2021}
}
@book{Japkowicz:2011vy,
abstract = {The field of machine learning has matured to the point where many sophisticated learning approaches can be applied to practical applications. Thus it is of critical importance that researchers have the proper tools to evaluate learning approaches and understand the underlying issues. This book examines various aspects of the evaluation process with an emphasis on classification algorithms. The authors describe several techniques for classifier performance assessment, error estimation and resampling, obtaining statistical significance as well as selecting appropriate domains for evaluation. They also present a unified evaluation framework and highlight how different components of evaluation are both significantly interrelated and interdependent. The techniques presented in the book are illustrated using R and WEKA facilitating better practical insight as well as implementation. Aimed at researchers in the theory and applications of machine learning, this book offers a solid basis for conducting performance evaluations of algorithms in practical settings.},
author = {Japkowicz, Nathalie and Shah, Mohak},
booktitle = {Evaluating Learning Algorithms: A Classification Perspective},
doi = {10.1017/CBO9780511921803},
isbn = {978-0-51-192180-3},
pages = {1--406},
publisher = {Cambridge University Press},
title = {{Evaluating learning algorithms: A classification perspective}},
volume = {9780521196},
year = {2011}
}
@inproceedings{hardt2016equality,
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
address = {Barcelona, Spain},
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
doi = {978-1-51-083881-9},
issn = {1049-5258},
month = {dec},
pages = {3323--3331},
publisher = {Curran Associates Inc.},
title = {{Equality of opportunity in supervised learning}},
year = {2016}
}
@misc{Mandel:2008ww,
annote = {Accessed: 28 August 2018},
author = {Mandel, Lawrence},
month = {may},
title = {{Describe REST Web services with WSDL 2.0}},
url = {https://ibm.co/313RoNV},
year = {2008}
}
@inproceedings{Cummaudo:2020fse,
address = {Virtual Event, USA},
author = {Cummaudo, Alex and Barnett, Scott and Vasa, Rajesh and Grundy, John and Abdelrazek, Mohamed},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3409688},
month = {nov},
pages = {269--280},
publisher = {ACM},
title = {{Beware the evolving `intelligent' web service! An integration architecture tactic to guard AI-first components}},
year = {2020}
}
@article{SiMergeSearchEngineResults,
abstract = {The proliferation of searchable text databases on local area networks and the Internet causes the problem of finding information that may be distributed among many disjoint text databases (distributed information retrieval). How to merge the results returned by selected databases is an important subproblem of the distributed information retrieval task. Previous research assumed that either resource providers cooperate to provide normalizing statistics or search clients down-load all retrieved documents and compute normalized scores without cooperation from resource providers. This article presents a semisupervised learning solution to the result merging problem. The key contribution is the observation that information used to create resource descriptions for resource selection can also be used to create a centralized sample database to guide the normalization of document scores returned by different databases. At retrieval time, the query is sent to the selected databases, which return database-specific document scores, and to a centralized sample database, which returns database-independent document scores. Documents that have both a database-specific score and a database-independent score serve as training data for learning to normalize the scores of other documents. An extensive set of experiments demonstrates that this method is more effective than the well-known CORI result-merging algorithm under a variety of conditions.},
address = {New York, NY, USA},
author = {Si, Luo and Callan, Jamie},
doi = {10.1145/944012.944017},
issn = {1046-8188},
journal = {ACM Transactions on Information Systems},
keywords = {Distributed information retrieval,Resource ranking,Resource selection,Results merging,Semisupervised learning method,Server selection},
month = {oct},
number = {4},
pages = {457--491},
publisher = {ACM},
title = {{A semisupervised learning method to merge search engine results}},
volume = {21},
year = {2003}
}
@article{Miles:1994ty,
abstract = {The latest edition of this best-selling textbook by Miles and Huberman not only is considerably expanded in content, but is now available in paperback. Bringing the art of qualitative analysis up-to-date, this edition adds hundreds of new techniques, ideas and references developed in the past decade. The increase in the use of computers in qualitative analysis is also reflected in this volume. There is an extensive appendix on criteria to choose from among the currently available analysis packages. Through examples from a host of social science and professional disciplines, Qualitative Data Analysis remains the most comprehensive and complete treatment of this topic currently available to scholars and applied researchers.},
author = {Schwandt, Thomas A},
doi = {10.1016/0149-7189(96)88232-2},
issn = {0149-7189},
journal = {Evaluation and Program Planning},
number = {1},
pages = {106--107},
title = {{Qualitative data analysis: An expanded sourcebook}},
volume = {19},
year = {1996}
}
@book{Juran:1988tg,
address = {New York, NY, USA},
author = {Juran, Joseph M.},
isbn = {978-0-02-916681-9},
publisher = {The Free Press},
title = {{Juran on Planning for Quality}},
year = {1988}
}
@article{Wickham:2010hy,
abstract = {A grammar of graphics is a tool that enables us to concisely describe the components of a graphic. Such a grammar allows us to move beyond named graphics (e.g., the "scatterplot") and gain insight into the deep structure that underlies statistical graphics. This article builds on Wilkinson, Anand, and Grossman (2005), describing extensions and refinements developed while building an open source implementation of the grammar of graphics for R, ggplot2. The topics in this article include an introduction to the grammar by working through the process of creating a plot, and discussing the components that we need. The grammar is then presented formally and compared toWilkinson's grammar, highlighting the hierarchy of defaults, and the implications of embedding a graphical grammar into a programming language. The power of the grammar is illustrated with a selection of examples that explore different components and their interactions, in more detail. The article concludes by discussing some perceptual issues, and thinking about how we can build on the grammar to learn how to create graphical "poems." Supplemental materials are available online. Copyright {\textcopyright}2010 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
author = {Wickham, Hadley},
doi = {10.1198/jcgs.2009.07098},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Grammar of graphics,Statistical graphics},
month = {jan},
number = {1},
pages = {3--28},
title = {{A Layered grammar of graphics}},
volume = {19},
year = {2010}
}
@article{Bangor2008,
abstract = {This article presents nearly 10 year's worth of System Usability Scale (SUS) data collected on numerous products in all phases of the development lifecycle. The SUS, developed by Brooke (1996), reflected a strong need in the usability community for a tool that could quickly and easily collect a user's subjective rating of a product's usability. The data in this study indicate that the SUS fulfills that need. Results from the analysis of this large number of SUS scores show that the SUS is a highly robust and versatile tool for usability professionals. The article presents these results and discusses their implications, describes nontraditional uses of the SUS, explains a proposed modification to the SUS to provide an adjective rating that correlates with a given score, and provides details of what constitutes an acceptable SUS score. Copyright {\textcopyright} Taylor {\&} Francis Group, LLC.},
author = {Bangor, Aaron and Kortum, Philip T. and Miller, James T.},
doi = {10.1080/10447310802205776},
issn = {10447318},
journal = {International Journal of Human-Computer Interaction},
title = {{An empirical evaluation of the system usability scale}},
year = {2008}
}
@article{Baruch:1999vf,
abstract = {A study was conducted to explore what could and should be a reasonable response rate in academic studies. One hundred and forty-one papers which included 175 different studies were examined. They were published in the Academy of Management Journal, Human Relations, Journal of Applied Psychology, Organizational Behavior and Human Decision Processes, and Journal of International Business Studies in the years 1975, 1985, and 1995, covering about 200,000 respondents. The average response rate was 55.6 with a standard deviation of 19.7. Variations among the journals such as the year of publication and other variables were discussed. Most notable is the decline through the years (average 48.4, standard deviation of 20.1, in 1995), the lower level found in studies involving top management or organizational representatives (average 36.1, standard deviation of 13.3), and the predominance of North American studies. It is suggested that the average and standard deviation found in this stud)' should be used as a norm for future studies, bearing in mind the specific reference group. It is also recommended that a distinction is made between surveys directed at individual participants and those targeting organizational representatives.},
author = {Baruch, Yehuda},
doi = {10.1177/001872679905200401},
issn = {0018-7267},
journal = {Human Relations},
keywords = {Empirical studies,Questionnaires,Research methods,Response rate,Return rate},
number = {4},
pages = {421--438},
title = {{Response rate in academic studies - A comparative analysis}},
volume = {52},
year = {1999}
}
@book{Calhoun:1995ww,
author = {Alway, Joan and Calhoun, Craig},
booktitle = {Contemporary Sociology},
doi = {10.2307/2076647},
issn = {0094-3061},
number = {1},
pages = {119--120},
publisher = {American Sociological Association},
title = {{Critical Social Theory: Culture, History, and the Challenge of Difference.}},
volume = {26},
year = {1997}
}
@article{Lima:2009tm,
abstract = {Companies' interest in customer relationship modelling and key issues such as customer lifetime value and churn has substantially increased over the years. However, the complexity of building, interpreting and applying these models creates obstacles for their implementation. The main contribution of this paper is to show how domain knowledge can be incorporated in the data mining process for churn prediction, viz. through the evaluation of coefficient signs in a logistic regression model, and secondly, by analysing a decision table (DT) extracted from a decision tree or rule-based classifier. An algorithm to check DTs for violations of monotonicity constraints is presented, which involves the repeated application of condition reordering and table contraction to detect counter-intuitive patterns. Both approaches are applied to two telecom data sets to empirically demonstrate how domain knowledge can be used to ensure the interpretability of the resulting models.Journal of the Operational Research Society (2009) 60, 1096-1106. doi:10.1057/jors.2008.161; published online 18 February 2009 {\textcopyright}2009 Operational Research Society Ltd.},
author = {Lima, E and Mues, C and Baesens, B},
doi = {10.1057/jors.2008.161},
issn = {0160-5682},
journal = {Journal of the Operational Research Society},
keywords = {Churn,Data mining,Decision tables,Domain knowledge},
number = {8},
pages = {1096--1106},
title = {{Domain knowledge integration in data mining using decision tables: Case studies in churn prediction}},
volume = {60},
year = {2009}
}
@inproceedings{Hosseini:2018jr,
abstract = {Google has recently introduced the Cloud Vision API for image analysis. According to the demonstration website, the API 'quickly classifies images into thousands of categories, detects individual objects and faces within images, and finds and reads printed words contained within images.' It can be also used to 'detect different types of inappropriate content from adult to violent content.' In this paper, we evaluate the robustness of Google Cloud Vision API to input perturbation. In particular, we show that by adding sufficient noise to the image, the API generates completely different outputs for the noisy image, while a human observer would perceive its original content. We show that the attack is consistently successful, by performing extensive experiments on different image types, including natural images, images containing faces and images with texts. For instance, using images from ImageNet dataset, we found that adding an average of 14.25{\%} impulse noise is enough to deceive the API. Our findings indicate the vulnerability of the API in adversarial environments. For example, an adversary can bypass an image filtering system by adding noise to inappropriate images. We then show that when a noise filter is applied on input images, the API generates mostly the same outputs for restored images as for original images. This observation suggests that cloud vision API can readily benefit from noise filtering, without the need for updating image analysis algorithms.},
address = {Cancun, Mexico},
archivePrefix = {arXiv},
arxivId = {1704.05051},
author = {Hosseini, Hossein and Xiao, Baicen and Poovendran, Radha},
booktitle = {Proceedings of the 16th IEEE International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2017.0-172},
eprint = {1704.05051},
isbn = {978-1-53-861417-4},
keywords = {Adversarial machine learning,Google Cloud Vision API,Image Noise,Machine learning},
month = {dec},
pages = {101--105},
publisher = {IEEE},
title = {{Google's cloud vision API is not robust to noise}},
year = {2017}
}
@article{Parasuraman:1988wh,
abstract = {This paper describes the development of a 22-item instrument (called SERVQUAL) for assessing customer perceptions of service quality in service and retailing organizations. After a discussion of the conceptualization and operationalization of the service quality construct, the procedures used in constructing and refining a multiple-item scale to measure the construct are described. Evidence of the scale's reliability, factor structure, and validity on the basis of analyzing data from four independent samples is presented next. The paper concludes with a discussion of potential applications of the scale.},
author = {Berry, Leonard L and Parasuraman, A and Zeithaml, Valarie A},
doi = {10.1016/S0148-2963(99)00084-3},
isbn = {00224359},
issn = {0022-4359},
journal = {Journal of Retailing},
number = {1},
pages = {12--40},
pmid = {6353339},
title = {{SERVQUAL: A multiple-item scale for measuring consumer perceptions of service quality}},
volume = {64},
year = {1988}
}
@phdthesis{Barnett:2018Kx,
address = {Hawthorn, VIC, Australia},
author = {Barnett, Scott},
school = {Swinburne University of Technology},
title = {{Extracting technical domain knowledge to improve software architecture}},
year = {2018}
}
@inproceedings{8100173,
address = {Honolulu, HI, USA},
author = {Redmon, J and Farhadi, A},
booktitle = {Proceedings of the 2017 Conference on Computer Vision and Pattern Recognition},
month = {jul},
pages = {6517--6525},
publisher = {IEEE},
title = {{YOLO9000: Better, Faster, Stronger}},
year = {2017}
}
@article{Bangor2008a,
abstract = {This article presents nearly 10 year's worth of System Usability Scale (SUS) data collected on numerous products in all phases of the development lifecycle. The SUS, developed by Brooke (1996), reflected a strong need in the usability community for a tool that could quickly and easily collect a user's subjective rating of a product's usability. The data in this study indicate that the SUS fulfills that need. Results from the analysis of this large number of SUS scores show that the SUS is a highly robust and versatile tool for usability professionals. The article presents these results and discusses their implications, describes nontraditional uses of the SUS, explains a proposed modification to the SUS to provide an adjective rating that correlates with a given score, and provides details of what constitutes an acceptable SUS score. Copyright {\textcopyright} Taylor {\&} Francis Group, LLC.},
author = {Bangor, Aaron and Kortum, Philip T. and Miller, James T.},
doi = {10.1080/10447310802205776},
issn = {10447318},
journal = {International Journal of Human-Computer Interaction},
title = {{An empirical evaluation of the system usability scale}},
year = {2008}
}
@article{Szafron:2004uf,
abstract = {Proteome Analyst (PA) (http://www.cs.ualberta.ca/{\~{}}bioinfo/PA/) is a publicly available, high-throughput, web-based system for predicting various properties of each protein in an entire proteome. Using machine-learned classifiers, PA can predict, for example, the GeneQuiz general function and Gene Ontology (GO) molecular function of a protein. In addition, PA is currently the most accurate and most comprehensive system for predicting subcellular localization, the location within a cell where a protein performs its main function. Two other capabilities of PA are notable. First, PA can create a custom classifier to predict a new property, without requiring any programming, based on labeled training data (i.e. a set of examples, each with the correct classification label) provided by a user. PA has been used to create custom classifiers for potassium-ion channel proteins and other general function ontologies. Second, PA provides a sophisticated explanation feature that shows why one prediction is chosen over another. The PA system produces a Na{\"{i}}ve Bayes classifier, which is amenable to a graphical and interactive approach to explanations for its predictions; transparent predictions increase the user's confidence in, and understanding of, PA. {\textcopyright}Oxford University Press 2004; all rights reserved.},
author = {Szafron, Duane and Lu, Paul and Greiner, Russell and Wishart, David S and Poulin, Brett and Eisner, Roman and Lu, Zhiyong and Anvik, John and Macdonell, Cam and Fyshe, Alona and Meeuwis, David},
doi = {10.1093/nar/gkh485},
issn = {0305-1048},
journal = {Nucleic Acids Research},
title = {{Proteome Analyst: Custom predictions with explanations in a web-based tool for high-throughput proteome annotations}},
volume = {32},
year = {2004}
}
@book{Wohlin:2012bu,
abstract = {Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors' book, which was published in 2000. In addition, substantial new material, e.g. concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with practical aspects. Researchers will also benefit from the book, learning more about how to conduct empirical studies, and likewise practitioners may use it as a "cookbook" when evaluating new methods or techniques before implementing them in their organization.},
address = {Berlin, Heidelberg},
author = {Wohlin, Claes and Runeson, Per and H{\"{o}}st, Martin and Ohlsson, Magnus C and Regnell, Bj{\"{o}}rn and Wessl{\'{e}}n, Anders},
doi = {10.1007/978-3-642-29044-2},
isbn = {978-3-64-229044-2},
issn = {0098-5589},
publisher = {Springer},
title = {{Experimentation in Software Engineering}},
year = {2012}
}
@article{rgensen:2016gl,
abstract = {Context The trustworthiness of research results is a growing concern in many empirical disciplines. Aim The goals of this paper are to assess how much the trustworthiness of results reported in software engineering experiments is affected by researcher and publication bias, given typical statistical power and significance levels, and to suggest improved research practices. Method First, we conducted a small-scale survey to document the presence of researcher and publication biases in software engineering experiments. Then, we built a model that estimates the proportion of correct results for different levels of researcher and publication bias. A review of 150 randomly selected software engineering experiments published in the period 2002-2013 was conducted to provide input to the model. Results The survey indicates that researcher and publication bias is quite common. This finding is supported by the observation that the actual proportion of statistically significant results reported in the reviewed papers was about twice as high as the one expected assuming no researcher and publication bias. Our models suggest a high proportion of incorrect results even with quite conservative assumptions. Conclusion Research practices must improve to increase the trustworthiness of software engineering experiments. A key to this improvement is to avoid conducting studies with unsatisfactory low statistical power.},
author = {J{\o}rgensen, Magne and Dyb{\aa}, Tore and Liest{\o}l, Knut and Sj{\o}berg, Dag I K},
doi = {10.1016/j.jss.2015.03.065},
issn = {0164-1212},
journal = {Journal of Systems and Software},
keywords = {Controlled experiments,Empirical software engineering,Statistical hypothesis testing},
pages = {133--145},
title = {{Incorrect results in software engineering experiments: How to improve research practices}},
volume = {116},
year = {2016}
}
@inproceedings{Kurakin:2016vw,
abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {1607.02533},
author = {Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
booktitle = {Proceedings of the 5th International Conference on Learning Representations},
eprint = {1607.02533},
month = {apr},
title = {{Adversarial examples in the physical world}},
year = {2017}
}
@misc{Classifi7:online,
annote = {Accessed: 5 February 2020},
author = {{Google LLC}},
title = {{Classification: Thresholding | Machine Learning Crash Course}},
url = {http://bit.ly/36oMgWb},
year = {2019}
}
@article{Boehm:2005vj,
author = {Boehm, Barry and Basili, Victor R},
chapter = {12},
doi = {10.1109/9780470049167.ch12},
isbn = {978-0-47-004916-7},
journal = {Software Management},
pages = {419--421},
title = {{Software defect reduction top 10 list}},
year = {2007}
}
@article{daMotaSilveira:2017vp,
abstract = {This paper brings to light an important discussion on how to create better assistive technologies for visually impaired people with the new advances in the computer vision field. Until very recently, assistive technology solutions required large and expensive hardware, as well as complex software. However, this scenario seems to be changing with web services on cloud that turns available computer vision features, offering image and video content analysis. The challenge is on how the results of these analyses will be processed and which will be the action or interaction displayed afterwards, showing the importance of appropriate user interface to visually impaired.},
author = {{da Mota Silveira}, Henrique and Martini, Luiz C{\'{e}}sar},
doi = {10.20897/jisem.201709},
issn = {2468-4376},
journal = {Journal of Information Systems Engineering {\&} Management},
number = {2},
pages = {1--3},
title = {{How the New Approaches on Cloud Computer Vision can Contribute to Growth of Assistive Technologies to Visually Impaired in the Following Years?}},
volume = {2},
year = {2017}
}
@article{Fung:2005we,
author = {Fung, Glenn and Sandilya, Sathyakama and Rao, R Bharat},
doi = {10.1007/978-3-540-75390-2_4},
journal = {Studies in Computational Intelligence},
number = {1},
pages = {83--107},
publisher = {Springer},
title = {{Rule extraction from linear support vector machines}},
volume = {80},
year = {2009}
}
@inproceedings{Storey2014,
address = {Hyderabad, India},
author = {Storey, Margaret Anne and Singer, Leif and Cleary, Brendan and Filho, Fernando Figueira and Zagalsky, Alexey},
booktitle = {Future of Software Engineering Proceedings},
doi = {10.1145/2593882.2593887},
keywords = {Collaboration,Social Media,Software Engineering},
month = {may},
pages = {100--116},
publisher = {ACM},
title = {{The (R)evolution of social media in software engineering}},
year = {2014}
}
@inproceedings{Domingos:1998ug,
abstract = {Occam's razor has been the subject of much controversy. This paper argues that this is partly because it has been interpreted in two quite different ways, the first of which (simplicity is a goal in itself) is essentially correct, while the second (simplicity leads to greater accuracy) is not. The paper reviews the large variety of theoretical arguments and empirical evidence for and against the "second razor", and concludes that the balance is strongly against it. In particular, it builds on the case of (Schaffer, 1993) and (Webb, 1996) by considering additional theoretical arguments and recent empirical evidence that the second razor fails in most domains. A version of the first razor more appropriate to KDD is proposed, and we argue that continuing to apply the second razor risks causing significant opportunities to be missed.},
address = {New York, NY, USA},
author = {Domingos, P},
booktitle = {Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining},
doi = {10.1.1.40.3278},
month = {aug},
pages = {37--43},
publisher = {AAAI},
title = {{Occam's Two Razors: The Sharp and the Blunt}},
year = {1998}
}
@article{Taulavuori:2004el,
abstract = {Product lines embody a strategic reuse of both intellectual effort and existing artefacts, such as software architectures and components. Third-party components are increasingly being used in product line based software engineering, in which case the integration is controlled by the product line architecture. However, the software integrators have difficulties in finding out the capabilities of components, because components are not documented in a standard way. Documentation is often the only way of assessing the applicability, credibility and quality of a third-party component. Our contribution is a standard documentation pattern for software components. The pattern provides guidelines and structure for component documentation and ensures the quality of documentation. The pattern has been validated by applying and analysing it in practice. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Taulavuori, Anne and Niemel{\"{a}}, Eila and Kallio, P{\"{a}}ivi},
doi = {10.1016/j.infsof.2003.10.004},
issn = {0950-5849},
journal = {Information and Software Technology},
keywords = {Component documentation,Software product line,Third-party component},
month = {jun},
number = {8},
pages = {535--546},
title = {{Component documentation - A key issue in software product lines}},
volume = {46},
year = {2004}
}
@incollection{Kitchenham:2007ux,
author = {Kitchenham, Barbara A and Pfleeger, Shari L},
booktitle = {Guide to Advanced Empirical Software Engineering},
chapter = {3},
doi = {10.1007/978-1-84800-044-5},
editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K.},
isbn = {978-1-84-800043-8},
month = {nov},
pages = {63--92},
publisher = {Springer},
title = {{Personal opinion surveys}},
year = {2007}
}
@misc{Bessin:2004vc,
author = {Bessin, J},
month = {jun},
publisher = {IBM developerWorks},
title = {{The Business Value of Quality}},
url = {https://ibm.co/2u0UDK0},
volume = {15},
year = {2004}
}
@techreport{murphy2008improving,
abstract = {—As machine learning (ML) applications become prevalent in various aspects of everyday life, their dependability takes on increasing importance. It is challenging to test such applications, however, because they are intended to learn properties of data sets where the correct answers are not already known. Our work is not concerned with testing how well an ML algorithm learns, but rather seeks to ensure that an application using the algorithm implements the specification correctly and fulfills the users' expectations. These are critical to ensuring the application's dependability. This paper presents three approaches to testing these types of applications. In the first, we create a set of limited test cases for which it is, in fact, possible to predict what the correct output should be. In the second approach, we use random testing to generate large data sets according to parameterization based on the application's equivalence classes. Our third approach is based on metamorphic testing, in which properties of the application are exploited to define transformation functions on the input, such that the new output can easily be predicted based on the original output. Here we discuss these approaches, and our findings from testing the dependability of three real-world ML applications.},
address = {New York, NY, USA},
author = {Murphy, Christian and Kaiser, Gail},
doi = {10.7916/D8RF62VN},
institution = {Department of Computer Science, Columbia University},
keywords = {Index Terms—Machine Learning,Metamorphic Testing,Non-Testable Programs,Oracle Problem,Quality Assurance,Random Testing,Software Dependability,Software Testing},
number = {Ml},
pages = {1--21},
title = {{Improving the Dependability of Machine Learning Applications}},
year = {2008}
}
@inproceedings{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
address = {Long Beach, CA, USA},
author = {Paszke, Adam and Chanan, Gregory and Lin, Zeming and Gross, Sam and Yang, Edward and Antiga, Luca and Devito, Zachary},
booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems},
doi = {10.1017/CBO9781107707221.009},
month = {dec},
publisher = {Curran Associates Inc.},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@article{Baehrens:2010tj,
abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method. {\textcopyright}2010 David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen and Klaus-Robert M{\"{u}}ller.},
author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"{u}}ller, Klaus Robert},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Ames mutagenicity,Black box model,Explaining,Kernel methods,Nonlinear},
pages = {1803--1831},
title = {{How to explain individual classification decisions}},
volume = {11},
year = {2010}
}
@inproceedings{Sculley2015,
abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a svariety of system-level anti-patterns.},
address = {Montreal, QC, Canada},
author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean Fran{\c{c}}ois and Dennison, Dan},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems},
doi = {10.5555/2969442.2969519},
issn = {1049-5258},
month = {dec},
pages = {2503--2511},
publisher = {Curran Associates Inc.},
title = {{Hidden technical debt in machine learning systems}},
year = {2015}
}
@article{IBMTripleModularRedendancy,
abstract = {One of the proposed techniques for meeting the severe reliability requirements inherent in certain future computer applications is described. This technique involves the use of triple-modular redundancy, which is essentially the use of the two-out-of-three voting concept at a low level. Effects of imperfect voting circuitry and of various interconnections of logical elements are assessed. A hypothetical triple-modular redundant computer is subjected to a Monte Carlo program on the IBM 704, which simulates component failures. Reliability is thereby determined and compared with reliability obtained by analytical calculations based on simplifying assumptions.},
author = {Lyons, R E and Vanderkulk, W},
doi = {10.1147/rd.62.0200},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
month = {apr},
number = {2},
pages = {200--209},
title = {{The Use of Triple-Modular Redundancy to Improve Computer Reliability}},
volume = {6},
year = {2010}
}
@article{Singh:2016wu,
abstract = {Recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility. However, it is not clear what kind of explanations, such as linear models, decision trees, and rule lists, are the appropriate family to consider, and different tasks and models may benefit from different kinds of explanations. Instead of picking a single family of representations, in this work we propose to use "programs" as model-agnostic explanations. We show that small programs can be expressive yet intuitive as explanations, and generalize over a number of existing interpretable families. We propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations. Finally, we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers.},
archivePrefix = {arXiv},
arxivId = {1611.07579},
author = {Singh, Sameer and Ribeiro, Marco Tulio and Guestrin, Carlos},
eprint = {1611.07579},
month = {nov},
title = {{Programs as Black-Box Explanations}},
year = {2016}
}
@article{Kononenko:1993td,
abstract = {Although successful in medical diagnostic problems, inductive learning systems were not widely accepted in medical practice. In this paper two different approaches to machine learning in medical applications are compared: the system for inductive learning of decision trees Assistant, and the naive Bayesian classifier. Both methodologies were tested in four medical diagnostic problems: localization of primary tumor, prognostics of recurrence of breast cancer, diagnosis of thyroid diseases, and rheumatology. The accuracy of automatically acquired diagnostic knowledge from stored data records is compared, and the interpretation of the knowledge and the explanation ability of the classification process of each system is discussed. Surprisingly, the naive Bayesian classifier is superior to Assistant in classification accuracy and explanation ability, while the interpretation of the acquired knowledge seems to be equally valuable. In addition, two extensions to naive Bayesian classifier are briefly described: dealing with continuous attributes, and discovering the dependencies among attributes. {\textcopyright}1993 Taylor {\&} Francis Group, LLC.},
author = {Kononenko, Igor},
doi = {10.1080/08839519308949993},
issn = {1087-6545},
journal = {Applied Artificial Intelligence},
number = {4},
pages = {317--337},
title = {{Inductive and bayesian learning in medical diagnosis}},
volume = {7},
year = {1993}
}
@inproceedings{Allamanis:2013is,
abstract = {Questions from Stack Overflow provide a unique opportunity to gain insight into what programming concepts are the most confusing. We present a topic modeling analysis that combines question concepts, types, and code. Using topic modeling, we are able to associate programming concepts and identifiers (like the String class) with particular types of questions, such as, "how to perform encoding". {\textcopyright}2013 IEEE.},
address = {San Francisco, CA, USA},
author = {Allamanis, Miltiadis and Sutton, Charles},
booktitle = {Proceedings of the 10th IEEE International Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2013.6624004},
isbn = {978-1-46-732936-1},
issn = {2160-1852},
month = {may},
pages = {53--56},
publisher = {IEEE},
title = {{Why, when, and what: Analyzing stack overflow questions by topic, type, and code}},
year = {2013}
}
@inproceedings{Piccioni:2013em,
abstract = {Modern software development extensively involves reusing library components accessed through their Application Programming Interfaces (APIs). Usability is therefore a fundamental goal of API design, but rigorous empirical studies of API usability are still relatively uncommon. In this paper, we present the design of an API usability study which combines interview questions based on the cognitive dimensions framework, with systematic observations of programmer behavior while solving programming tasks based on ''tokens''. We also discuss the implementation of the study to assess the usability of a persistence library API (offering functionalities such as storing objects into relational databases). The study involved 25 programmers (including students, researchers, and professionals), and provided additional evidence to some critical features evidenced by related studies, such as the difficulty of finding good names for API features and of discovering relations between API types. It also discovered new issues relevant to API design, such as the impact of flexibility, and confirmed the crucial importance of accurate documentation for usability. {\textcopyright}2013 IEEE.},
address = {Baltimore, MD, USA},
author = {Piccioni, Marco and Furia, Carlo A and Meyer, Bertrand},
booktitle = {Proceedings of the 13th International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2013.14},
issn = {1949-3770},
keywords = {application programming interfaces},
month = {oct},
pages = {5--14},
publisher = {IEEE},
title = {{An empirical study of API usability}},
year = {2013}
}
@inproceedings{Baesens:2003we,
abstract = {Credit scoring, decision tables, rule extraction, neural networks Abstract: Accuracy and comprehensibility are two important criteria when developing decision support systems for credit scoring. In this paper, we focus on the second criterion and propose the use of decision tables as an alternative knowledge visualisation formalism which lends itself very well to building intelligent and userfriendly credit scoring systems. Starting from a set of propositional if-then rules extracted by a neural network rule extraction algorithm, we construct decision tables and demonstrate their efficiency and user-friendliness for two real-life credit scoring cases.},
address = {Angers, France},
author = {Baesens, Bart and Mues, Christophe and {De Backer}, Manu and Vanthienen, Jan and Setiono, Rudy},
booktitle = {Proceedings of the 5th International Conference on Enterprise Information Systems},
doi = {10.1007/1-4020-2673-0_15},
isbn = {9-72-988161-8},
month = {apr},
pages = {19--25},
publisher = {IEEE},
title = {{Building intelligent credit scoring systems using decision tables}},
volume = {2},
year = {2003}
}
@inproceedings{Garg:2011gw,
abstract = {With the growth of Cloud Computing, more and more companies are offering different cloud services. From the customer's point of view, it is always difficult to decide whose services they should use, based on users' requirements. Currently there is no software framework which can automatically index cloud providers based on their needs. In this work, we propose a framework and a mechanism, which measure the quality and prioritize Cloud services. Such framework can make significant impact and will create healthy competition among Cloud providers to satisfy their Service Level Agreement (SLA) and improve their Quality of Services (QoS). {\textcopyright}2011 IEEE.},
address = {Melbourne, Australia},
author = {Garg, Saurabh Kumar and Versteeg, Steve and Buyya, Rajkumar},
booktitle = {Proceedings of the 4th IEEE International Conference on Utility and Cloud Computing},
doi = {10.1109/UCC.2011.36},
isbn = {978-0-76-954592-9},
keywords = {Cloud Computing,Quality of Service,Service Measurement},
month = {dec},
pages = {210--218},
publisher = {IEEE},
title = {{SMICloud: A framework for comparing and ranking cloud services}},
year = {2011}
}
@inproceedings{Watson:2012uy,
abstract = {Computer technology has made amazing advances in the past few decades; however, the software documentation of today still looks strikingly similar to the software documentation used 30 years ago. If this continues into the 21st century, more and more soft-ware developers could be using 20 th-century-style documentation to solve 21 st-century problems with 21 st-century technologies. Is 20 th-century- style documentation up to the challenge? How can that be measured? This paper seeks to answer those questions by developing a heuristic to identify whether the documentation set for an application programming interface (API) contains the key elements of API reference documentation that help software developers learn an API. The resulting heuristic was tested on a collection of software documentation that was chosen to provide a diverse set of examples with which to validate the heuristic. In the course of testing the heuristic, interesting patterns in the API documentation were observed. For example, twenty-five percent of the documentation sets studied did not have any overview information, which, according to studies, is one of the most basic elements an API documentation set needs to help software developers learn to use the API. The heuristic produced by this research can be used to evaluate large sets of API documentation, track trends in API documentation, and facilitate additional research. Copyright {\textcopyright}2012 ACM.},
address = {Seattle, WA, USA},
author = {Watson, Robert},
booktitle = {Proceedings of the 30th ACM International Conference on Design of Communication},
doi = {10.1145/2379057.2379112},
isbn = {978-1-45-031497-8},
keywords = {API,API reference documentation,Application programming interface,Software documentation,Software libraries},
month = {oct},
pages = {295--302},
publisher = {ACM},
title = {{Development and application of a heuristic to assess trends in API documentation}},
year = {2012}
}
@inproceedings{Elazmeh:2007tp,
abstract = {The paper presents ongoing issues, challenges, and difficulties we face in applying machine learning methods to retrospectively collected clinical data. The objective of our research is to build a reliable prediction model for early assessment of emergency pediatric asthma exacerbations. This predictive model should be able to distinguish between patients with mild or moderate/severe asthma attacks at a medically acceptable level of performance. Our real-life data set presents us with some difficult challenges which we communicate in this paper. Our approach to overcoming some of these difficulties is to use external expert knowledge to aid with classification by decomposing the classification problem into a two-tier concept, where concepts can be explicitly described in terms of the external knowledge source. Such an approach also has the advantage of significantly reducing the size of the training set required. Copyright {\textcopyright}2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
address = {Vancouver, BC, Canada},
author = {Elazmeh, William and Matwin, Stan and O'Sullivan, Dympna and Michalowski, Wojtek and Farion, Ken},
booktitle = {Proceedings of the 22nd Conference on Artificial Intelligence},
isbn = {978-1-57-735332-4},
month = {jul},
pages = {10--15},
publisher = {AAAI},
title = {{Insights from predicting pediatric asthma exacerbations from retrospective clinical data}},
volume = {WS-07-05},
year = {2007}
}
@inproceedings{4659256,
abstract = {Change is an essential characteristic of software development, as software systems must respond to evolving requirements, platforms, and other environmental pressures. In this paper, we discuss the concept of software evolution from several perspectives. We examine how it relates to and differs from software maintenance. We discuss insights about software evolution arising from Lehman's laws of software evolution and the staged lifecycle model of Bennett and Rajlich. We compare software evolution to other kinds of evolution, from science and social sciences, and we examine the forces that shape change. Finally, we discuss the changing nature of software in general as it relates to evolution, and we propose open challenges and future directions for software evolution research. {\textcopyright}2008 IEEE.},
address = {Beijing, China},
author = {Godfrey, Michael W and German, Daniel M},
booktitle = {Proceedings of the 2008 Frontiers of Software Maintenance},
doi = {10.1109/FOSM.2008.4659256},
isbn = {978-1-42-442655-3},
keywords = {software development management,software maintenan},
month = {oct},
pages = {129--138},
title = {{The past, present, and future of software evolution}},
year = {2008}
}
@inproceedings{Stevens:2013vf,
address = {San Francisco, CA, USA},
author = {Stevens, Ryan and Ganz, Jonathan and Filkov, Vladimir and Devanbu, Premkumar and Chen, Hao},
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
isbn = {978-1-46-732936-1},
month = {may},
pages = {31--40},
publisher = {IEEE},
title = {{Asking for (and about) permissions used by Android apps}},
year = {2013}
}
@inproceedings{Treude:2011fh,
abstract = {Question and Answer (Q{\&}A) websites, such as Stack Overflow, use social media to facilitate knowledge exchange between programmers and fill archives with millions of entries that contribute to the body of knowledge in software development. Understanding the role of Q{\&}A websites in the documentation landscape will enable us to make recommendations on how individuals and companies can leverage this knowledge effectively. In this paper, we analyze data from Stack Overflow to categorize the kinds of questions that are asked, and to explore which questions are answered well and which ones remain unanswered. Our preliminary findings indicate that Q{\&}A websites are particularly effective at code reviews and conceptual questions. We pose research questions and suggest future work to explore the motivations of programmers that contribute to Q{\&}A websites, and to understand the implications of turning Q{\&}A exchanges into technical mini-blogs through the editing of questions and answers. {\textcopyright}2011 ACM.},
address = {Honolulu, HI, USA},
author = {Treude, Christoph and Barzilay, Ohad and Storey, Margaret Anne},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
doi = {10.1145/1985793.1985907},
isbn = {978-1-45-030445-0},
issn = {0270-5257},
keywords = {q{\&}a,questions,social media,stack overflow},
month = {may},
pages = {804--807},
publisher = {ACM},
title = {{How do programmers ask and answer questions on the web?}},
year = {2011}
}
@book{Simon:1996uw,
abstract = {Continuing his exploration of the organization of complexity and the science of design, this new edition of Herbert Simon's classic work on artificial intelligence adds a chapter that sorts out the current themes and tools—chaos, adaptive systems, genetic algorithms—for analyzing complexity and complex systems. There are updates throughout the book as well. These take into account important advances in cognitive psychology and the science of design while confirming and extending the book's basic thesis: that a physical symbol system has the necessary and sufficient means for intelligent action. The chapter "Economic Reality" has also been revised to reflect a change in emphasis in Simon's thinking about the respective roles of organizations and markets in economic systems.},
author = {Michalos, Alex C and Simon, Herbert A},
booktitle = {Technology and Culture},
doi = {10.2307/3102825},
issn = {0040165X},
number = {1},
pages = {118},
publisher = {MIT press},
title = {{The Sciences of the Artificial}},
volume = {11},
year = {1970}
}
@article{Freitas:2010vk,
abstract = {The literature on protein function prediction is currently dominated by works aimed at maximizing predictive accuracy, ignoring the important issues of validation and interpretation of discovered knowledge, which can lead to new insights and hypotheses that are biologically meaningful and advance the understanding of protein functions by biologists. The overall goal of this paper is to critically evaluate this approach, offering a refreshing new perspective on this issue, focusing not only on predictive accuracy but also on the comprehensibility of the induced protein function prediction models. More specifically, this paper aims to offer two main contributions to the area of protein function prediction. First, it presents the case for discovering comprehensible protein function prediction models from data, discussing in detail the advantages of such models, namely, increasing the confidence of the biologist in the system's predictions, leading to new insights about the data and the formulation of new biological hypotheses, and detecting errors in the data. Second, it presents a critical review of the pros and cons of several different knowledge representations that can be used in order to support the discovery of comprehensible protein function prediction models. {\textcopyright}2006 IEEE.},
author = {Freitas, Alex A and Wieser, Daniela C and Apweiler, Rolf},
doi = {10.1109/TCBB.2008.47},
issn = {1545-5963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
keywords = {Biology,Classifier design and evaluation,Induction,Machine learning,Machine learning.},
number = {1},
pages = {172--182},
title = {{On the importance of comprehensible classification models for protein function prediction}},
volume = {7},
year = {2010}
}
@book{Bass:2003wi,
abstract = {This report is another in a series of Software Engineering Institute (SEISM) case studies of organizations that have adopted the software product line approach for developing systems. It details the story of Salion, Inc., an enterprise software company providing Revenue Acquisition Management solutions tailored to the unique needs of automotive suppliers. Salion's solutions enable suppliers to organize and manage their disparate customer-interfacing activities as one coordinated business process, resulting in higher revenues, profit margins, and customer satisfaction. This case study is unique in that Salion did not have substantial experience in its application area, although its key designers and strategists were knowledgeable about related domains. Salion pursued a reactive approach to its product line that let it respond flexibly to spontaneous business opportunities and that significantly lowered the cost of adopting the product line paradigm to its software system development. This case study describes relevant dimensions of Salion's context, how it approached several product line practice areas that were key to its strategy, the benefits gained through its product line, lessons learned, and the major thematic aspects of the Salion story.},
author = {Bass, Len and Clements, Paul and Kazman, Rick},
booktitle = {Software Architecture},
edition = {2nd},
isbn = {0-32-115495-9},
pages = {560},
publisher = {Addison-Wesley},
title = {{Software Architecture in Practice}},
year = {2003}
}
@article{Landis:1977kv,
abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
author = {Landis, J Richard and Koch, Gary G},
doi = {10.2307/2529310},
issn = {0006-341X},
journal = {Biometrics},
month = {mar},
number = {1},
pages = {159},
title = {{The Measurement of Observer Agreement for Categorical Data}},
volume = {33},
year = {1977}
}
@techreport{BernersLee:2004vf,
author = {Berners-Lee, Tim and Fielding, Roy and Masinter, Larry},
title = {{Uniform resource identifier (URI): Generic syntax}},
year = {2004}
}
@article{Narayanan:2018ud,
abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
annote = {In Press},
archivePrefix = {arXiv},
arxivId = {1802.00682},
author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
eprint = {1802.00682},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {InPress},
mendeley-tags = {InPress},
title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
year = {2018}
}
@book{SWEBOK,
address = {Washington, DC, USA},
edition = {3rd},
editor = {Bourque, Pierre and Fairley, Richard E},
isbn = {978-0-7695-5166-1},
pages = {346},
publisher = {IEEE},
title = {{Guide to the Software Engineering Body of Knowledge}},
year = {2014}
}
@misc{wiki:dataset-list,
author = {{Wikipedia Contributors}},
title = {{List of datasets for machine-learning research — Wikipedia, The Free Encyclopedia}},
url = {https://bit.ly/3cZgwLb},
year = {2020}
}
@book{Wong:2006ve,
abstract = {Data mining involves the non-trivial extraction of implicit, previously unknown, and potentially useful information from databases. Genetic Programming (GP) and Inductive Logic Programming (ILP) are two of the approaches for data mining. This book first sets the necessary backgrounds for the reader, including an overview of data mining, evolutionary algorithms and inductive logic programming. It then describes a framework, called GGP (Generic Genetic Programming), that integrates GP and ILP based on a formalism of logic grammars. The formalism is powerful enough to represent context- sensitive information and domain-dependent knowledge. This knowledge can be used to accelerate the learning speed and/or improve the quality of the knowledge induced. A grammar-based genetic programming system called LOGENPRO (The LOGic grammar based GENetic PROgramming system) is detailed and tested on many problems in data mining. It is found that LOGENPRO outperforms some ILP systems. We have also illustrated how to apply LOGENPRO to emulate Automatically Defined Functions (ADFs) to discover problem representation primitives automatically. By employing various knowledge about the problem being solved, LOGENPRO can find a solution much faster than ADFs and the computation required by LOGENPRO is much smaller than that of ADFs. Moreover, LOGENPRO can emulate the effects of Strongly Type Genetic Programming and ADFs simultaneously and effortlessly. Data Mining Using Grammar Based Genetic Programming and Applications is appropriate for researchers, practitioners and clinicians interested in genetic programming, data mining, and the extraction of data from databases.},
author = {Wong, Man Leung and Leung, Kwong Sak},
booktitle = {Data Mining Using Grammar Based Genetic Programming and Applications},
doi = {10.1007/b116131},
isbn = {978-0-79-237746-7},
publisher = {Springer},
title = {{Data Mining Using Grammar Based Genetic Programming and Applications}},
year = {2002}
}
@article{GAROUSI2019101,
abstract = {Context: A Multivocal Literature Review (MLR) is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in software engineering (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results. Objective: There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE. Method: To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example. Results: The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations. Conclusion: Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences.},
archivePrefix = {arXiv},
arxivId = {1707.02553},
author = {Garousi, Vahid and Felderer, Michael and M{\"{a}}ntyl{\"{a}}, Mika V},
doi = {10.1016/j.infsof.2018.09.006},
eprint = {1707.02553},
issn = {0950-5849},
journal = {Information and Software Technology},
keywords = {Evidence-based software engineering,Grey literature,Guidelines,Literature study,Multivocal literature review,Systematic literature review,Systematic mapping study},
pages = {101--121},
title = {{Guidelines for including grey literature and conducting multivocal literature reviews in software engineering}},
volume = {106},
year = {2019}
}
@article{Uddin:2015hn,
abstract = {Formal documentation can be a crucial resource for learning to how to use an API. However, producing high-quality documentation can be nontrivial. Researchers investigated how 10 common documentation problems manifested themselves in practice. The results are based on two surveys of a total of 323 professional software developers and analysis of 179 API documentation units. The three severest problems were ambiguity, incompleteness, and incorrectness of content. The respondents often mentioned six of the 10 problems as 'blockers" that forced them to use another API.},
author = {Uddin, Gias and Robillard, Martin P},
doi = {10.1109/MS.2014.80},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {API,documentation,software development,software engineering,user study},
month = {jun},
number = {4},
pages = {68--75},
title = {{How API Documentation Fails}},
volume = {32},
year = {2015}
}
@article{Wachter:2017hx,
abstract = {The aim of this contribution is to analyse the real borderlines of the 'right to explanation' in the GDPR and to discretely distinguish between dif ferent levels of information and of consumers' awareness in the 'black box society. In order to combine transparency and comprehensibility we propose the new concept of algorithm 'legibility'. We argue that a systemic interpretation is needed in this field, since it can be beneficial not only for individuals but also for businesses. This may be an opportunity for auditing algorithms and correcting unknown machine biases, thus similarly enhancing the quality of decision-making outputs. Accordingly, we show how a systemic interpretation of Articles 13-15 and 22 GDPR is necessary, considering in particular that: The threshold of minimum human intervention required so that the decision-making is 'solely' automated (Article 22(1)) can also include nominal human intervention; the envisaged 'significant effects' on individuals (Article 22(1)) can encompass as well marketing manipulation, price discrimination, etc; 'meaningful information' that should be pro-vided to data subjects about the logic, signifi-cance and consequences of decision-making (Article 15(1 )(h){\textgreater}should be read as 'legibility' of 'architecture' and 'implementation' of algorith-mic processing; trade secret protection might limit the right of access of data subjects, but there is a general legal favour for data protection rights that should reduce the impact of trade secrets protection. In addition, we recommend a 'legibility test' that data controllers should perform in order to com-ply with the duty to provide meaningful information about the logic involved in an automated decision-making.},
author = {Malgieri, Gianclaudio and Comand{\'{e}}, Giovanni},
doi = {10.1093/idpl/ipx019},
issn = {2044-4001},
journal = {International Data Privacy Law},
month = {jun},
number = {4},
pages = {243--265},
title = {{Why a right to legibility of automated decision-making exists in the general data protection regulation}},
volume = {7},
year = {2017}
}
@inproceedings{Otero:2013ul,
abstract = {Most ant colony optimization (ACO) algorithms for inducing classification rules use a ACO-based procedure to create a rule in a one-at-a-time fashion. An improved search strategy has been proposed in the cAnt-MinerPB algorithm, where an ACO-based procedure is used to create a complete list of rules (ordered rules), i.e., the ACO search is guided by the quality of a list of rules instead of an individual rule. In this paper we propose an extension of the cAnt-MinerPB algorithm to discover a set of rules (unordered rules). The main motivations for this work are to improve the interpretation of individual rules by discovering a set of rules and to evaluate the impact on the predictive accuracy of the algorithm. We also propose a new measure to evaluate the interpretability of the discovered rules to mitigate the fact that the commonly used model size measure ignores how the rules are used to make a class prediction. Comparisons with state-of-the-art rule induction algorithms, support vector machines, and the cAnt-MinerPB producing ordered rules are also presented.},
author = {Otero, Fernando E B and Freitas, Alex A},
booktitle = {Evolutionary Computation},
doi = {10.1162/EVCO_a_00155},
issn = {1530-9304},
keywords = {Ant colony optimization,Classification,Comprehensibility,Data mining,Sequential covering,Unordered rules},
number = {3},
pages = {385--409},
pmid = {26066807},
publisher = {ACM},
title = {{Improving the interpretability of classification rules discovered by an ant colony algorithm: Extended results}},
volume = {24},
year = {2016}
}
@inproceedings{Boehm:1978vv,
abstract = {The study reported in this paper establishes a conceptual framework and some key initial results in the analysis of the characteristics of software quality. Its main results and conclusions are: • Explicit attention to characteristics of software quality can lead to significant savings in software life-cycle costs. • The current software state-of-the-art imposes specific limitations on our ability to automatically and quantitatively evaluate the quality of software. • A definitive hierarchy of well-defined, well-differentiated characteristics of software quality is developed. Its higher-level structure reflects the actual uses to which software quality evaluation would be put; its lower-level characteristics are closely correlated with actual software metric evaluations which can be performed. • A large number of software quality-evaluation metrics have been defined, classified, and evaluated with respect to their potential benefits, quantifiability, and ease of automation. • Particular software life-cycle activities have been identified which have significant leverage on software quality. Most importantly, we believe that the study reported in this paper provides for the first time a clear, well-defined framework for assessing the often slippery issues associated with software quality, via the consistent and mutually supportive sets of definitions, distinctions, guidelines, and experiences cited. This framework is certainly not complete, but it has been brought to a point sufficient to serve as a viable basis for future refinements and extensions.},
address = {San Francisco, California, USA},
author = {Boehm, B W and Brown, J R and Lipow, M},
booktitle = {Proceedings of the 2nd International Conference on Software Engineering},
issn = {0270-5257},
keywords = {Management by objectives,Quality assurance,Quality characteristics,Quality metrics,Software engineering,Software measurement and evaluation,Software quality,Software reliability,Software standards,Testing},
month = {oct},
pages = {592--605},
publisher = {IEEE},
title = {{Quantitative evaluation of software quality}},
year = {1976}
}
@article{Usman:2017hn,
abstract = {Context: Software Engineering (SE) is an evolving discipline with new subareas being continuously developed and added. To structure and better understand the SE body of knowledge, taxonomies have been proposed in all SE knowledge areas. Objective: The objective of this paper is to characterize the state-of-the-art research on SE taxonomies. Method: A systematic mapping study was conducted, based on 270 primary studies. Results: An increasing number of SE taxonomies have been published since 2000 in a broad range of venues, including the top SE journals and conferences. The majority of taxonomies can be grouped into the following SWEBOK knowledge areas: construction (19.55{\%}), design (19.55{\%}), requirements (15.50{\%}) and maintenance (11.81{\%}). Illustration (45.76{\%}) is the most frequently used approach for taxonomy validation. Hierarchy (53.14{\%}) and faceted analysis (39.48{\%}) are the most frequently used classification structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53{\%}) these procedures are not described in sufficient detail. The majority of the taxonomies (97{\%}) target unique subject matters and many taxonomy-papers are cited frequently. Most SE taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way. Conclusion: There is a strong interest in taxonomies in SE, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classification structures, procedures and descriptive bases are usually not well described and motivated.},
author = {Usman, Muhammad and Britto, Ricardo and B{\"{o}}rstler, J{\"{u}}rgen and Mendes, Emilia},
doi = {10.1016/j.infsof.2017.01.006},
issn = {0950-5849},
journal = {Information and Software Technology},
keywords = {Classification,Software engineering,Systematic mapping study,Taxonomy},
month = {may},
pages = {43--59},
title = {{Taxonomies in software engineering: A Systematic mapping study and a revised taxonomy development method}},
volume = {85},
year = {2017}
}
@article{Weiss2004,
author = {Weiss, Gary M},
doi = {10.1145/1007730.1007734},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {7--19},
publisher = {Association for Computing Machinery (ACM)},
title = {{Mining with rarity}},
url = {https://dl.acm.org/doi/10.1145/1007730.1007734},
volume = {6},
year = {2004}
}
@inproceedings{wrobel2013,
address = {Sopot, Poland},
author = {Wrobel, Michal R},
booktitle = {Proceedings of 6th International Conference on Human System Interactions},
doi = {10.1109/HSI.2013.6577875},
month = {jun},
pages = {518--523},
publisher = {IEEE},
title = {{Emotions in the software development process}},
year = {2013}
}
@article{Rosenfeld:2018ut,
archivePrefix = {arXiv},
arxivId = {1808.03305},
author = {Rosenfeld, Amir and Zemel, Richard and Tsotsos, John K},
eprint = {1808.03305},
title = {{The Elephant in the Room}},
year = {2018}
}
@inproceedings{Robillard:hk,
abstract = {We advocate for a paradigm shift in supporting the information needs of developers, centered around the concept of automated on-demand developer documentation. Currently, developer information needs are fulfilled by asking experts or consulting documentation. Unfortunately, traditional documentation practices are inefficient because of, among others, the manual nature of its creation and the gap between the creators and consumers. We discuss the major challenges we face in realizing such a paradigm shift, highlight existing research that can be leveraged to this end, and promote opportunities for increased convergence in research on software documentation.},
address = {Shanghai, China},
author = {Robillard, Martin P. and Marcus, Andrian and Treude, Christoph and Bavota, Gabriele and Chaparro, Oscar and Ernst, Neil and Gerosall, Marco Aur{\'{e}}lio and Godfrey, Michael and Lanza, Michele and Linares-V{\'{a}}squez, Mario and Murphy, Gail C. and Moreno, Laura and Shepherd, David and Wong, Edmund},
booktitle = {Proceedings of the 33rd IEEE International Conference on Software Maintenance and Evolution},
doi = {10.1109/ICSME.2017.17},
month = {sep},
pages = {479--483},
publisher = {IEEE},
title = {{On-demand developer documentation}},
year = {2017}
}
@inproceedings{ortu2016,
address = {Austin, TX, USA},
author = {Ortu, Marco and Murgia, Alessandro and Destefanis, Giuseppe and Tourani, Parastou and Tonelli, Roberto and Marchesi, Michele and Adams, Bram},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
doi = {10.1145/2901739.2903505},
month = {may},
organization = {ACM},
pages = {480--483},
publisher = {ACM},
title = {{The emotional side of software developers in JIRA}},
year = {2016}
}
@article{Krizhevsky:2012wl,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
issn = {1557-7317},
journal = {Communications of the ACM},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
volume = {60},
year = {2017}
}
@book{Pham:2000ua,
author = {Pham, Hoang},
edition = {1st},
isbn = {978-1-84-628295-9},
publisher = {Springer},
title = {{System Software Reliability}},
year = {2000}
}
@misc{ISO9000:2015,
author = {{International Organization for Standardization}},
title = {{ISO 9000:2015 Quality management systems – Fundamentals and vocabulary}},
url = {http://bit.ly/37O4oKo},
year = {2015}
}
@inproceedings{Tahir:2018ks,
abstract = {This paper investigates how developers discuss code smells and anti-patterns over Stack Overflow to understand better their perceptions and understanding of these two concepts. Understanding developers' perceptions of these issues are important in order to inform and align future research efforts and direct tools vendors in the area of code smells and anti-patterns. In addition, such insights could lead the creation of solutions to code smells and anti-patterns that are better fit to the realities developers face in practice. We applied both quantitative and qualitative techniques to analyse discussions containing terms associated with code smells and anti-patterns. Our findings show that developers widely use Stack Overflow to ask for general assessments of code smells or anti-patterns, instead of asking for particular refactoring solutions. An interesting finding is that developers very often ask their peers 'to smell their code' (i.e., ask whether their own code 'smells' or not), and thus, utilize Stack Overflow as an informal, crowd-based code smell/anti-pattern detector. We conjecture that the crowd-based detection approach considers contextual factors, and thus, tends to be more trusted by developers over automated detection tools. We also found that developers often discuss the downsides of implementing specific design patterns, and 'flag' them as potential anti-patterns to be avoided. Conversely, we found discussions on why some anti-patterns previously considered harmful should not be flagged as anti-patterns. Our results suggest that there is a need for: 1) more context-based evaluations of code smells and anti-patterns, and 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry.},
address = {Christchurch, New Zealand},
author = {Tahir, Amjed and Yamashita, Aiko and Licorish, Sherlock and Dietrich, Jens and Counsell, Steve},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering},
doi = {10.1145/3210459.3210466},
isbn = {978-1-45-036403-4},
keywords = {Anti-patterns,Code smells,Empirical study,Mining software repositories,Stack Overflow},
month = {jun},
pages = {68--78},
publisher = {ACM},
title = {{Can you tell me if it smells? A study on how developers discuss code smells and anti-patterns in Stack Overflow}},
year = {2018}
}
@phdthesis{Nelson:1981ue,
author = {Nelson, Bruce Jay},
school = {Carnegie Mellon University},
title = {{Remote Procedure Call}},
year = {1981}
}
@phdthesis{curumsing2017,
address = {Hawthorn, VIC, Australia},
author = {Curumsing, Maheswaree Kissoon},
school = {Swinburne University of Technology},
title = {{Emotion-Oriented Requirements Engineering}},
year = {2017}
}
@incollection{Brooke:1996ua,
abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for " quick and dirty " methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
address = {Cornwall, England, UK},
author = {Brooke, John},
booktitle = {Usability Evaluation in Industry},
chapter = {21},
isbn = {978-0-74-840460-5},
pages = {189--194},
publisher = {Taylor {\&} Francis Ltd},
title = {{SUS-A quick and dirty usability scale}},
year = {1996}
}
@inproceedings{calefato2017,
address = {San Antonio, TX, USA},
author = {Calefato, Fabio and Lanubile, Filippo and Novielli, Nicole},
booktitle = {Proceedings of the 7th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos},
doi = {10.1109/ACIIW.2017.8272591},
month = {oct},
pages = {79--80},
publisher = {IEEE},
title = {{EmoTxt: a toolkit for emotion recognition from text}},
year = {2017}
}
@article{Wohlin:2014jq,
abstract = {Several factors make empirical research in software engineering particularly challenging as it requires studying not only technology but its stakeholders' activities while drawing concepts and theories from social science. Researchers, in general, agree that selecting a research design in empirical software engineering research is challenging, because the implications of using individual research methods are not well recorded. The main objective of this article is to make researchers aware and support them in their research design, by providing a foundation of knowledge about empirical software engineering research decisions, in order to ensure that researchers make well-founded and informed decisions about their research designs. This article provides a decision-making structure containing a number of decision points, each one of them representing a specific aspect on empirical software engineering research. The article provides an introduction to each decision point and its constituents, as well as to the relationships between the different parts in the decision-making structure. The intention is the structure should act as a starting point for the research design before going into the details of the research design chosen. The article provides an in-depth discussion of decision points in relation to the research design when conducting empirical research.},
author = {Wohlin, Claes and Aurum, Ayb{\"{u}}ke},
doi = {10.1007/s10664-014-9319-7},
issn = {1573-7616},
journal = {Empirical Software Engineering},
keywords = {Empirical software engineering research,Research design,Research methods,Selecting research method},
month = {may},
number = {6},
pages = {1427--1455},
title = {{Towards a decision-making structure for selecting a research design in empirical software engineering}},
volume = {20},
year = {2015}
}
@article{Taylor:1968tq,
author = {Taylor, R S},
doi = {10.5860/crl_29_03_178},
journal = {College and Research Libraries},
number = {3},
title = {{Question-Negotiation and Information Seeking in Libraries}},
volume = {29},
year = {1968}
}
@inproceedings{Abadi:2016vn,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
address = {Savannah, GA, USA},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation},
eprint = {1605.08695},
isbn = {978-1-93-197133-1},
pages = {265--283},
publisher = {ACM},
title = {{TensorFlow: A system for large-scale machine learning}},
year = {2016}
}
@inproceedings{Boz:2002uv,
abstract = {Neural Networks are successful in acquiring hidden knowledge in datasets. Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans. Researchers tried to address this problem by extracting rules from trained Neural Networks. Most of the proposed rule extraction methods required specialized type of Neural Networks; some required binary inputs and some were computationally expensive. Craven proposed extracting MofN type Decision Trees from Neural Networks. We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex. In this paper, we introduced a new method for extracting regular C4.5 like Decision Trees from trained Neural Networks. We showed that the new method (DecText) is effective in extracting high fidelity trees from trained networks. We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity.},
address = {Edmonton, AB, Canada},
author = {Boz, Olcay},
booktitle = {Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/775107.775113},
month = {jul},
pages = {456--461},
publisher = {ACM},
title = {{Extracting decision trees from trained neural networks}},
year = {2002}
}
@inproceedings{Bussone:2015wm,
abstract = {Clinical decision support systems (CDSS) are increasingly used by healthcare professionals for evidence-based diagnosis and treatment support. However, research has suggested that users often over-rely on system suggestions - even if the suggestions are wrong. Providing explanations could potentially mitigate misplaced trust in the system and over-reliance. In this paper, we explore how explanations are related to user trust and reliance, as well as what information users would find helpful to better understand the reliability of a system's decision-making. We investigated these questions through an exploratory user study in which healthcare professionals were observed using a CDSS prototype to diagnose hypothetic cases using fictional patients suffering from a balance-related disorder. Our results show that the amount of system confidence had only a slight effect on trust and reliance. More importantly, giving a fuller explanation of the facts used in making a diagnosis had a positive effect on trust but also led to over-reliance issues, whereas less detailed explanations made participants question the system's reliability and led to self-reliance problems. To help them in their assessment of the reliability of the system's decisions, study participants wanted better explanations to help them interpret the system's confidence, to verify that the disorder fit the suggestion, to better understand the reasoning chain of the decision model, and to make differential diagnoses. Our work is a first step toward improved CDSS design that better supports clinicians in making correct diagnoses.},
address = {Dallas, TX, USA},
author = {Bussone, Adrian and Stumpf, Simone and O'Sullivan, Dympna},
booktitle = {Proceedings of the 2015 IEEE International Conference on Healthcare Informatics},
doi = {10.1109/ICHI.2015.26},
isbn = {978-1-46-739548-9},
keywords = {CDSS,Explanations,Reliability,Reliance,Trust,User study},
month = {oct},
pages = {160--169},
publisher = {IEEE},
title = {{The role of explanations on trust and reliance in clinical decision support systems}},
year = {2015}
}
@inproceedings{Dorn:2010wl,
abstract = {This paper reports on a study of professional web designers and developers. We provide a detailed characterization of their knowledge of fundamental programming concepts elicited through card sorting. Additionally, we present qualitative findings regarding their motivation to learn new concepts and the learning strategies they employ. We find a high level of recognition of basic concepts, but we identify a number of concepts that they do not fully understand, consider difficult to learn, and use infrequently. We also note that their learning process is motivated by work projects and often follows a pattern of trial and error. We conclude with implications for end-user programming researchers. {\textcopyright}2010 ACM.},
address = {Atlanta, GA, USA},
author = {Dorn, Brian and Guzdial, Mark},
booktitle = {Proceedings of the 28th ACM Conference on Human Factors in Computing Systems},
doi = {10.1145/1753326.1753430},
isbn = {978-1-60-558929-9},
keywords = {informal learning,web development},
month = {apr},
pages = {703--712},
publisher = {ACM},
title = {{Learning on the job: Characterizing the programming knowledge and learning strategies of web designers}},
volume = {2},
year = {2010}
}
@article{Lau:1999vs,
abstract = {Based on recent reviews regarding its use in information systems (IS) studies, this paper argues that action research is still not well recognized by IS researchers and mainstream IS journals especially in North America. To make the situation worse, existing criteria used to assess the quality of action research studies are found to be inadequate when applied to IS. In order to advance its understanding and use by IS researchers and practitioners, the IS action research framework proposed recently by Lau is refined and presented as a set of guidelines in this paper. The implications of this refined framework on IS research and practice are discussed. {\textcopyright}1999, MCB UP Limited},
author = {Lau, Francis},
doi = {10.1108/09593849910267206},
issn = {0959-3845},
journal = {Information Technology {\&} People},
keywords = {Action research,Assessment,Information systems,Methodology,Quality,Research},
number = {2},
pages = {148--176},
title = {{Toward a framework for action research in information systems studies}},
volume = {12},
year = {1999}
}
@misc{McGowen:2019vt,
author = {McGowen, Bret},
month = {jan},
title = {{Machine learning with Google APIs}},
url = {http://bit.ly/3aUQpo2},
year = {2019}
}
@book{Horch:2003uv,
author = {Horch, John W},
isbn = {978-1-58-053604-2},
pages = {286},
publisher = {Artech House},
title = {{Practical Guide To Software Quality Management}},
year = {2003}
}
@inproceedings{Szegedy:2013vw,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
address = {Banff, AB, Canada},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
booktitle = {Proceedings of the 2nd International Conference on Learning Representations},
eprint = {1312.6199},
month = {apr},
publisher = {ACM},
title = {{Intriguing properties of neural networks}},
year = {2014}
}
@inproceedings{Nykaza:2002td,
abstract = {This paper steps the reader through a needs assessment of programmers that was conducted by instructional designers. The assessment's purpose was to identify what learning support programmers need and want to successfully use a new software development kit (SDK). The paper includes the challenges the researchers encountered, the questions asked and the responses, the types of individuals interviewed, and the conclusions reached from the research. Recommendations also are presented. Those responsible with developing documentation, training, and other learning support systems for programmers may find this assessment helpful. Marketing, product development and customer support people may also find value in learning more about the needs of this unique audience.},
address = {Toronto, ON, Canada},
author = {Nykaza, Janet and Messinger, Rhonda and Boehme, Fran and Norman, Cherie L and Mace, Matthew and Gordon, Manuel},
booktitle = {Proceedings of the 20th Annual International Conference on Computer Documentation},
doi = {10.1145/584955.584976},
keywords = {API documentation,Developer documentation,Needs analysis,Needs assessment,Programmer documentation,SDK documentation},
month = {oct},
pages = {133--141},
publisher = {ACM},
title = {{What programmers really want: Results of a needs assessment for SDK documentation}},
year = {2002}
}
@book{Sommerville:2011uc,
address = {Boston, MA, USA},
author = {Sommerville, Ian},
edition = {9th},
isbn = {978-0-13-703515-1},
publisher = {Addison-Wesley},
title = {{Software Engineering}},
year = {2011}
}
@article{Blake:1998vd,
abstract = {This paper describes the hardware implementations of fuzzy systems, neural networks and fuzzy neural networks (FNNs) using Xilinx Field Programmable Gate Arrays (FPGAs). The validity of these approaches is demonstrated by their application to a non-linear function approximation problem. The various elements of each system are discussed and implemented in hardware. The architectures were also implemented in software using the MATLAB neural network toolbox. The results are analysed in terms of an accuracy performance index and in the dimensions of the hardware required. {\textcopyright}1998 Elsevier Science Inc. All rights reserved.},
author = {Blake, J J and Maguire, L P and McGinnity, T M and Roche, B and McDaid, L J},
doi = {10.1016/S0020-0255(98)10029-4},
issn = {0020-0255},
journal = {Information Sciences},
number = {1-4},
pages = {151--168},
title = {{The implementation of fuzzy systems, neural networks and fuzzy neural networks using FPGAs}},
volume = {112},
year = {1998}
}
@article{Heckerman:2000uw,
abstract = {We describe a graphical model for probabilistic relationships - an alternative to the Bayesian network - called a dependency network. The graph of a dependency network, unlike a Bayesian network, is potentially cyclic. The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents. We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships. {\textcopyright}2000 David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, {\&} Carl Kadie.},
author = {Heckerman, David and Chickering, David Maxwell and Meek, Christopher and Rounthwaite, Robert and Kadie, Carl},
doi = {10.1162/153244301753344614},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian networks,Collaborative filtering,Data visualization,Dependency networks,Exploratory data analysis,Gibbs sampling,Graphical models,Probabilistic inference},
number = {1},
pages = {49--75},
title = {{Dependency networks for inference, collaborative filtering, and data visualization}},
volume = {1},
year = {2001}
}
@inproceedings{Reboucas:2016tw,
abstract = {Recently, Apple released Swift, a modern programming language built to be the successor of Objective-C. In less than a year and a half after its first release, Swift became one of the most popular programming languages in the world, considering different popularity measures. A significant part of this success is due to Apple's strict control over its ecosystem, and the clear message that it will replace Objective-C in a near future. According to Apple, "Swift is a powerful and intuitive programming language[...]. Writing Swift code is interactive and fun, the syntax is concise yet expressive." However, little is known about how Swift developers perceive these benefits. In this paper, we conducted two studies aimed at uncovering the questions and strains that arise from this early adoption. First, we perform a thorough analysis on 59,156 questions asked about Swift on StackOverflow. Second, we interviewed 12 Swift developers to cross-validate the initial results. Our study reveals that developers do seem to find the language easy to understand and adopt, although 17.5{\%} of the questions are about basic elements of the language. Still, there are many questions about problems in the toolset (compiler, Xcode, libraries). Some of our interviewees reinforced these problems.},
address = {Suita, Japan},
author = {Reboucas, Marcel and Pinto, Gustavo and Ebert, Felipe and Torres, Weslley and Serebrenik, Alexander and Castor, Fernando},
booktitle = {Proceedings of the 23rd International Conference on Software Analysis, Evolution, and Reengineering},
doi = {10.1109/saner.2016.66},
month = {mar},
pages = {634--638},
publisher = {IEEE},
title = {{An Empirical Study on the Usage of the Swift Programming Language}},
year = {2016}
}
@inproceedings{Bajaj:2014wg,
abstract = {Modern web applications consist of a significant amount of clientside code, written in JavaScript, HTML, and CSS. In this paper, we present a study of common challenges and misconceptions among web developers, by mining related questions asked on Stack Overflow. We use unsupervised learning to categorize the mined questions and define a ranking algorithm to rank all the Stack Overflow questions based on their importance. We analyze the top 50 questions qualitatively. The results indicate that (1) the overall share of web development related discussions is increasing among developers, (2) browser related discussions are prevalent; however, this share is decreasing with time, (3) form validation and other DOM related discussions have been discussed consistently over time, (4) web related discussions are becoming more prevalent in mobile development, and (5) developers face implementation issues with new HTML5 features such as Canvas. We examine the implications of the results on the development, research, and standardization communities. Copyright is held by the author/owner(s). Publication rights licensed to ACM.},
address = {Hyderabad, India},
author = {Bajaj, Kartik and Pattabiraman, Karthik and Mesbah, Ali},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
doi = {10.1145/2597073.2597083},
isbn = {978-1-45-032863-0},
keywords = {Stack Overflow,Text mining,Topic modeling,Web developers},
month = {may},
pages = {112--121},
publisher = {ACM},
title = {{Mining questions asked by web developers}},
year = {2014}
}
@misc{FileShad33:online,
annote = {Accessed: 25 January 2019},
author = {BusinessWire},
month = {jul},
title = {{FileShadow Delivers Machine Learning to End Users with Google Vision API | Business Wire}},
url = {https://bwnews.pr/2O5qv78},
year = {2018}
}
@article{Aalst:2015gv,
abstract = {As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the 'evidence' hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.},
author = {{Van Der Aalst}, Wil and Damiani, Ernesto},
doi = {10.1109/TSC.2015.2493732},
issn = {1939-1374},
journal = {IEEE Transactions on Services Computing},
keywords = {Big Data,Cloud Computing,Data Science,Process Mining,Process Science,Service Orientation},
month = {nov},
number = {6},
pages = {810--819},
title = {{Processes Meet Big Data: Connecting Data Science with Process Science}},
volume = {8},
year = {2015}
}
@misc{ISO25010:2011,
author = {{International Organization for Standardization}},
title = {{ISO25010:2011 - Systems and software engineering -- Systems and software Quality Requirements and Evaluation (SQuaRE) -- System and software quality models}},
year = {2011}
}
@article{McHugh:2012up,
abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
author = {McHugh, Mary L},
doi = {10.11613/bm.2012.031},
issn = {1330-0962},
journal = {Biochemia Medica},
keywords = {Interrater,Kappa,Rater,Reliability},
number = {3},
pages = {276--282},
pmid = {23092060},
title = {{Interrater reliability: The kappa statistic}},
volume = {22},
year = {2012}
}
@article{Richards:2001vw,
abstract = {This paper describes the analysis of a database of diabetic patients' clinical records and death certificates. The objective of the study was to find rules that describe associations between observations made of patients at their first visit to the hospital and early mortality. Pre-processing was carried out and a knowledge discovery in databases (KDD) package, developed by the Lanner Group and the University of East Anglia, was used for rule induction using simulated annealing. The most significant discovered rules describe an association that was not generally known or accepted by the medical community, however, recent independent studies confirm their validity. {\textcopyright}2001 Elsevier Science B.V.},
author = {Richards, G and Rayward-Smith, V J and S{\"{o}}nksen, P H and Carey, S and Weng, C},
doi = {10.1016/S0933-3657(00)00110-X},
issn = {0933-3657},
journal = {Artificial Intelligence in Medicine},
keywords = {Data mining,Diabetes,Neuropathy,Rule induction},
number = {3},
pages = {215--231},
title = {{Data mining for indicators of early mortality in a database of clinical records}},
volume = {22},
year = {2001}
}
@incollection{Krig2016,
abstract = {This chapter discusses several topics pertaining to ground truth data, the basis for computer vision metric analysis. We look at examples to illustrate the importance of ground truth data design and use, including manual and automated methods. We then propose a method and corresponding ground truth dataset for measuring interest point detector response as compared to human visual system response and human expectations. Also included here are example applications of the general robustness criteria and the general vision taxonomy developed in Chap. 5as applied to the preparation of hypothetical ground truth data. Lastly, we look at the current state of the art, its best practices, and a survey of available ground truth datasets.},
address = {Cham},
author = {Krig, Scott},
booktitle = {Computer Vision Metrics: Textbook Edition},
doi = {10.1007/978-3-319-33762-3_7},
isbn = {978-3-319-33762-3},
pages = {247--271},
publisher = {Springer International Publishing},
title = {{Ground Truth Data, Content, Metrics, and Analysis}},
year = {2016}
}
@phdthesis{Kim:2015vo,
author = {Kim, Been},
school = {Massachusetts Institute of Technology},
title = {{Interactive and Interpretable Machine Learning Models for Human Machine Collaboration}},
year = {2015}
}
@article{Krosnick:1999wt,
author = {Krosnick, Jon A},
doi = {10.1146/annurev.psych.50.1.537},
issn = {0066-4308},
journal = {Annual Review of Psychology},
month = {feb},
number = {1},
pages = {537--567},
title = {{Survey Research}},
volume = {50},
year = {1999}
}
@inproceedings{nishi2018test,
abstract = {As machine learning (ML) technology continues to spread by rapid evolution, the system or service using Machine Learning technology, called ML product, makes big impact on our life, society and economy. Meanwhile, Quality Assurance (QA) for ML product is quite more difficult than hardware, non-ML software and service because performance of ML technology is much better than non-ML technology in exchange for the characteristics of ML product, e.g. low explainability. We must keep rapid evolution and reduce quality risk of ML product simultaneously. In this paper, we show a Quality Assurance Framework for Machine Learning product. Scope of QA in this paper is limited to product evaluation. First, a policy of QA for ML Product is proposed. General principles of product evaluation is introduced and applied to ML product evaluation as a part of the policy. They are composed of A-ARAI: Allowability, Achievability, Robustness, Avoidability and Improvability. A strategy of ML Product Evaluation is constructed as another part of the policy. Quality Integrity Level for ML product is also modelled. Second, we propose a test architecture of ML product testing. It consists of test levels and fundamental test types of ML product testing, including snapshot testing, learning testing and confrontation testing. Finally, we defines QA activity levels for ML product.},
address = {V{\"{a}}ster{\aa}s, Sweden},
author = {Nishi, Yasuharu and Masuda, Satoshi and Ogawa, Hideto and Uetsuki, Keiji},
booktitle = {Proceedings of the 11th International Conference on Software Testing, Verification and Validation Workshops},
doi = {10.1109/ICSTW.2018.00060},
isbn = {978-1-53-866352-3},
keywords = {Artificial intelligence,Functional safety,Machine learning,Quality assurance,Test architecture,Test design,Test level,Test type},
month = {apr},
pages = {273--278},
publisher = {IEEE},
title = {{A test architecture for machine learning product}},
year = {2018}
}
@inproceedings{novielli2018,
address = {Gothenburg, Sweden},
author = {Novielli, Nicole and Calefato, Fabio and Lanubile, Filippo},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
doi = {10.1145/3196398.3196453},
isbn = {9781450357166},
month = {may},
pages = {14--17},
publisher = {ACM},
title = {{A gold standard for emotion annotation in stack overflow}},
year = {2018}
}
@article{DoshiVelez:2017wu,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
eprint = {1702.08608},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
year = {2017}
}
@article{Gamer:tj,
author = {Gamer, M and Lemon, J and Fellows, I and Singh, P},
journal = {R package version 0.83},
title = {{Irr: various coefficients of interrater reliability}},
year = {2010}
}
@inproceedings{Kaufman:1999vg,
abstract = {In concept learning or data mining tasks, the learner is typically faced with a choice of many possible hypotheses characterizing the data. If one can assume that the training data are noise-free, then the generated hypothesis should be complete and consistent with regard to the data. In real-world problems, however, data are often noisy, and an insistence on full completeness and consistency is no longer valid. The problem then is to determine a hypothesis that represents the “best” trade-off between completeness and consistency. This paper presents an approach to this problem in which a learner seeks rules optimizing a description quality criterion that combines completeness and consistency gain, a measure based on consistency that reflects the rule's benefit. The method has been implemented in the AQ18 learning and data mining system and compared to several other methods. Experiments have indicated the flexibility and power of the proposed method.},
address = {Warsaw, Poland},
author = {Kaufman, Kenneth A and Michalski, Ryszard S},
booktitle = {Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases},
doi = {10.1007/BFb0095128},
isbn = {3-540-65965-X},
issn = {1611-3349},
month = {sep},
pages = {411--419},
publisher = {Springer},
title = {{Learning from inconsistent and noisy data: The AQ18 approach}},
volume = {1609},
year = {1999}
}
@article{Garvin:1984vf,
author = {Garvin, David A},
issn = {0019-848X},
journal = {MIT Sloan Management Review},
number = {1},
pages = {25--43},
title = {{What Does `Product Quality' Really Mean?}},
volume = {26},
year = {1984}
}
@inproceedings{Cummaudo:2020icse,
address = {Seoul, Republic of Korea},
author = {Cummaudo, Alex and Vasa, Rajesh and Barnett, Scott and Grundy, John and Abdelrazek, Mohamed},
booktitle = {Proceedings of the 42nd International Conference on Software Engineering},
doi = {10.1145/3377811.3380404},
month = {jun},
pages = {1584--1596},
publisher = {ACM},
title = {{Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow}},
year = {2020}
}
@inproceedings{Ridgeway:1998ud,
abstract = {Voting methods such as boosting and bagging provide substantial improvements in classification performance in many problem domains. However, the resulting predictions can prove inscrutable to end-users. This is especially problematic in domains such as medicine, where end-user acceptance often depends on the ability of a classifier to explain its reasoning. Here we propose a variant of the boosted na{\"{i}}ve Bayes classifier that facilitates explanations while retaining predictive performance.},
address = {New York, NY, USA},
author = {Ridgeway, Greg and Madigan, David and Richardson, Thomas and O'Kane, John},
booktitle = {Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining},
pages = {101--104},
publisher = {AAAI},
title = {{Interpretable Boosted Na{\"{i}}ve Bayes Classification}},
year = {1998}
}
@inproceedings{Dibia:2017iy,
abstract = {Existing research highlight the myriad of benefits realized when technology is sufficiently democratized and made accessible to non-technical or novice users. However, democratizing complex technologies such as artificial intelligence (AI) remains hard. In this work, we draw on theoretical underpinnings from the democratization of innovation, in exploring the design of maker kits that help introduce novice users to complex technologies. We report on our work designing TJBot: an open source cardboard robot that can be programmed using pre-built AI services. We highlight principles we adopted in this process (approachable design, simplicity, extensibility and accessibility), insights we learned from showing the kit at workshops (66 participants) and how users interacted with the project on GitHub over a 12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in attracting novice users (40{\%} of users who forked the project are new to GitHub) and a variety of demographics are interested in prototyping use cases such as home automation, task delegation, teaching and learning.},
address = {Denver, CO, USA},
archivePrefix = {arXiv},
arxivId = {1805.10723},
author = {Dibia, Victor and Cox, Aaron and Weisz, Justin},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
eprint = {1805.10723},
month = {may},
pages = {381--384},
publisher = {ACM},
title = {{Designing for Democratization: Introducing Novices to Artificial Intelligence Via Maker Kits}},
year = {2017}
}
@inproceedings{lin2018sentiment,
address = {Gothenburg, Sweden},
author = {Lin, Bin and Zampetti, Fiorella and Bavota, Gabriele and {Di Penta}, Massimiliano and Lanza, Michele and Oliveto, Rocco},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
doi = {10.1145/3180155.3180195},
month = {may},
pages = {94--104},
publisher = {ACM},
title = {{Sentiment analysis for software engineering: How far can we go?}},
year = {2018}
}
@techreport{Kitchenham:2007dd,
address = {Keele, UK},
author = {Kitchenham, Barbara and Charters, S},
booktitle = {EBSE Technical Report},
institution = {Software Engineering Group, Keele University and Department of Computer Science, University of Durham},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
year = {2007}
}
@article{Chillarege:1992tm,
abstract = {This paper describes orthogonal defect classification (ODC), a concept that enables in-process feedback to developers by extracting signatures on the development process from defects. The ideas are evolved from an earlier finding that demonstrates the use of semantic information from defects to extract cause-effect relationships in the development process. This finding is leveraged to develop a systematic framework for building measurement and analysis methods. This paper • defines ODC and discusses the necessary and sufficient conditions required to provide feedback to a developer; • illustrates the use of the defect type distribution to measure the progress of a product through a process; • illustrates the use of the defect trigger distribution to evaluate the effectiveness and eventually the completeness of verification processes such as inspection or testing; • provides sample results from pilot projects using ODC; •opens the doors to a wide variety of analysis techniques for providing effective and fast feedback based on the concepts of ODC. {\textcopyright}1992 IEEE},
author = {Chillarege, Ram and Bhandari, Inderpal S and Chaar, Jarir K and Halliday, Michael J and Ray, Bonnie K and Moebus, Diane S},
doi = {10.1109/32.177364},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {11},
pages = {943--956},
title = {{Orthogonal Defect Classification—A Concept for In-Process Measurements}},
volume = {18},
year = {1992}
}
@inproceedings{Hou:2013jf,
abstract = {Text categorization, automatically labeling natural language text with pre-defined semantic categories, is an essential task for managing the abundant online data. An example of such data in Software Engineering is the large, ever-growing volume of forum discussions on how to use particular APIs. We have conducted a study to explore the question as to how well machine learning algorithms can be applied to categorize API discussions based on their content. Our goal is two-fold: (1) Can a relatively straightforward algorithm such as Naive Bayes work sufficiently well for this task? (2) If yes, how can we control its performance? We have achieved the best test accuracy mean (TAM) of 94.1{\%} with our largest training data set for the AWT/Swing API, which consists of 833 forum discussions distributed over eight categories/topics. We have also investigated factors that impact classification accuracy, with the most important two being the size of the training set and multi-label documents (the phenomenon that some discussions involve more than one category). {\textcopyright}2013 IEEE.},
address = {Eindhoven, Netherlands},
author = {Hou, Daqing and Mo, Lingfeng},
booktitle = {Proceedings of the 29th International Conference on Software Maintenance},
doi = {10.1109/ICSM.2013.17},
keywords = {APIs,AWT/Swing,MALLET,Naive Bayes,Online Forums,Text Categorization},
month = {sep},
pages = {60--69},
publisher = {IEEE},
title = {{Content categorization of API discussions}},
year = {2013}
}
@article{Kotula:1998wp,
author = {Kotula, Jeffrey},
doi = {10.1109/52.663791},
issn = {0740-7459},
journal = {IEEE Software},
number = {2},
pages = {84--92},
title = {{Using patterns to create component documentation}},
volume = {15},
year = {1998}
}
@inproceedings{Quinlan:1999ue,
address = {Bled, Slovenia},
author = {Quinlan, J R},
booktitle = {Proceedings of the 9th International Workshop on Inductive Logic Programming},
doi = {10.1007/3-540-48751-4_3},
isbn = {3-54-066109-3},
issn = {1611-3349},
month = {jun},
pages = {15--18},
publisher = {Springer},
title = {{Some elements of machine learning}},
volume = {1634},
year = {1999}
}
@article{boyd2018just,
abstract = {Objective To compare voice-activated internet searches by smartphone (two digital assistants) with laptop ones for information and advice related to smoking cessation. Design Responses to 80 questions on a range of topics related to smoking cessation (including the FAQ from a NHS website), compared for quality. Setting Smartphone and internet searches as performed in New Zealand. Main outcome measures Ranked responses to the questions. Results Google laptop internet searches came first (or first equal) for best quality smoking cessation advice for 83{\%} (66/80) of the responses. Voiced questions to Google Assistant (“OK Google”) came first/first equal 76{\%} of the time vs Siri (Apple) at 28{\%}. Google and Google Assistant were statistically significantly better than Siri searches (odds ratio 12.4 and 8.5 respectively, p{\textless}0.0001 in each comparison). When asked FAQs from the National Health Service website, or to find information the Centers for Disease Control has made videos on, the best search results used expert sources 59{\%} (31/52) of the time, “some expertise” (eg, Wikipedia) 18{\%} of the time, but also magazines and other low quality sources 19{\%} of the time. Using all three methods failed to find relevant information 8{\%} (6/80) of the time, with Siri having the most failed responses (53{\%} of the time). Conclusion Google internet searches and Google Assistant were found to be significantly superior to the Siri digital assistant for smoking cessation information. While expert content was returned over half the time, there is still substantial room for improvement in how these software systems deliver smoking cessation advice.},
author = {Boyd, Matt and Wilson, Nick},
doi = {10.1371/journal.pone.0194811},
issn = {1932-6203},
journal = {PLoS ONE},
number = {3},
publisher = {Public Library of Science},
title = {{Just ask Siri? A pilot study comparing smartphone digital assistants and laptop Google searches for smoking cessation advice}},
volume = {13},
year = {2018}
}
@article{Sokolova:2009vu,
abstract = {This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier's evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies. {\textcopyright}2009 Elsevier Ltd. All rights reserved.},
author = {Sokolova, Marina and Lapalme, Guy},
doi = {10.1016/j.ipm.2009.03.002},
issn = {0306-4573},
journal = {Information Processing and Management},
keywords = {Machine Learning,Performance evaluation,Text classification},
number = {4},
pages = {427--437},
title = {{A systematic analysis of performance measures for classification tasks}},
volume = {45},
year = {2009}
}
@inproceedings{Sen:1995uk,
address = {Montreal, QC, Canada},
author = {Sen, Sandip and Knight, Leslie},
booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
month = {aug},
pages = {725--733},
publisher = {Morgan Kaufmann},
title = {{A genetic prototype learner}},
year = {1995}
}
@article{Verbeke:2011vo,
abstract = {Customer churn prediction models aim to detect customers with a high propensity to attrite. Predictive accuracy, comprehensibility, and justifiability are three key aspects of a churn prediction model. An accurate model permits to correctly target future churners in a retention marketing campaign, while a comprehensible and intuitive rule-set allows to identify the main drivers for customers to churn, and to develop an effective retention strategy in accordance with domain knowledge. This paper provides an extended overview of the literature on the use of data mining in customer churn prediction modeling. It is shown that only limited attention has been paid to the comprehensibility and the intuitiveness of churn prediction models. Therefore, two novel data mining techniques are applied to churn prediction modeling, and benchmarked to traditional rule induction techniques such as C4.5 and RIPPER. Both AntMiner+ and ALBA are shown to induce accurate as well as comprehensible classification rule-sets. AntMiner+ is a high performing data mining technique based on the principles of Ant Colony Optimization that allows to include domain knowledge by imposing monotonicity constraints on the final rule-set. ALBA on the other hand combines the high predictive accuracy of a non-linear support vector machine model with the comprehensibility of the rule-set format. The results of the benchmarking experiments show that ALBA improves learning of classification techniques, resulting in comprehensible models with increased performance. AntMiner+ results in accurate, comprehensible, but most importantly justifiable models, unlike the other modeling techniques included in this study. {\textcopyright}2010 Elsevier Ltd. All rights reserved.},
author = {Verbeke, Wouter and Martens, David and Mues, Christophe and Baesens, Bart},
doi = {10.1016/j.eswa.2010.08.023},
issn = {0957-4174},
journal = {Expert Systems with Applications},
keywords = {ALBA,Ant Colony Optimization,Churn prediction,Classification,Comprehensible rule induction,Data mining},
number = {3},
pages = {2354--2364},
title = {{Building comprehensible customer churn prediction models with advanced rule induction techniques}},
volume = {38},
year = {2011}
}
@article{Ashby:1957db,
author = {Ashby, W Ross and Pierce, J R},
journal = {Physics Today},
month = {jul},
number = {7},
pages = {34--36},
title = {{An Introduction to Cybernetics}},
volume = {10},
year = {1957}
}
@article{BenDavid:1995up,
abstract = {Decision trees that are based on information-theory are useful paradigms for learning from examples. However, in some real-world applications, known information-theoretic methods frequently generate nonmonotonic decision trees, in which objects with better attribute values are sometimes classified to lower classes than objects with inferior values. This property is undesirable for problem solving in many application domains, such as credit scoring and insurance premium determination, where monotonicity of subsequent classifications is important. An attribute-selection metric is proposed here that takes both the error as well as monotonicity into account while building decision trees. The metric is empirically shown capable of significantly reducing the degree of non-monotonicity of decision trees without sacrificing their inductive accuracy. {\textcopyright}1995, Kluwer Academic Publishers. All rights reserved.},
author = {Ben-David, Arie},
doi = {10.1023/A:1022655006810},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {accuracy,consistency,information theory,monotonic classification problems,monotonic decision trees},
number = {1},
pages = {29--43},
title = {{Monotonicity Maintenance in Information-Theoretic Machine Learning Algorithms}},
volume = {19},
year = {1995}
}
@article{Liu:2018fa,
abstract = {CONTEXT.— Nodal metastasis of a primary tumor influences therapy decisions for a variety of cancers. Histologic identification of tumor cells in lymph nodes can be laborious and error-prone, especially for small tumor foci. OBJECTIVE.— To evaluate the application and clinical implementation of a state-of-the-art deep learning-based artificial intelligence algorithm (LYmph Node Assistant or LYNA) for detection of metastatic breast cancer in sentinel lymph node biopsies. DESIGN.— Whole slide images were obtained from hematoxylin-eosin-stained lymph nodes from 399 patients (publicly available Camelyon16 challenge dataset). LYNA was developed by using 270 slides and evaluated on the remaining 129 slides. We compared the findings to those obtained from an independent laboratory (108 slides from 20 patients/86 blocks) using a different scanner to measure reproducibility. RESULTS.— LYNA achieved a slide-level area under the receiver operating characteristic (AUC) of 99{\%} and a tumor-level sensitivity of 91{\%} at 1 false positive per patient on the Camelyon16 evaluation dataset. We also identified 2 "normal" slides that contained micrometastases. When applied to our second dataset, LYNA achieved an AUC of 99.6{\%}. LYNA was not affected by common histology artifacts such as overfixation, poor staining, and air bubbles. CONCLUSIONS.— Artificial intelligence algorithms can exhaustively evaluate every tissue patch on a slide, achieving higher tumor-level sensitivity than, and comparable slide-level performance to, pathologists. These techniques may improve the pathologist's productivity and reduce the number of false negatives associated with morphologic detection of tumor cells. We provide a framework to aid practicing pathologists in assessing such algorithms for adoption into their workflow (akin to how a pathologist assesses immunohistochemistry results).},
author = {Liu, Yun and Kohlberger, Timo and Norouzi, Mohammad and Dahl, George E and Smith, Jenny L and Mohtashamian, Arash and Olson, Niels and Peng, Lily H and Hipp, Jason D and Stumpe, Martin C},
doi = {10.5858/arpa.2018-0147-OA},
issn = {1543-2165},
journal = {Archives of Pathology {\&} Laboratory Medicine},
month = {jul},
number = {7},
pages = {859--868},
pmid = {30295070},
title = {{Artificial Intelligence-Based Breast Cancer Nodal Metastasis Detection.}},
volume = {143},
year = {2017}
}
@techreport{McCall:1977uy,
abstract = {An hierarchical definition of factors affecting software quality was compiled after an extensive literature search. The definition covers the complete range of software development and is broken down into non-oriented and software-oriented characteristics. For the lowest level of the software-oriented factors, metrics were developed that would be independent of the programming language. These measurable criteria were collected and validated using actual Air Force data bases. A handbook was generated that will be useful to Air Force acquisition managers for specifying the overall quality of a software system.},
address = {Griffiss Air Force Base, NY, USA},
author = {McCall, Jim A and Richards, Paul K and Walters, Gene F},
booktitle = {Technical Report: Rome Air Development Center, Air Force Systems Command},
institution = {General Electric Company},
month = {nov},
number = {RADC-TR-77-369},
pages = {689--1699},
title = {{Factors in Software Quality: Concept and Definitions of Software Quality}},
volume = {1},
year = {1977}
}
@inproceedings{Eykholt:2018vk,
abstract = {Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100{\%} of the images obtained in lab settings, and in 84.8{\%} of the captured video frames obtained on a moving vehicle (field test) for the target classifier.},
address = {Honolulu, HI, USA},
author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
booktitle = {Proceedings of the 2017 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00175},
isbn = {978-1-53-866420-9},
issn = {1063-6919},
month = {jul},
pages = {1625--1634},
title = {{Robust Physical-World Attacks on Deep Learning Visual Classification}},
year = {2018}
}
@inproceedings{Shepperd:2018hr,
abstract = {Context: There is growing interest in establishing software engineering as an evidence-based discipline. To that end, replication is often used to gain confidence in empirical findings, as opposed to reproduction where the goal is showing the correctness, or validity of the published results. Objective: To consider what is required for a replication study to confirm the original experiment and apply this understanding in software engineering. Method: Simulation is used to demonstrate why the prediction interval for confirmation can be surprisingly wide. This analysis is applied to three recent replications. Results: It is shown that because the prediction intervals are wide, almost all replications are confirmatory, so in that sense there is no 'replication crisis', however, the contributions to knowledge are negligible. Conclusion: Replicating empirical software engineering experiments, particularly if they are under-powered or under-reported, is a waste of scientific resources. By contrast, meta-analysis is strongly advocated so that all relevant experiments are combined to estimate the population effect.},
address = {Gothenburg, Sweden},
archivePrefix = {arXiv},
arxivId = {1802.04580},
author = {Shepperd, Martin},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
doi = {10.1145/3183399.3183423},
eprint = {1802.04580},
isbn = {978-1-45-035662-6},
issn = {0270-5257},
keywords = {Software engineering,empirical study,evidence,replication},
month = {may},
pages = {73--76},
publisher = {ACM},
title = {{Replication studies considered harmful}},
year = {2018}
}
@book{demeyer2008software,
abstract = {Software has become omnipresent and vital in our information-based society, so all software producers should assume responsibility for its reliability. While "reliable" originally assumed implementations that were effective and mainly error-free, additional issues like adaptability and maintainability have gained equal importance recently. For example, the 2004 ACM/IEEE Software Engineering Curriculum Guidelines list software evolution as one of ten key areas of software engineering education.Mens and Demeyer, both international authorities in the field of software evolution, together with the invited contributors, focus on novel trends in software evolution research and its relations with other emerging disciplines such as model-driven software engineering, service-oriented software development, and aspect-oriented software development. They do not restrict themselves to the evolution of source code but also address the evolution of other, equally important software artifacts such as databases and database schemas, design models, software architectures, and process management. The contributing authors provide broad overviews of related work, and they also contribute to a comprehensive glossary, a list of acronyms, and a list of books, journals, websites, standards and conferences that together represent the community's body of knowledge. Combining all these features, this book is the indispensable source for researchers and professionals looking for an introduction and comprehensive overview of the state of the art. In addition, it is an ideal basis for an advanced course on software evolution. {\textcopyright}Springer-Verlag Berlin Heidelberg 2008.},
address = {Berlin, Heidelberg},
author = {Mens, Tom and Demeyer, Serge},
doi = {10.1007/978-3-540-76440-3},
isbn = {978-3-54-076439-7},
publisher = {Springer},
title = {{Software Evolution}},
year = {2008}
}
@inproceedings{Barnett:2015ut,
abstract = {Modern IDEs provide limited support for developers when starting a new data-driven mobile app. App developers are currently required to write copious amounts of boilerplate code, scripts, organise complex directories, and author actual functionality. Although this scenario is ripe for automation, current tools are yet to address it adequately. In this paper we present RAPPT, a tool that generates the scaffolding of a mobile app based on a high level description specified in a Domain Specific Language (DSL). We demonstrate the feasibility of our approach by an example case study and feedback from a professional development team. Demo at: https://www.youtube.com/watch?v=ffquVgBYpLM.},
address = {Florence, Italy},
author = {Barnett, Scott and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 37th International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.216},
isbn = {978-1-47-991934-5},
issn = {0270-5257},
keywords = {Code Generation,Mobile App Prototyping,Model Driven Development},
month = {may},
pages = {657--660},
publisher = {IEEE},
title = {{Bootstrapping Mobile App Development}},
year = {2015}
}
@article{Bellazzi:2008tv,
abstract = {Background: The widespread availability of new computational methods and tools for data analysis and predictive modeling requires medical informatics researchers and practitioners to systematically select the most appropriate strategy to cope with clinical prediction problems. In particular, the collection of methods known as 'data mining' offers methodological and technical solutions to deal with the analysis of medical data and construction of prediction models. A large variety of these methods requires general and simple guidelines that may help practitioners in the appropriate selection of data mining tools, construction and validation of predictive models, along with the dissemination of predictive models within clinical environments. Purpose: The goal of this review is to discuss the extent and role of the research area of predictive data mining and to propose a framework to cope with the problems of constructing, assessing and exploiting data mining models in clinical medicine. Methods: We review the recent relevant work published in the area of predictive data mining in clinical medicine, highlighting critical issues and summarizing the approaches in a set of learned lessons. Results: The paper provides a comprehensive review of the state of the art of predictive data mining in clinical medicine and gives guidelines to carry out data mining studies in this field. Conclusions: Predictive data mining is becoming an essential instrument for researchers and clinical practitioners in medicine. Understanding the main issues underlying these methods and the application of agreed and standardized procedures is mandatory for their deployment and the dissemination of results. Thanks to the integration of molecular and clinical data taking place within genomic medicine, the area has recently not only gained a fresh impulse but also a new set of complex problems it needs to address. {\textcopyright}2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Bellazzi, Riccardo and Zupan, Blaz},
doi = {10.1016/j.ijmedinf.2006.11.006},
issn = {1386-5056},
journal = {International Journal of Medical Informatics},
keywords = {Clinical medicine,Data analysis,Data mining,Data mining process,Predictive models},
number = {2},
pages = {81--97},
title = {{Predictive data mining in clinical medicine: Current issues and guidelines}},
volume = {77},
year = {2008}
}
@book{Tassey:2002vu,
abstract = {N/A},
author = {Tassey, Gregory},
booktitle = {National Institute of Standards and Technology (NIST)},
doi = {10.1080/10438590500197315},
isbn = {978-0-75-672618-8},
month = {sep},
publisher = {National Institute of Standards and Technology},
title = {{The economic impacts of inadequate infrastructure for software testing}},
year = {2002}
}
@inproceedings{Ribeiro:2016gg,
address = {San Francisco, CA, USA},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {2939672.2939778},
month = {aug},
pages = {1135--1144},
publisher = {ACM},
title = {{`Why Should I Trust You?': Explaining the Predictions of Any Classifier}},
year = {2016}
}
@article{Arnold2019FactSheets:Conformity,
author = {Arnold, M and Piorkowski, D and Reimer, D and Richards, J and Tsay, J and Varshney, K R and Bellamy, R K E and Hind, M and Houde, S and Mehta, S and Mojsilovic, A and Nair, R and Ramamurthy, K Natesan and Olteanu, A},
doi = {10.1147/JRD.2019.2942288},
journal = {IBM Journal of Research and Development},
number = {4-5},
pages = {6:1 -- 6:13},
publisher = {IBM Corporation},
title = {{FactSheets: Increasing trust in AI services through supplier's declarations of conformity}},
volume = {63},
year = {2019}
}
@inproceedings{Hasan2014UsingMessages,
address = {New York, NY, USA},
author = {Hasan, Maryam and Agu, Emmanuel and Rundensteiner, Elke},
booktitle = {Proceedings of the 2014 ACM SIGKDD Workshop on Healthcare Informatics},
month = {aug},
pages = {187--193},
publisher = {ACM},
title = {{Using Hashtags as Labels for Supervised Learning of Emotions in Twitter Messages}},
year = {2014}
}
@inproceedings{VanAssche:2007wc,
abstract = {Ensemble methods are popular learning methods that are usually able to increase the predictive accuracy of a classifier. On the other hand, this comes at the cost of interpretability, and insight in the decision process of an ensemble is hard to obtain. This is a major reason why ensemble methods have not been extensively used in the setting of inductive logic programming. In this paper we aim to overcome this issue of comprehensibility by learning a single first order interpretable model that approximates the first order ensemble. The new model is obtained by exploiting the class distributions predicted by the ensemble. These are employed to compute heuristics for deciding which tests are to be used in the new model. As such we obtain a model that is able to give insight in the decision process of the ensemble, while being more accurate than the single model directly learned on the data. {\textcopyright}2008 Springer-Verlag Berlin Heidelberg.},
address = {Corvallis, OR, USA},
author = {{Van Assche}, Anneleen and Blockeel, Hendrik},
booktitle = {Proceedings of the 17th International Conference on Inductive Logic Programming},
doi = {10.1007/978-3-540-78469-2_26},
isbn = {3-54-078468-3},
issn = {0302-9743},
keywords = {Comprehensibility,Ensembles,First order decision trees},
month = {jun},
pages = {269--279},
publisher = {Springer},
title = {{Seeing the forest through the trees learning a comprehensible model from a first order ensemble}},
year = {2007}
}
@inproceedings{Kim:2014ui,
abstract = {We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the "quintessential" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in inter-pretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.},
address = {Montreal, QC, Canada},
archivePrefix = {arXiv},
arxivId = {1503.01161},
author = {Kim, Been and Rudin, Cynthia and Shah, Julie},
booktitle = {Proceedings of the 28th Conference on Neural Information Processing Systems},
eprint = {1503.01161},
issn = {1049-5258},
month = {dec},
pages = {1952--1960},
title = {{The Bayesian case model: A generative approach for case-based reasoning and prototype classification}},
year = {2014}
}
@inproceedings{Haselbock:2018jd,
abstract = {Design space analysis is a method for identifying and organizing potential design options and related concepts. So far, we have used decision models for the design space analysis of various areas of microservice design. Based on the feedback we have received, we refine our approach for design space analysis and extend it to support decision documentation. To validate the refined design space analysis approach and the approach we developed for decision documentation, we conduct a case study of microservice API management together with an industry partner. We present the identified design spaces and decision models created during the design space analysis, and show how the decision models were used for decision documentation. In addition, we draw general conclusions from applying the presented approach and concepts in an industrial context.},
address = {Paris, France},
author = {Haselbock, Stefan and Weinreich, Rainer and Buchgeher, Georg and Kriechbaum, Thomas},
booktitle = {Proceedings of the 11th International Conference on Service-Oriented Computing and Applications},
doi = {10.1109/SOCA.2018.00008},
keywords = {Decision models,Design decision documentation,Design space analysis,Microservices API management},
month = {nov},
pages = {1--8},
title = {{Microservice Design Space Analysis and Decision Documentation: A Case Study on API Management}},
year = {2019}
}
@inproceedings{sculley2011detecting,
address = {San Diego, CA, USA},
author = {Sculley, D and Otey, Matthew Eric and Pohl, Michael and Spitznagel, Bridget and Hainsworth, John and Zhou, Yunkai},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2020408.2020455},
month = {aug},
pages = {274--282},
publisher = {ACM},
title = {{Detecting adversarial advertisements in the wild}},
year = {2011}
}
@inproceedings{Subramanian:2014bg,
abstract = {Application Programming Interfaces (APIs) provide powerful abstraction mechanisms that enable complex functionality to be used by client programs. However, this abstraction does not come for free: understanding how to use an API can be difficult. While API documentation can help, it is often insufficient on its own. Online sites like Stack Overflow and Github Gists have grown to fill the gap between traditional API documentation and more example-based resources. Unfortunately, these two important classes of documentation are independent. In this paper we describe an iterative, deductive method of linking source code examples to API documentation. We also present an implementation of this method, called Baker, that is highly precise (0.97) and supports both Java and JavaScript. Baker can be used to enhance traditional API documentation with up-to-date source code examples; it can also be used to incorporate links to the API documentation into the code snippets that use the API.},
address = {Hyderabad, India},
author = {Subramanian, Siddharth and Inozemtseva, Laura and Holmes, Reid},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
doi = {10.1145/2568225.2568313},
issn = {0270-5257},
keywords = {Source code examples,documentation,source code search},
month = {may},
pages = {643--652},
publisher = {ACM},
title = {{Live API documentation}},
year = {2014}
}
@inproceedings{murgia2014,
address = {Hyderabad, India},
author = {Murgia, Alessandro and Tourani, Parastou and Adams, Bram and Ortu, Marco},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
doi = {10.1145/2597073.2597086},
month = {may},
pages = {262--271},
publisher = {ACM},
title = {{Do developers feel emotions? an exploratory analysis of emotions in software artifacts}},
year = {2014}
}
@inproceedings{Zhang:2008vfa,
abstract = {Text extraction in video documents, as an important research field of content-based information indexing and retrieval, has been developing rapidly since 1990s. This has led to much progress in text extraction, performance evaluation, and related applications. By reviewing the approaches proposed during the past five years, this paper introduces the progress made in this area and discusses promising directions for future research.},
address = {Nara, Japan},
author = {Zhang, Jing and Kasturi, Rangachar},
booktitle = {Proceedings of the 8th International Workshop on Document Analysis Systems},
doi = {10.1109/das.2008.49},
month = {sep},
pages = {5--17},
publisher = {IEEE},
title = {{Extraction of Text Objects in Video Documents: Recent Progress}},
year = {2008}
}
@inproceedings{Mitchell:2018in,
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
address = {Atlanta, GA, USA},
archivePrefix = {arXiv},
arxivId = {1810.03993},
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
booktitle = {Proceedings of the 2nd Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3287560.3287596},
eprint = {1810.03993},
isbn = {978-1-45-036125-5},
keywords = {Datasheets,Disaggregated evaluation,Documentation,Ethical considerations,Fairness evaluation,ML model evaluation,Model cards},
month = {jan},
pages = {220--229},
publisher = {ACM},
title = {{Model cards for model reporting}},
year = {2019}
}
@misc{ISO25010:2011,
author = {{International Organization for Standardization}},
title = {{ISO/IEC 25010:2011 Systems and software engineering – Systems and software Quality Requirements and Evaluation (SQuaRE) – System and software quality models}},
url = {http://bit.ly/2S4yzGs},
year = {2011}
}
@inproceedings{Bigham2006,
abstract = {Images without alternative text are a barrier to equal web access for blind users. To illustrate the problem, we conducted a series of studies that conclusively show that a large fraction of significant images have no alternative text. To ameliorate this problem, we introduce WebInSight, a system that automatically creates and inserts alternative text into web pages on-the-fly. To formulate alternative text for images, we present three labeling modules based on web context analysis, enhanced optical character recognition (OCR) and human labeling. The system caches alternative text in a local database and can add new labels seamlessly after a web page is downloaded, resulting in minimal impact to the browsing experience. Copyright 2006 ACM.},
address = {Portland, OR, USA},
author = {Bigham, Jeffrey P. and Kaminsky, Ryan S. and Ladner, Richard E. and Danielsson, Oscar M. and Hempton, Gordon L.},
booktitle = {Proceedings of the 8th International ACM SIGACCESS Conference on Computers and Accessibility},
doi = {10.1145/1168987.1169018},
keywords = {Optical character recognition,Transformation proxy,Web accessibility,Web studies},
month = {oct},
pages = {181--188},
publisher = {ACM},
title = {{WebInSight: Making web images accessible}},
year = {2006}
}
@article{mcleod2011factors,
abstract = {Determining the factors that have an influence on software systems development and deployment project outcomes has been the focus of extensive and ongoing research for more than 30 years. We provide here a survey of the research literature that has addressed this topic in the period 1996-2006, with a particular focus on empirical analyses. On the basis of this survey we present a new classification framework that represents an abstracted and synthesized view of the types of factors that have been asserted as influencing project outcomes. {\textcopyright}2011 ACM.},
author = {McLeod, Laurie and MacDonell, Stephen G},
doi = {10.1145/1978802.1978803},
issn = {0360-0300},
journal = {ACM Computing Surveys},
keywords = {Development processes,Institutional context,People and action,Project content,Project outcomes},
number = {4},
pages = {24},
publisher = {ACM},
title = {{Factors that affect software systems development project outcomes: A survey of research}},
volume = {43},
year = {2011}
}
@article{Bouwers2010,
abstract = {Architecture evaluations offer many benefits, including the early detection of problems and a better understanding of a system's possibilities. Although many methods for evaluating architectures are available, studies have shown that industry's adoption of architecture evaluations is low. A reason for this lack of adoption is the limited out-of-the-box process and tool support available to start performing architecture reviews. This article introduces the lightweight sanity check for implemented architectures (LiSCIA) evaluation method. LiSCIA can be used out of the box to perform a first architectural evaluation of a system. The check is based on years of experience in evaluating the maintainability of software systems. By periodically performing this check, developers and project managers can control the implemented architecture's erosion as the system (and its requirements) evolves over time. {\textcopyright} 2006 IEEE.},
author = {Bouwers, Eric and van Deursen, Arie},
doi = {10.1109/MS.2010.60},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {Architecture erosion,Software architecture evaluation,Software architectures,Software quality},
month = {jul},
number = {4},
pages = {44--50},
title = {{A Lightweight Sanity Check for Implemented Architectures}},
volume = {27},
year = {2010}
}
@inproceedings{McCarthy:1960:PCS:889202,
abstract = {Interesting work is being done in programming computers to solve problems which require a high degree of intelligence in humans. However, certain elementary verbal reasoning processes so simple that they can be carried out by any non-feeble minded human have yet to be simulated by machine programs.},
address = {Cambridge, MA, USA},
author = {McCarthy, J},
booktitle = {Proceedings of the Symposium on the Mechanization of Thought Processes},
pages = {1--15},
title = {{Programs with common sense}},
year = {1963}
}
@article{tversky1974judgment,
author = {Tversky, Amos and Kahneman, Daniel},
journal = {Science},
number = {4157},
pages = {1124--1131},
publisher = {American Association for the Advancement of Science},
title = {{Judgment under uncertainty: Heuristics and biases}},
volume = {185},
year = {1974}
}
@book{Crosby:1979uy,
abstract = {Nontechnical in approach, this how-to manual for managers with accountability for product performance specifies ways in which quality problems can be prevented at each stage of production},
author = {Crosby, Philip B},
isbn = {978-0-07-014512-2},
publisher = {McGraw-Hill},
title = {{Quality is free: The art of making quality certain}},
year = {1979}
}
@inproceedings{Arpteg2018,
abstract = {Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.},
address = {Prague, Czech Republic},
author = {Arpteg, Anders and Brinne, Bj{\"{o}}rn and Crnkovic-Friis, Luka and Bosch, Jan},
booktitle = {Proceedings of the 44th Euromicro Conference on Software Engineering and Advanced Applications},
doi = {10.1109/SEAA.2018.00018},
isbn = {978-1-53-867382-9},
keywords = {Artificial intelligence,Deep learning,Machine learning,Software engineering challenges},
month = {aug},
pages = {50--59},
publisher = {IEEE},
title = {{Software engineering challenges of deep learning}},
year = {2018}
}
@misc{Jimerson:2017vh,
address = {San Francisco, CA, USA},
author = {Jimerson, Brian and Gregory, Brian},
month = {dec},
title = {{Pivotal Cloud Foundry, Google ML, and Spring}},
url = {http://bit.ly/2RUBIIL},
year = {2017}
}
@techreport{RightScaleInc:2018kJ,
abstract = {In January 2016, RightScale surveyed 1,060 technical professionals across a broad cross-section of organizations about their adoption of cloud computing. The company published its annual State of the Cloud Report on February 9, 2016. We also asked a number of additional questions about their adoption of DevOps and use of DevOps tools, including Docker. In this DevOps Trends report, we offer a deep dive into those responses as well as some additional analysis about DevOps. The 2016 State of the Cloud Survey identified several key findings: DevOps growing especially in the enterprise. • DevOps adoption increased from 66 percent in 2015 to 74 percent in 2016. • DevOps adoption is strongest in the enterprise (81 percent of enterprises adopting DevOps compared to 70 percent in SMBs). • Enterprises are adopting DevOps from the bottom up: projects or teams (29 percent) and business units or divisions (31 percent), company-wide (21 percent). Docker},
author = {{RightScale Inc.}},
pages = {1--19},
title = {{State of the Cloud Report: DevOps Trends}},
year = {2016}
}
@article{Dromey:1995wy,
abstract = {A model for software product quality is defined. It has been formulated by associating a set of quality-carrying properties with each of the structural forms that are used to define the statements and statement components of a programming language. These quality-carrying properties are in turn linked to the high-level quality attributes of the International Standard for Software Product Evaluation ISO-9126. The model supports building quality into software, definition of language-specific coding standards, systematically classifying quality defects, and the development of automated code auditors for detecting defects in software.},
author = {Dromey, R Geoff},
doi = {10.1109/32.345830},
isbn = {978-1-11-815666-7},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {2},
pages = {146--162},
title = {{A model for software product quality}},
volume = {21},
year = {1995}
}
@inproceedings{Ahasanuzzaman:2018kv,
abstract = {The design and maintenance of APIs are complex tasks due to the constantly changing requirements of its users. Despite the efforts of its designers, APIs may suffer from a number of issues (such as incomplete or erroneous documentation, poor performance, and backward incompatibility). To maintain a healthy client base, API designers must learn these issues to fix them. Question answering sites, such as Stack Overflow (SO), has become a popular place for discussing API issues. These posts about API issues are invaluable to API designers, not only because they can help to learn more about the problem but also because they can facilitate learning the requirements of API users. However, the unstructured nature of posts and the abundance of non-issue posts make the task of detecting SO posts concerning API issues difficult and challenging. In this paper, we first develop a supervised learning approach using a Conditional Random Field (CRF), a statistical modeling method, to identify API issue-related sentences. We use the above information together with different features of posts and experience of users to build a technique, called CAPS, that can classify SO posts concerning API issues. Evaluation of CAPS using carefully curated SO posts on three popular API types reveals that the technique outperforms all three baseline approaches we consider in this study. We also conduct studies to test the generalizability of CAPS results and to understand the effects of different sources of information on it.},
address = {Campobasso, Italy},
author = {Ahasanuzzaman, Md and Asaduzzaman, Muhammad and Roy, Chanchal K and Schneider, Kevin A},
booktitle = {Proceedings of the 25th International Conference on Software Analysis, Evolution and Reengineering},
doi = {10.1109/SANER.2018.8330213},
isbn = {978-1-53-864969-5},
keywords = {API Issue,Stack Overflow,feature extraction,text classification,unstructured data mining},
month = {mar},
pages = {244--254},
publisher = {IEEE},
title = {{Classifying stack overflow posts on API issues}},
year = {2018}
}
@article{DBLP:journals/corr/abs-1907-04135,
abstract = {A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.},
archivePrefix = {arXiv},
arxivId = {1907.04135},
author = {Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Viegas, Fernanda and Wilson, Jimbo},
doi = {10.1109/tvcg.2019.2934619},
eprint = {1907.04135},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {1},
pages = {56--65},
title = {{The What-If Tool: Interactive Probing of Machine Learning Models}},
volume = {26},
year = {2019}
}
@inproceedings{Ross:2017vn,
abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classifiers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
address = {Melbourne, Australia},
archivePrefix = {arXiv},
arxivId = {1703.03717},
author = {Ross, Andrew Slavin and Hughes, Michael C and Doshi-Velez, Finale},
booktitle = {Proceedings of the 26th International Joint Conferences on Artificial Intelligence},
doi = {10.24963/ijcai.2017/371},
eprint = {1703.03717},
isbn = {978-0-99-924110-3},
issn = {1045-0823},
month = {aug},
pages = {2662--2670},
title = {{Right for the right reasons: Training differentiable models by constraining their explanations}},
year = {2017}
}
@inproceedings{LinaresVasquez:2014vj,
abstract = {The growing number of questions related to mobile development in StackOverow highlights an increasing interest of software developers in mobile programming. For the Android platform, 213,836 questions were tagged with Android-related labels in StackOverow between July 2008 and August 2012. This paper aims at investigating how changes occurring to Android APIs trigger questions and activity in StackOverflow, and whether this is particularly true for certain kinds of changes. Our findings suggest that Android developers usually have more questions when the behavior of APIs is modified. In addition, deleting public methods from APIs is a trigger for questions that are (i) more discussed and of major interest for the community, and (ii) posted by more experienced developers. In general, results of this paper provide important insights about the use of social media to learn about changes in software ecosystems, and establish solid foundations for building new recommenders for notifying developers/managers about important changes and recommending them relevant crowdsourced solutions.},
address = {Hyderabad, India},
author = {Linares-V{\'{a}}squez, Mario and Bavota, Gabriele and {Di Penta}, Massimiliano and Oliveto, Rocco and Poshyvanyk, Denys},
booktitle = {Proceedings of the 22nd International Conference on Program Comprehension},
doi = {10.1145/2597008.2597155},
isbn = {978-1-45-032879-1},
keywords = {API Changes,Android,Social Media,StackOverflow},
month = {jun},
pages = {83--94},
publisher = {ACM},
title = {{How do API changes trigger stack overflow discussions? A study on the android SDK}},
year = {2014}
}
@inproceedings{Parnas:2007fb,
abstract = {This experience and research based paper discusses the reasons that software cannot be trusted and then explains how the use of greatly improved documentation can make software more trustworthy. It shows how tabular expressions can be used to prepare software documents that are both precise and easily used by developers, inspectors, and testers. The paper reviews a number of "tried and true" ideas and illustrates some new refinements in the methods that resulted from recent research. It is intended both to tell developers of techniques available to them and to suggest new research areas. {\textcopyright} 2007 IEEE.},
address = {Plano, TX, USA},
author = {Parnas, David L. and Vilkomir, Sergiy A.},
booktitle = {Proceedings of 10th IEEE International Symposium on High Assurance Systems Engineering},
doi = {10.1109/HASE.2007.63},
issn = {1530-2059},
keywords = {Critical software,Documentation,Specifications,Testing},
month = {nov},
pages = {237--244},
publisher = {IEEE},
title = {{Precise documentation of critical software}},
year = {2007}
}
@article{5416726,
abstract = {BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they may do so in different ways. This provides evidence that, in this context at least, the systematic review is a robust research method. {\textcopyright}2010 IEEE.},
author = {MacDonell, Stephen and Shepperd, Martin and Kitchenham, Barbara and Mendes, Emilia},
doi = {10.1109/TSE.2010.28},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Empirical software engineering,cost estimation,meta-analysis,systematic review},
month = {sep},
number = {5},
pages = {676--687},
title = {{How reliable are systematic reviews in empirical software engineering?}},
volume = {36},
year = {2010}
}
@inproceedings{tu2000evolution,
abstract = {Most studies of software evolution have been performed on systems developed within a single company using traditional management techniques. With the widespread availability of several large software systems that have been developed using an 'open source' development approach, we now have a chance to examine these systems in detail, and see if their evolutionary narratives are significantly different from commercially developed systems. This paper summarizes our preliminary investigations into the evolution of the best known open source system: the Linux operating system kernel. Because Linux is large (over two million lines of code in the most recent version) and because its development model is not as tightly planned and managed as most industrial software processes, we had expected to find that Linux was growing more slowly as it got bigger and more complex. Instead, we have found that Linux has been growing at a super-linear rate for several years. In this paper, we explore the evolution of the Linux kernel both at the system level and within the major subsystems, and we discuss why we think Linux continues to exhibit such strong growth.},
address = {San Jose, CA, USA},
author = {Godfrey, Michael W and Tu, Qiang},
booktitle = {Conference on Software Maintenance},
doi = {10.1109/icsm.2000.883030},
month = {aug},
organization = {IEEE},
pages = {131--142},
title = {{Evolution in open source software: a case study}},
year = {2000}
}
@article{Lethbridge:2005jv,
abstract = {Software engineering is an intensively people-oriented activity, yet too little is known about how designers, maintainers, requirements analysts and all other types of software engineers perform their work. In order to improve software engineering tools and practice, it is therefore essential to conduct field studies, i.e. to study real practitioners as they solve real problems. To do so effectively, however, requires an understanding of the techniques most suited to each type of field study task. In this paper, we provide a taxonomy of techniques, focusing on those for data collection. The taxonomy is organized according to the degree of human intervention each requires. For each technique, we provide examples from the literature, an analysis of some of its advantages and disadvantages, and a discussion of how to use it effectively. We also briefly talk about field study design in general, and data analysis. {\textcopyright}2005 Springer Science + Business Media, Inc.},
author = {Lethbridge, Timothy C and Sim, Susan Elliott and Singer, Janice},
doi = {10.1007/s10664-005-1290-x},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Empirical software engineering,Field studies,Work practices},
month = {jul},
number = {3},
pages = {311--341},
title = {{Studying software engineers: Data collection techniques for software field studies}},
volume = {10},
year = {2005}
}
@inproceedings{Iyengar:2017fb,
abstract = {A wide variety of services are available over the Web which can dramatically improve the functionality of applications. These services include information retrieval (including data lookups from a variety of sources and Web searches), natural language understanding, visual recognition, and data storage. A key problem is how to provide support for applications which use these services. This paper presents a rich software development kit (SDK) which accesses these services and provides a variety of features applications need to use these services, optimize performance, and compare them. A key aspect of our SDK is its support for natural language understanding services. We also present a personalized knowledge base built on top of our rich SDK that uses publically available data sources as well as private information. The knowledge base supports data analysis and reasoning over data.},
address = {Atlanta, GA, USA},
author = {Iyengar, Arun},
booktitle = {Proceedings of the 37th International Conference on Distributed Computing Systems},
doi = {10.1109/ICDCS.2017.172},
isbn = {978-1-53-861791-5},
keywords = {Cloud client,Cloud service ranking,Cognitive client,Cognitive services,Service ranking,Software development kit},
month = {jun},
pages = {1856--1864},
publisher = {IEEE},
title = {{Supporting Data Analytics Applications Which Utilize Cognitive Services}},
year = {2017}
}
@inproceedings{LaForge:2018tm,
address = {London, England, UK},
author = {Laforge, Guillaume},
booktitle = {QCon},
month = {jun},
title = {{Machine Intelligence at Google Scale}},
url = {http://bit.ly/2S0xC1B},
year = {2018}
}
@article{Freitas:2014ic,
abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
author = {Freitas, Alex A},
doi = {10.1145/2594473.2594475},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
month = {mar},
number = {1},
pages = {1--10},
title = {{Comprehensible classification models}},
volume = {15},
year = {2014}
}
@inproceedings{Aghajani:2019bo,
abstract = {(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavourable take on documentation. Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.},
address = {Montreal, QC, Canada},
author = {Aghajani, Emad and Nagy, Csaba and Vega-Marquez, Olga Lucero and Linares-Vasquez, Mario and Moreno, Laura and Bavota, Gabriele and Lanza, Michele},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
doi = {10.1109/ICSE.2019.00122},
isbn = {978-1-72-810869-8},
issn = {0270-5257},
keywords = {Documentation,Empirical Study},
month = {may},
pages = {1199--1210},
publisher = {IEEE},
title = {{Software Documentation Issues Unveiled}},
year = {2019}
}
@inproceedings{NIPS2019_9015,
address = {Vancouver, BC, Canada},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Proceedings of the 33rd International Conference on the Advances of Neural Information Processing Systems},
eprint = {1912.01703},
month = {dec},
pages = {8026--8037},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
year = {2019}
}
@article{Fleiss:1971ff,
author = {Fleiss, Joseph L},
doi = {10.1037/h0031619},
journal = {Psychological Bulletin},
number = {5},
pages = {378--382},
title = {{Measuring nominal scale agreement among many raters}},
volume = {76},
year = {1971}
}
@inproceedings{Shaw:2003aa,
address = {Portland, OR, USA},
author = {Shaw, M},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
isbn = {978-0-76-951877-0},
month = {may},
pages = {726--736},
publisher = {IEEE},
title = {{Writing good software engineering research papers}},
year = {2003}
}
@book{Weerawarana:2005wx,
abstract = {``Other books claim to present the complete Web services platform architecture, but this is the first one I've seen that really does. The authors have been intimately involved in the creation of the architecture. Who better to write this book?'' - Anne Thomas Manes, Vice President and Research Director, Burton Group ``This is a very important book, providing a lot of technical detail and background that very few (if any) other books will be able to provide. The list of authors includes some of the top experts in the various specifications covered, and they have done an excellent job explaining the background motivation for and pertinent details of each specification. The benefit of their perspectives and collective expertise alone make the book worth reading.'' - Eric Newcomer, CTO, IONA Technologies},
address = {Crawfordsville, IN, USA},
author = {Weerawarana, Sanjiva and Curbera, Francisco and Leymann, Frank and Storey, Tony and Ferguson, Donald F},
isbn = {0-13-148874-0},
pages = {456},
publisher = {Prentice-Hall},
title = {{Web Services Platform Architecture}},
year = {2005}
}
@article{Karwath:2002tv,
abstract = {Background: The inference of homology between proteins is a key problem in molecular biology The current best approaches only identify ∼50{\%} of homologies (with a false positive rate set at 1/1000). Results: We present Homology Induction (HI), a new approach to inferring homology. HI uses machine learning to bootstrap from standard sequence similarity search methods. First a standard method is run, then HI learns rules which are true for sequences of high similarity to the target (assumed homologues) and not true for general sequences, these rules are then used to discriminate sequences in the twilight zone. To learn the rules HI describes the sequences in a novel way based on a bioinformatic knowledge base, and the machine learning method of inductive logic programming. To evaluate HI we used the PDB40D benchmark which lists sequences of known homology but low sequence similarity. We compared the HI methodoly with PSI-BLAST alone and found HI performed significantly better. In addition, Receiver Operating Characteristic (ROC) curve analysis showed that these improvements were robust for all reasonable error costs. The predictive homology rules learnt by HI by can be interpreted biologically to provide insight into conserved features of homologous protein families. Conclusions: HI is a new technique for the detection of remote protein homolgy - a central bioinformatic problem. HI with PSI-BLAST is shown to outperform PSI-BLAST for all error costs. It is expect that similar improvements would be obtained using HI with any sequence similarity method. {\textcopyright}2002 Karwath and King; licensee BioMed Central Ltd.},
author = {Karwath, Andreas and King, Ross D},
doi = {10.1186/1471-2105-3-11},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {11},
title = {{Homology induction: The use of machine learning to improve sequence similarity searches}},
volume = {3},
year = {2002}
}
@incollection{Singer:2007tu,
author = {Singer, Janice and Sim, Susan E and Lethbridge, Timothy C},
booktitle = {Guide to Advanced Empirical Software Engineering},
chapter = {1},
doi = {10.1007/978-1-84800-044-5},
editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K.},
isbn = {978-1-84-800043-8},
month = {nov},
pages = {9--34},
publisher = {Springer},
title = {{Software engineering data collection for field studies}},
year = {2007}
}
@article{Biggs:2014ur,
abstract = {Incluye bibliograf{\'{i}}a e {\'{i}}ndice},
author = {Biggs, J and Collis, K},
doi = {10.1177/089202068700100412},
isbn = {0-12-097551-1},
issn = {0892-0206},
journal = {Management in Education},
number = {4},
pages = {20},
title = {{Evaluating the Quality of Learning: The SOLO Taxonomy (Structure of the Observed Learning Outcome)}},
volume = {1},
year = {1987}
}
@inproceedings{Cummaudo:2021semotion,
abstract = {Copyright {\textcopyright} 2020, arXiv, All rights reserved. Software developers are increasingly using machine learning APIs to implement ‘intelligent' features. Studies show that incorporating machine learning into an application increases technical debt, creates data dependencies, and introduces uncertainty due to non-deterministic behaviour. However, we know very little about the emotional state of software developers who deal with such issues. In this paper, we do a landscape analysis of emotion found in 1,245 Stack Overflow posts about computer vision APIs. We investigate the application of an existing emotion classifier EmoTxt and manually verify our results. We found that the emotion profile varies for different question categories.},
address = {Virtual Event, USA},
annote = {In Review},
author = {Cummaudo, A. and Graetsch, U.M. and Curumsing, M.K. and Barnett, S. and Vasa, R. and Grundy, J.},
booktitle = {Proceedings of the Sixth International Workshop on Emotion Awareness in Software Engineering},
keywords = {Computer vision,Emotion mining,Empirical study,Intelligent services,Pain points,Software developer emotions,Stack overflow},
publisher = {IEEE},
title = {{Manual and Automatic Emotion Analysis of Computer Vision Service Pain-Points}},
year = {2021}
}
@inproceedings{covington2016deep,
abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
address = {Boston, MA, USA},
author = {Covington, Paul and Adams, Jay and Sargin, Emre},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
doi = {10.1145/2959100.2959190},
isbn = {978-1-45-034035-9},
keywords = {Deep learning,Recommender system,Scalability},
month = {sep},
pages = {191--198},
publisher = {ACM},
title = {{Deep neural networks for youtube recommendations}},
year = {2016}
}
@inproceedings{Feelders:2000ve,
abstract = {A common form of prior knowledge in economic modelling concerns the monotonicity of relations between the dependent and explanatory variables. Monotonicity may also be an important requirement with a view toward explaining and justifying decisions based on such models. We explore the use of monotonicity constraints in classification tree algorithms.We present an application of monotonic classification trees to a problem in house pricing. In this preliminary study we found that the monotonic trees were only slightly worse in classification performance, but were much simpler than their non-monotonic counterparts.},
address = {Lyon, France},
author = {Feelders, A J},
booktitle = {Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery},
doi = {10.1007/3-540-45372-5_42},
isbn = {978-3-54-041066-9},
issn = {1611-3349},
month = {sep},
pages = {395--400},
publisher = {Springer},
title = {{Prior knowledge in economic applications of data mining}},
volume = {1910},
year = {2000}
}
@article{Gebru:2018wh,
abstract = {Currently there is no standard way to identify how a dataset was created, and what characteristics, motivations, and potential skews it represents. To begin to address this issue, we propose the concept of a datasheet for datasets, a short document to accompany public datasets, commercial APIs, and pretrained models. The goal of this proposal is to enable better communication between dataset creators and users, and help the AI community move toward greater transparency and accountability. By analogy, in computer hardware, it has become industry standard to accompany everything from the simplest components (e.g., resistors), to the most complex microprocessor chips, with datasheets detailing standard operating characteristics, test results, recommended usage, and other information. We outline some of the questions a datasheet for datasets should answer. These questions focus on when, where, and how the training data was gathered, its recommended use cases, and, in the case of human-centric datasets, information regarding the subjects' demographics and consent as applicable. We develop prototypes of datasheets for two well-known datasets: Labeled Faces in The Wild and the Pang {\$}\backslashbackslashbackslash{\{}\backslash{\{}{\}}\backslashbackslash{\{}\backslash{\$}{\}}{\{}$\backslash${\}}{\}}{\&} Lee Polarity Dataset.},
archivePrefix = {arXiv},
arxivId = {1803.09010},
author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume{\'{e}}, Hal and Crawford, Kate},
eprint = {1803.09010},
title = {{Datasheets for Datasets}},
year = {2018}
}
@book{pyle1999data,
author = {Pyle, Dorian},
edition = {1st},
isbn = {978-15-5-860529-9},
pages = {560},
publisher = {Morgan Kaufmann},
title = {{Data Preparation for Data Mining}},
year = {1994}
}
@article{Martens:2011uh,
abstract = {This paper proposes a complete framework to assess the overall performance of classification models from a user perspective in terms of accuracy, comprehensibility, and justifiability. A review is provided of accuracy and comprehensibility measures, and a novel metric is introduced that allows one to measure the justifiability of classification models. Furthermore, taxonomy of domain constraints is introduced, and an overview of the existing approaches to impose constraints and include domain knowledge in data mining techniques is presented. Finally, justifiability metric is applied to a credit scoring and customer churn prediction case. {\textcopyright}2011 Elsevier B.V. All rights reserved.},
author = {Martens, David and Vanthienen, Jan and Verbeke, Wouter and Baesens, Bart},
doi = {10.1016/j.dss.2011.01.013},
issn = {0167-9236},
journal = {Decision Support Systems},
keywords = {Classification,Comprehensibility,Data mining,Justifiability,Metrics},
number = {4},
pages = {782--793},
title = {{Performance of classification models from a user perspective}},
volume = {51},
year = {2011}
}
@book{Bramer:2007vg,
address = {London, England, UK},
author = {Bramer, Max},
doi = {10.1007/978-1-4471-7307-6},
isbn = {978-1-44-717306-9},
publisher = {Springer},
series = {Undergraduate Topics in Computer Science},
title = {{Principles of Data Mining}},
volume = {180},
year = {2016}
}
@incollection{Barzilay:2013cn,
abstract = {The open source community, as well as numerous technical blogs and community web sites, put online vast quantities of free source code, ranging from snippets to full-blown products. This code embodies the software development community's domain knowledge, and mirrors the structure of the Internet: it is distributed rather than hierarchical; it is chaotic, incomplete, and inconsistent. StackOverflow.com is a Question and Answer (Q{\&}A) website which uses social media to facilitate knowledge exchange between programmers by mitigating the pitfalls involved in using code from the Internet. Its design nurtures a community of developers, and enables crowd sourced software engineering activities ranging from documentation to providing useful, high quality code snippets to be used in production. In this chapter we review Stack Overflow from three perspectives: (1) its design and its social media characteristics, (2) the role it plays in the software documentation landscape, and (3) the use of Stack Overflow in the context of the example centric programming paradigm.},
author = {Barzilay, Ohad and Treude, Christoph and Zagalsky, Alexey},
booktitle = {Finding Source Code on the Web for Remix and Reuse},
doi = {10.1007/978-1-4614-6596-6_15},
isbn = {978-1-46-146596-6},
number = {4},
pages = {289--308},
title = {{Facilitating crowd sourced software engineering via stack overflow}},
year = {2014}
}
@article{Sendak2020PresentingLabels,
abstract = {There is tremendous enthusiasm surrounding the potential for machine learning to improve medical prognosis and diagnosis. However, there are risks to translating a machine learning model into clinical care and clinical end users are often unaware of the potential harm to patients. This perspective presents the “Model Facts” label, a systematic effort to ensure that front-line clinicians actually know how, when, how not, and when not to incorporate model output into clinical decisions. The “Model Facts” label was designed for clinicians who make decisions supported by a machine learning model and its purpose is to collate relevant, actionable information in 1-page. Practitioners and regulators must work together to standardize presentation of machine learning model information to clinical end users in order to prevent harm to patients. Efforts to integrate a model into clinical practice should be accompanied by an effort to clearly communicate information about a machine learning model with a “Model Facts” label.},
author = {Sendak, Mark P and Gao, Michael and Brajer, Nathan and Balu, Suresh},
doi = {10.1038/s41746-020-0253-3},
issn = {2398-6352},
journal = {npj Digital Medicine},
number = {1},
pages = {41},
title = {{Presenting machine learning model information to clinical end users with model facts labels}},
volume = {3},
year = {2020}
}
@article{McLellan:1998vu,
abstract = {Imagine hypothetically, just for a moment, that programmers are humans," writes Steven Pemberton in a July 1997 magazine devoted to human-computer interaction design and development. "Now suppose for a moment, also for the sake of the argument, that their chief method of communicating and interacting with computers was with programming languages. What would we, as HCI people, then do? Run screaming in the other direction...." 1 It is a good question and, unfortunately, an all too common response. It's hard enough for us to ensure that product interfaces, like those for Excel or Word, are easy to use and learn. But programmers are users, too. They need application and system libraries that are just as easy to learn and use as the products they build from these libraries. Listen to this customer: "I think it would be worthwhile if all developers would spend maybe a couple of hours a year seeing how the[ir] product is used by...customers. Just watching them. And while they're watching ...the customer would say, 'I don't like the way this works....'You need to see how they use it." 2 Now ask yourself: why is it easier to visualize the customer who's purchased a financial accounting package from a neighborhood computer outlet, rather than a programmer whose company has just purchased a new Java class library? Wouldn't the developer of this library find it worthwhile to watch programmers work with it?},
author = {McLellan, Samuel G. and Roesler, Alvin W. and Tempest, Joseph T. and Spinuzzi, Clay I.},
doi = {10.1109/52.676963},
issn = {0740-7459},
journal = {IEEE Software},
number = {3},
pages = {78--86},
title = {{Building more usable APIs}},
volume = {15},
year = {1998}
}
@article{Zupan:2000tp,
author = {Zupan, Blaz and Dem{\v{s}}ar, Janez and Kattan, Michael W and Beck, J Robert and Bratko, Ivan},
journal = {Artificial intelligence in medicine},
number = {1},
pages = {59--75},
title = {{Machine learning for survival analysis: a case study on recurrence of prostate cancer}},
volume = {20},
year = {2000}
}
@inproceedings{Sauro:2011aj,
abstract = {When designing questionnaires there is a tradition of including items with both positive and negative wording to minimize acquiescence and extreme response biases. Two disadvantages of this approach are respondents accidentally agreeing with negative items (mistakes) and researchers forgetting to reverse the scales (miscoding). The original System Usability Scale (SUS) and an all positively worded version were administered in two experiments (n=161 and n=213) across eleven websites. There was no evidence for differences in the response biases between the different versions. A review of 27 SUS datasets found 3 (11{\%}) were miscoded by researchers and 21 out of 158 questionnaires (13{\%}) contained mistakes from users. We found no evidence that the purported advantages of including negative and positive items in usability questionnaires outweigh the disadvantages of mistakes and miscoding. It is recommended that researchers using the standard SUS verify the proper coding of scores and include procedural steps to ensure error-free completion of the SUS by users. Researchers can use the all positive version with confidence because respondents are less likely to make mistakes when responding, researchers are less likely to make errors in coding, and the scores will be similar to the standard SUS. Copyright 2011 ACM.},
address = {Vancouver, BC, Canada},
author = {Sauro, Jeff and Lewis, James R.},
booktitle = {Proceedings of the 2011 SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/1978942.1979266},
keywords = {Acquiescent bias,Satisfaction measures,Standardized questionnaires,System Usability Scale (SUS),Usability evaluation},
month = {may},
pages = {2215--2223},
title = {{When designing usability questionnaires, does it hurt to be positive?}},
year = {2011}
}
@book{Rutten:2004a,
author = {Rutten, J and Kwiatkowska, M and Norman, G and Parker, D},
editor = {Panangaden, P and van Breugel, F},
publisher = {American Mathematical Society},
series = {CRM Monograph Series},
title = {{Mathematical techniques for analyzing concurrent and probabilistic systems}},
volume = {23},
year = {2004}
}
@article{Turhan2012OnModels,
author = {Turhan, Burak and Shepperd, Martin and Menzies, Tim},
doi = {10.1007/s10664-011-9182-8},
journal = {Empirical Software Engineering},
pages = {62--74},
title = {{On the dataset shift problem in software engineering prediction models}},
volume = {17},
year = {2012}
}
@article{8506423,
abstract = {One source of software project challenges and failures is the systematic errors introduced by human cognitive biases. Although extensively explored in cognitive psychology, investigations concerning cognitive biases have only recently gained popularity in software engineering research. This paper therefore systematically maps, aggregates and synthesizes the literature on cognitive biases in software engineering to generate a comprehensive body of knowledge, understand state of the art research and provide guidelines for future research and practise. Focusing on bias antecedents, effects and mitigation techniques, we identified 65 articles (published between 1990 and 2016), which investigate 37 cognitive biases. Despite strong and increasing interest, the results reveal a scarcity of research on mitigation techniques and poor theoretical foundations in understanding and interpreting cognitive biases. Although bias-related research has generated many new insights in the software engineering community, specific bias mitigation techniques are still needed for software professionals to overcome the deleterious effects of cognitive biases on their work.},
author = {Mohanani, R and Salman, I and Turhan, B and Rodr{\'{i}}guez, P and Ralph, P},
doi = {10.1109/TSE.2018.2877759},
issn = {1939-3520},
journal = {IEEE Transactions on Software Engineering},
keywords = {Antecedents of cognitive bias;cognitive bias;debia},
pages = {1},
title = {{Cognitive Biases in Software Engineering: A Systematic Mapping Study}},
year = {2018}
}
@misc{Albrecht2013,
abstract = {Background: Communication between patients and medical staff can be challenging if both parties have different cultural and linguistic backgrounds. Specialized applications can potentially alleviate these problems and significantly contribute to an effective, improved care process when foreign language patients are involved. Objective: The objective for this paper was to discuss the experiences gained from a study carried out at the Hannover Medical School regarding the use of a mobile translation application in hospital wards. The conditions for successfully integrating these technologies in the care process are discussed. Methods: iPads with a preinstalled copy of an exemplary multilingual assistance tool ('xprompt') designed for use in medical care were deployed on 10 wards. Over a period of 6 weeks, approximately 160 employees of the care staff had the opportunity to gather experiences with the devices while putting them to use during their work. Afterwards, the participants were asked to fill out an anonymous, paper-based questionnaire (17 questions) covering the usability of the iPads, translation apps in general, and the exemplary chosen application specifically. For questions requiring a rating, Likert scales were employed. The retained data were entered into an electronic survey system and exported to Microsoft Excel 2007 for further descriptive analysis. Results: Of 160 possible participants, 42 returned the questionnaire and 39 completed the questions concerning the chosen app. The demographic data acquired via the questionnaire (ie, age, professional experience, gender) corresponded to the values for the entire care staff at the Hannover Medical School. Most respondents (35/39, 90{\%}) had no previous experience with an iPad. On a 7-point scale, the participants generally rated mobile translation tools as helpful for communicating with foreign language patients (36/39, 92{\%}; median=5, IQR=2). They were less enthusiastic about xprompt's practical use (36/39, median=4, IQR=2.5), although the app was perceived as easy-to-use (36/39, median=6, IQR=3) and there were no obvious problems with the usability of the device (36/39, median=6, IQR=2). Conclusions: The discrepancy between the expert ratings for xprompt (collected from the App Store and online) and the opinions of the study's participants can probably be explained by the differing approaches of the two user groups. The experts had clear expectations, whereas, without a more thorough introduction, our study participants perceived using the app as too time consuming in relation to the expected benefit. The introduction of such tools in today's busy care settings should therefore be more carefully planned to heighten acceptance of new tools. Still, the low return rate of the questionnaires only allows for speculations on the data, and further research is necessary. Trial Registration: This study was approved by the local institutional review board (IRB), Trial ID number: 1145-2011.},
author = {Albrecht, Urs Vito and Behrends, Marianne and Matthies, Herbert K. and {Von Jan}, Ute},
booktitle = {Journal of Medical Internet Research},
doi = {10.2196/mhealth.2268},
issn = {14388871},
keywords = {Cultural deprivation,Medical informatics applications,Nursing care},
title = {{Usage of multilingual mobile translation applications in clinical settings}},
year = {2013}
}
@inproceedings{Craven:1995wg,
abstract = {A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm, Trepan, for extracting comprehensible, symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that Trepan is able to produce decision trees that maintain a high level of fidelity to their respective...},
address = {Denver, CO, USA},
author = {Craven, Mark W and Shavlik, Jude W},
booktitle = {Proceedings of the 8th International Conference on Neural Information Processing Systems},
isbn = {978-0-26-220107-0},
month = {dec},
pages = {24--30},
publisher = {MIT Press},
title = {{Extracting tree-structured representations of trained neural networks}},
volume = {8},
year = {1996}
}
@book{Rokach:2008wc,
abstract = {This is the first comprehensive book dedicated entirely to the field of decision trees in data mining and covers all aspects of this important technique. Decision trees have become one of the most powerful and popular approaches in knowledge discovery and data mining, the science and technology of exploring large and complex bodies of data in order to discover useful patterns. The area is of great importance because it enables modeling and knowledge extraction from the abundance of data available. Both theoreticians and practitioners are continually seeking techniques to make the process more efficient, cost-effective and accurate. Decision trees, originally implemented in decision theory and statistics, are highly effective tools in other areas such as data mining, text mining, information extraction, machine learning, and pattern recognition.This book invites readers to explore the many benefits in data mining that decision trees offer: self-explanatory and easy to follow when compacted; able to handle a variety of input data: nominal, numeric and textual; able to process datasets that may have errors or missing values; high predictive performance for a relatively small computational effort; available in many data mining packages over a variety of platforms; and, useful for various tasks, such as classification, regression, clustering and feature selection.},
author = {Lori, Rokach and Oded, Maimon},
isbn = {978-9-81-277171-1},
pages = {244},
publisher = {World Scientific Publishing Company},
title = {{Data mining with decision trees}},
volume = {69},
year = {2008}
}
@inproceedings{Suri:2007wl,
abstract = {In this paper we consider the task of prototype selection whose primary goal is to reduce the storage and computational requirements of the Nearest Neighbor classifier while achieving better classification accuracies. We propose a solution to the prototype selection problem using techniques from cooperative game theory and show its efficacy experimentally. {\textcopyright}Springer-Verlag Berlin Heidelberg 2007.},
address = {Warsaw, Poland},
author = {{Rama Suri}, N and Srinivas, V S and {Narasimha Murty}, M},
booktitle = {Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases},
doi = {10.1007/978-3-540-74976-9_58},
isbn = {978-3-54-074975-2},
issn = {0302-9743},
month = {sep},
pages = {556--564},
publisher = {Springer},
title = {{A cooperative game theoretic approach to prototype selection}},
year = {2007}
}
@misc{Cigital:2003tl,
author = {Cigital},
title = {{Case Study: Finding defects earlier yields enormous savings}},
url = {http://bit.ly/36Il2cE},
year = {2003}
}
@book{Pirsig:1974vs,
author = {Pirsig, Robert M},
edition = {1st},
isbn = {9-780-06-058946-2},
publisher = {HarperTorch},
title = {{Zen and the art of motorcycle maintenance: An inquiry into values}},
year = {1974}
}
@inproceedings{Beyer:2018fm,
abstract = {Software developers frequently solve development issues with the help of question and answer web forums, such as Stack Overflow (SO). While tags exist to support question searching and browsing, they are more related to technological aspects than to the question purposes. Tagging questions with their purpose can add a new dimension to the investigation of topics discussed in posts on SO. In this paper, we aim to automate such a classification of SO posts into seven question categories. As a first step, we have manually created a curated data set of 500 SO posts, classified into the seven categories. Using this data set, we apply machine learning algorithms (Random Forest and Support Vector Machines) to build a classification model for SO questions. We then experiment with 82 different configurations regarding the preprocessing of the text and representation of the input data. The results of the best performing models show that our models can classify posts into the correct question category with an average precision and recall of 0.88 and 0.87 when using Random Forest and the phrases indicating a question category as input data for the training. The obtained model can be used to aid developers in browsing SO discussions or researchers in building recommenders based on SO.},
address = {Gothenburg, Sweden},
author = {Beyer, Stefanie and MacHo, Christian and Pinzger, Martin and {Di Penta}, Massimiliano},
booktitle = {Proceedings of the 26th International Conference on Program Comprehension},
doi = {10.1145/3196321.3196333},
isbn = {978-1-45-035714-2},
issn = {0270-5257},
month = {may},
pages = {211--221},
publisher = {ACM},
title = {{Automatically classifying posts into question categories on stack overflow}},
year = {2018}
}
@book{Jin:2006uf,
address = {Berlin, Heidelberg},
author = {Jin, Yaochu},
doi = {10.1007/3-540-33019-4},
isbn = {978-3-54-030676-4},
publisher = {Springer},
series = {Studies in Computational Intelligence},
title = {{Multi-Objective Machine Learning}},
year = {2006}
}
@article{Oreskes:1994gn,
abstract = {Verification and validation of numerical models of natural systems is impossible. This is because natural systems are never closed and because model results are always non-unique. Models can be confirmed by the demonstration of agreement between observation and prediction, but confirmation is inherently partial. Complete confirmation is logically precluded by the fallacy of affirming the consequent and by incomplete access to natural phenomena. Models can only be evaluated in relative terms, and their predictive value is always open to question. The primary value of models is heuristic.},
author = {Oreskes, Naomi and Shrader-Frechette, Kristin and Belitz, Kenneth},
doi = {10.1126/science.263.5147.641},
issn = {0036-8075},
journal = {Science},
number = {5147},
pages = {641--646},
title = {{Verification, validation, and confirmation of numerical models in the earth sciences}},
volume = {263},
year = {1994}
}
@inproceedings{Johansson:2009uo,
abstract = {Some data mining problems require predictive models to be not only accurate but also comprehensible. Comprehensibility enables human inspection and understanding of the model, making it possible to trace why individual predictions are made. Since most high-accuracy techniques produce opaque models, accuracy is, in practice, regularly sacrificed for comprehensibility. One frequently studied technique, often able to reduce this accuracy vs. comprehensibility tradeoff, is rule extraction, i.e., the activity where another, transparent, model is generated from the opaque. In this paper, it is argued that techniques producing transparent models, either directly from the dataset, or from an opaque model, could benefit from using an oracle guide. In the experiments, genetic programming is used to evolve decision trees, and a neural network ensemble is used as the oracle guide. More specifically, the datasets used by the genetic programming when evolving the decision trees, consist of several different combinations of the original training data and "oracle data", i.e., training or test data instances, together with corresponding predictions from the oracle. In total, seven different ways of combining regular training data with oracle data were evaluated, and the results, obtained on 26 UCI datasets, clearly show that the use of an oracle guide improved the performance. As a matter of fact, trees evolved using training data only had the worst test set accuracy of all setups evaluated. Furthermore, statistical tests show that two setups, both using the oracle guide, produced significantly more accurate trees, compared to the setup using training data only. {\textcopyright}2009 IEEE.},
address = {Nashville, TN, USA},
author = {Johansson, Ulf and Niklasson, Lars},
booktitle = {Proceedings of the 2009 IEEE Symposium on Computational Intelligence and Data Mining},
doi = {10.1109/CIDM.2009.4938655},
isbn = {978-1-42-442765-9},
month = {may},
pages = {238--244},
publisher = {IEEE},
title = {{Evolving decision trees using oracle guides}},
year = {2009}
}
@inproceedings{Wang:2018vl,
abstract = {Transfer learning is a powerful approach that allows users to quickly build accurate deep-learning (Student) models by "learning" from centralized (Teacher) models pretrained with large datasets, e.g. Google's Inception V3. We hypothesize that the centralization of model training increases their vulnerability to misclas-sification attacks leveraging knowledge of publicly accessible Teacher models. In this paper, we describe our efforts to understand and experimentally validate such attacks in the context of image recognition. We identify techniques that allow attackers to associate Student models with their Teacher counterparts, and launch highly effective misclassification attacks on black-box Student models. We validate this on widely used Teacher models in the wild. Finally, we propose and evaluate multiple approaches for defense, including a neuron-distance technique that successfully defends against these attacks while also obfuscates the link between Teacher and Student models.},
address = {Baltimore, MD, USA},
author = {Wang, Bolun and Yao, Yuanshun and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
booktitle = {Proceedings of the 27th USENIX Security Symposium},
isbn = {978-1-93-913304-5},
month = {jul},
pages = {1281--1297},
publisher = {USENIX Association},
title = {{With great training comes great vulnerability: Practical attacks against transfer learning}},
year = {2018}
}
@inproceedings{Meyers2019,
author = {Meyers, J and Cain, A and Renzella, J and Cummaudo, A},
booktitle = {Proceedings of 2018 IEEE International Conference on Teaching, Assessment, and Learning for Engineering, TALE 2018},
doi = {10.1109/TALE.2018.8615174},
title = {{A Proposal for Integrating Gamification into Task-Oriented Portfolio Assessment}},
year = {2019}
}
@inproceedings{Ko:2011fb,
abstract = {While many studies have investigated the challenges that developers face in finding and using API documentation, few have considered the role of developers' conceptual knowledge in these tasks. We designed a study in which developers were asked to explore the feasibility of two requirements concerning networking protocols and application platforms that most participants were unfamiliar with, observing the effect that a lack of conceptual knowledge had on their use of documentation. Our results show that without conceptual knowledge, developers struggled to formulate effective queries and to evaluate the relevance or meaning of content they found. Our results suggest that API documentation should not only include detailed examples of API use, but also thorough introductions to the concepts, standards, and ideas manifested in an API's data structures and functionality. {\textcopyright}2011 IEEE.},
address = {Pittsburgh, PA, USA},
author = {Ko, Andrew J and Riche, Yann},
booktitle = {Proceedings of the 2011 IEEE Symposium on Visual Languages and Human Centric Computing},
doi = {10.1109/VLHCC.2011.6070395},
isbn = {978-1-45-771245-6},
keywords = {API usability,documentation,feasibility},
month = {sep},
pages = {173--176},
publisher = {IEEE},
title = {{The role of conceptual knowledge in API usability}},
year = {2011}
}
@inproceedings{Allahyari:2011ud,
abstract = {This paper reviews methods for evaluating and analyzing the understandability of classification models in the context of data mining. The motivation for this study is the fact that the majority of previous work on evaluation and optimization of classification models has focused on assessing or increasing the accuracy of the models and thus user-oriented properties such as comprehensibility and understandability have been largely overlooked. We conduct a quantitative survey to examine the concept of understandability from the user's point of view. The survey results are analyzed using the analytic hierarchy process (AHP) to rank models according to their understandability. The results indicate that decision tree models are perceived as more understandable than rule-based models. Using the survey results regarding understandability of a number of models in conjunction with quantitative measurements of the complexity of the models, we are able to establish a negative correlation between the complexity and understandability of the classification models, at least for one of the two studied data sets. ?? 2011 The authors and IOS Press. All rights reserved.},
address = {Trondheim, Norway},
author = {Allahyari, Hiva and Lavesson, Niklas},
booktitle = {Proceedings of the 11th Scandinavian Conference on Artificial Intelligence},
doi = {10.3233/978-1-60750-754-3-11},
isbn = {978-1-60-750753-6},
issn = {0922-6389},
keywords = {Classification,evaluation,understandability},
month = {may},
pages = {11--19},
publisher = {IOS Press},
title = {{User-oriented assessment of classification model understandability}},
volume = {227},
year = {2011}
}
@article{DoshiVelez:2017vm,
abstract = {The ubiquity of systems using artificial intelligence or "AI" has brought increasing attention to how those systems should be regulated. The choice of how to regulate AI systems will require care. AI systems have the potential to synthesize large amounts of data, allowing for greater levels of personalization and precision than ever before|applications range from clinical decision support to autonomous driving and predictive policing. That said, common sense reasoning [McCarthy, 1960] remains one of the holy grails of AI, and there exist legitimate concerns about the intentional and unintentional negative consequences of AI systems [Bostrom, 2003, Amodei et al., 2016, Sculley et al., 2014]. There are many ways to hold AI systems accountable. In this work, we focus on one: explanation. Questions about a legal right to explanation from AI systems was recently debated in the EU General Data Protection Regulation [Goodman and Flaxman, 2016, Wachter et al., 2017], and thus thinking carefully about when and how explanation from AI systems might improve accountability is timely. Good choices about when to demand explanation can help prevent negative consequences from AI systems, while poor choices may not only fail to hold AI systems accountable but also hamper the development of much-needed beneficial AI systems. Below, we briefly review current societal, moral, and legal norms around explanation, and then focus on the different contexts under which explanation is currently required under the law. We find that there exists great variation around when explanation is demanded, but there also exists important consistencies: when demanding explanation from humans, what we typically want to know is how and whether certain input factors affected the final decision or outcome. These consistencies allow us to list the technical considerations that must be considered if we desired AI systems that could provide kinds of explanations that are currently required of humans under the law. Contrary to popular wisdom of AI systems as indecipherable black boxes, we find that this level of explanation should often be technically feasible but may sometimes be practically onerous|there are certain aspects of explanation that may be simple for humans to provide but challenging for AI systems, and vice versa. As an interdisciplinary team of legal scholars, computer scientists, and cognitive scientists, we recommend that for the present, AI systems can and should be held to a similar standard of explanation as humans currently are; in the future we may wish to hold an AI to a different standard.},
annote = {In Press},
archivePrefix = {arXiv},
arxivId = {1711.01134},
author = {Doshi-Velez, Finale and Kortz, Mason and Budish, Ryan and Bavitz, Christopher and Gershman, Samuel J and O'Brien, David and Shieber, Stuart and Waldo, Jim and Weinberger, David and Wood, Alexandra},
doi = {10.2139/ssrn.3064761},
eprint = {1711.01134},
journal = {SSRN Electronic Journal},
month = {nov},
title = {{Accountability of AI Under the Law: The Role of Explanation}},
year = {2017}
}
@incollection{Easterbrook:2007ws,
author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
booktitle = {Guide to Advanced Empirical Software Engineering},
chapter = {11},
doi = {10.1007/978-1-84800-044-5},
editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K.},
isbn = {978-1-84-800043-8},
month = {nov},
pages = {285--311},
publisher = {Springer},
title = {{Selecting empirical methods for software engineering research}},
year = {2007}
}
@inproceedings{10.1145/2370216.2370437,
address = {Pittsburgh, PA, USA},
author = {Ba{\~{n}}os, Oresti and Damas, Miguel and Pomares, H{\'{e}}ctor and Rojas, Ignacio and T{\'{o}}th, M{\'{a}}t{\'{e}} Attila and Amft, Oliver},
booktitle = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing},
doi = {10.1145/2370216.2370437},
isbn = {9781450312240},
keywords = {activity recognition,benchmark dataset,fitness exercises,motion sensors,sensor displacement},
pages = {1026--1035},
publisher = {ACM},
title = {{A Benchmark Dataset to Evaluate Sensor Displacement in Activity Recognition}},
year = {2012}
}
@article{Maalej2013,
abstract = {Reading reference documentation is an important part of programming with application programming interfaces (APIs). Reference documentation complements the API by providing information not obvious from the API syntax. To improve the quality of reference documentation and the efficiency with which the relevant information it contains can be accessed, we must first understand its content. We report on a study of the nature and organization of knowledge contained in the reference documentation of the hundreds of APIs provided as a part of two major technology platforms: Java SDK 6 and.NET 4.0. Our study involved the development of a taxonomy of knowledge types based on grounded methods and independent empirical validation. Seventeen trained coders used the taxonomy to rate a total of 5,574 randomly sampled documentation units to assess the knowledge they contain. Our results provide a comprehensive perspective on the patterns of knowledge in API documentation: observations about the types of knowledge it contains and how this knowledge is distributed throughout the documentation. The taxonomy and patterns of knowledge we present in this paper can be used to help practitioners evaluate the content of their API documentation, better organize their documentation, and limit the amount of low-value content. They also provide a vocabulary that can help structure and facilitate discussions about the content of APIs. {\textcopyright}1976-2012 IEEE.},
author = {Maalej, Walid and Robillard, Martin P},
doi = {10.1109/TSE.2013.12},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {.NET,API documentation,Java,content analysis,data mining,empirical study,grounded method,pattern mining,software documentation},
title = {{Patterns of knowledge in API reference documentation}},
year = {2013}
}
@article{Myers:2011bt,
abstract = {All software today is written using application programming interfaces (APIs). We performed a user study of the online documentation of a large and complex API for Enterprise Service-Oriented Architecture (eSOA), which identified many issues and recommendations for making API documentation easier to use. eSOA is an appropriate testbed because the target users include high-level business experts who do not have significant programming expertise and thus can be classified as “end-user developers.” Our study showed that the participants' background influenced how they navigated the documentation. Lack of familiarity with business terminology was a barrier for developers without business application experience. Both groups avoided areas of the documentation that had an inconsistent visual design. A new design for the documentation that supports flexible navigation strategies seems to be required to support the wide range of users for eSOA. This paper summarizes our study and provides recommendations for future documentation for APIs.},
author = {Myers, Brad A and Jeong, Sae Young and Xie, Yingyu and Beaton, Jack and Stylos, Jeff and Ehret, Ralf and Karstens, Jan and Efeoglu, Arkin and Busse, Daniela K},
doi = {10.4018/joeuc.2010101903},
issn = {1546-2234},
journal = {Journal of Organizational and End User Computing},
keywords = {Api design,Business solution architects,Documentation,Natural programming,Service-oriented architecture,Usability,Web services},
month = {jan},
number = {1},
pages = {23--51},
publisher = {IGI Global},
title = {{Studying the Documentation of an API for Enterprise Service-Oriented Architecture}},
volume = {22},
year = {2010}
}
@inproceedings{Michie:1988te,
address = {Glasgow, Scotland, UK},
author = {Michie, D},
booktitle = {Proceedings of the 3rd European Conference on European Working Session on Learning},
isbn = {978-0-27-308800-4},
month = {oct},
pages = {107--122},
publisher = {Pitman Publishing, Inc.},
title = {{Machine learning in the next five years}},
year = {1988}
}
@article{Jaspers:2011hy,
abstract = {Objective: To synthesize the literature on clinical decision-support systems' (CDSS) impact on healthcare practitioner performance and patient outcomes. Design: Literature search on Medline, Embase, Inspec, Cinahl, Cochrane/Dare and analysis of high-quality systematic reviews (SRs) on CDSS in hospital settings. Two-stage inclusion procedure: (1) selection of publications on predefined inclusion criteria; (2) independent methodological assessment of preincluded SRs by the 11-item measurement tool, AMSTAR. Inclusion of SRs with AMSTAR score 9 or above. SRs were thereafter rated on level of evidence. Each stage was performed by two independent reviewers. Results: 17 out of 35 preincluded SRs were of high methodological quality and further analyzed. Evidence that CDSS significantly impacted practitioner performance was found in 52 out of 91 unique studies of the 16 SRs examining this effect (57{\%}). Only 25 out of 82 unique studies of the 16 SRs reported evidence that CDSS positively impacted patient outcomes (30{\%}). Conclusions: Few studies have found any benefits on patient outcomes, though many of these have been too small in sample size or too short in time to reveal clinically important effects. There is significant evidence that CDSS can positively impact healthcare providers' performance with drug ordering and preventive care reminder systems as most clear examples. These outcomes may be explained by the fact that these types of CDSS require a minimum of patient data that are largely available before the advice is (to be) generated: at the time clinicians make the decisions.},
author = {Jaspers, Monique W M and Smeulers, Marian and Vermeulen, Hester and Peute, Linda W},
doi = {10.1136/amiajnl-2011-000094},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association},
number = {3},
pages = {327--334},
pmid = {21422100},
title = {{Effects of clinical decision-support systems on practitioner performance and patient outcomes: A synthesis of high-quality systematic review findings}},
volume = {18},
year = {2011}
}
@book{Spector:1992uj,
abstract = {Summated scales have four characteristics: multiple items; quantitative measurement; no right answer; and, each item of scale is a statement. Mutliple item scales are used for reliability and precision. "Reliability assures that a scale can consistently measure something, but it does not assure that it will measure what it is designed to measure. This property (that a scale measures its intended construct) is validity" (6-7). While reliability is evaluated with test-retest, internal consistency or other types of reliability tests, validity if evaluated in reference to theoretical context and hypothesized relationships with the construct and other variables. The theory of summated rating scales is described in chapter 2, where it is noted that one of the most troublesome sources of biases (or non-random error) is social desirability. Another is acquiescence responses, where people tend to agree with all items regardless of content. Chapter 3 addresses the importance of defining the construct to be measured, and chapter 4 desiging a scale. Approach recommended is inductive one in which theoretical ideas guide validation strategy. The deductive approach is more exploratory and leaves much room for misinterpretation. Constructs vary from being specific and narrow to being multi-dimensional and involving sub-scales."Locus of control" reserach is used as an example throughout this monograph to discuss scale construction. Three types of response choices are described: agreement, evaluation, and frequency. Scales can be unipolar or bipolar. Attitudes are often bipolar because there is a positive, neutral, negative response. These scales can be numbered from 1to x or from -x to +x. Five rules for good items are: express only one idea; use both positively and negatively worded items; avoid expressions, jargon, etc.; consider reading level of respondents; and, avoid use of negatives to reverse wording. Chapter 5 discusses conducting the item analysis, in order to determine internal consistency of the scale and eliminate inconsistent items. The item-remainder (or part-whole or item-whole) coefficient measures how well each item relates to other items in the analysis. Items should be scaled in same direction, so some scales may need to be reversed before the analysis. Two methods can be used to decide if items should be retained: m-items chosen or criterion for coefficient (e.g., 0.40). Coefficient alpha also measures consistency, and is a function of number of items and their degree of correlation. Note that internal consistency can result if more than one highly corrrelated measure constructs comprise the scale. In choosing scale items, both item-remainder and alpha coefficient are used. Items may also be deleted based on their correlations with external variables (such as a social desirability measure). When item analysis results in too few items, it may be helpful to estimate number of items needed to achieve acceptable level of internal consistency, using, e.g., Spearman-Brown prophesy formula. Multidimensional scales are discussed starting on pg. 39. Many constructs are broad and may contain multiple aspects. Attitudes toward complex things wuch as gov't often contain many dimensions. Subscales are useful for these constructs. Shared items in scales may cause problems because relationships among the sub-scale constructs may be due to a real relationship or to shared items. Chapter 6 discusses validation. Typical strategy involves testing scale in context of hypothesized interrelations between construct and other variables. This is similar to testing a theory, in that validity cannot be proven. Evidence is simply collected to support or refute validity. As with a theory, a construct is tentatively accepted because it is useful. A few methods of validity evaluation are described: criterion-related validity, discrimnant and convergent validities, and factor analysis. For criterion-related validity, scale is validated in relation to theoretically-based hypotheses. Concurrent validity involve simultaneously collected data on scale and related variables, while predictive validity involves collecting data on scale of interest prior to criterion variables. Known-groups validity assess hypothesized differences between certain groups using t-tests, ANOVA, etc. Convergent validity means different measures of same construct are strongly related, while discriminant validity means measures of different constructs should relate only modestly. An example of Multitrait-Multimethod Matrix is presented. Two types of factor analysis are discussed: confirmatory, where there is a hypothesized structure, and exploratory. Basic idea of factor analysis is to reduce number of items to smaller number of underlying gorups of items, called factors. For multidimensional scales, additional items tend to produce stronger factors that account for more variable. Poor items and response biases can wreak havoc on factor solution. The eigenvalue represents relative proportion of variance accounted for by factor. If items don't correlate, eigenvalues will reflect only variance in original items and will equal 1. If perfectly correlated, single factor is produced with eigenvalue equal to number of items and other eigenvalues with equal 0. If items for several factors, each will have eigenvalue greater than one, meaning it is accounting for more variance than a single item. Once factors identified, orthogonal rotation procedure is applied, in order to produce clusters of items based on mathematical criteria. Loading matrix produces factor loadings that are correlations of item with each factor. Minimum value of {\~{}}0.30-.35 suggests that an item loads significantly onto a factor. One difficulty is the subjective judgement necessary to determine number of factors. Note that factor analysis can be sensitive to total set of items, and adding/deleting single item can have profound effects. Factor analysis is not likely to be useful for scales with few items. With exploratory factor analysis, best fitting factor structure is fit to data. With confirmatory factor analsyis, structure is hypothesized in advance. Covariance structure modeling structures such as LISREL and EQS are described. Lastly, reliability and norms are discussed in chapter 7.},
address = {Newbury Park, CA, USA},
author = {Spector, Paul},
booktitle = {Summated Rating Scale Construction},
doi = {10.4135/9781412986038},
isbn = {978-0-80-394341-4},
publisher = {SAGE},
title = {{Summated Rating Scale Construction}},
year = {1992}
}
@article{Lavrac:1999tf,
abstract = {Widespread use of medical information systems and explosive growth of medical databases require traditional manual data analysis to be coupled with methods for efficient computer-assisted analysis. This paper presents selected data mining techniques that can be applied in medicine, and in particular some machine learning techniques including the mechanisms that make them better suited for the analysis of medical databases (derivation of symbolic rules, use of background knowledge, sensitivity and specificity of induced descriptions). The importance of the interpretability of results of data analysis is discussed and illustrated on selected medical applications.},
author = {Lavra{\v{c}}, Nada},
doi = {10.1016/S0933-3657(98)00062-1},
issn = {0933-3657},
journal = {Artificial Intelligence in Medicine},
keywords = {Data mining,Machine learning,Medical applications},
number = {1},
pages = {3--23},
pmid = {10225344},
title = {{Selected techniques for data mining in medicine}},
volume = {16},
year = {1999}
}
@inproceedings{Rubey:1968fg,
address = {Las Vegas, NV, USA},
author = {Rubey, Raymond J and Hartwick, R Dean},
booktitle = {Proceedings of the 1968 23rd ACM National Conference},
doi = {10.1145/800186.810631},
isbn = {978-1-45-037486-6},
month = {aug},
pages = {671--677},
publisher = {ACM},
title = {{Quantitative measurement of program quality}},
year = {1968}
}
@article{Ritzer:1991ge,
abstract = {"The contributors and editor of this work are to be commended for their successful efforts in delineating many of the concerns current within education and in calling for frank debate on these issues by all interested parties. Furthermore, they have stimulated good scholarship by readily admitting to the current state of affairs being one of more questions than answers and more confusion than clarity. They thus remind us that the search for knowledge is one fraught with conflict in a public arena. "The appropriate audience for this volume is assessed to be the reader who derives satisfaction from critical thinking. It would be appropriate for graduate students in education, human services, social sciences, or theology, or any person committed to the endeavor and process of education. "The Paradigm Dialog is one of those rare books that simultaneously stretches the mind while projecting one into self-reflection. For the applied practitioner, whether teacher, counselor, or consultant, the possibility of gaining further insight into the underlying assumptions which constrain one's pedagogy or practice is highly possible upon a critical reading." -The Journal of Applied Rehabilitation Counseling Is scientific positivism, long the reigning paradigm for research in the social sciences, the "best way" to conduct social research? This is the central question examined in The Paradigm Dialog. Recently three key challengers have appeared-postpositivism, critical theory, and constructivism. All three offer researchers new methodological approaches, and all three present fundamental questions that must be addressed. Can research be conducted between paradigms? Are they equally useful in answering questions of applied research? What constitutes good, or ethical, research in each? In this volume, these and other significant questions are examined by a multidisciplinary group of leading figures in qualitative research. Not surprisingly, there is no agreement on the "best" paradigm question, but the dialog offered in this compelling volume deftly explores important issues in selecting the proper paradigm for tackling a variety of research questions. With a group of contributors that reads like a veritable who's who in qualitative research, The Paradigm Dialog is a must for anyone conducting research in the social sciences.},
author = {Ritzer, George and Guba, Egon},
doi = {10.2307/3340973},
issn = {0318-6431},
journal = {Canadian Journal of Sociology},
number = {4},
pages = {446},
title = {{The Paradigm Dialog}},
volume = {16},
year = {1991}
}
@article{calefato2018,
author = {Calefato, Fabio and Lanubile, Filippo and Maiorano, Federico and Novielli, Nicole},
doi = {10.1007/s10664-017-9546-9},
journal = {Empirical Software Engineering},
number = {3},
pages = {1352--1382},
publisher = {Springer},
title = {{Sentiment polarity detection for software development}},
volume = {23},
year = {2018}
}
@inproceedings{Law2017,
author = {Law, C.-Y. and Grundy, J and Cain, A and Vasa, R and Cummaudo, A},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3013499.3013502},
title = {{User perceptions of using an open learner model visualisation tool for facilitating self-regulated learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-85014894003{\&}partnerID=MN8TOARS},
year = {2017}
}
@article{Lecun:1998hy,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@inproceedings{murphy2007approach,
abstract = {Some machine learning applications are intended to learn properties of data sets where the correct answers are not already known to human users. It is challenging to test such ML software, because there is no reliable test oracle. We describe a software testing approach aimed at addressing this problem. We present our findings from testing implementations of two different ML ranking algorithms: Support Vector Machines and Marti Rank. Copyright {\textcopyright}(2007) by Knowledge Systems Institute (KSI).},
address = {Boston, MA, USA},
author = {Murphy, Christian and Kaiser, Gail and Arias, Marta},
booktitle = {Proceedings of the 19th International Conference on Software Engineering and Knowledge Engineering},
isbn = {978-1-62-748661-3},
month = {jul},
pages = {167--172},
title = {{An approach to software testing of machine learning applications}},
year = {2007}
}
@article{Frey:2007hs,
abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such "exemplars" can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called "affinity propagation," which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},
author = {Frey, Brendan J and Dueck, Delbert},
doi = {10.1126/science.1136800},
issn = {0036-8075},
journal = {Science},
month = {feb},
number = {5814},
pages = {972--976},
title = {{Clustering by passing messages between data points}},
volume = {315},
year = {2007}
}
@inproceedings{Caruana:2015jk,
abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
address = {Sydney, Australia},
author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, No{\'{e}}mie},
booktitle = {Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2783258.2788613},
isbn = {978-1-45-033664-2},
keywords = {Additive models,Classification,Healthcare,Intelligibility,Interaction detection,Logistic regression,Risk prediction},
month = {aug},
pages = {1721--1730},
publisher = {ACM},
title = {{Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission}},
volume = {2015-Augus},
year = {2015}
}
@inproceedings{Beyer:2014ec,
abstract = {While many tutorials, code examples, and documentation about Android APIs exist, developers still face various problems with the implementation of Android Apps. Many of these issues are discussed on Q{\&}A-sites, such as Stack Overflow. In this paper we present a manual categorization of 450 Android related posts of Stack Overflow concerning their question and problem types. The idea is to find dependencies between certain problems and question types to get better insights into issues of Android App development. The categorization is developed using card sorting with three experienced Android App developers. An initial approach to automate the classification of Stack Overflow posts using Lucene is also presented. The study highlights that the most common question types are 'How to⋯?' and 'What is the problem⋯?'. The problems that are discussed most often are related to 'User Interface' and 'Core Elements'. In particular, the problem category 'Layout' is often related to 'What is the problem⋯?' and 'Frameworks' issues often come with 'Is it possible⋯?' questions.},
address = {Victoria, BC, Canada},
author = {Beyer, Stefanie and Pinzger, Martin},
booktitle = {Proceedings of the 30th International Conference on Software Maintenance and Evolution},
doi = {10.1109/ICSME.2014.88},
isbn = {978-0-76-955303-0},
month = {sep},
pages = {531--535},
publisher = {IEEE},
title = {{A manual categorization of android app development issues on stack overflow}},
year = {2014}
}
@inproceedings{Ortiz:2017wg,
author = {Ortiz, Andres L Martinez},
booktitle = {EIAPortugal},
month = {jul},
title = {{Curating Content with Google Machine Learning Application Programming Interfaces}},
url = {http://bit.ly/2S40er8},
year = {2017}
}
@inproceedings{Petersen:2019ji,
abstract = {Background - Validity threats should be considered and consistently reported to judge the value of an empirical software engineering research study. The relevance of specific threats for a particular research study depends on the worldview or philosophical worldview of the researchers of the study. Problem/Gap - In software engineering, different categorizations exist, which leads to inconsistent reporting and consideration of threats. Contribution - In this paper, we relate different worldviews to software engineering research methods, identify generic categories for validity threats, and provide a categorization of validity threats with respect to their relevance for different world views. Thereafter, we provide a checklist aiding researchers in identifying relevant threats. Method - Different threat categorizations and threats have been identified in literature, and are reflected on in relation to software engineering research. Results - Software engineering is dominated by the pragmatist worldviews, and therefore use multiple methods in research. Maxwell's categorization of validity threats has been chosen as very suitable for reporting validity threats in software engineering research. Conclusion - We recommend to follow a checklist approach, and reporting first the philosophical worldview of the researcher when doing the research, the research methods and all threats relevant, including open, reduced, and mitigated threats. {\textcopyright}2013 IEEE.},
address = {Ankara, Turkey},
author = {Petersen, Kai and Gencel, Cigdem},
booktitle = {Proceedings of the Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement},
doi = {10.1109/IWSM-Mensura.2013.22},
isbn = {978-0-76-955078-7},
month = {oct},
pages = {81--89},
publisher = {IEEE},
title = {{Worldviews, research methods, and their relationship to validity in empirical software engineering research}},
year = {2013}
}
@inproceedings{Nybom:2018ef,
abstract = {Background: Application Programming Interfaces (APIs) are key to software reuse. Software developers can link functionality and behaviour found in other software with their own software by taking an API into use. However, figuring out how an API works is usually demanding, and may require that the developers spend a notable amount of time familiarizing themselves with the API. Good API documentation is of key importance to simplify this task. Objective: To present a comprehensive, unbiased overview of the state-of-the-art on tools and approaches for API documentation generation. Method: A systematic mapping study on published tools and approaches that can be used for generating API documentation, or for assisting in the API documentation process. Results: 36 studies on API documentation generation tools and approaches analyzed and categorized in a variety of ways. Among other things, the paper presents an overview of what kind of tools have been developed, what kind of documentation they generate, and what sources the documentation approaches require. Conclusion: Out of the identified approaches, many contribute to API documentation in the areas of natural language documentation and code examples and templates. Many of the approaches contribute to ease API users' understanding and learning of the API, but also to the maintenance and generation of API documentation. Most of the approaches are automatic, simplifying the API documentation generation notably, under the assumption that relevant sources for the generation are available. Most of the API documentation approaches are evaluated either by exercise of the approach followed by analysis of the results, or by empirical evaluation methods.},
address = {Prague, Czech Republic},
author = {Nybom, Kristian and Ashraf, Adnan and Porres, Ivan},
booktitle = {Proceedings of the 44th Euromicro Conference on Software Engineering and Advanced Applications},
doi = {10.1109/SEAA.2018.00081},
isbn = {978-1-53-867382-9},
keywords = {API,API documentation,Systematic mapping study},
month = {aug},
pages = {462--469},
publisher = {IEEE},
title = {{A systematic mapping study on API documentation generation approaches}},
year = {2018}
}
@inproceedings{Petersen:2008td,
abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
booktitle = {Proceedings of the 12th International Conference on Evaluation and Assessment in Software Engineering, EASE 2008},
doi = {10.14236/ewic/ease2008.8},
keywords = {Evidence based software engineering,Systematic mapping studies,Systematic reviews},
pages = {68--77},
title = {{Systematic mapping studies in software engineering}},
year = {2008}
}
@book{RamanAnandHoder2015,
address = {Sebastopol, CA, USA},
author = {{Raman Anand; Hoder}, Chris},
booktitle = {Building Intelligent Apps with Cognitive APIs},
edition = {1st},
isbn = {978-1-49-205862-5},
pages = {97},
publisher = {O'Reilly Media, Inc.},
title = {{Building Intelligent Apps with Cognitive APIs}},
year = {2019}
}
@article{Robillard:2011uv,
abstract = {Large APIs can be hard to learn, and this can lead to decreased programmer productivity. But what makes APIs hard to learn? We conducted a mixed approach, multi-phased study of the obstacles faced by Microsoft developers learning a wide variety of new APIs. The study involved a combination of surveys and in-person interviews, and collected the opinions and experiences of over 440 professional developers. We found that some of the most severe obstacles faced by developers learning new APIs pertained to the documentation and other learning resources. We report on the obstacles developers face when learning new APIs, with a special focus on obstacles related to API documentation. Our qualitative analysis elicited five important factors to consider when designing API documentation: documentation of intent; code examples; matching APIs with scenarios; penetrability of the API; and format and presentation. We analyzed how these factors can be interpreted to prioritize API documentation development efforts {\textcopyright}2010 Springer Science+Business Media, LLC.},
author = {Robillard, Martin P and Deline, Robert},
doi = {10.1007/s10664-010-9150-8},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Application programming interfaces,Documentation,Programming,Software libraries},
number = {6},
pages = {703--732},
title = {{A field study of API learning obstacles}},
volume = {16},
year = {2011}
}
@article{Jiang:2005ua,
abstract = {Background: Determining the functions of uncharacterized proteins is one of the most pressing problems in the post-genomic era. Large scale protein-protein interaction assays, global mRNA expression analyses and systematic protein localization studies provide experimental information that can be used for this purpose. The data from such experiments contain many false positives and false negatives, but can be processed using computational methods to provide reliable information about protein-protein relationships and protein function. An outstanding and important goal is to predict detailed functional annotation for all uncharacterized proteins that is reliable enough to effectively guide experiments. Results: We present AVID, a computational method that uses a multi-stage learning framework to integrate experimental results with sequence information, generating networks reflecting functional similarities among proteins. We illustrate use of the networks by making predictions of detailed Gene Ontology (GO) annotations in three categories: molecular function, biological process, and cellular component. Applied to the yeast Saccharomyces cerevisiae, AVID provides 37,451 pair-wise functional linkages between 4,191 proteins. These relationships are ∼65-78{\%} accurate, as assessed by cross-validation testing. Assignments of highly detailed functional descriptors to proteins, based on the networks, are estimated to be ∼67{\%} accurate for GO categories describing molecular function and cellular component and ∼52{\%} accurate for terms describing biological process. The predictions cover 1,490 proteins with no previous annotation in GO and also assign more detailed functions to many proteins annotated only with less descriptive terms. Predictions made by AVID are largely distinct from those made by other methods. Out of 37,451 predicted pair-wise relationships, the greatest number shared in common with another method is 3,413. Conclusions: AVID provides three networks reflecting functional associations among proteins. We use these networks to generate new, highly detailed functional predictions for roughly half of the yeast proteome that are reliable enough to drive targeted experimental investigations. The predictions suggest many specific, testable hypotheses. All of the data are available as downloadable files as well as through an interactive website at http://bmc-140.mit.edu/avid. Thus, AVID will be a valuable resource for experimental biologists. {\textcopyright}2005 Jiang and Keating, licensee BioMed Central Ltd.},
author = {Jiang, Taijiao and Keating, Amy E},
doi = {10.1186/1471-2105-6-136},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {136},
title = {{AVID: An integrative framework for discovering functional relationship among proteins}},
volume = {6},
year = {2005}
}
@article{Pearl:2018uv,
abstract = {THE DRAMATIC SUCCESS In machine learning has led to an explosion of artificial intelligence (AI) applications and increasing expectations for autonomous systems that exhibit human-level intelligence. These expectations have, however, met with fundamental obstacles that cut across many application areas. One such obstacle is adaptability, or robustness. Machine learning researchers have noted current systems lack the ability to recognize or react to new circumstances they have not been specifically programmed or trained for.},
author = {Pearl, Judea},
doi = {10.1145/3241036},
issn = {1557-7317},
journal = {Communications of the ACM},
number = {3},
pages = {54--60},
title = {{The seven tools of causal inference, with reflections on machine learning}},
volume = {62},
year = {2019}
}
@article{Friedman:1997vs,
abstract = {Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.},
author = {Friedman, Nir and Geiger, Dan and Goldszmidt, Moises},
doi = {10.1002/9780470400531.eorms0099},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Bayesian networks,Classification},
number = {2-3},
pages = {131--163},
title = {{Bayesian Network Classifiers}},
volume = {29},
year = {1997}
}
@book{Krathwohl:2001wr,
author = {Bloom, Benjamin Samuel},
edition = {2nd},
isbn = {978-0-58-228010-6},
publisher = {Addison-Wesley Longman},
title = {{Taxonomy of Educational Objectives, Handbook 1: Cognitive Domain}},
year = {1956}
}
@article{Huysmans:2011gq,
abstract = {An important objective of data mining is the development of predictive models. Based on a number of observations, a model is constructed that allows the analysts to provide classifications or predictions for new observations. Currently, most research focuses on improving the accuracy or precision of these models and comparatively little research has been undertaken to increase their comprehensibility to the analyst or end-user. This is mainly due to the subjective nature of 'comprehensibility', which depends on many factors outside the model, such as the user's experience and his/her prior knowledge. Despite this influence of the observer, some representation formats are generally considered to be more easily interpretable than others. In this paper, an empirical study is presented which investigates the suitability of a number of alternative representation formats for classification when interpretability is a key requirement. The formats under consideration are decision tables, (binary) decision trees, propositional rules, and oblique rules. An end-user experiment was designed to test the accuracy, response time, and answer confidence for a set of problem-solving tasks involving the former representations. Analysis of the results reveals that decision tables perform significantly better on all three criteria, while post-test voting also reveals a clear preference of users for decision tables in terms of ease of use. {\textcopyright}2010 Elsevier B.V. All rights reserved.},
author = {Huysmans, Johan and Dejaeger, Karel and Mues, Christophe and Vanthienen, Jan and Baesens, Bart},
doi = {10.1016/j.dss.2010.12.003},
issn = {0167-9236},
journal = {Decision Support Systems},
keywords = {Classification,Comprehensibility,Data mining,Decision tables,Knowledge representation},
month = {apr},
number = {1},
pages = {141--154},
title = {{An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models}},
volume = {51},
year = {2011}
}
@article{Wieringa:2006vd,
author = {Wieringa, Roel and Maiden, Neil and Mead, Nancy and Rolland, Colette},
doi = {10.1007/s00766-005-0021-6},
issn = {0947-3602},
journal = {Requirements Engineering},
keywords = {Paper classification,Paper evaluation criteria,Requirements engineering research,Research methods},
month = {mar},
number = {1},
pages = {102--107},
title = {{Requirements engineering paper classification and evaluation criteria: a proposal and a discussion}},
volume = {11},
year = {2006}
}
@article{Aversano:2017ic,
abstract = {Software documentation is a basic component of the software development process and it is very important in all the phases of a software system life cycle. It is plays a very important role from the point of view of both the software engineer and user. Software documentation usually includes textual documentation required by the Software engineering standards, API documentation, Wiki pages and source code comments. Surveys and studies indicate that the documentation is not always available and, if available, only partially addresses the developers' needs, as it is often wrong, incomplete, out-of-date and ambiguous. In the context of ERP - Enterprise Resource Planning, the relevance of the software documentation is even more important due to the complexity of such a kind of software systems and the strategic role they have within operative organizations. This paper focuses on the quality assessment of the documentation of ERP open source systems with the aim of understanding if they include high quality documentation for adequately support anyone want to adopt them and/or executing maintenance activities. Specifically, a quality model is defined and its application to three Open source software system is performed.},
author = {Aversano, Lerina and Guardabascio, Daniela and Tortorella, Maria},
doi = {10.1016/j.procs.2017.11.057},
issn = {1877-0509},
journal = {Procedia Computer Science},
keywords = {Documentation Quality,Documentation maintenance,ERP Open source software,Software documentation,Software metrics},
month = {jan},
pages = {423--430},
title = {{Analysis of the Documentation of ERP Software Projects}},
volume = {121},
year = {2017}
}
@article{Judice2016,
abstract = {Purpose: Modern lifestyles require people to spend prolonged periods of sitting, and public health messages recommend replacing sitting with as much standing as is feasible. The metabolic/energy cost (MEC) of sitting and standing is poorly understood, and MEC associated with a transition from sitting to standing has not been reported. Thus, we carefully quantified the MEC for sitting, standing and sit/stand transitions, adjusting for age and fat-free mass (FFM) in a sample of adults with no known disease. Methods: Participants (N = 50; 25 women), 20–64 years, randomly performed three conditions for 10 min each (sitting, standing, 1 sit/stand transition min−1 and then sitting back down). MEC was measured by indirect calorimetry and FFM by dual-energy X-ray absorptiometry. Results: V̇O2 (ml kg−1 min−1) for sitting (2.93 ± 0.61; 2.87 ± 0.37 in men and women respectively), standing (3.16 ± 0.63; 3.03 ± 0.40), and steady-state cost of repeated sit/stand transitions (1 min−1) (3.86 ± 0.75; 3.79 ± 0.57) were significantly different regardless of sex and weight (p {\textless} 0.001). EE (kcal min−1) also differed from sitting (1.14 ± 0.18; 0.88 ± 0.11), to standing (1.23 ± 0.19; 0.92 ± 0.13), and sit/stand transitions (1 min−1) (1.49 ± 0.25; 1.16 ± 0.16). Heart-rate increased from sitting to standing ({\~{}}13 bpm; p {\textless} 0.001). Neither sex nor FFM influenced the results (p ≥ 0.05). Conclusions: This study found in a sample of adults with no known disease that continuous standing raised MEC 0.07 kcal min−1 above normal sitting. The transition from sitting to standing (and return to sitting) had a metabolic cost of 0.32 kcal min−1 above sitting. Therefore, public health messages recommending to interrupt sitting frequently should be informed of the modest energetic costs regardless of sex and body composition.},
author = {J{\'{u}}dice, Pedro B. and Hamilton, Marc T. and Sardinha, Lu{\'{i}}s B. and Zderic, Theodore W. and Silva, Analiza M.},
doi = {10.1007/s00421-015-3279-5},
file = {::},
issn = {14396319},
journal = {European Journal of Applied Physiology},
keywords = {Body composition,Breaks,Expenditure,Heart rate,Sedentary behavior},
month = {feb},
number = {2},
pages = {263--273},
publisher = {Springer Verlag},
title = {{What is the metabolic and energy cost of sitting, standing and sit/stand transitions?}},
volume = {116},
year = {2016}
}
@article{Schutte2000,
abstract = {The factorial validity of the Maslach Burnout Inventory-General Survey (MBI-GS) was investigated among employees of a multinational company in the forest industry. The present study includes data from Finnish, Swedish and Dutch employees (total N = 9055). The hypothesized three-factor model of the MBI-GS (Exhaustion, Cynicism, Professional Efficacy) was replicated across occupational groups (i.e. managers, clerks, foremen technicians, blue-collar workers) and nations. The fit of this model to the data was superior to alternative one- and two-factor models in all samples under investigation. In addition, the three-factor structure of the MBI-GS proved invariant across all occupational groups. The internal consistencies of the three subscales are satisfactory, except for the cynicism scale in some subsamples. Therefore, it is suggested to exclude one - ambiguous - cynicism item. Finally, some differences in levels of burnout are found between nations and occupational groups that are consistent with earlier findings.},
author = {Schutte, Nico and Toppinen, Salla and Kalimo, Raija and Schaufeli, Wilmar},
doi = {10.1348/096317900166877},
issn = {09631798},
journal = {Journal of Occupational and Organizational Psychology},
month = {mar},
number = {1},
pages = {53--66},
publisher = {British Psychological Society},
title = {{The factorial validity of the Maslach Burnout Inventory-General Survey (MBI-GS) across occupational groups and nations}},
url = {http://doi.wiley.com/10.1348/096317900166877},
volume = {73},
year = {2000}
}
@article{DeVries2017,
abstract = {Objectives The present study evaluated the efficacy of an exercise intervention to reduce work-related fatigue (emotional exhaustion, overall fatigue, and need for recovery). The effects of exercise on self-efficacy, sleep, work ability, cognitive functioning and aerobic fitness (secondary outcomes) were also investigated. Methods Employees with high levels of work-related fatigue were randomly assigned to either a 6-week exercise intervention (EI; N=49) or a wait-list control group (WLC; N=47). All participants were measured pre- (T0) and post-intervention (T1). EI participants were also measured 6 (T2) and 12 weeks (T3) after the end of the intervention. Analyses were based on intention-to-treat (ITT) and per-protocol (PP). PP analyses only included EI participants (N=31) who completed the intervention and WLC participants (N= 35) who did not increase their exercise level during the wait period. Results Analyses of covariance (ANCOVA) revealed that, at T1, the EI group reported lower emotional exhaustion and overall fatigue than the WLC group, however, only according to PP analyses. Both according to ITT and PP analyses, EI participants showed higher sleep quality, work ability, and self-reported cognitive functioning at T1 compared to WLC participants. Intervention effects were maintained at T2 and T3. Conclusions The exercise intervention had enduring effects on work-related fatigue and broader indicators of employee well-being. This study demonstrates that, in case of work-related fatigue, exercise does constitute a powerful medicine for those who comply with the treatment.},
author = {de Vries, Juriena D. and van Hooff, Madelon L.M. and Geurts, Sabine A.E. and Kompier, Michiel A.J.},
doi = {10.5271/sjweh.3634},
file = {::},
issn = {1795990X},
journal = {Scandinavian Journal of Work, Environment and Health},
keywords = {Burnout,Emotional exhaustion,Employee fatique,Employee well-being,Intervention,Need for recovery,RCT,Randomized controlled trial},
number = {4},
pages = {337--349},
publisher = {Nordic Association of Occupational Safety and Health},
title = {{Exercise to reduce work-related fatigue among employees: A randomized controlled trial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28323305},
volume = {43},
year = {2017}
}
@article{SandJensen2015,
abstract = {In this position paper we discuss optimization in the HCI domain based on our experiences with Bayesian methods for modeling and optimization of audio systems, including challenges related to evaluating, designing, and optimizing such interfaces. We outline and demonstrate how a combined Bayesian modeling and optimization approach provides a flexible framework for integrating various user and content attributes, while also supporting model-based optimization of HCI systems. Finally, we discuss current and future research direction and applications, such as inferring user needs and optimizing interfaces for computer assisted teaching.},
author = {{Sand Jensen}, Bj{\o}rn and {Brehm Nielsen}, Jens and Larsen, Jan},
file = {::},
journal = {Workshop on Principles, Techniques and Perspectives on Optimization and HCI. CHI 2015.},
keywords = {Author Keywords Bayes,Gaussian process priors,HCI)],Miscellaneous,Optimization},
pages = {1--4},
title = {{Perspectives on Bayesian Optimization for HCI}},
year = {2015}
}
@article{Wickham2010,
author = {Wickham, Hadley},
doi = {10.1198/jcgs.2009.07098},
file = {::},
keywords = {Grammar of graphics,Statistical graphics},
title = {{A Layered Grammar of Graphics}},
year = {2010}
}
@inproceedings{Cohen2018,
address = {New York, New York, USA},
author = {Cohen, Eldan and Consens, Mariano P.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories - MSR '18},
doi = {10.1145/3196398.3196436},
file = {::},
isbn = {9781450357166},
pages = {426--436},
publisher = {ACM Press},
title = {{Large-scale analysis of the co-commit patterns of the active developers in github's top repositories}},
url = {http://dl.acm.org/citation.cfm?doid=3196398.3196436},
year = {2018}
}
@article{Amodei2016,
annote = {AI is everywhere - medicine, science, transportation, privacy, security, fairness, economic, military
AI - robustness, risk sensitivity and safe exploration
AI Safety - supervised classification, reinforcement learning
Social impacts of AI and accidents in ML systems

Safety problems are where in the process things went wrong. 

Five broad categories of AI Safety research areas are avoiding side effects, avoiding reward hacking, scalable supervision, safe exploration and distributional shift. Having wrong objective function will cause negative side effects (Accomplishing some specific tasks), reward hacking(Designer admits some clever easy solution), scalable oversight(how to ensure safe behavior, even with limited access to the true objective function), safe exploration(negative/irrecoverable consequences that outweigh the long term value of exploration), robustness to distributional shift(how to avoid having ML systems make bad decisions)

A cleaning robot at office workspace is an illustrative example of AI Safety

Need for AI Safety - increasing promise of Reinforcement Learning (RL), trending more complex agents and environments and increasing autonomy in AI Systems

Avoiding negative side effects - many distruptive things, common sense constraints on the environment
Define an impact regularizer - formalize change to the environment, penalize state distance, choice of representation and distance metric
Learn a generalized impact regularizer
Penalize influence - not get into positions where there are side effects
Multi agent approaches - Inclusion of human agents in coperative inverse RL, reward autoencoder
Reward uncertainty - to avoid unanticipated side effects
Define a baseline policy around which changes are being considered

Avoiding reward hacking - Partially observed goals (Assumption on RL is that reward is directly experienced, but only partially observed), complicated systems, abstract rewards (hacking one dimensional space), Goodhart's Law(targeting highly correlated function to accomplish specific task), feedback loops(self-amplifying component), environmental embedding (human in the reward loop, giving the agent incentive)

Adversarial reward functions - exploit the problem with intension of high reward
Model lookahead - reward based on anticipated future states, not the current one
Adversarial blinding - blind a model to certain variables
Counterexample resistance - adversarial training
Trip wires - deliberate introduction of some plausible vulnerabilities 

Scalable oversight - semi-supervised RL, evaluation based on reward from all episodes, optimization based on limited reward samples

Safe exploration - destroy the agent or trap it in states it can't get out of it, cannot hard code all problems, reduce the need for domain specific engineering, optimise worst case performance, off policy estimation, inverse RL {\&} apprenticeship learning (Learning algorithm is provided with expert trajectories of near-optimal behavior)

Robustness to Distributional Change - recognize our own ignorance, trained on one distribution, deployed on different test distribution, limited by variance of the important estimate, covariate shift assumption (very strong and untestable), build a generative model for distribution(drawback - this is fragile), partially specified models with assumptions on some aspects of a distribution

Modeling distribution of errors of a model, assumption - errors are independent and gaussian distributed, a good example is speech systems},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.06565v2},
author = {Amodei, Dario and Steinhardt, Jacob and Man, Dan and Christiano, Paul},
eprint = {arXiv:1606.06565v2},
file = {::},
pages = {1--29},
title = {{Concrete Problems in AI Safety}},
year = {2016}
}
@article{Vtyurina2018,
abstract = {Voice-based conversational assistants are growing in popularity on ubiquitous mobile and stationary devices. Cortana, as well as Google Home, Amazon Echo, and others, can provide support for various tasks from managing reminders to booking a hotel. However, with few exceptions, user input is limited to explicit queries or commands. In this work, we explore the role of implicit conversational cues in guided task completion scenarios. In a Wizard of Oz study, we found that, for the task of cooking a recipe, nearly one-quarter of all user-assistant exchanges were initiated from implicit conversational cues rather than from plain questions. Given that these implicit cues occur in such high frequency, we conclude by presenting a set of design implications for the design of guided task experiences in contemporary conversational assistants.},
author = {Vtyurina, Alexandra and Fourney, Adam},
doi = {10.1145/3173574.3173782},
file = {::},
isbn = {9781450356206},
keywords = {conversational cues,conversational systems,task support},
title = {{Exploring the Role of Conversational Cues in Guided Task Support with Virtual Assistants}},
url = {https://doi.org/10.1145/3173574.3173782},
year = {2018}
}
@article{Ahmad2018,
abstract = {Purpose: Software developers extensively use stack overflow (SO) for knowledge sharing on software development. Thus, software engineering researchers have started mining the structured/unstructured data present in certain software repositories including the Q{\&}A software developer community SO, with the aim to improve software development. The purpose of this paper is show that how academics/practitioners can get benefit from the valuable user-generated content shared on various online social networks, specifically from Q{\&}A community SO for software development. Design/methodology/approach: A comprehensive literature review was conducted and 166 research papers on SO were categorized about software development from the inception of SO till June 2016. Findings: Most of the studies revolve around a limited number of software development tasks; approximately 70 percent of the papers used millions of posts data, applied basic machine learning methods, and conducted investigations semi-automatically and quantitative studies. Thus, future research should focus on the overcoming existing identified challenges and gaps. Practical implications: The work on SO is classified into two main categories; “SO design and usage” and “SO content applications.” These categories not only give insights to Q{\&}A forum providers about the shortcomings in design and usage of such forums but also provide ways to overcome them in future. It also enables software developers to exploit such forums for the identified under-utilized tasks of software development. Originality/value: The study is the first of its kind to explore the work on SO about software development and makes an original contribution by presenting a comprehensive review, design/usage shortcomings of Q{\&}A sites, and future research challenges.},
author = {Ahmad, Arshad and Feng, Chong and Ge, Shi and Yousif, Abdallah},
doi = {10.1108/DTA-07-2017-0054},
file = {::},
issn = {25149288},
journal = {Data Technologies and Applications},
keywords = {Information retrieval,Mining,Software development,Software repositories,Stack overflow,Text mining},
number = {2},
pages = {190--247},
title = {{A survey on mining stack overflow: question and answering (Q{\&}A) community}},
volume = {52},
year = {2018}
}
@article{Weinman2011,
abstract = {This chapter describes a GPU-based implementation of a discriminative maximum entropy learning algorithm that can improve runtime on large datasets by a factor of over 200. A typical machine-learning algorithm creates a classification function that inductively generalizes from training examples-input features and associated classification labels-to previously unseen examples requiring labels. It is used on a variety of problems, including time series prediction for financial forecasting, machine translation, character and speech recognition, and even conservation biology. Although there are many techniques for performing such classifications, the aforementioned approaches all use one type of model: the maximum entropy classifier, also known as multinomial logistic regression. Optimizing the prediction accuracy of the learned function for complex problems can require massive amounts of training data. A learner is implemented that utilizes the parallelism of a GPU for the most common scenarios. It is likely that in the cases where matrix multiplication is not the bottleneck, performance can be improved even further by optimizing the max and sum reductions at the heart of the other kernels. Unrolling loops and/or initializing extraneous values for nonpower of two block sizes so that special cases can be eliminated should yield improved runtimes. The motivation for employing MaxEnt has been to improve character recognition in arbitrary images of natural scenes. This task is more complex than typical document-based character recognition because it involves a wide variety of fonts and uncontrolled viewing conditions. {\textcopyright} 2011 Copyright {\textcopyright} 2011 NVIDIA Corporation and Wen-mei W. Hwu Published by Elsevier Inc. All rights reserved..},
archivePrefix = {arXiv},
arxivId = {1603.04467v2},
author = {Weinman, Jerod J. and Lidaka, Augustus and Aggarwal, Shitanshu},
doi = {1603.04467},
eprint = {1603.04467v2},
file = {::},
isbn = {9780123849885},
issn = {01420615},
journal = {GPU Computing Gems Emerald Edition},
pages = {277--291},
title = {{TensorFlow: Large-scale machine learning}},
year = {2011}
}
@article{Montelisciani2014,
abstract = {The access to external expertise and collaboration initiatives became an undeniable strategy for highly innovative sectors. Innovation intermediaries have gained a prominent role in this scenario, and crowdsourcing platforms reached notable results. However, identifying right competencies with demanded innovation is a critical issue. Research is needed in the domain of matchmaking mechanisms for identifying the necessary skills to fulfil different kinds of problems in order to build effective collaborative teams. This paper investigates different approaches adopted in crowdsourcing and main characteristics of teams operating in these contexts. Thus, a set of critical issues is highlighted and a structured team-building methodology for finding suitable solvers in crowdsourcing challenges is presented. Such method is grounded on natural language processing (NLP) and semantic ontologies fulfilling some of the identified criticalities. The description is supported by a case study conducted within a self-developed crowdsourcing platform. {\textcopyright}2014 Taylor {\&} Francis.},
author = {Montelisciani, Gabriele and Gabelloni, Donata and Tazzini, Giacomo and Fantoni, Gualtiero},
doi = {10.1080/09537325.2014.923095},
file = {::},
issn = {0953-7325},
journal = {Technology Analysis {\&} Strategic Management},
keywords = {collaborative innovation,competencies management,crowdsourcing,matchmaking,team building},
month = {jul},
number = {6},
pages = {687--702},
publisher = {Routledge},
title = {{Skills and wills: the keys to identify the right team in collaborative innovation platforms}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09537325.2014.923095},
volume = {26},
year = {2014}
}
@article{Pearl2019,
abstract = {THE DRAMATIC SUCCESS In machine learning has led to an explosion of artificial intelligence (AI) applications and increasing expectations for autonomous systems that exhibit human-level intelligence. These expectations have, however, met with fundamental obstacles that cut across many application areas. One such obstacle is adaptability, or robustness. Machine learning researchers have noted current systems lack the ability to recognize or react to new circumstances they have not been specifically programmed or trained for.},
author = {Pearl, Judea},
doi = {10.1145/3241036},
file = {::},
issn = {15577317},
journal = {Communications of the ACM},
number = {3},
pages = {54--60},
title = {{The seven tools of causal inference, with reflections on machine learning}},
volume = {62},
year = {2019}
}
@techreport{Ullah,
abstract = {Epilepsy is a neurological disorder and for its detection, encephalography (EEG) is a commonly used clinical approach. Manual inspection of EEG brain signals is a time-consuming and laborious process, which puts heavy burden on neurologists and affects their performance. Several automatic techniques have been proposed using traditional approaches to assist neurologists in detecting binary epilepsy scenarios e.g. seizure vs. non-seizure or normal vs. ictal. These methods do not perform well when classifying ternary case e.g. ictal vs. normal vs. inter-ictal; the maximum accuracy for this case by the state-of-the-art-methods is 97±1{\%}. To overcome this problem, we propose a system based on deep learning, which is an ensemble of pyramidal one-dimensional convolutional neural network (P-1D-CNN) models. In a CNN model, the bottleneck is the large number of learnable parameters. P-1D-CNN works on the concept of refinement approach and it results in 60{\%} fewer parameters compared to traditional CNN models. Further to overcome the limitations of small amount of data, we proposed augmentation schemes for learning P-1D-CNN model. In almost all the cases concerning epilepsy detection, the proposed system gives an accuracy of 99.1±0.9{\%} on the University of Bonn dataset.},
author = {Ullah, Ihsan and Hussain, Muhammad and Qazi, Emad-Ul-Haq and Aboalsamh, Hatim},
file = {::},
keywords = {1D-CNN,Electroencephalogram (EEG),epilepsy,ictal,interictal,seizure},
title = {{An Automated System for Epilepsy Detection using EEG Brain Signals based on Deep Learning Approach}}
}
@article{Chen2016,
abstract = {Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q{\&}A posts, which often contain overwhelming or out-of-date information. In this paper, we present a new approach to recommend analogical libraries based on a knowledge base of analogical libraries mined from tags of millions of Stack Overflow questions. The novelty of our approach is to solve analogical-libraries questions by combining state-of-the-art word embedding technique and domain-specific relational and categorical knowledge mined from Stack Overflow. We implement our approach in a proof-of-concept web application (https://graphofknowledge.appspot.com/similartech). The evaluation results show that our approach can make accurate recommendation of analogical libraries (Precision@1=0.81 and Precision@5=0.67). Google Analytics of the website traffic provides initial evidence of the potential usefulness of our web application for software developers.},
author = {Chen, Chunyang and Gao, Sa and Xing, Zhenchang},
doi = {10.1109/saner.2016.21},
file = {::},
isbn = {9781509018550},
keywords = {-analogical libraries,categorical knowledge,graph,knowledge,relational knowledge,word embedding},
pages = {338--348},
publisher = {IEEE},
title = {{Mining Analogical Libraries in Q{\&}A Discussions -- Incorporating Relational and Categorical Knowledge into Word Embedding}},
year = {2016}
}
@inproceedings{Karlson2010,
abstract = {The impact of interruptions on workflow and productivity has been extensively studied in the PC domain, but while fragmented user attention is recognized as an inherent aspect of mobile phone usage, little formal evidence exists of its effect on mobile productivity. Using a survey and a screenshot-based diary study we investigated the types of barriers people face when performing tasks on their mobile phones, the ways they follow up with such suspended tasks, and how frustrating the experience of task disruption is for mobile users. From 386 situated samples provided by 12 iPhone and 12 Pocket PC users, we distill a classification of barriers to the completion of mobile tasks. Our data suggest that moving to a PC to complete a phone task is common, yet not inherently problematic, depending on the task. Finally, we relate our findings to prior design guidelines for desktop workflow, and discuss how the guidelines can be extended to mitigate disruptions to mobile taskflow. {\textcopyright} 2010 ACM.},
author = {Karlson, Amy K. and Iqbal, Shamsi T. and Meyers, Brian and Ramos, Gonzalo and Lee, Kathy and Tang, John C.},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/1753326.1753631},
isbn = {9781605589299},
keywords = {cross-device tasks,diary study,mobile taskflow},
pages = {2009--2018},
title = {{Mobile taskflow in context: A screenshot study of smartphone usage}},
volume = {3},
year = {2010}
}
@article{Harper2019,
abstract = {This article examines some of the mystique surrounding AI, including the interrelated notions of explainability and complexity, and argues that these notions suggest that designing human-centered AI is difficult. It explains how, once these are put aside, an HCI perspective can help define interaction between AI and users that can enhance rather than substitute one important aspect of human life: creativity. Key to developing such creative interactions are abstractions and grammars of action and other notions; the article explores the history of these in HCI and how they are to be used in the contemporary interaction and design space, in relation to AI. The article is programmatic rather than empirical though its argument uses real-world examples.},
author = {Harper, Richard H.R. R},
doi = {10.1080/10447318.2019.1631527},
file = {::},
issn = {15327590},
journal = {International Journal of Human-Computer Interaction},
title = {{The Role of HCI in the Age of AI}},
volume = {7318},
year = {2019}
}
@inproceedings{Manning2015,
abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
address = {Stroudsburg, PA, USA},
author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
doi = {10.3115/v1/p14-5010},
file = {::},
month = {jun},
pages = {55--60},
publisher = {Association for Computational Linguistics (ACL)},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://aclweb.org/anthology/P14-5010},
year = {2015}
}
@inproceedings{He2017,
abstract = {We define notions of stability for learning algorithms and derive bounds on their generalization error based on the empirical error and the leave-one-out error. We then study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hilbert space regularization and Kullback-Liebler regularization. We then apply the results to SVM for regression and classification and to maximum entropy discrimination.},
archivePrefix = {arXiv},
arxivId = {1532-4435},
author = {He, Xing and Sun, Yufeng and Li, Yaqiu},
booktitle = {Proceedings of 2016 11th International Conference on Reliability, Maintainability and Safety: Integrating Big Data, Improving Reliability and Serving Personalization, ICRMS 2016},
doi = {10.1109/ICRMS.2016.8050166},
eprint = {1532-4435},
file = {::},
isbn = {9781509027149},
issn = {15324435},
keywords = {Complex system,Importance degree computation,Reliability allocation},
title = {{An improved AGREE method with reliability mathematical model for complex system importance degree computation}},
year = {2017}
}
@misc{Benbadis2008,
abstract = {Background/Aims: The overinterpretation of EEGs is common and is an important contributor to the misdiagnosis of epilepsy. We reviewed our experience in order to clarify which EEG patterns are commonly overread as epileptiform. Methods: We identified patients who were seen at our epilepsy clinic and were ultimately diagnosed as having conditions other than epilepsy. We selected those who had previously had an EEG read as showing epileptiform discharges and whose EEG was available for our own re-review. Results: 37 patients met the above criteria. Eventual diagnoses were psychogenic nonepileptic seizures (10), syncope (7), other miscellaneous diagnoses (5) and unexplained nonspecific symptoms (15). None of the EEGs had epileptiform discharges. The descriptions of the abnormalities included 'temporal sharp waves' in 30, 'frontal sharp waves' in 2 and 'generalized spike-wave complexes' in 2. Three had no reports available to identify the alleged abnormality. The benign patterns mistaken for temporal (30) and frontal (2) sharp waves were simple fluctuations of background activity with temporal phase reversals. Conclusions: By far the most common patterns overread as epileptiform are nonspecific fluctuations of background in the temporal regions, which are misread as temporal sharp waves. Copyright {\textcopyright} 2008 S. Karger AG.},
author = {Benbadis, Selim R. and Lin, Kaiwen},
booktitle = {European Neurology},
doi = {10.1159/000115641},
file = {::},
issn = {00143022},
keywords = {Electroencephalography,Epilepsy, misdiagnosis},
month = {apr},
number = {5},
pages = {267--271},
pmid = {18264016},
title = {{Errors in EEG interpretation and misdiagnosis of epilepsy: Which EEG patterns are overread?}},
volume = {59},
year = {2008}
}
@article{Colusso2019,
abstract = {Using scientific discoveries to inform design practice is an important, but difficult, objective in HCI. In this paper, we provide an overview of Translational Science in HCI by triangulating literature related to the research-practice gap with interview data from many parties engaged (or not) in translating HCI knowledge. We propose a model for Translational Science in HCI based on the concept of a continuum to describe how knowledge progresses (or stalls) through multiple steps and translations until it can influence design practice. The model offers a conceptual framework that can be used by researchers and practitioners to visualize and describe the progression of HCI knowledge through a sequence of translations. Additionally, the model may facilitate a precise identification of translational barriers, which allows devising more effective strategies to increase the use of scientific findings in design practice.},
author = {Colusso, Lucas and Munson, Sean A. and Jones, Ridley and Hsieh, Gary},
doi = {10.1145/3290605.3300231},
file = {::},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Research-practice gap,Translational research,Translational science},
pages = {1--13},
title = {{A translational science model for HCI}},
year = {2019}
}
@article{Yu2017,
abstract = {The attention mechanisms in deep neural networks are inspired by human's attention that sequentially focuses on the most relevant parts of the information over time to generate prediction output. The attention parameters in those models are implicitly trained in an end-to-end manner, yet there have been few trials to explicitly incorporate human gaze tracking to supervise the attention models. In this paper, we investigate whether attention models can benefit from explicit human gaze labels, especially for the task of video captioning. We collect a new dataset called VAS, consisting of movie clips, and corresponding multiple descriptive sentences along with human gaze tracking data. We propose a video captioning model named Gaze Encoding Attention Network (GEAN) that can leverage gaze tracking information to provide the spatial and temporal attention for sentence generation. Through evaluation of language similarity metrics and human assessment via Amazon mechanical Turk, we demonstrate that spatial attentions guided by human gaze data indeed improve the performance of multiple captioning methods. Moreover, we show that the proposed approach achieves the state-of-the-art performance for both gaze prediction and video captioning not only in our VAS dataset but also in standard datasets (e.g. LSMDC [24] and Hollywood2 [18]).},
archivePrefix = {arXiv},
arxivId = {1707.06029},
author = {Yu, Youngjae and Choi, Jongwook and Kim, Yeonhwa and Yoo, Kyung and Lee, Sang Hun and Kim, Gunhee},
doi = {10.1109/CVPR.2017.648},
eprint = {1707.06029},
file = {::},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {6119--6127},
title = {{Supervising neural attention models for video captioning by human gaze data}},
volume = {2017-Janua},
year = {2017}
}
@article{Zhang2020,
abstract = {Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.},
archivePrefix = {arXiv},
arxivId = {2001.06684},
author = {Zhang, Amy X. and Muller, Michael and Wang, Dakuo},
doi = {10.1145/3392826},
eprint = {2001.06684},
file = {::},
issn = {25730142},
journal = {Proceedings of the ACM on Human-Computer Interaction},
keywords = {collaboration,collaborative data science,data science,data scientists,human-centered data science,machine learning,teams},
number = {CSCW1},
pages = {1--23},
title = {{How do Data Science Workers Collaborate? Roles, Workflows, and Tools}},
volume = {4},
year = {2020}
}
@article{Boucher2017,
abstract = {Hypertension is an important risk factor of cardiovascular diseases, the leading cause of death worldwide. Adverse effects of psychosocial factors at work might increase the risk of masked hypertension, but evidences are still scarce. The objective of this study is then to determine whether adverse psychosocial work factors from the effort-reward imbalance (ERI) model are associated with the prevalence of masked hypertension in a population of white-collar workers. White-collar workers were recruited from three public organizations. Blood pressure was measured at the workplace for manually operated measurements (mean of the first three readings taken by a trained assistant) followed by ambulatory measurements (mean of all subsequent readings taken during the working day). Masked hypertension was defined as manually operated BP{\textless}140/90 mm Hg and ambulatory BP ≥135/85 mm Hg. ERI exposure at work was measured using Siegrist's validated questionnaire. Blood pressure readings were obtained from 2369 workers (participation proportion: 85{\%}). ERI exposure (OR: 1.53 (95{\%} CI: 1.16-2.02) and high efforts at work (OR: 1.61 (95{\%} CI: 1.13-1.29) were associated with masked hypertension, after adjusting for sociodemographic and cardiovascular risk factors. Workers exposed to an imbalance between efforts spent at work and reward had a higher prevalence of masked hypertension. High efforts at work might be of particular importance in explaining this association. Future studies should be designed to investigate how clinicians can include questions on psychosocial work factors to screen for masked hypertension and how workplace interventions can decrease adverse psychosocial exposures to lower BP.},
author = {Boucher, P. and Gilbert-Ouimet, M. and Trudel, X. and Duchaine, C. S. and Milot, A. and Brisson, C.},
doi = {10.1038/jhh.2017.42},
issn = {14765527},
journal = {Journal of Human Hypertension},
keywords = {Hypertension,Risk factors},
month = {oct},
number = {10},
pages = {620--626},
publisher = {Nature Publishing Group},
title = {{Masked hypertension and effort-reward imbalance at work among 2369 white-collar workers}},
volume = {31},
year = {2017}
}
@inproceedings{Caesar2019,
abstract = {Previous research demonstrated that Bluetooth Low Energy beacons enable very accurate indoor positioning. This leads to the question whether a Bluetooth Mesh network could inadvertently serve the same purpose, leading to unexpected location privacy violations. We analyze the information broadcasted by a typical Bluetooth Mesh installation and show that it can indeed be utilized by a potentially malicious smartphone app in order to localize a smartphone user within a building. This is facilitated by the unique advertising address regularly emitted by each mesh node. Further, we show that implementing address randomization on the side of the mesh network completely prevents this type of positioning without having a negative impact on the functioning of the mesh network.},
address = {New York, New York, USA},
author = {Caesar, Matthias and Steffan, Jan},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3339252.3340507},
isbn = {9781450371643},
keywords = {Bluetooth Mesh Protocol,Location Privacy,Tracking},
month = {aug},
pages = {1--7},
publisher = {Association for Computing Machinery},
title = {{A location privacy analysis of bluetooth mesh}},
url = {http://dl.acm.org/citation.cfm?doid=3339252.3340507},
year = {2019}
}
@article{Barnett2019a,
abstract = {Interest in mobile application development has significantly increased. The need for rapid, iterative development coupled with the diversity of platforms, technologies and frameworks impacts on the productivity of developers. In this paper we propose a new approach and tool support, Rapid APPlication Tool (RAPPT), that enables rapid development of mobile applications. It employs Domain Specific Visual Languages and Modeling techniques to help developers define the characteristics of their applications using high level visual notations. Our approach also provides multiple views of the application to help developers have a better understanding of the different aspects of their application. Our user evaluation of RAPPT demonstrates positive feedback ranging from expert to novice developers.},
author = {Barnett, Scott and Avazpour, Iman and Vasa, Rajesh and Grundy, John},
doi = {10.1016/j.cola.2019.02.001},
issn = {25901184},
journal = {Journal of Computer Languages},
month = {apr},
pages = {88--96},
title = {{Supporting multi-view development for mobile applications}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1045926X15300458},
volume = {51},
year = {2019}
}
@inproceedings{SriramSrinivasan2018,
address = {San Jose},
author = {{Sriram Srinivasan}},
booktitle = {DataWorks Summit},
title = {{Applying Software engineering practices for the data science and machine learning lifecycle}},
url = {https://www.slideshare.net/Hadoop{\_}Summit/software-engineering-practices-for-the-data-science-and-machine-learning-lifecycle},
year = {2018}
}
@article{Christiansen2013,
abstract = {Type providers [16], pioneered in the F{\#} programming language, are a practical and powerful means of gaining the benefits of a modern static type system when working with data schemas that are defined outside of the programming language, such as relational databases. F{\#} type providers are implemented using a form of compile-time code generation, which requires the compiler to expose an internal API and can undermine type safety. We show that with dependent types it is possible to define a type provider mechanism that does not rely on code generation. Using this mechanism, a type provider becomes a kind of generic program that is instantiated for a particular external schema, which can be represented using an ordinary datatype. Because these dependent type providers use the ordinary abstraction mechanisms of the type system, they preserve its safety properties. We evaluate the practicality of this technique and explore future extensions.},
author = {Christiansen, David Raymond},
doi = {10.1145/2502488.2502495},
file = {::},
isbn = {9781450323895},
journal = {Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP},
keywords = {Dependent types,Generic programming,Metaprogramming,Type providers},
pages = {25--34},
title = {{Dependent type providers}},
year = {2013}
}
@article{Bafatakis2019,
abstract = {Software developers all over the world use Stack Overflow (SO) to interact and exchange code snippets. Research also uses SO to harvest code snippets for use with recommenda- tion systems. However, previous work has shown that code on SO may have quality issues, such as security or license problems. We analyse Python code on SO to determine its coding style compliance. From 1,962,535 code snippets tagged with ‘python', we extracted 407,097 snippets of at least 6 statements of Python code. Surprisingly, 93.87{\%} of the extracted snippets contain style violations, with an average of 0.7 violations per statement and a huge number of snippets with a considerably higher ratio. Researchers and developers should, therefore, be aware that code snippets on SO may not representative of good coding style. Furthermore, while user reputation seems to be unrelated to coding style compliance, for posts with vote scores in the range between -10 and 20, we found a strong correlation (r = −0.87, p {\textless} 10−7) between the vote score a post received and the average number of violations per statement for snippets in such posts.},
author = {Bafatakis, Nikolaos and Boecker, Niels and Boon, Wenjie and Salazar, Martin Cabello and Krinke, Jens and Oznacar, Gazi and White, Robert},
doi = {10.1109/MSR.2019.00042},
file = {::},
journal = {In Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {210--214},
title = {{Python Coding Style Compliance on Stack Overflow}},
year = {2019}
}
@article{Hunter2016,
abstract = {Surprisingly little research investigates employee breaks at work, and even less research provides prescriptive suggestions for better workday breaks in terms of when, where, and how break activities are most beneficial. Based on the effort-recovery model and using experience sampling methodology, we examined the characteristics of employee workday breaks with 95 employees across 5 workdays. In addition, we examined resources as a mediator between break characteristics and well-being. Multilevel analysis results indicated that activities that were preferred and earlier in the work shift related to more resource recovery following the break. We also found that resources mediated the influence of preferred break activities and time of break on health symptoms and that resource recovery benefited person-level outcomes of emotional exhaustion, job satisfaction, and organizational citizenship behavior. Finally, break length interacted with the number of breaks per day such that longer breaks and frequent short breaks were associated with more resources than infrequent short breaks.},
author = {Hunter, Emily M. and Wu, Cindy},
doi = {10.1037/apl0000045},
issn = {00219010},
journal = {Journal of Applied Psychology},
keywords = {Breaks,Effort-recovery model,Health,Resources},
month = {feb},
number = {2},
pages = {302--311},
pmid = {26375961},
publisher = {American Psychological Association Inc.},
title = {{Give me a Better break: Choosing workday break activities to maximize resource recovery}},
volume = {101},
year = {2016}
}
@techreport{Grossman2012,
author = {Grossman},
file = {::},
title = {{(12) United States Patent (30) Foreign Application Priority Data OTHER PUBLICATIONS}},
url = {www.ip.com,},
year = {2012}
}
@article{Jung2013,
abstract = {The purpose of this study was to investigate the relationship between job stressors and job burnout, and to explore the mediating effect of mood regulation on these two variables. A total of 532 employees working as information technology professionals in South Korea participated and data from 499 participants who completed all of the questionnaires were used for the data analyses. The results showed that each job burnout factor (i.e., emotional exhaustion and cynicism) was predicted by job stressors (i.e., role overload, role insufficiency, and role boundary). Also, the mediating effects of mood regulation on the relationship between job burnout and job stressors were apparent, but the relationships varied across the two factors. For instance, there was a direct mediating effect of mood regulation on the relationship between role insufficiency, role boundary, and emotional exhaustion, and partial mediating effects on the relationships between role overload, role insufficiency, and role boundary, and cynicism. The findings suggest that the associations between job burnout and job stressors are mediated by mood regulation among information technology professionals. The implication of these research findings and limitations of this study are discussed. {\textcopyright} 2013 Copyright Taylor and Francis Group, LLC.},
author = {Jung, Eunju},
doi = {10.1080/15555240.2013.779502},
issn = {15555240},
journal = {Journal of Workplace Behavioral Health},
keywords = {burnout,information technology professional,job stress,mediator,mood regulation},
month = {apr},
number = {2},
pages = {94--106},
publisher = { Taylor {\&} Francis Group },
title = {{Work Stress and Burnout: The Mediating Role of Mood Regulation Among Information Technology Professionals}},
volume = {28},
year = {2013}
}
@article{Siddiqui2019,
abstract = {Epilepsy is a common neurological disorder, and epileptic seizure detection is a scientific challenge since sometimes patient do not experience any alert. The objective of this research is to reduce the seizure detection time while maintaining high accuracy, and locate the brain hemisphere that is mostly affected by seizure. We argue that by using a decision forest (i.e., an ensemble of carefully built decision trees), instead of a single classifier such as a decision tree, we can afford to reduce epoch lengths (used for converting the ECoG and EEG signal into datasets) without compromising accuracy. This will allow us to build the future records in a shorter time resulting in a quicker seizure detection. In this paper, we apply two decision forest classifiers, called SysFor and Forest CERN, on an ECoG brain dataset. Our initial experiments on the dataset of a single patient indicate that decision forest algorithms such as SysFor and Forest CERN can reduce the seizure detection time significantly while maintaining 100{\%} accuracy. They can also be used to identify the region of the brain of a patient that is mostly affected by seizure.},
author = {Siddiqui, Mohammad Khubeb and Islam, Md Zahidul and Kabir, Muhammad Ashad},
doi = {10.1007/s00521-018-3381-9},
file = {::},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Brain data mining,Classification,Data mining,Decision forest,Decision tree,Epilepsy,Quick seizure detection,Seizure localization},
month = {sep},
number = {9},
pages = {5595--5608},
publisher = {Springer London},
title = {{A novel quick seizure detection and localization through brain data mining on ECoG dataset}},
volume = {31},
year = {2019}
}
@inproceedings{Cummaudo:2019esem,
abstract = {Background: Good API documentation facilitates the development process, improving productivity and quality. While the topic of API documentation quality has been of interest for the last two decades, there have been few studies to map the specific constructs needed to create a good document. In effect, we still need a structured taxonomy that captures such knowledge systematically.Aims: This study reports emerging results of a systematic mapping study. We capture key conclusions from previous studies that assess API documentation quality, and synthesise the results into a single framework.Method: By conducting a systematic review of 21 key works, we have developed a five dimensional taxonomy based on 34 categorised weighted recommendations.Results: All studies utilise field study techniques to arrive at their recommendations, with seven studies employing some form of interview and questionnaire, and four conducting documentation analysis. The taxonomy we synthesise reinforces that usage description details (code snippets, tutorials, and reference documents) are generally highly weighted as helpful in API documentation, in addition to design rationale and presentation.Conclusions: We propose extensions to this study aligned to developer utility for each of the taxonomy's categories.},
address = {Porto de Galinhas, Recife, Brazil},
author = {Cummaudo, Alex and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 13th International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2019.8870148},
isbn = {978-1-72-812968-6},
issn = {1949-3789},
keywords = {API,DevX,documentation,systematic mapping study,taxonomy},
month = {oct},
pages = {1--6},
publisher = {IEEE},
title = {{What should I document? A preliminary systematic mapping study into API documentation knowledge}},
year = {2019}
}
@article{Yuan2017a,
abstract = {Purpose Automatic seizure detection is significant for the diagnosis of epilepsy and the reduction of massive workload for reviewing continuous EEG recordings. Methods Compared with the long non-seizure periods, the durations of the seizure events are much shorter in the continuous EEG recordings. So the seizure detection task can be regarded as an imbalanced classification problem. In this paper, a novel method based on the weighted extreme learning machine (ELM) is proposed for seizure detection with imbalanced EEG data distribution. Firstly, the wavelet packet transform is employed to analyze the EEG data and obtain the time and frequency domain features, and the pattern match regularity statistic (PMRS) is used as the nonlinear feature to quantify the complexity of the EEG time series. After that, the EEG feature vectors are discriminated by the weighted ELM. It can assign different weights for the EEG feature samples according to the class distribution, so that to effectively moderate the bias in performance caused by imbalanced class distribution. Results The metric G-mean which takes into account of both the sensitivity and specificity is used to evaluate the performance of this method. The G-mean of 93.96{\%}, event-based sensitivity of 97.73{\%} and false alarm rate of 0.37/h are yielded on the publicly available EEG dataset. Conclusion The comparison with other detection methods shows the superior performance of this method, which indicates its potential for detecting seizure events in clinical practice. Additionally, much larger amounts of true continuous EEG data will be used to test the proposed method further in the future work.},
author = {Yuan, Qi and Zhou, Weidong and Zhang, Liren and Zhang, Fan and Xu, Fangzhou and Leng, Yan and Wei, Dongmei and Chen, Meina},
doi = {10.1016/j.seizure.2017.05.018},
file = {::},
issn = {15322688},
journal = {Seizure},
keywords = {EEG,Imbalanced classification,Seizure detection,Wavelet packet transform,Weighted ELM},
month = {aug},
pages = {99--108},
pmid = {28649016},
publisher = {W.B. Saunders Ltd},
title = {{Epileptic seizure detection based on imbalanced classification and wavelet packet transform}},
volume = {50},
year = {2017}
}
@article{Sulir2017,
abstract = {Source code is a primary artifact where program-mers are looking when they try to comprehend a program. However, to improve program comprehension efficiency, tools often associate parts of source code with metadata collected from static and dynamic analysis, communication artifacts and many other sources. In this article, we present a systematic mapping study of approaches and tools labeling source code elements with metadata and presenting them to developers in various forms. We selected 25 from more than 2,000 articles and categorized them. A taxonomy with four dimensions – source, target, presentation and persistence – was formed. Based on the survey results, we also identified interesting future research challenges.},
author = {Sulir, Matus and Poruban, Jaroslav},
doi = {10.15439/2017F229},
file = {::},
isbn = {9788394625375},
journal = {Proceedings of the 2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017},
pages = {721--729},
title = {{Labeling source code with metadata: A survey and taxonomy}},
volume = {11},
year = {2017}
}
@article{Abdul,
abstract = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasingly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explainable systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorithmic accountability , interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research towards this goal.},
author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y and Kankanhalli, Mohan},
doi = {10.1145/3173574.3174156},
file = {::},
isbn = {9781450356206},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Explainable artificial intelligence,Intelligibility,explanations,interpretable machine learning.},
title = {{Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda}},
url = {https://doi.org/10.1145/3173574.3174156},
year = {2018}
}
@article{Tenenbaum2006,
abstract = {Human perception and memory are often explained as optimal statistical inferences that are informed by accurate prior probabilities. In contrast, cognitive judgments are usually viewed as following error-prone heuristics that are insensitive to priors.We examined the optimality of human cognition in a more realistic context than typical laboratory studies, asking people to make predictions about the duration or extent of everyday phenomena such as human life spans and the box-office take of movies. Our results suggest that everyday cognitive judgments follow the same optimal statistical principles as perception and memory, and reveal a close correspondence between people's implicit probabilistic models and the statistics of the world.},
author = {Tenenbaum, Joshua B and Griffiths, Thomas L},
file = {::},
journal = {Psychological Science},
number = {9},
pages = {767--773},
title = {{Optimal Predictions in Everyday Cognition}},
volume = {17},
year = {2006}
}
@article{Kaptein2012,
abstract = {CHI researchers typically use a significance testing approach to statistical analysis when testing hypotheses during usability evaluations. However, the appropriateness of this approach is under increasing criticism, with statisticians, economists, and psychologists arguing against the use of routine interpretation of results using "canned" p values. Three problems with current practice - the fallacy of the transposed conditional, a neglect of power, and the reluctance to interpret the size of effects - can lead us to build weak theories based on vaguely specified hypothesis, resulting in empirical studies which produce results that are of limited practical or scientific use. Using publicly available data presented at CHI 2010 [19] as an example we address each of the three concerns and promote consideration of the magnitude and actual importance of effects, as opposed to statistical significance, as the new criteria for evaluating CHI research. Copyright 2012 ACM.},
author = {Kaptein, Maurits and Robertson, Judy},
doi = {10.1145/2207676.2208557},
file = {::},
isbn = {9781450310154},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Bayesian statistics,Research methods,Usability evaluation},
pages = {1105--1113},
title = {{Rethinking statistical analysis methods for CHI}},
year = {2012}
}
@article{Amershi2019,
abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
doi = {10.1145/3290605.3300233},
file = {::},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {AI-infused systems,Design guidelines,Human-AI interaction},
pages = {1--13},
title = {{Guidelines for human-AI interaction}},
year = {2019}
}
@article{Furbass2015,
abstract = {Aims of the study: Continuous EEG from critical care patients needs to be evaluated time efficiently to maximize the treatment effect. A computational method will be presented that detects rhythmic and periodic patterns according to the critical care EEG terminology (CCET) of the American Clinical Neurophysiology Society (ACNS). The aim is to show that these detected patterns support EEG experts in writing neurophysiological reports. Materials and methods: First of all, three case reports exemplify the evaluation procedure using graphically presented detections. Second, 187. hours of EEG from 10 critical care patients were used in a comparative trial study. For each patient the result of a review session using the EEG and the visualized pattern detections was compared to the original neurophysiology report. Results: In three out of five patients with reported seizures, all seizures were reported correctly. In two patients, several subtle clinical seizures with unclear EEG correlation were missed. Lateralized periodic patterns (LPD) were correctly found in 2/2 patients and EEG slowing was correctly found in 7/9 patients. In 8/10 patients, additional EEG features were found including LPDs, EEG slowing, and seizures. Conclusion: The use of automatic pattern detection will assist in review of EEG and increase efficiency. The implementation of bedside surveillance devices using our detection algorithm appears to be feasible and remains to be confirmed in further multicenter studies.},
author = {F{\"{u}}rbass, F. and Hartmann, M. M. and Halford, J. J. and Koren, J. and Herta, J. and Gruber, A. and Baumgartner, C. and Kluge, T.},
doi = {10.1016/j.neucli.2015.08.001},
file = {::},
issn = {17697131},
journal = {Neurophysiologie Clinique},
keywords = {ACNS ICU terminology,Automatic detection,Critical care,EEG,Rhythmic and periodic patterns},
number = {3},
pages = {203--213},
publisher = {Elsevier Masson SAS},
title = {{Automatic detection of rhythmic and periodic patterns in critical care EEG based on American Clinical Neurophysiology Society (ACNS) standardized terminology}},
volume = {45},
year = {2015}
}
@techreport{Drake1993,
abstract = {Sudden unexpected death in epilepsy (SUDEP) has been ascribed to cardiac arrhythmia, possibly triggered by cerebral events. Young, noncompliant, substance-abusing males with convulsions may be at risk. EEG/ECG studies have not shown significant cardiac arrhythmias in these and other seizure patients. We reviewed resting ECGs in 75 epilepsy patients and compared ventricular rate, PR interval, QRS duration, and QT interval corrected for heart rate (QTC) with normal ECGs recorded in age-matched patients without cardiac or neurologic disorders. No potentially lethal arrhythmias were noted in the seizure patients. Patients who fit the previously-described profile of high risk of SUDEP had more abnormal ECGs and ventricular rate was faster in these patients than in other epileptics. Patients with complex partial and secondarily generalized seizures had faster ventricular rates than other epileptics. No differences were noted in QRS duration or PR interval. QT was longer in patients with complex partial seizures than in control ECGs or other epileptic patients. These findings suggest that resting ECG has low diagnostic yield in epilepsy patients without cardiac symptoms. The factors possibly predisposing to SUDEP may relatively increase resting heart rate, however, and relatively increased QT interval with complex partial seizures may indicate some differences, possibly neurally-mediated, in cardiac excitability which could contribute to SUDEP.},
author = {Drake, Miles E and Reider, Carson R and Kay, Amparo},
booktitle = {Seizure},
file = {::},
keywords = {arrhythmia,electrocardiogram,epilepsy,seizures,sudden death},
pages = {63--65},
title = {{Electrocardiography in epilepsy patients without cardiac symptoms*}},
volume = {2},
year = {1993}
}
@article{Leyshon2010,
abstract = {Introduction: Ergonomic interventions designed for office and computer work have become widely available and heavily marketed but there is little evidence to support their use with workers who already have a musculoskeletal disorder (MSD). The purpose of any ergonomic intervention can be to improve worker comfort, safety and/or productivity. The ergonomic research in secondary prevention typically focuses outcomes on improved worker comfort but less if any emphasis has been put on productivity and safety. The purpose of this study was to determine the level and quality of evidence supporting ergonomic interventions to improve the comfort, safety and/or productivity of office workers with symptoms of MSDs. Method: A search of the ergonomic intervention literature based on MSDs of four body areas (low back, upper limb, eye and neck) was employed. The studies underwent two levels of analysis for inclusion in a best-evidence synthesis approach, which included a priori evaluation of specific interventions relative to outcomes of comfort, safety and/or productivity. Results: Twenty-seven out of 202 articles were synthesized based on relevance, quality and significant results. Only 8 articles were determined high quality and no strong levels of evidence were identified. Levels of evidence for specific ergonomic interventions ranged from insufficient to moderate. Generally outcomes were focused mostly on improved comfort of workers. Conclusions: There is still limited quality research that addresses ergonomic interventions designed for secondary prevention. Further high quality studies are needed to support evidence-based ergonomic interventions in practice. For all stakeholders to fully evaluate the usefulness of the ergonomic intervention studies need to attend to outcomes not only of worker comfort but also to productivity and safety. {\textcopyright} 2010 - IOS Press and the authors. All rights reserved.},
author = {Leyshon, Rhysa and Chalova, Katrina and Gerson, Leigh and Savtchenko, Alex and Zakrzewski, Remik and Howie, Andrew and Shaw, Lynn},
doi = {10.3233/WOR-2010-0994},
issn = {10519815},
journal = {Work},
keywords = {Ergonomics,Intervention,Musculoskeletal disorder,Office work,Outcome},
month = {jan},
number = {3},
pages = {335--348},
publisher = {IOS Press},
title = {{Ergonomic interventions for office workers with musculoskeletal disorders: A systematic review}},
volume = {35},
year = {2010}
}
@book{allbee2018hands,
author = {Allbee, Brian},
publisher = {Packt Publishing Ltd},
title = {{Hands-On Software Engineering with Python: Move beyond basic programming and construct reliable and efficient software with complex code}},
year = {2018}
}
@article{Shreve2011,
abstract = {Classification analysis utilizes features for separating observations into distinct groups for decision-making purposes. This study provides a systematic design for comparing the performance of six classification methods using Monte Carlo simulations and illustrates that the variable selection process is integral in comparing methodologies to ensure minimal bias, enhanced stability, and optimize performance. We quantify the variable selection bias and show that, for sufficiently large samples, this bias is minimized so that methods can be compared. We address topics relevant to model building and provide prescriptions for future comparisons so as to build a body of evidence for recommending their use. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Shreve, J. and Schneider, H. and Soysal, O.},
doi = {10.1016/j.dss.2011.08.001},
file = {::},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Classification,Comparison,F-measure,Prediction,Reliability,Validity,Variable selection},
title = {{A methodology for comparing classification methods through the assessment of model stability and validity in variable selection}},
year = {2011}
}
@article{Cole2015,
abstract = {Background: Workplace sedentary behaviour is a priority target for health promotion. However, little is known about how to effect change. We aimed to explore desk-based office workers' perceptions of factors that influenced sedentary behaviour at work and to explore the feasibility of using a novel mobile phone application to track their behaviours. Methods: We invited office employees (n = 12) and managers (n = 2) in a software engineering company to participate in semi-structured interviews to explore perceived barriers and facilitators affecting workplace sedentary behaviour. We assessed participants' sedentary behaviours using an accelerometer before and after they used a mobile phone application to record their activities at self-selected time intervals daily for 2 weeks. Interviews were analysed using a thematic framework. Results: Software engineers (5 employees; 2 managers) were interviewed; 13 tested the mobile phone application; 8 returned feedback. Major barriers to reducing workplace sedentary behaviour included the pressure of 'getting the job done', the nature of their work requiring sitting at a computer, personal preferences for the use of time at and after work, and a lack of facilities, such as a canteen, to encourage moving from their desks. Facilitators for reduced sedentariness included having a definite reason to leave their desks, social interaction and relief of physical and mental symptoms of prolonged sitting. The findings were similar for participants with different levels of overall physical activity. Valid accelerometer data were tracked for four participants: all reduced their sedentary behaviour. Participants stated that recording data using the phone application added to their day's work but the extent to which individuals perceived this as a burden varied and was counter-balanced by its perceived value in increasing awareness of sedentary behaviour. Individuals expressed a wish for flexibility in its configuration. Conclusions: These findings indicate that employers' and employees' perceptions of the cultural context and physical environment of their work, as well as personal factors, must be considered in attempting to effect changes that reduce workplace sedentary behaviour. Further research should investigate appropriate individually tailored approaches to this challenge, using a framework of behaviour change theory which takes account of specific work practices, preferences and settings.},
author = {Cole, Judith A. and Tully, Mark A. and Cupples, Margaret E.},
doi = {10.1186/s13104-015-1670-2},
file = {::},
issn = {1756-0500},
journal = {BMC Research Notes},
keywords = {Barriers,Behaviour change,Incentives,Sedentary,Sitting time,Workplace},
month = {dec},
number = {1},
pages = {683},
publisher = {BioMed Central Ltd.},
title = {{“They should stay at their desk until the work's done”: a qualitative study examining perceptions of sedentary behaviour in a desk-based occupational setting}},
url = {http://www.biomedcentral.com/1756-0500/8/683},
volume = {8},
year = {2015}
}
@article{Dobslaw,
archivePrefix = {arXiv},
arxivId = {arXiv:2001.06652v1},
author = {Dobslaw, Felix and Gomes, Francisco and Neto, De Oliveira and Feldt, Robert},
eprint = {arXiv:2001.06652v1},
file = {::},
pages = {1--8},
title = {{Boundary Value Exploration for Software Analysis}}
}
@article{Syntetos2009,
abstract = {Forecasting and planning for inventory management has received considerable attention from the Operational Research (OR) community over the last 50 years because of its implications for decision making, both at the strategic level of an organization and at the operational level. Many influential contributions have been made in this area, reflecting different perspectives that have evolved in divergent strands of the literature, namely: system dynamics, control theory and forecasting theory (both statistical and judgemental). Although this pluralism is healthy in terms of knowledge advancement, it also signifies the fragmentation of the OR discipline and the lack of cross-fertilization of ideas to develop more comprehensive approaches towards the resolution of the same issues. In this paper, the relevant literature is reviewed and synthesized to promote some convergence between these different approaches to inventory forecasting and planning. The review concludes with an inter-disciplinary agenda for further research.},
author = {Syntetos, A. A. and Boylan, J. E. and Disney, S. M.},
doi = {10.1057/jors.2008.173},
file = {::},
issn = {01605682},
journal = {Journal of the Operational Research Society},
keywords = {Control theory,Forecasting,Inventory management,System dynamics},
number = {SUPPL. 1},
title = {{Forecasting for inventory planning: A 50-year review}},
volume = {60},
year = {2009}
}
@misc{Besharati2018,
abstract = {Purpose. The aim of this study was to investigate musculoskeletal disorders (MSDs) and associated factors among Iranian office personnel. Materials and methods. In this cross-sectional study, 359 Iranian office workers were included. Data were gathered using a demographic questionnaire, the Nordic musculoskeletal questionnaire, the numeric rating scale, rapid office strain assessment (ROSA) and the NASA task load index (NASA-TLX). Results. Our findings showed that the highest prevalence rate of MSDs within the last 12 months and the highest pain/discomfort severity were related to the participants' necks. The mean performance, mental demand and effort subscale scores of the NASA-TLX were higher than other subscales (physical demand, temporal demand and frustration level). ROSA scores showed that 53.8{\%} of the participants were in action level 1 (low MSD risk) and the rest (46.2{\%}) were in action level 2 (high MSD risk). The pain/discomfort severity in the shoulders, elbows, wrists/hands, thighs and ankles/feet was correlated to the final ROSA score. Age, gender, body mass index and some NASA-TLX subscales (effort, mental demand and performance) were associated with symptoms of MSDs in different body regions. Conclusions. Improving workplace conditions (both mentally and physically) is suggested for reducing and eliminating musculoskeletal problems among office workers.},
author = {Besharati, Alireza and Daneshmandi, Hadi and Zareh, Khodabakhsh and Fakherpour, Anahita and Zoaktafi, Mojgan},
booktitle = {International Journal of Occupational Safety and Ergonomics},
doi = {10.1080/10803548.2018.1501238},
issn = {10803548},
keywords = {NASA task load index,musculoskeletal disorders,office workers,pain,rapid office strain assessment},
publisher = {Taylor and Francis Ltd.},
title = {{Work-related musculoskeletal problems and associated factors among office workers}},
year = {2018}
}
@article{Song2018b,
abstract = {Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2018.},
author = {Song, Hyunjoon},
file = {::},
keywords = {Electrical Engineering and Computer Science.,Thesis},
number = {2017},
title = {{AutoFE : efficient and robust automated feature engineering}},
url = {https://dspace.mit.edu/handle/1721.1/119919},
year = {2018}
}
@article{Ackoglu2020,
abstract = {The present study developed a feature selection (FS)-based decision support system using the electroencephalography (EEG) signals recorded from neonates with and without seizures. The study employed 10 different FS algorithms to reduce the classification cost by using fewer features and to improve the classification performance of the model by removing the irrelevant features. In doing so, the classification performance of each FS algorithm on each EEG channel difference was also evaluated. The dataset used in the study included EEG measurements and visual EEG annotations that were recorded from 79 term neonates. Multiple features were extracted from each channel difference using the Feature extraction (FE). Subsequently, a novel feature subset was generated for the classification using FS algorithms. The classification performance of each selected feature was assessed based on multiple criteria. The use of features extracted by the combined use of FS algorithms showed higher performance compared to the use of all features. In this study, 18 channel differences were analyzed. Better performance was achieved by using 3 of the selected 14 features or 2 of the selected features. The C4-P4 channel difference showed the highest classification performance (98.8{\%}) among all channel differences. In the literature, FE has already been performed for the classification of the dataset used in the present study. The primary aim of the present study was to perform the same classification with the minimum number of features. The results indicated that feature reduction reduced the cost and also improved the performance of the classification. These results seem to be highly promising and thus can be used in clinical practice and shed light for future studies.},
author = {A{\c{c}}ıkoğlu, Merve and Tuncer, Seda Arslan},
doi = {10.1016/j.mehy.2019.109464},
file = {::},
issn = {15322777},
journal = {Medical Hypotheses},
keywords = {Feature selection,Machine learning,Neonatal seizure,Ranking},
month = {feb},
publisher = {Churchill Livingstone},
title = {{Incorporating feature selection methods into a machine learning-based neonatal seizure diagnosis}},
volume = {135},
year = {2020}
}
@article{Zeng2006,
abstract = {BACKGROUND: The text descriptions in electronic medical records are a rich source of information. We have developed a Health Information Text Extraction (HITEx) tool and used it to extract key findings for a research study on airways disease.{\$}\backslash{\$}n{\$}\backslash{\$}nMETHODS: The principal diagnosis, co-morbidity and smoking status extracted by HITEx from a set of 150 discharge summaries were compared to an expert-generated gold standard.{\$}\backslash{\$}n{\$}\backslash{\$}nRESULTS: The accuracy of HITEx was 82{\%} for principal diagnosis, 87{\%} for co-morbidity, and 90{\%} for smoking status extraction, when cases labeled "Insufficient Data" by the gold standard were excluded.{\$}\backslash{\$}n{\$}\backslash{\$}nCONCLUSION: We consider the results promising, given the complexity of the discharge summaries and the extraction tasks.},
author = {Zeng, Qing T. and Goryachev, Sergey and Weiss, Scott and Sordo, Margarita and Murphy, Shawn N. and Lazarus, Ross},
doi = {10.1186/1472-6947-6-30},
file = {::},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
month = {jul},
title = {{Extracting principal diagnosis, co-morbidity and smoking status for asthma research: Evaluation of a natural language processing system}},
volume = {6},
year = {2006}
}
@inproceedings{Dormuth2019,
abstract = {Many software analysis methods have come to rely on machine learning approaches. Code segmentation - the process of decomposing source code into meaningful blocks - can augment these methods by featurizing code, reducing noise, and limiting the problem space. Traditionally, code segmentation has been done using syntactic cues; current approaches do not intentionally capture logical content. We develop a novel deep learning approach to generate logical code segments regardless of the language or syntactic correctness of the code. Due to the lack of logically segmented source code, we introduce a unique data set construction technique to approximate ground truth for logically segmented code. Logical code segmentation can improve tasks such as automatically commenting code, detecting software vulnerabilities, repairing bugs, labeling code functionality, and synthesizing new code.},
archivePrefix = {arXiv},
arxivId = {1907.08615},
author = {Dormuth, Jacob and Gelman, Ben and Moore, Jessica and Slater, David},
booktitle = {Proceedings of the 31st International Conference on Software Engineering and Knowledge Engineering},
doi = {10.18293/SEKE2019-026},
eprint = {1907.08615},
file = {::},
title = {{Logical Segmentation of Source Code}},
url = {http://arxiv.org/abs/1907.08615{\%}0Ahttp://dx.doi.org/10.18293/SEKE2019-026},
year = {2019}
}
@article{Baldassano2017,
abstract = {There exist significant clinical and basic research needs for accurate, automated seizure detection algorithms. These algorithms have translational potential in responsive neurostimulation devices and in automatic parsing of continuous intracranial electroencephalography data. An important barrier to developing accurate, validated algorithms for seizure detection is limited access to high-quality, expertly annotated seizure data from prolonged recordings. To overcome this, we hosted a kaggle.com competition to crowdsource the development of seizure detection algorithms using intracranial electroencephalography from canines and humans with epilepsy. The top three performing algorithms from the contest were then validated on out-of-sample patient data including standard clinical data and continuous ambulatory human data obtained over several years using the implantable NeuroVista seizure advisory system. Two hundred teams of data scientists from all over the world participated in the kaggle.com competition. The top performing teams submitted highly accurate algorithms with consistent performance in the out-of-sample validation study. The performance of these seizure detection algorithms, achieved using freely available code and data, sets a new reproducible benchmark for personalized seizure detection. We have also shared a 'plug and play' pipeline to allow other researchers to easily use these algorithms on their own datasets. The success of this competition demonstrates how sharing code and high quality data results in the creation of powerful translational tools with significant potential to impact patient care.},
author = {Baldassano, Steven N. and Brinkmann, Benjamin H. and Ung, Hoameng and Blevins, Tyler and Conrad, Erin C. and Leyde, Kent and Cook, Mark J. and Khambhati, Ankit N. and Wagenaar, Joost B. and Worrell, Gregory A. and Litt, Brian},
doi = {10.1093/brain/awx098},
file = {::},
issn = {14602156},
journal = {Brain},
keywords = {Crowdsourcing,Epilepsy,Experimental models,Intracranial EEG,Seizure detection},
number = {6},
pages = {1680--1691},
publisher = {Oxford University Press},
title = {{Crowdsourcing seizure detection: Algorithm development and validation on human implanted device recordings}},
volume = {140},
year = {2017}
}
@article{Stamatatos2008,
annote = {The main idea in this paper was to combine all the documents for each of the classes. Many short sentences were then sampled from the minoroity classes where as less longer sentences were sampled form the majority classes. This resulted in inverting the class imbalance problem slightly but showed better results than down sampling all classes and undersampling the majority.},
author = {Stamatatos},
file = {::},
keywords = {author identification,class imbalance,text categorization},
number = {2},
pages = {790--799},
title = {{Using text sampling to handle the class imbalance problem}},
volume = {44},
year = {2008}
}
@article{Usman2017,
abstract = {Context: Software Engineering (SE) is an evolving discipline with new subareas being continuously developed and added. To structure and better understand the SE body of knowledge, taxonomies have been proposed in all SE knowledge areas. Objective: The objective of this paper is to characterize the state-of-the-art research on SE taxonomies. Method: A systematic mapping study was conducted, based on 270 primary studies. Results: An increasing number of SE taxonomies have been published since 2000 in a broad range of venues, including the top SE journals and conferences. The majority of taxonomies can be grouped into the following SWEBOK knowledge areas: construction (19.55{\%}), design (19.55{\%}), requirements (15.50{\%}) and maintenance (11.81{\%}). Illustration (45.76{\%}) is the most frequently used approach for taxonomy validation. Hierarchy (53.14{\%}) and faceted analysis (39.48{\%}) are the most frequently used classification structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53{\%}) these procedures are not described in sufficient detail. The majority of the taxonomies (97{\%}) target unique subject matters and many taxonomy-papers are cited frequently. Most SE taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way. Conclusion: There is a strong interest in taxonomies in SE, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classification structures, procedures and descriptive bases are usually not well described and motivated.},
author = {Usman, Muhammad and Britto, Ricardo and B{\"{o}}rstler, J{\"{u}}rgen and Mendes, Emilia},
doi = {10.1016/j.infsof.2017.01.006},
file = {::},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Classification,Software engineering,Systematic mapping study,Taxonomy},
pages = {43--59},
publisher = {Elsevier B.V.},
title = {{Taxonomies in software engineering: A Systematic mapping study and a revised taxonomy development method}},
url = {http://dx.doi.org/10.1016/j.infsof.2017.01.006},
volume = {85},
year = {2017}
}
@article{Zhang2018b,
abstract = {Explainable recommendation attempts to develop models that generate not only high-quality recommendations but also intuitive explanations. The explanations may either be post-hoc or directly come from an explainable model. Explainable recommendation tries to address the problem of why: by providing explanations to users or system designers, it helps humans to understand why certain items are recommended by the algorithm, where the human can either be users or system designers. Explainable recommendation helps to improve the transparency, persuasiveness, effectiveness, trustworthiness, and satisfaction of recommendation systems. It also facilitates system designers for better system debugging. In recent years, a large number of explainable recommendation approaches -- especially model-based methods -- have been proposed and applied in real-world systems. In this survey, we provide a comprehensive review for the explainable recommendation research. We highlight the position of explainable recommendation in recommender system research by categorizing recommendation problems into the 5W, i.e., what, when, who, where, and why. We then conduct a comprehensive survey of explainable recommendation on three perspectives: 1) We provide a chronological research timeline of explainable recommendation, including user study approaches in the early years and more recent model-based approaches. 2) We provide a two-dimensional taxonomy to classify existing explainable recommendation research: one dimension is the information source of the explanations, and the other dimension is the algorithmic mechanism to generate explainable recommendations. 3) We summarize how explainable recommendation applies to different recommendation tasks, such as product, social, and POI recommendations. We also devote a section to discuss the future directions to promote the explainable recommendation research.},
archivePrefix = {arXiv},
arxivId = {1804.11192},
author = {Zhang, Yongfeng and Chen, Xu},
eprint = {1804.11192},
file = {::},
number = {Xx},
pages = {1--100},
title = {{Explainable Recommendation: A Survey and New Perspectives}},
url = {http://arxiv.org/abs/1804.11192},
volume = {XX},
year = {2018}
}
@article{Silvers1994,
abstract = {The goal of a computerized statistical advisory system is to incorporate into software specific and sound advice on appropriately applying statistical techniques and interpreting their results for a given set of data. At the heart of such a system is the statistical strategy, the conceptual model used to represent the reasoning of an expert statistician. this paper presents our experience in developing a statistical strategy for a prototype knowledge-based advisory system. The statistical strategy was conceived in two stages. First, a hierarchical structure of decision trees at different levels of abstraction, together with rules for traversing the structure, was developed as a vehicle for eliciting the strategy from expert statisticians. In the second stage, the underlying objects and essential modes of decision-making implicitly incorporated in the decision-tree structure were extracted and a specific taxonomy for the problem was developed. A more general, object-oriented paradigm based on the results of this second stage was used to implement the prototype model. The statistical strategy was developed with a specific user audience in mind and focused on a well-defined subset of statistical techniques often used by that audience. {\textcopyright} 1994.},
author = {Silvers, Abraham and Herrmann, Nira and Godfrey, Kathy and Roberts, Bruce and Cerys, Daniel},
doi = {10.1016/0167-9473(94)90068-X},
file = {::},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
month = {oct},
number = {3},
pages = {341--355},
title = {{A prototype statistical advisory system for biomedical researchers I: Overview}},
url = {https://linkinghub.elsevier.com/retrieve/pii/016794739490068X},
volume = {18},
year = {1994}
}
@inproceedings{Dutta2018a,
abstract = {Probabilistic programming systems (PP systems) allow developers to model stochastic phenomena and perform efficient inference on the models. The number and adoption of probabilistic programming systems is growing significantly. However, there is no prior study of bugs in these systems and no methodology for systematically testing PP systems. Yet, testing PP systems is highly non-trivial, especially when they perform approximate inference. In this paper, we characterize 118 previously reported bugs in three open-source PP systems-Edward, Pyro and Stan-and propose ProbFuzz, an extensible system for testing PP systems. Prob-Fuzz allows a developer to specify templates of probabilistic models, from which it generates concrete probabilistic programs and data for testing. ProbFuzz uses language-specific translators to generate these concrete programs, which use the APIs of each PP system. ProbFuzz finds potential bugs by checking the output from running the generated programs against several oracles, including an accuracy checker. Using ProbFuzz, we found 67 previously unknown bugs in recent versions of these PP systems. Developers already accepted 51 bug fixes that we submitted to the three PP systems, and their underlying systems, PyTorch and TensorFlow. CCS CONCEPTS • Software and its engineering → Software testing;},
author = {Dutta, Saikat and Legunsen, Owolabi and Huang, Zixin and Misailovic, Sasa},
doi = {10.1145/3236024.3236057},
title = {{Testing probabilistic programming systems}},
year = {2018}
}
@article{Simmonds2017,
abstract = {A living systematic review (LSR) should keep the review current as new research evidence emerges. Any meta-analyses included in the review will also need updating as new material is identified. If the aim of the review is solely to present the best current evidence standard meta-analysis may be sufficient, provided reviewers are aware that results may change at later updates. If the review is used in a decision-making context, more caution may be needed. When using standard meta-analysis methods, the chance of incorrectly concluding that any updated meta-analysis is statistically significant when there is no effect (the type I error) increases rapidly as more updates are performed. Inaccurate estimation of any heterogeneity across studies may also lead to inappropriate conclusions. This paper considers four methods to avoid some of these statistical problems when updating meta-analyses: two methods, that is, law of the iterated logarithm and the Shuster method control primarily for inflation of type I error and two other methods, that is, trial sequential analysis and sequential meta-analysis control for type I and II errors (failing to detect a genuine effect) and take account of heterogeneity. This paper compares the methods and considers how they could be applied to LSRs.},
author = {Simmonds, Mark and Salanti, Georgia and McKenzie, Joanne and Elliott, Julian and Agoritsas, Thomas and Hilton, John and Perron, Caroline and Akl, Elie and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and Kolakowsky-Hayner, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Salanti, Georgia and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch{\"{u}}nemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Shemilt, Ian and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and MacLehose, Harriet and Simmonds, Mark and Cossi, Marie Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Marshall, Iain and Soares-Weiser, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Garc{\'{i}}a, Laura Mart{\'{i}}nez and Synnot, Anneliese and Danko, Kristen and Mavergames, Chris and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Thomas, James and Egan, Cathy and McDonald, Steve and Tritton, Roger and Elliott, Julian and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Meerpohl, Joerg and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Turgeon, Alexis and Featherstone, Robin and Mondello, Stefania and Turner, Tari and Foxlee, Ruth and Morley, Richard and van Valkenhoef, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Wallace, Byron and Glasziou, Paul and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and Noel-Storr, Anna and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and {Yepes Nu{\~{n}}ez}, Juan Jos{\'{e}} and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
doi = {10.1016/j.jclinepi.2017.08.008},
file = {::},
issn = {18785921},
journal = {Journal of Clinical Epidemiology},
keywords = {Heterogeneity,Living systematic review,Meta-analysis,Type I error,Type II error},
pages = {38--46},
pmid = {28912004},
title = {{Living systematic reviews: 3. Statistical methods for updating meta-analyses}},
volume = {91},
year = {2017}
}
@article{Ashmore2019,
abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our paper provides a comprehensive survey of the state-of-the-art in the assurance of ML, i.e. in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e. of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The paper begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
annote = {From Duplicate 1 (Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges - Ashmore, Rob; Calinescu, Radu; Paterson, Colin)

Desiderata - something that is needed/wanted

vision - ML is everywhere, it is determined by the level of assurance provided for ML
ML errors in smart cameras can be deleted, the selection of odd word in translation system can be ignored. But ML errors in medical diagnosis/self driving cars are unacceptable.

Understand the complex, iterative process of ML and use ML.
4 stages of ML Lifecycle - Data Management, Model Learning, Model Verification, Model Deployment
ML Learning - supervised, unsupervised, reinforcement
Convolutional NN is used in classification of road signs in self driving cars

ML represents the automated extraction of models/patterns from data.
Data Management - collection, augmentation, preprocessing and analysis of data
Augmentation - add further data samples
Model selection depends on problem type, volume {\&} structure of training data and personal experience.
Hyperparameter selection can cause overfitting, underfitting and model complexity.
Loss function is a measure of training errors.

Feature extraction can be subcategorised as numerical (blood sugar level in a healthcare system), ordinal (position in a queue in a traffic management system), categorical (an element(bus, car, truck) from the set in a self driving car)

MAPE control system - Monitor Analyze Plan Execute Control System
Learning - Deep learning, reinforcement learning, transfer learning, ensemble learning
Data Management - collection (source), preprocessing (one to one mapping), augmentation (one to many mapping)

Desiderata - relevant, complete, balanced, accurate
Relevant - the intersection between the dataset {\&} the desired behavior in intended operational domain
Complete - the way samples are distributed across input domain and subspaces of it.
Input domain space, operational domain space, failure domain space, adversarial domain space
Balanced - the distribution of features 
Accurate - how measurement issues can affect the way that samples reflect the intended operational domain

Challenges in relevancy - detection of hidden backdoors, demonstrating that synthetic data is appropriate, detecting {\&} correcting for data leakage

To consider whether a data set is complete, we can plot marginal distribution of each feature, ratio of sampling density between densely sampled {\&} sparsely sampled

Challenges in completeness - understanding completeness across operational domain, finding verifiable ways of achieving augmentation, demonstrating completeness across adversarial domain, labeling discrepancies introduced by human

Model categories are classification, regression, clustering and reinforcement.
Objective function reflects the requirements for the model.
Hyper parameter selection strategies - initialisation with values offered by ML frameworks, manual configuration based on recommendations from literature/experience, trial and error
NN model, used for facial recognition can be reused for operator fatigue.

Prediction errors have three components such as irreducible error, bias error and variance error.

Model specific parameters restrict the users the choice of model.},
archivePrefix = {arXiv},
arxivId = {1905.04223},
author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
eprint = {1905.04223},
file = {::},
month = {may},
title = {{Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges}},
url = {http://arxiv.org/abs/1905.04223},
year = {2019}
}
@inproceedings{Wang2018,
abstract = {Emotion recognition is the task of recognizing a person's emotional state. EEG, as a physiological signal, can provide more detailed and complex information for emotion recognition task. Meanwhile, EEG can't be changed and hidden intentionally makes EEG-based emotion recognition achieve more effective and reliable result. Unfortunately, due to the cost of data collection, most EEG datasets have small number of EEG data. The lack of data makes it difficult to predict the emotion states with the deep models, which requires enough number of training data. In this paper, we propose to use a simple data augmentation method to address the issue of data shortage in EEG-based emotion recognition. In experiments, we explore the performance of emotion recognition with the shallow and deep computational models before and after data augmentation on two standard EEG-based emotion datasets. Our experimental results show that the simple data augmentation method can improve the performance of emotion recognition based on deep models effectively.},
author = {Wang, Fang and Zhong, Sheng Hua and Peng, Jianfeng and Jiang, Jianmin and Liu, Yan},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-73600-6_8},
file = {::},
isbn = {9783319735993},
issn = {16113349},
keywords = {Data augmentation,EEG,Emotion recognition},
pages = {82--93},
publisher = {Springer Verlag},
title = {{Data augmentation for eeg-based emotion recognition with deep convolutional neural networks}},
volume = {10705 LNCS},
year = {2018}
}
@article{Lipton2018,
abstract = {Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal p(y) changes but the conditional p(x$\backslash$y) does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution p(y). BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, orun- calibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.},
archivePrefix = {arXiv},
arxivId = {1802.03916},
author = {Lipton, Zachary C. and Wang, Yu Xiang and Smola, Alexander J.},
eprint = {1802.03916},
file = {::},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {4887--4897},
title = {{Detecting and correcting for label shift with black box predictors}},
volume = {7},
year = {2018}
}
@article{Gao2016,
abstract = {Reducing sitting time by means of sit–stand workstations is an emerging trend, but further evidence is needed regarding their health benefits. This cross-sectional study compared work time muscle activity patterns and spinal shrinkage between office workers (aged 24–62, 58.3{\%} female) who used either a sit–stand workstation (Sit–Stand group, n = 10) or a traditional sit workstation (Sit group, n = 14) for at least the past three months. During one typical workday, muscle inactivity and activity from quadriceps and hamstrings were monitored using electromyography shorts, and spinal shrinkage was measured using stadiometry before and after the workday. Compared with the Sit group, the Sit–Stand group had less muscle inactivity time (66.2 ± 17.1{\%} vs. 80.9 ± 6.4{\%}, p = 0.014) and more light muscle activity time (26.1 ± 12.3{\%} vs. 14.9 ± 6.3{\%}, p = 0.019) with no significant difference in spinal shrinkage (5.62 ± 2.75 mm vs. 6.11 ± 2.44 mm). This study provides evidence that working with sit–stand workstations can promote more light muscle activity time and less inactivity without negative effects on spinal shrinkage. Practitioner Summary: This cross-sectional study compared the effects of using a sit–stand workstation to a sit workstation on muscle activity patterns and spinal shrinkage in office workers. It provides evidence that working with a sit–stand workstation can promote more light muscle activity time and less inactivity without negative effects on spinal shrinkage.},
author = {Gao, Ying and Cronin, Neil J. and Pesola, Arto J. and Finni, Taija},
doi = {10.1080/00140139.2016.1139750},
issn = {13665847},
journal = {Ergonomics},
keywords = {Sit–stand workstation,muscle inactivity and muscle activity,office workers,spinal shrinkage},
month = {oct},
number = {10},
pages = {1267--1274},
publisher = {Taylor and Francis Ltd.},
title = {{Muscle activity patterns and spinal shrinkage in office workers using a sit–stand workstation versus a sit workstation}},
volume = {59},
year = {2016}
}
@article{Horbach2020,
abstract = {Peer review of journal submissions has become one of the most important pillars of quality management in academic publishing. Because of growing concerns with the quality and effectiveness of the system, a host of enthusiastic innovators has proposed and experimented with new procedures and technologies. However, little is known about whether these innovations manage to convince other journal editors. This paper will address open questions regarding the implementation of new review procedures, the occurrence rate of various peer review procedures and their distribution over scientific disciplines or academic publishers, as well as the motivations for editors or publishers to engage in novel review procedures. It shows that in spite of enthusiastic innovation, the adoption of new peer review procedures is in fact very slow, with the exception of text similarity scanners. For now, peer review innovations appear to be restricted to specific niches in academic publishing. Analysing these niches, the article concludes with a reflection on the circumstances in which innovations might be more widely implemented.},
author = {Horbach, Serge P. J. M. and Halffman, Willem},
doi = {10.1007/s11024-019-09388-z},
file = {::},
isbn = {0123456789},
issn = {0026-4695},
journal = {Minerva},
keywords = {Expectations,Implementation,Innovation,Peer review,Scientific publishing},
month = {jun},
number = {2},
pages = {139--161},
publisher = {Springer Netherlands},
title = {{Journal Peer Review and Editorial Evaluation: Cautious Innovator or Sleepy Giant?}},
volume = {58},
year = {2020}
}
@article{Barratt2016,
author = {Barratt, HS and Campbell, M and Moore, L and Zwarenstein, M and Bower, P},
doi = {10.3310/hsdr04160-19},
journal = {Health Services and Delivery Research},
number = {16},
publisher = {NIHR Journals Library},
title = {{Challenges, solutions and future directions in the evaluation of service innovations in health care and public health}},
volume = {4},
year = {2016}
}
@techreport{Black2019,
abstract = {“Statistical software” encompasses several distinct classes of software. This report explains what formal methods, tools, and approaches may be able to increase assurance of results of using statistical software and implementing differential privacy. To provide context, we present an exemplary process for assured results. The parts are, data assurance, algorithm design, software production, correctness proofs, postproduction assurance of software, and result checking. We note a workshop we organized to support this paper and finish with recommended formal methods, tools, and researchers doing particularly pertinent work.},
address = {Gaithersburg, MD},
author = {Black, Paul E},
doi = {10.6028/NIST.IR.8274},
file = {::},
institution = {National Institute of Standards and Technology},
keywords = {correctness proofs,differential privacy,formal methods,software assurance,software quality,static analysis,static source code analyzers,statistical software.},
month = {oct},
title = {{Formal methods for statistical software}},
url = {https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8274.pdf},
year = {2019}
}
@misc{Wagenaar2015,
abstract = {Technological advances are dramatically advancing translational research in Epilepsy. Neurophysiology, imaging, and metadata are now recorded digitally in most centers, enabling quantitative analysis. Basic and translational research opportunities to use these data are exploding, but academic and funding cultures prevent this potential from being realized. Research on epileptogenic networks, antiepileptic devices, and biomarkers could progress rapidly if collaborative efforts to digest this "big neuro data" could be organized. Higher temporal and spatial resolution data are driving the need for novel multidimensional visualization and analysis tools. Crowd-sourced science, the same that drives innovation in computer science, could easily be mobilized for these tasks, were it not for competition for funding, attribution, and lack of standard data formats and platforms. As these efforts mature, there is a great opportunity to advance Epilepsy research through data sharing and increase collaboration between the international research community.},
author = {Wagenaar, Joost B. and Worrell, Gregory A. and Ives, Zachary and Matthias, D{\"{u}}mpelmann and Litt, Brian and Schulze-Bonhage, Andreas},
booktitle = {Journal of Clinical Neurophysiology},
doi = {10.1097/WNP.0000000000000159},
file = {::},
issn = {15371603},
keywords = {Cloud computing,Data Repositories,Data sharing,EEG,Epilepsy},
month = {jun},
number = {3},
pages = {235--239},
pmid = {26035676},
publisher = {Lippincott Williams and Wilkins},
title = {{Collaborating and sharing data in epilepsy research}},
volume = {32},
year = {2015}
}
@article{Roelen2014,
abstract = {Purpose: Prolonged fatigue adversely affects an individual's performance and functioning. The present study investigated the prospective associations between prolonged fatigue and sickness absence (SA) during 1-year follow-up. Methods: At baseline, a convenience sample of white-collar employees received the 20-item Checklist Individual Strength (CIS), which measures prolonged fatigue by covering the dimensions fatigue severity, reduced concentration, reduced motivation, and reduced physical activity. SA episodes were registered during the 1-year follow-up distinguishing between short-term (1-7 days) and long-term ({\textgreater}7 days) SA episodes. Baseline CIS scores were linked to SA during follow-up by negative binomial regression models in which age, gender, job grade, and prior SA were controlled for. Results: Six hundred and thirty-three (56 {\%}) employees participated in the study of which 598 had complete data and were eligible for analysis. Gender was a significant effect modifier of the relationship between prolonged fatigue and SA. Therefore, the results were stratified for men (N = 365) and women (N = 233). In white-collar men, fatigue severity and reduced concentration were positively associated with the number of long-term SA episodes, while other fatigue dimensions were not significantly related to SA. In white-collar women, prolonged fatigue was not associated with SA during 1-year follow-up. Conclusion: The results of this study warrant more attention for prolonged fatigue in occupational healthcare practice and research. Early identification of and treatment for prolonged fatigue might prevent future health problems and SA, especially in white-collar men. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Roelen, Corn{\'{e}} A.M. and {Van Rhenen}, Willem and Groothoff, Johan W. and {Van Der Klink}, Jac J.L. and B{\"{u}}ltmann, Ute},
doi = {10.1007/s00420-013-0856-y},
file = {::},
issn = {03400131},
journal = {International Archives of Occupational and Environmental Health},
keywords = {Absenteeism,Checklist Individual Strength,Office workers,Sick leave},
month = {feb},
number = {3},
pages = {257--263},
publisher = {Springer Verlag},
title = {{Prolonged fatigue is associated with sickness absence in men but not in women: Prospective study with 1-year follow-up of white-collar employees}},
volume = {87},
year = {2014}
}
@article{O&039;Carroll2020,
abstract = {Coronavirus disease 2019 (COVID-19), an illness caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection, has spread rapidly worldwide, resulting in significant mortality and placing major strain on healthcare systems. Although the clinical course is variable, one in five patients will require hospitalisation for management, with older age and the presence of comorbidities increasing the risk of more severe disease [1–3]. The median time from first onset of symptoms to development of acute respiratory distress syndrome in those who progress to severe disease is estimated to be 8.0 days [4].Remote monitoring of oxygen saturation in cases of COVID-19 pneumonia may facilitate discharge, relieving burden on bed demand and allowing safe follow-up for this disease in which the sequelae are unknown https://bit.ly/3cTXnZU},
author = {O{\&}{\#}039;Carroll, Orla and MacCann, Rachel and O{\&}{\#}039;Reilly, Aoife and Dunican, Eleanor M and Feeney, Eoin R and Ryan, Silke and Cotter, Aoife and Mallon, Patrick W and Keane, Michael P and Butler, Marcus W and McCarthy, Cormac},
doi = {10.1183/13993003.01492-2020},
journal = {European Respiratory Journal},
month = {aug},
number = {2},
pages = {2001492},
title = {{Remote monitoring of oxygen saturation in individuals with COVID-19 pneumonia}},
url = {http://erj.ersjournals.com/content/56/2/2001492.abstract},
volume = {56},
year = {2020}
}
@article{Avazpour2014,
abstract = {Recommendation systems support users and developers of various computer and software systems to overcome information overload, perform information discovery tasks, and approximate computation, among others. They have recently become popular and have attracted a wide variety of application scenarios ranging from business process modeling to source code manipulation. Due to this wide variety of application domains, different approaches and metrics have been adopted for their evaluation. In this chapter, we review a range of evaluation metrics and measures as well as some approaches used for evaluating recommendation systems. The metrics presented in this chapter are grouped under sixteen different dimensions, e.g., correctness, novelty, coverage. We review these metrics according to the dimensions to which they correspond. A brief overview of approaches to comprehensive evaluation using collections of recommendation system dimensions and associated metrics is presented. We also provide suggestions for key future research and practice directions.},
author = {Avazpour, Iman and Pitakrat, Teerat and Grunske, Lars and Grundy, John},
doi = {10.1007/978-3-642-45135-5_10},
file = {::},
isbn = {9783642451355},
journal = {Recommendation Systems in Software Engineering},
number = {c},
pages = {245--273},
title = {{Dimensions and metrics for evaluating recommendation systems}},
year = {2014}
}
@article{Esfahani2013,
abstract = {The ever-growing complexity of software systems coupled with their stringent availability requirements are challenging the manual management of software after its deployment. This has motivated the development of self-adaptive software systems. Self-adaptation endows a software system with the ability to satisfy certain objectives by automatically modifying its behavior at runtime. While many promising approaches for the construction of self-adaptive software systems have been developed, the majority of them ignore the uncertainty underlying the adaptation. This has been one of the key inhibitors to widespread adoption of self-adaption techniques in risk-averse real-world applications. Uncertainty in this setting is a vaguely understood term. In this paper, we characterize the sources of uncertainty in self-adaptive software system, and demonstrate its impact on the system's ability to satisfy its objectives. We then provide an alternative notion of optimality that explicitly incorporates the uncertainty underlying the knowledge (models) used for decision making. We discuss the state-of-the-art for dealing with uncertainty in this setting, and conclude with a set of challenges, which provide a road map for future research.},
author = {Esfahani, Naeem and Malek, Sam},
doi = {10.1007/978-3-642-35813-5_9},
file = {::},
isbn = {9783642358128},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Self-Adaptive Software Systems,Uncertainty},
pages = {214--238},
title = {{Uncertainty in self-adaptive software systems}},
volume = {7475 LNCS},
year = {2013}
}
@article{Ruano-Ordas2018a,
abstract = {Internet e-mail service emerged in the late seventies to implement fast message exchanging through computer networks. Network users immediately discovered the value of this service (sometimes for improper purposes such as spamming). As e-mail became indispensable to increase personal productivity, the volume of spam deliveries was constantly growing. With the passage of time, a great number of proposals and tools have emerged to fight against spam. However, the vast majority of them do not properly take into consideration the inner attributes of spam and ham messages such as the noise or the presence of concept drift. In this work, we provide a detailed empirical study of concept drift in the e-mail domain taking into consideration two key aspects: existing types of concept drift and the real class of messages (spam and ham). As a result, our study reveals different weaknesses of multiple e-mail filtering alternatives and other relevant works in this domain and identifies new strategies to develop more accurate filters. Finally, the experimentation carried out in this work has motivated the development of a concept drift analyser tool for the e-mail domain that can be freely downloaded from https://github.com/sing-group/conceptDriftAnalyser.git.},
author = {Ruano-Ord{\'{a}}s, David and Fdez-Riverola, Florentino and M{\'{e}}ndez, Jos{\'{e}} R.},
doi = {10.1016/j.ins.2017.10.049},
file = {::},
issn = {00200255},
journal = {Information Sciences},
keywords = {Classification,Concept drift analysis,E-mail,Spam filtering,Text mining},
pages = {120--135},
title = {{Concept drift in e-mail datasets: An empirical study with practical implications}},
volume = {428},
year = {2018}
}
@article{virtanen2020scipy,
author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and Others},
journal = {Nature methods},
number = {3},
pages = {261--272},
publisher = {Nature Publishing Group},
title = {{SciPy 1.0: fundamental algorithms for scientific computing in Python}},
volume = {17},
year = {2020}
}
@article{Zanaty2018,
abstract = {Background: Code review is a well-established software quality practice where developers critique each others' changes. A shift towards automated detection of low-level issues (e.g., integration with linters) has, in theory, freed reviewers up to focus on higher level issues, such as software design. Yet in practice, little is known about the extent to which design is discussed during code review. Aim: To bridge this gap, in this paper, we set out to study the frequency and nature of design discussions in code reviews. Method: We perform an empirical study on the code reviews of the OpenStack Nova (provisioning management) and Neutron (networking abstraction) projects. We manually classify 2,817 review comments from a randomly selected sample of 220 code reviews. We then train and evaluate classifiers to automatically label review comments as design related or not. Finally, we apply the classifiers to a larger sample of 2,506,308 review comments to study the characteristics of reviews that include design discussions. Results: Our manual analysis indicates that (1) design discussions are still quite rare, with only 9{\%} and 14{\%} of Nova and Neutron review comments being related to software design, respectively; and (2) design feedback is often constructive, with 73{\%} of the design-related comments also providing suggestions to address the concerns. Furthermore, our classifiers achieve a precision of 59{\%}-66{\%} and a recall of 70{\%}-78{\%}, outperforming baselines like zeroR by 43 percentage points in terms of F1-score. Finally, code changes that have design-related feedback have a statistically significantly increased rate of abandonment (Pearson 2 test, DF=1, p {\textless} 0.001). Conclusion: Design-related discussion during code review is still rare. Since design discussion is a primary motivation for conducting code review, more may need to be done to encourage such discussions among contributors.},
annote = {A good paper for automatic text labelling},
author = {Zanaty, Farida El and Hirao, Toshiki and McIntosh, Shane and Ihara, Akinori and Matsumoto, Kenichi},
doi = {10.1145/3239235.3239525},
file = {::},
isbn = {9781450358231},
issn = {19493789},
journal = {International Symposium on Empirical Software Engineering and Measurement},
keywords = {Code review,Mining software repositories,Software design},
title = {{An empirical study of design discussions in code review}},
year = {2018}
}
@book{Pasquini1999,
abstract = {Cyber attacks are the core of any security assessment of ICT-based systems. One of the more promising research fields in this{\$}\backslash{\$}n context is related to the representation of the attack patterns. Several are the models proposed to represent them; these{\$}\backslash{\$}n models usually provide a generic representation of attacks. Conversely, the experience shows that attack profiles are strongly{\$}\backslash{\$}n dependent upon several “boundary conditions”. This paper defends that from the security assessment perspective, it is necessary{\$}\backslash{\$}n to integrate the knowledge contained in the attack patterns with “boundary” knowledge related to vulnerability of the target{\$}\backslash{\$}n system and to the potential threats. In this paper, after a characterization of this “boundary knowledge”, we propose an n-dimensional{\$}\backslash{\$}n view of the attack tree approach, integrating information on threats and vulnerabilities. Moreover, we show how to use this{\$}\backslash{\$}n view to derive knowledge about the security exposure of a target system.{\$}\backslash{\$}n {\$}\backslash{\$}n Keywords: Security assessment, Attack Pattern.},
author = {Pasquini, Alberto},
doi = {10.1007/3-540-48249-0},
file = {::},
isbn = {978-3-540-66488-8},
keywords = {Artificial intelligence,Dependability,Safety engin,artificial intelligence,data quality,dependability,empirical modelling,model validation,safety engineering},
pages = {431--438},
publisher = {Springer International Publishing},
title = {{Computer Safety, Reliability and Security}},
url = {http://link.springer.com/10.1007/3-540-48249-0},
volume = {1698},
year = {1999}
}
@incollection{Krawczyk2018,
author = {Krawczyk, Bartosz and Prati, Ronaldo C. and Herrera, Francisco and Garc{\'{i}}a, Salvador and Galar, Mikel and Fern{\'{a}}ndez, Alberto},
booktitle = {Learning from Imbalanced Data Sets},
doi = {10.1007/978-3-319-98074-4_8},
title = {{Imbalanced Classification with Multiple Classes}},
year = {2018}
}
@inproceedings{Pimentel2019,
abstract = {Jupyter Notebooks have been widely adopted by many different communities, both in science and industry. They support the creation of literate programming documents that combine code, text, and execution results with visualizations and all sorts of rich media. The self-documenting aspects and the ability to reproduce results have been touted as significant benefits of notebooks. At the same time, there has been growing criticism that the way notebooks are being used leads to unexpected behavior, encourage poor coding practices, and that their results can be hard to reproduce. To understand good and bad practices used in the development of real notebooks, we studied 1.4 million notebooks from GitHub. We present a detailed analysis of their characteristics that impact reproducibility. We also propose a set of best practices that can improve the rate of reproducibility and discuss open challenges that require further research and development.},
author = {Pimentel, Jo{\~{a}}o Felipe and Murta, Leonardo and Braganholo, Vanessa and Freire, Juliana},
booktitle = {2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2019.00077},
file = {::},
isbn = {978-1-7281-3412-3},
issn = {21601860},
keywords = {Github,Jupyter notebook,Reproducibility},
month = {may},
pages = {507--517},
publisher = {IEEE},
title = {{A Large-Scale Study About Quality and Reproducibility of Jupyter Notebooks}},
url = {https://ieeexplore.ieee.org/document/8816763/},
year = {2019}
}
@article{Kume2017,
abstract = {It is widely accepted that listening to music improves subjective feelings and reduces fatigue sensations, and different kinds of music lead to different activations of these feelings. Recently, cardiac autonomic nervous modulation has been proposed as a useful objective indicator of fatigue. However, scientific considerations of the relation between feelings of fatigue and cardiac autonomic nervous modulation while listening to music are still lacking. In this study, we examined which subjective feelings of fatigue are related to participants' cardiac autonomic nervous function while they listen to music. We used an album of comfortable and relaxing environmental music, with blended sounds from a piano and violin as well as natural sound sources. We performed a crossover trial of environmental music and silent sessions for 20 healthy subjects, 12 females, and 8 males, after their daily work shift. We measured changes in eight types of subjective feelings, including healing, fatigue, sleepiness, relaxation, and refreshment, using the KOKORO scale, a subjective mood measurement system for self-reported feelings. Further, we obtained measures of cardiac autonomic nervous function on the basis of heart rate variability before and after the sessions. During the music session, subjective feelings significantly shifted toward healing and a secure/relaxed feeling and these changes were greater than those in the silent session. Heart rates ($\Delta$HR) in the music session significantly decreased compared with those in the silent session. Other cardiac autonomic parameters such as high-frequency (HF) component and the ratio of low-frequency (LF) and HF components (LF/HF) were similar in the two sessions. In the linear regression analysis of the feelings with $\Delta$HR and changes in LF/HF ($\Delta$LF/HF), increases and decreases in $\Delta$HR were correlated to the feeling axes of Fatigue-Healing and Anxiety/Tension-Security/Relaxation, whereas those in $\Delta$LF/HF were related to the feeling axes of Sleepiness-Wakefulness and Gloomy-Refreshed. This indicated that listening to music improved the participants' feelings of fatigue and decreased their heart rates. However, it did not reduce the cardiac LF/HF, suggesting that cardiac LF/HF might show a delayed response to fatigue. Thus, we demonstrated changes in cardiac autonomic nervous functions based on feelings of fatigue.},
author = {Kume, Satoshi and Nishimura, Yukako and Mizuno, Kei and Sakimoto, Nae and Hori, Hiroshi and Tamura, Yasuhisa and Yamato, Masanori and Mitsuhashi, Rika and Akiba, Keigo and Koizumi, Jun-ichi and Watanabe, Yasuyoshi and Kataoka, Yosky},
doi = {10.3389/fnins.2017.00108},
file = {::},
issn = {1662-453X},
journal = {Frontiers in Neuroscience},
keywords = {Cardiac autonomic function,Fatigue,Healing,Music,Subjective feelings},
month = {mar},
number = {MAR},
pages = {108},
publisher = {Frontiers Research Foundation},
title = {{Music Improves Subjective Feelings Leading to Cardiac Autonomic Nervous Modulation: A Pilot Study}},
url = {http://journal.frontiersin.org/article/10.3389/fnins.2017.00108/full},
volume = {11},
year = {2017}
}
@article{Horemans2004,
abstract = {Horemans HL, Nollet F, Beelen A, Lankhorst GJ. A comparison of 4 questionnaires to measure fatigue in postpoliomyelitis syndrome. Arch Phys Med Rehabil 2004;85: 392-8. Objective: To assess the comparability and reproducibility of 4 questionnaires used to measure fatigue in postpoliomyeli-tis syndrome (PPS). Design: Repeated-measures at a 3-week interval. Setting: University hospital. Participants: Convenience sample of 65 patients with PPS. Interventions: Not applicable. Main Outcome Measures: The Fatigue Severity Scale (FSS), the Nottingham Health Profile (NHP) energy category, the Polio Problem List (PPL) fatigue item, and the Dutch Short Fatigue Questionnaire (SFQ). Results: Correlations of scores between questionnaires were all significant (P.01) and ranged from .43 (between the NHP energy category and the PPL fatigue item) to .68 (between the PPL fatigue item and the SFQ). Scores on the second visit, normalized to a 0 to 100 scale, were: FSS, 7815; NHP energy category, 4735; PPL fatigue item, 8117; and SFQ, 6522. Except for the difference between the FSS and the PPL fatigue item, the differences in scores between the questionnaires were significant (P.01). Scale analysis indicated that all questionnaires measured the same unidimensional construct. The repro-ducibility of the FSS, the PPL fatigue item, and the SFQ was moderate. The smallest detectable change was 1.5 points for the FSS, 2.0 points for the PPL fatigue item, and 1.9 points for the SFQ. Conclusions: Although the questionnaires measure the same fatigue construct in PPS, the results are not interchangeable because the ranges of measurement differ. The NHP energy category, in particular, appeared to have a high detection threshold. The moderate reproducibility of the questionnaires indicates a lack of precision, especially when applied at the individual patient level.},
author = {Horemans, Herwin L and Nollet, Frans and Beelen, Anita and Lankhorst, Gustaaf J},
doi = {10.1016/j.apmr.2003.06.007},
file = {::},
keywords = {Fatigue,Postpoliomyelitis syndrome,Ques-tionnaires,Rehabilitation,Reliability and validity},
title = {{A Comparison of 4 Questionnaires to Measure Fatigue in Postpoliomyelitis Syndrome}},
year = {2004}
}
@article{Wilming2017,
abstract = {We present a dataset of free-viewing eye-movement recordings that contains more than 2.7 million fixation locations from 949 observers on more than 1000 images from different categories. This dataset aggregates and harmonizes data from 23 different studies conducted at the Institute of Cognitive Science at Osnabr{\"{u}}ck University and the University Medical Center in Hamburg-Eppendorf. Trained personnel recorded all studies under standard conditions with homogeneous equipment and parameter settings. All studies allowed for free eye-movements, and differed in the age range of participants (∼7-80 years), stimulus sizes, stimulus modifications (phase scrambled, spatial filtering, mirrored), and stimuli categories (natural and urban scenes, web sites, fractal, pink-noise, and ambiguous artistic figures). The size and variability of viewing behavior within this dataset presents a strong opportunity for evaluating and comparing computational models of overt attention, and furthermore, for thoroughly quantifying strategies of viewing behavior. This also makes the dataset a good starting point for investigating whether viewing strategies change in patient groups.},
author = {Wilming, Niklas and Onat, Selim and Ossand{\'{o}}n, Jos{\'{e}} P. and A{\c{c}}ik, Alper and Kietzmann, Tim C. and Kaspar, Kai and Gameiro, Ricardo R. and Vormberg, Alexandra and K{\"{o}}nig, Peter},
doi = {10.1038/sdata.2016.126},
file = {::},
issn = {20524463},
journal = {Scientific Data},
pages = {1--11},
title = {{An extensive dataset of eye movements during viewing of complex images}},
volume = {4},
year = {2017}
}
@inproceedings{posnett2011simpler,
author = {Posnett, Daryl and Hindle, Abram and Devanbu, Premkumar},
booktitle = {Proceedings of the 8th working conference on mining software repositories},
pages = {73--82},
title = {{A simpler model of software readability}},
year = {2011}
}
@inproceedings{Kross2019a,
author = {Kross, Sean and Guo, Philip J.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI '19},
doi = {10.1145/3290605.3300493},
file = {::},
isbn = {9781450359702},
keywords = {2019,acm reference format,data science education,guo,j,practitioners teaching data,sean kross and philip,teaching programming},
pages = {1--14},
publisher = {ACM Press},
title = {{Practitioners teaching data science in industry and academia: Expectations, workflows, and challenges}},
url = {http://dl.acm.org/citation.cfm?doid=3290605.3300493},
year = {2019}
}
@article{Kiral-Kornek2018,
abstract = {Background: Seizure prediction can increase independence and allow preventative treatment for patients with epilepsy. We present a proof-of-concept for a seizure prediction system that is accurate, fully automated, patient-specific, and tunable to an individual's needs. Methods: Intracranial electroencephalography (iEEG) data of ten patients obtained from a seizure advisory system were analyzed as part of a pseudoprospective seizure prediction study. First, a deep learning classifier was trained to distinguish between preictal and interictal signals. Second, classifier performance was tested on held-out iEEG data from all patients and benchmarked against the performance of a random predictor. Third, the prediction system was tuned so sensitivity or time in warning could be prioritized by the patient. Finally, a demonstration of the feasibility of deployment of the prediction system onto an ultra-low power neuromorphic chip for autonomous operation on a wearable device is provided. Results: The prediction system achieved mean sensitivity of 69{\%} and mean time in warning of 27{\%}, significantly surpassing an equivalent random predictor for all patients by 42{\%}. Conclusion: This study demonstrates that deep learning in combination with neuromorphic hardware can provide the basis for a wearable, real-time, always-on, patient-specific seizure warning system with low power consumption and reliable long-term performance.},
author = {Kiral-Kornek, Isabell and Roy, Subhrajit and Nurse, Ewan and Mashford, Benjamin and Karoly, Philippa and Carroll, Thomas and Payne, Daniel and Saha, Susmita and Baldassano, Steven and O'Brien, Terence and Grayden, David and Cook, Mark and Freestone, Dean and Harrer, Stefan},
doi = {10.1016/j.ebiom.2017.11.032},
file = {::},
issn = {23523964},
journal = {EBioMedicine},
keywords = {Artificial intelligence,Deep neural networks,Epilepsy,Mobile medical devices,Precision medicine,Seizure prediction},
month = {jan},
pages = {103--111},
publisher = {Elsevier B.V.},
title = {{Epileptic Seizure Prediction Using Big Data and Deep Learning: Toward a Mobile System}},
volume = {27},
year = {2018}
}
@article{Marculescu2019,
abstract = {In the context of robustness testing, the boundary between the valid and invalid regions of the input space can be an interesting source of erroneous inputs. Knowing where a specific software under test (SUT) has a boundary is essential for validation in relation to requirements. However, finding where a SUT actually implements the boundary is a non-trivial problem that has not gotten much attention. This paper proposes a method of finding the boundary between the valid and invalid regions of the input space. The proposed method consists of two steps. First, test data generators, directed by a search algorithm to maximise distance to known, valid test cases, generate valid test cases that are closer to the boundary. Second, these valid test cases undergo mutations to try to push them over the boundary and into the invalid part of the input space. This results in a pair of test sets, one consisting of test cases on the valid side of the boundary and a matched set on the outer side, with only a small distance between the two sets. The method is evaluated on a number of examples from the standard library of a modern programming language. We propose a method of determining the boundary between valid and invalid regions of the input space and apply it on a SUT that has a non-contiguous valid region of the input space. From the small distance between the developed pairs of test sets, and the fact that one test set contains valid test cases and the other invalid test cases, we conclude that the pair of test sets described the boundary between the valid and invalid regions of that input space. Differences of behaviour can be observed between different distances and sets of mutation operators, but all show that the method is able to identify the boundary between the valid and invalid regions of the input space. This is an important step towards more automated robustness testing.},
annote = {Defining boundaries for input space varies at different stages of software development and these boundaries are rich sources of software errors.

Finding boundaries will help to identify test cases which are near to the borderline of valid and invalid regions. This facilitates robustness and continual operation of the software, irrespective of correctness.

The research was conducted in two stages, namely generating automated data and mutating the valid test cases using property switching search.

GodelTest framework is utilized to develop an automated test data generator and it is assumed that the generator is fault free. The fitness function is calculated between the current candidate and the highest candidate of among already identified candidates and the fitness function needs to be maximized. 

In the second step, closed paired sets of test cases on either side of the boundary are selected and are swapped. Each candidate is started with the desired property, which is validity, and is mutated until the desired property is changed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1810.06720v1},
author = {Marculescu, Bogdan and Feldt, Robert},
doi = {10.1109/APSEC.2018.00031},
eprint = {arXiv:1810.06720v1},
file = {::},
isbn = {9781728119700},
issn = {15301362},
journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
keywords = {search based software testing,software testing},
pages = {169--178},
title = {{Finding a Boundary between Valid and Invalid Regions of the Input Space}},
volume = {2018-Decem},
year = {2019}
}
@article{Bach,
abstract = {Data comics for data-driven storytelling are inspired by the visual language of comics and aim to communicate insights in data through visualizations. While comics are widely known, few examples of data comics exist and there has not been any structured analysis nor guidance for their creation. We introduce data-comic design-patterns, each describing a set of panels with a specific narrative purpose, that allow for rapid storyboarding of data comics while showcasing their expressive potential. Our patterns are derived from i) analyzing common patterns in infographics, datavideos, and existing data comics, ii) our experiences creating data comics for different scenarios. Our patterns demonstrate how data comics allow an author to combine the best of both worlds: spatial layout and overview from infographics as well as linearity and narration from videos and presentations.},
author = {Bach, Benjamin and Wang, Zezhong and Farinella, Matteo and Murray-Rust, Dave and Riche, Nathalie Henry},
doi = {10.1145/3173574.3173612},
file = {::},
isbn = {9781450356206},
keywords = {Authors' choice,by semicolons,include commas,of terms,required.,separated,within terms only},
title = {{Design Patterns for Data Comics}},
url = {http://dx.doi.org/10.1145/3173574.3173612}
}
@article{Le2019,
abstract = {Software Engineering researchers are increasingly using Natural Language Processing (NLP) techniques to automate Software Vulnerabilities (SVs) assessment using the descriptions in public repositories. However, the existing NLP-based approaches suffer from concept drift. This problem is caused by a lack of proper treatment of new (out-of-vocabulary) terms for the evaluation of unseen SVs over time. To perform automated SVs assessment with concept drift using SVs' descriptions, we propose a systematic approach that combines both character and word features. The proposed approach is used to predict seven Vulnerability Characteristics (VCs). The optimal model of each VC is selected using our customized time-based cross-validation method from a list of eight NLP representations and six well-known Machine Learning models. We have used the proposed approach to conduct large-scale experiments on more than 100,000 SVs in the National Vulnerability Database (NVD). The results show that our approach can effectively tackle the concept drift issue of the SVs' descriptions reported from 2000 to 2018 in NVD even without retraining the model. In addition, our approach performs competitively compared to the existing word-only method. We also investigate how to build compact concept-drift-aware models with much fewer features and give some recommendations on the choice of classifiers and NLP representations for SVs assessment.},
author = {Le, Triet Huynh Minh and Sabir, Bushra and Babar, Muhammad Ali},
doi = {10.1109/MSR.2019.00063},
file = {::},
isbn = {9781728134123},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Machine learning,Mining software repositories,Multi-class classification,Natural language processing,Software vulnerability},
pages = {371--382},
title = {{Automated software vulnerability assessment with concept drift}},
volume = {2019-May},
year = {2019}
}
@article{Cha2017,
abstract = {A number of image processing techniques (IPTs) have been implemented for detecting civil infrastructure defects to partially replace human-conducted onsite inspections. These IPTs are primarily used to manipulate images to extract defect features, such as cracks in concrete and steel surfaces. However, the extensively varying real-world situations (e.g., lighting and shadow changes) can lead to challenges to the wide adoption of IPTs. To overcome these challenges, this article proposes a vision-based method using a deep architecture of convolutional neural networks (CNNs) for detecting concrete cracks without calculating the defect features. As CNNs are capable of learning image features automatically, the proposed method works without the conjugation of IPTs for extracting features. The designed CNN is trained on 40 K images of 256 × 256 pixel resolutions and, consequently, records with about 98{\%} accuracy. The trained CNN is combined with a sliding window technique to scan any image size larger than 256 × 256 pixel resolutions. The robustness and adaptability of the proposed approach are tested on 55 images of 5,888 × 3,584 pixel resolutions taken from a different structure which is not used for training and validation processes under various conditions (e.g., strong light spot, shadows, and very thin cracks). Comparative studies are conducted to examine the performance of the proposed CNN using traditional Canny and Sobel edge detection methods. The results show that the proposed method shows quite better performances and can indeed find concrete cracks in realistic situations.},
author = {Cha, Young Jin and Choi, Wooram and B{\"{u}}y{\"{u}}k{\"{o}}zt{\"{u}}rk, Oral},
doi = {10.1111/mice.12263},
file = {::},
issn = {14678667},
journal = {Computer-Aided Civil and Infrastructure Engineering},
number = {5},
pages = {361--378},
title = {{Deep Learning-Based Crack Damage Detection Using Convolutional Neural Networks}},
volume = {32},
year = {2017}
}
@article{Abelein2013,
abstract = {User participation and involvement in software development are considered to be essential for a successful software system. Three research areas, human aspects of software engineering, requirements engineering, and information systems, study these topics from various perspectives. We think it is important to analyze user participation and involvement in software engineering comprehensively to encourage further research in this area. We investigate the evidence on effects of user participation and involvement on system success and we explore which methods are available in literature. A systematic mapping study was conducted. The systematic search yielded 3,698 hits, from which we identified 289 unique papers. These papers were reviewed by the first author based on inclusion and exclusion criteria. The second author validated the selection of papers by reviewing the reasons for exclusion and inclusion and the corresponding papers on a sample base. 58 of the 289 papers were selected (22 statistical survey and meta-study papers and 36 methods papers). Based on the empirical evidence of the surveys and meta-studies, we developed a meta-analysis of structural equation models. This overview demonstrates that most papers showed positive correlations between aspects of development processes (including user participation) and human aspects (including user involvement) and system success. The analysis of the proposed solutions from the method papers revealed a wide variety of user participation and involvement practices for most activities within software development.},
annote = {Research Areas - Human aspects of software engineering, Requirements engineering, information systems
User Participation and Involvement (UPI)
Particpatory design, user centered design, ethnography, contextual design
Review Method - A defined search strategy, A defined search string based on anonyms ANDs and ORs,
a broad collection of search sources, a strict documentation of the search, 
quantitative and qualitative papers should be analysed separately, 
explicit inclusion and exclusion criteria, paper selection should be checked by two researchers.
Four databases - Web of Science, Science Direct, Business Source Premier and Scopus
Studies split into two - survey papers and method papers
Software development activities are planning {\&} project management, SW specification {\&} Requirement Engineering, 
SW Design {\&} Implementation, SW Verification {\&} Validation, SW Evolution 
Calculation of correlation between studies},
author = {Abelein, Ulrike and Paech, Barbara},
doi = {10.1007/s10664-013-9278-4},
file = {::},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Literature review,Meta analysis,Software development,Systematic mapping study,User involvement,User participation},
number = {1},
pages = {28--81},
publisher = {Kluwer Academic Publishers},
title = {{Understanding the Influence of User Participation and Involvement on System Success – a Systematic Mapping Study}},
volume = {20},
year = {2013}
}
@inproceedings{Kekenen2017,
abstract = {We arranged a case study in order to examine whether tetrapolar bioimpedance measurement could be applied for evaluating the healing of a surgical wound. We measured the donor site surgical wound of a patient who had undergone a breast reconstruction surgery. The measurements were conducted three times in a nine days period, starting from the first postoperative day. As a reference, the impedance of an unaffected site was also measured. The electrodes were placed at equal distances, four centimetres apart in a parallel formation. The results show that, at low frequencies, the impedance of the wound increases with time. At higher frequencies, the situation is opposite; the impedance of the wound is initially higher than the reference and decreases with time. Both ends seem to approach the reference impedance as the healing proceeds. Our results are in accordance with the normal course of surgical wound healing and more specifically appear to be related to the diminishing swelling around the wound site. We conclude that the obtained results are interesting in a level that calls for further investigation.},
author = {Kekenen, A. and Bergelin, M. and Erikssen, J. E. and Kaartinen, I. and Viik, J.},
booktitle = {IFMBE Proceedings},
doi = {10.1007/978-981-10-5122-7_112},
isbn = {9789811051210},
issn = {16800737},
keywords = {Bioimpedance,Healing,Monitoring,Surgical wound,Tetrapolar},
pages = {446--449},
publisher = {Springer Verlag},
title = {{Method for evaluation of surgical wound healing: A case study}},
url = {https://link.springer.com/chapter/10.1007/978-981-10-5122-7{\_}112},
volume = {65},
year = {2017}
}
@inproceedings{Allamanis2014,
abstract = {Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project's coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94{\%} accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1402.4182},
author = {Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering - FSE 2014},
doi = {10.1145/2635868.2635883},
eprint = {1402.4182},
file = {::},
isbn = {9781450330565},
keywords = {Coding conventions,Naturalness of software},
pages = {281--293},
publisher = {ACM Press},
title = {{Learning natural coding conventions}},
url = {http://dl.acm.org/citation.cfm?doid=2635868.2635883},
year = {2014}
}
@inproceedings{leff2001web,
author = {Leff, Avraham and Rayfield, James T},
booktitle = {Proceedings fifth IEEE international enterprise distributed object computing conference},
organization = {IEEE},
pages = {118--127},
title = {{Web-application development using the model/view/controller design pattern}},
year = {2001}
}
@article{Truong2017,
abstract = {Detecting seizure using brain neuroactivations recorded by intracranial electroencephalogram (iEEG) has been widely used for monitoring, diagnosing, and closed-loop therapy of epileptic patients, however, computational efficiency gains are needed if state-of-the-art methods are to be implemented in implanted devices. We present a novel method for automatic seizure detection based on iEEG data that outperforms current state-of-the-art seizure detection methods in terms of computational efficiency while maintaining the accuracy. The proposed algorithm incorporates an automatic channel selection (ACS) engine as a pre-processing stage to the seizure detection procedure. The ACS engine consists of supervised classifiers which aim to find iEEG channels which contribute the most to a seizure. Seizure detection stage involves feature extraction and classification. Feature extraction is performed in both frequency and time domains where spectral power and correlation between channel pairs are calculated. Random Forest is used in classification of interictal, ictal and early ictal periods of iEEG signals. Seizure detection in this paper is retrospective and patient-specific. iEEG data is accessed via Kaggle, provided by International Epilepsy Electro-physiology Portal. The dataset includes a training set of 6.5 h of interictal data and 41 min in ictal data and a test set of 9.14 h. Compared to the state-of-the-art on the same dataset, we achieve 2 times faster in run-time seizure detection. The proposed model is able to detect a seizure onset at 89.40{\%} sensitivity and 89.24{\%} specificity with a mean detection delay of 2.63 s for the test set. The area under the ROC curve (AUC) is 96.94{\%}, that is comparable to the current state-of-the-art with AUC of 96.29{\%}.},
archivePrefix = {arXiv},
arxivId = {1701.08968},
author = {Truong, Nhan Duy and Kuhlmann, Levin and Bonyadi, Mohammad Reza and Yang, Jiawei and Faulks, Andrew and Kavehei, Omid},
doi = {10.1016/j.eswa.2017.05.055},
eprint = {1701.08968},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Automatic channel selection,Random Forest,Seizure detection,iEEG},
month = {nov},
pages = {199--207},
publisher = {Elsevier Ltd},
title = {{Supervised learning in automatic channel selection for epileptic seizure detection}},
volume = {86},
year = {2017}
}
@article{Sivathamboo2018a,
author = {Sivathamboo, Niveshan and Hitchcock, Alison and Graham, Janet and Sivathamboo, Shobi and Chen, Zhibin and O'Brien, Terence J. and Vajda, Frank J. E.},
doi = {10.1111/epi.14539},
file = {::},
issn = {00139580},
journal = {Epilepsia},
keywords = {antiepileptic drugs,anxiety,birth defects,congenital malformations,depression,seizure control},
month = {sep},
number = {9},
pages = {1696--1704},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{The use of antidepressant drugs in pregnant women with epilepsy: A study from the Australian Pregnancy Register}},
url = {http://doi.wiley.com/10.1111/epi.14539},
volume = {59},
year = {2018}
}
@article{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
file = {::},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {452--459},
title = {{Probabilistic machine learning and artificial intelligence}},
volume = {521},
year = {2015}
}
@article{NELSON1976,
author = {NELSON, T. M. and LADAN, C. J.},
doi = {10.1111/j.2044-8325.1976.tb00331.x},
issn = {03058107},
journal = {Journal of Occupational Psychology},
month = {jun},
number = {2},
pages = {65--74},
title = {{Patterns and correlates of fatigue among office workers}},
url = {http://doi.wiley.com/10.1111/j.2044-8325.1976.tb00331.x},
volume = {49},
year = {1976}
}
@inproceedings{Sculley20152,
abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran{\c{c}}ois and Dennison, Dan},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
pages = {2503--2511},
title = {{Hidden Technical Debt in Machine Learning Systems}},
url = {http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf},
year = {2015}
}
@article{Cai2013,
abstract = {An application of dynamic Bayesian networks for quantitative risk assessment of human factors on offshore blowouts is presented. Human error is described using human factor barrier failure (HFBF), which consists of three categories of factors, including individual factor barrier failure (IFBF), organizational factor barrier failure (OFBF) and group factor barrier failure (GFBF). The structure of human factors is illustrated using pseudo-fault tree, which is defined by incorporating the intermediate options into fault tree in order to eliminate the binary restriction. A methodology of translating pseudo-fault tree into Bayesian networks and dynamic Bayesian networks taking repair into consideration is proposed and the propagation is performed. The results show that the human factor barrier failure probability only increases within the first two weeks and rapidly reaches a stable level when the repair is considered, whereas it increases continuously when the repair action is not considered. The results of mutual information show that the important degree sequences for the three categories of human factors on HFBF are: GFBF, OFBF and IFBF. In addition, each individual human factor contributes different to the HFBF, those which contribute much should given more attention in order to improve the human reliability and prevent the potential accident occurring. {\textcopyright}2013 Elsevier Ltd.},
author = {Cai, Baoping and Liu, Yonghong and Zhang, Yunwei and Fan, Qian and Liu, Zengkai and Tian, Xiaojie},
doi = {10.1016/j.jlp.2013.01.001},
file = {::},
issn = {09504230},
journal = {Journal of Loss Prevention in the Process Industries},
keywords = {Dynamic Bayesian networks,Human factors,Offshore blowouts,Quantitative risk assessment},
number = {4},
pages = {639--649},
publisher = {Elsevier Ltd},
title = {{A dynamic Bayesian networks modeling of human factors on offshore blowouts}},
url = {http://dx.doi.org/10.1016/j.jlp.2013.01.001},
volume = {26},
year = {2013}
}
@article{McCabe1976a,
author = {McCabe, T.J.},
doi = {10.1109/TSE.1976.233837},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {dec},
number = {4},
pages = {308--320},
title = {{A Complexity Measure}},
url = {http://ieeexplore.ieee.org/document/1702388/},
volume = {SE-2},
year = {1976}
}
@article{Gibaldi2017,
abstract = {The Tobii Eyex Controller is a new low-cost binocular eye tracker marketed for integration in gaming and consumer applications. The manufacturers claim that the system was conceived for natural eye gaze interaction, does not require continuous recalibration, and allows moderate head movements. The Controller is provided with a SDK to foster the development of new eye tracking applications. We review the characteristics of the device for its possible use in scientific research. We develop and evaluate an open source Matlab Toolkit that can be employed to interface with the EyeX device for gaze recording in behavioral experiments. The Toolkit provides calibration procedures tailored to both binocular and monocular experiments, as well as procedures to evaluate other eye tracking devices. The observed performance of the EyeX (i.e. accuracy {\textless} 0.6°, precision {\textless} 0.25°, latency {\textless} 50 ms and sampling frequency ≈55 Hz), is sufficient for some classes of research application. The device can be successfully employed to measure fixation parameters, saccadic, smooth pursuit and vergence eye movements. However, the relatively low sampling rate and moderate precision limit the suitability of the EyeX for monitoring micro-saccadic eye movements or for real-time gaze-contingent stimulus control. For these applications, research grade, high-cost eye tracking technology may still be necessary. Therefore, despite its limitations with respect to high-end devices, the EyeX has the potential to further the dissemination of eye tracking technology to a broad audience, and could be a valuable asset in consumer and gaming applications as well as a subset of basic and clinical research settings.},
author = {Gibaldi, Agostino and Vanegas, Mauricio and Bex, Peter J. and Maiello, Guido},
doi = {10.3758/s13428-016-0762-9},
file = {::},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Binocular,Eye movements,Eye tracking,Low cost,Saccade,Smooth pursuit,Vergence},
number = {3},
pages = {923--946},
title = {{Evaluation of the Tobii EyeX Eye tracking controller and Matlab toolkit for research}},
volume = {49},
year = {2017}
}
@article{Babic2019,
author = {Babic, Boris and Gerke, Sara and Evgeniou, Theodoros and {Glenn Cohen}, I.},
doi = {10.1126/science.aay9547},
file = {::},
issn = {10959203},
journal = {Science},
month = {dec},
number = {6470},
pages = {1202--1204},
pmid = {31806804},
publisher = {American Association for the Advancement of Science},
title = {{Algorithms on regulatory lockdown in medicine}},
volume = {366},
year = {2019}
}
@book{Kulesza2012,
abstract = {What does a user need to know to productively work with an intelligent agent? Intelligent agents and recommender systems are gaining widespread use, potentially creating a need for end users to understand how these systems operate in order to fix their agent's personalized behavior. This paper explores the effects of mental model soundness on such personalization by providing structural knowledge of a music recommender system in an empirical study. Our findings show that participants were able to quickly build sound mental models of the recommender system's reasoning, and that participants who most improved their mental models during the study were significantly more likely to make the recommender operate to their satisfaction. These results suggest that by helping end users understand a system's reasoning, intelligent agents may elicit more and better feedback, thus more closely aligning their output with each user's intentions.},
author = {Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Kwan, Irwin},
file = {::},
isbn = {9781450310154},
keywords = {ACM Classification Keywords H5m [Information inter,Author Keywords Recommenders,debugging,intelligent agents,mental models,music,personalization},
title = {{Tell Me More? The Effects of Mental Model Soundness on Personalizing an Intelligent Agent}},
url = {http://delivery.acm.org/10.1145/2210000/2207678/p1-kulesza.pdf?ip=139.132.188.30{\&}id=2207678{\&}acc=ACTIVE SERVICE{\&}key=65D80644F295BC0D.B242904781996EBA.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1550547247{\_}1c0effff3273731dbb3166f888f38a67},
year = {2012}
}
@misc{Pallmann2018,
abstract = {Adaptive designs can make clinical trials more flexible by utilising results accumulating in the trial to modify the trial's course in accordance with pre-specified rules. Trials with an adaptive design are often more efficient, informative and ethical than trials with a traditional fixed design since they often make better use of resources such as time and money, and might require fewer participants. Adaptive designs can be applied across all phases of clinical research, from early-phase dose escalation to confirmatory trials. The pace of the uptake of adaptive designs in clinical research, however, has remained well behind that of the statistical literature introducing new methods and highlighting their potential advantages. We speculate that one factor contributing to this is that the full range of adaptations available to trial designs, as well as their goals, advantages and limitations, remains unfamiliar to many parts of the clinical community. Additionally, the term adaptive design has been misleadingly used as an all-encompassing label to refer to certain methods that could be deemed controversial or that have been inadequately implemented. We believe that even if the planning and analysis of a trial is undertaken by an expert statistician, it is essential that the investigators understand the implications of using an adaptive design, for example, what the practical challenges are, what can (and cannot) be inferred from the results of such a trial, and how to report and communicate the results. This tutorial paper provides guidance on key aspects of adaptive designs that are relevant to clinical triallists. We explain the basic rationale behind adaptive designs, clarify ambiguous terminology and summarise the utility and pitfalls of adaptive designs. We discuss practical aspects around funding, ethical approval, treatment supply and communication with stakeholders and trial participants. Our focus, however, is on the interpretation and reporting of results from adaptive design trials, which we consider vital for anyone involved in medical research. We emphasise the general principles of transparency and reproducibility and suggest how best to put them into practice.},
author = {Pallmann, Philip and Bedding, Alun W. and Choodari-Oskooei, Babak and Dimairo, Munyaradzi and Flight, Laura and Hampson, Lisa V. and Holmes, Jane and Mander, Adrian P. and Odondi, Lang'o and Sydes, Matthew R. and Villar, Sof{\'{i}}a S. and Wason, James M.S. and Weir, Christopher J. and Wheeler, Graham M. and Yap, Christina and Jaki, Thomas},
booktitle = {BMC Medicine},
doi = {10.1186/s12916-018-1017-7},
file = {::},
issn = {17417015},
keywords = {Adaptive design,Design modification,Flexible design,Interim analysis,Seamless design,Statistical methods},
month = {feb},
number = {1},
pages = {1--15},
publisher = {BioMed Central Ltd.},
title = {{Adaptive designs in clinical trials: Why use them, and how to run and report them}},
volume = {16},
year = {2018}
}
@article{Andrzejak2001b,
abstract = {We compare dynamical properties of brain electrical activity from different recording regions and from different physiological and pathological brain states. Using the nonlinear prediction error and an estimate of an effective correlation dimension in combination with the method of iterative amplitude adjusted surrogate data, we analyze sets of electroencephalographic (EEG) time series: surface EEG recordings from healthy volunteers with eyes closed and eyes open, and intracranial EEG recordings from epilepsy patients during the seizure free interval from within and from outside the seizure generating area as well as intracranial EEG recordings of epileptic seizures. As a preanalysis step an inclusion criterion of weak stationarity was applied. Surface EEG recordings with eyes open were compatible with the surrogates' null hypothesis of a Gaussian linear stochastic process. Strongest indications of nonlinear deterministic dynamics were found for seizure activity. Results of the other sets were found to be inbetween these two extremes. {\textcopyright} 2001 The American Physical Society.},
author = {Andrzejak, Ralph G. and Lehnertz, Klaus and Mormann, Florian and Rieke, Christoph and David, Peter and Elger, Christian E.},
doi = {10.1103/PhysRevE.64.061907},
file = {::},
issn = {1063-651X},
journal = {Physical Review E},
month = {nov},
number = {6},
pages = {061907},
pmid = {11736210},
publisher = {American Physical Society},
title = {{Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state}},
url = {https://link.aps.org/doi/10.1103/PhysRevE.64.061907},
volume = {64},
year = {2001}
}
@article{Isaak2018,
abstract = {With the revelation that Facebook handed over personally identifiable information of more than 87 million users to Cambridge Analytica, it is now imperative that comprehensive privacy policy laws be developed. Technologists, researchers, and innovators should meaningfully contribute to the development of these policies.},
author = {Isaak, Jim and Hanna, Mina J.},
doi = {10.1109/MC.2018.3191268},
file = {::},
issn = {15580814},
journal = {Computer},
keywords = {Cambridge Analytica,Facebook,Internet/Web technologies,PII,The Policy Corner,cybercrime,data privacy,data security,online security,personally identifiable information,privacy,privacy protection,security,social media,user data privacy},
number = {8},
pages = {56--59},
publisher = {IEEE},
title = {{User Data Privacy: Facebook, Cambridge Analytica, and Privacy Protection}},
volume = {51},
year = {2018}
}
@article{Spinellis2011,
abstract = {The style of our code, encompassing formatting, the ordering of the program's elements, and the naming of our identifiers, is a key aspect of its maintainability. Expertly styled code is more expressive, making us more productive when reading or writing code, while avoiding distractions. To improve code style, acquaint yourself with the style guidelines of each language you use or software you edit and apply them religiously. Also, learn from other people's code; don't rely on formatting tools to do the job for you. {\textcopyright} 2011 IEEE.},
author = {Spinellis, Diomidis},
doi = {10.1109/MS.2011.31},
file = {::},
issn = {07407459},
journal = {IEEE Software},
keywords = {formatting guidelines,programming style},
number = {2},
pages = {103--104},
title = {{elytS edoC}},
volume = {28},
year = {2011}
}
@article{Lipton2018,
abstract = {Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal p(y) changes but the conditional p(x$\backslash$y) does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution p(y). BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, orun- calibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.},
archivePrefix = {arXiv},
arxivId = {1802.03916},
author = {Lipton, Zachary C. and Wang, Yu Xiang and Smola, Alexander J.},
eprint = {1802.03916},
file = {::},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {4887--4897},
title = {{Detecting and correcting for label shift with black box predictors}},
volume = {7},
year = {2018}
}
@article{Sharmila2016,
abstract = {Electroencephalogram (EEG) comprises valuable details related to the different physiological state of the brain. In this paper, a framework is offered for detecting the epileptic seizures from EEG data recorded from normal subjects and epileptic patients. This framework is based on a discrete wavelet transform (DWT) analysis of EEG signals using linear and nonlinear classifiers. The performance of the 14 different combinations of two-class epilepsy detection is studied using na{\"{i}}ve Bayes (NB) and k-nearest neighbor (k-NN) classifiers for the derived statistical features from DWT. It has been found that the NB classifier performs better and shows an accuracy of 100{\%} for the individual and combined statistical features derived from the DWT values of normal eyes open and epileptic EEG data provided by the University of Bonn, Germany. It has been found that the computation time of NB classifier is lesser than k-NN to provide better accuracy. So, the detection of an epileptic seizure based on DWT statistical features using NB classifiers is more suitable in real time for a reliable, automatic epileptic seizure detection system to enhance the patient's care and the quality of life.},
author = {Sharmila, A. and Geethanjali, P.},
doi = {10.1109/ACCESS.2016.2585661},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Electroencephalograms (EEG),discrete wavelet transform (DWT),epilepsy,k-nearest neighbor (k-NN),na{\"{i}}ve Bayes (NB)},
pages = {7716--7727},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{DWT Based Detection of Epileptic Seizure from EEG Signals Using Naive Bayes and k-NN Classifiers}},
volume = {4},
year = {2016}
}
@techreport{Harati,
abstract = {The Neural Engineering Data Consortium (NEDC) is releasing its first major big data corpus-the Temple University Hospital EEG Corpus. This corpus consists of over 25,000 EEG studies, and includes a neurologist's interpretation of the test, a brief patient medical history and demographic information about the patient such as gender and age. For the first time, there is a sufficient amount of data to support the application of state of the art machine learning algorithms. In this paper, we present pilot results of experiments on the prediction of some basic attributes of an EEG from the raw EEG signal data using a 3,762 session subset of the corpus. Standard machine learning approaches are shown to be capable of predicting commonly occurring events from simple features with high accuracy on closed-loop testing, and can deliver error rates below 50{\%} on a 6-way open set classification problem. This is very promising performance since commercial technology fails on this data.},
author = {Harati, A and L{\'{o}}pez, S and Obeid, I and Picone, J and Jacobson, M P and Tobochnik, S},
file = {::},
title = {{THE TUH EEG CORPUS: A Big Data Resource for Automated EEG Interpretation}}
}
@article{Das2018,
abstract = {Most of the traditional pattern classifiers assume their input data to be well-behaved in terms of similar underlying class distributions, balanced size of classes, the presence of a full set of observed features in all data instances, etc. Practical datasets, however, show up with various forms of irregularities that are, very often, sufficient to confuse a classifier, thus degrading its ability to learn from the data. In this article, we provide a bird's eye view of such data irregularities, beginning with a taxonomy and characterization of various distribution-based and feature-based irregularities. Subsequently, we discuss the notable and recent approaches that have been taken to make the existing stand-alone as well as ensemble classifiers robust against such irregularities. We also discuss the interrelation and co-occurrences of the data irregularities including class imbalance, small disjuncts, class skew, missing features, and absent (non-existing or undefined) features. Finally, we uncover a number of interesting future research avenues that are equally contextual with respect to the regular as well as deep machine learning paradigms.},
annote = {Contains a list of future research directions that are well worth looking over again at some point.

Defines: Missing features and Absent features

small disjuncts - sub clusters inside the main clusters

class distribution skew - difficult to visualise when there are more than 3 dimensions 


Solutions:
Data Level
- Undersampling
- Oversampling
- Hybrid

Algorithm Level
- Cost sensitive learning
- Boundary shifting methods
- Single class learning
- Active learning
- Kernel perturbation techniques
- Discriminative regression based supervised learning models},
author = {Das, Swagatam and Datta, Shounak and Chaudhuri, Bidyut B.},
doi = {10.1016/j.patcog.2018.03.008},
file = {::},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Absent features,Class imbalance,Class-distribution skew,Data irregularities,Missing features,Small disjuncts},
number = {June},
pages = {674--693},
title = {{Handling data irregularities in classification: Foundations, trends, and future challenges}},
volume = {81},
year = {2018}
}
@article{Moody2009,
abstract = {Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.},
author = {Moody, Daniel},
doi = {10.1109/TSE.2009.67},
file = {::},
isbn = {978-1-60558-719-6},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Analysis,Communication,Computer industry,Concrete,Concrete syntax,Design optimization,Diagrams,Flowcharts,Humans,Modeling,Physics,Problem-solving,Software engineering,Unified modeling language,Visual syntax,Visualization,analysis,communication,concrete syntax.,design flaws,diagrams,physics of notations,visual notations,visual representation,visual syntax},
mendeley-tags = {Computer industry,Concrete,Design optimization,Flowcharts,Humans,Modeling,Physics,Problem-solving,Software engineering,Unified modeling language,Visualization,analysis,communication,concrete syntax.,design flaws,diagrams,physics of notations,visual notations,visual representation,visual syntax},
month = {nov},
number = {6},
pages = {756--779},
shorttitle = {The {\#}x0201C;Physics {\#}x0201D; of Notations},
title = {{The physics of notations: Toward a scientific basis for constructing visual notations in software engineering}},
volume = {35},
year = {2009}
}
@article{Mirza2019,
abstract = {Information gathering comprises actions whose (sensory) consequences resolve uncertainty (i.e., are salient). In other words, actions that solicit salient information cause the greatest shift in beliefs (i.e., information gain) about the causes of our sensations. However, not all information is relevant to the task at hand: this is especially the case in complex, naturalistic scenes. This paper introduces a formal model of selective attention based on active inference and contextual epistemic foraging. We consider a visual search task with a special emphasis on goal-directed and task-relevant exploration. In this scheme, attention modulates the expected fidelity (precision) of the mapping between observations and hidden states in a state-dependent or context-sensitive manner. This ensures task-irrelevant observations have little expected information gain, and so the agent – driven to reduce expected surprise (i.e., uncertainty) – does not actively seek them out. Instead, it selectively samples task-relevant observations, which inform (task-relevant) hidden states. We further show, through simulations, that the atypical exploratory behaviours in conditions such as autism and anxiety may be due to a failure to appropriately modulate sensory precision in a context-specific way.},
author = {Mirza, M. Berk and Adams, Rick A. and Friston, Karl and Parr, Thomas},
doi = {10.1038/s41598-019-50138-8},
file = {::},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--22},
title = {{Introducing a Bayesian model of selective attention based on active inference}},
volume = {9},
year = {2019}
}
@article{Kalra2012,
abstract = {Public mental health incorporates a number of strategies from mental well-being promotion to primary prevention and other forms of prevention. There is considerable evidence in the literature to suggest that early interventions and public education can work well for reducing psychiatric morbidity and resulting burden of disease. Educational strategies need to focus on individual, societal and environmental aspects. Targeted interventions at individuals will also need to focus on the whole population. A nested approach with the individual at the heart of it surrounded by family surrounded by society at large is the most suitable way to approach this. This Guidance should be read along with the European Psychiatric Association (EPA) Guidance on Prevention. Those at risk of developing psychiatric disorders also require adequate interventions as well as those who may have already developed illness. However, on the model of triage, mental health and well-being promotion need to be prioritized to ensure that, with the limited resources available, these activities do not get forgotten. One possibility is to have separate programmes for addressing concerns of a particular population group, another that is relevant for the broader general population. Mental health promotion as a concept is important and this will allow prevention of some psychiatric disorders and, by improving coping strategies, is likely to reduce the burden and stress induced by mental illness. {\textcopyright} 2011 Elsevier Masson SAS.},
author = {Kalra, G. and Christodoulou, G. and Jenkins, R. and Tsipas, V. and Christodoulou, N. and Lecic-Tosevski, D. and Mezzich, J. and Bhugra, D.},
doi = {10.1016/j.eurpsy.2011.10.001},
issn = {09249338},
journal = {European Psychiatry},
keywords = {Intervention,Mental health,Mental health promotion,Mental well-being},
month = {feb},
number = {2},
pages = {81--86},
publisher = {No longer published by Elsevier},
title = {{Mental health promotion: Guidance and strategies}},
volume = {27},
year = {2012}
}
@article{Ju2008a,
author = {Ju, Wendy and Leifer, Larry},
doi = {10.1162/desi.2008.24.3.72},
file = {::},
issn = {0747-9360},
journal = {Design Issues},
month = {jul},
number = {3},
pages = {72--84},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{The Design of Implicit Interactions: Making Interactive Systems Less Obnoxious}},
url = {http://www.mitpressjournals.org/doi/10.1162/desi.2008.24.3.72},
volume = {24},
year = {2008}
}
@article{Raghuraman2019,
abstract = {The benefits of modeling the design to improve the quality and maintainability of software systems have long been advocated and recognized. Yet, the empirical evidence on this remains scarce. In this paper, we fill this gap by reporting on an empirical study of the relationship between UML modeling and software defect proneness in a large sample of open-source GitHub projects. Using statistical modeling, and controlling for confounding variables, we show that projects containing traces of UML models in their repositories experience, on average, a statistically minorly different number of software defects (as mined from their issue trackers) than projects without traces of UML models.},
author = {Raghuraman, Adithya and Ho-Quang, Truong and {V Chaudron Chalmers}, Michel R and Serebrenik, Alexander and Vasilescu, Bogdan},
file = {::},
journal = {16th International Conference on Mining Software Repositories},
keywords = {UML,open source software,software design,software quality},
title = {{Does UML Modeling Associate with Lower Defect Proneness?: A Preliminary Empirical Investigation}},
url = {https://pypi.org/project/langdetect/},
year = {2019}
}
@article{Sato2020,
abstract = {Although the prior literature has examined the relationship between work schedule characteristics and worker mental health, establishing the causal effect of work schedule characteristics is challenging because of endogeneity issues. This paper investigates how various work schedule characteristics affect workers' mental health using employee surveys and actual working hours recorded over seventeen months in a Japanese manufacturing company. Our sample includes 1334 white-collar workers and 786 blue-collar workers observed from 2015 to 2016. Our major findings are as follows: long working hours cause the mental health of white-collar workers to deteriorate even after controlling for individual fixed effects. Furthermore, working on weekends is associated with mental ill health—the negative effect of an hour increase in weekend work is one and a half to two times larger than that of weekday overtime work for white-collar workers. On the other hand, short rest periods are not associated with mental health for them. Our results indicate that taking a relatively long rest period on weekends is more important for keeping white-collar workers healthy than ensuring a sufficient daily rest period. Regarding blue-collar workers, our analysis reveals that working after midnight is associated with mental ill health, whereas short rest periods are not associated with their mental health. This suggests that the strain of night work is a more important determinant of mental health for blue-collar workers. The differences in the relationship between work schedule characteristics and workers' mental health for white-collar and blue-collar workers can be explained in terms of different work styles, different expectations, and different degrees of selection. We conclude that working for long hours or irregular hours deteriorates the mental health of workers but its impact is likely to differ significantly across job types.},
author = {Sato, Kaori and Kuroda, Sachiko and Owan, Hideo},
doi = {10.1016/j.socscimed.2019.112774},
file = {::},
issn = {18735347},
journal = {Social Science and Medicine},
keywords = {Healthy worker effect,Japan,Mental health,Nightwork,Short rest period,Weekend work,Working hours},
month = {feb},
pages = {112774},
publisher = {Elsevier Ltd},
title = {{Mental health effects of long work hours, night and weekend work, and short rest periods}},
volume = {246},
year = {2020}
}
@article{ChadwickD1990,
abstract = {
Chadwick, D.W., 1990. Diagnosis of epilepsy. The Lancet, },
author = {{Chadwick D}},
file = {::},
journal = {Lancet},
pages = {291--295.},
title = {{Diagnosis of epilepsy}},
volume = {336(8710)},
year = {1990}
}
@article{Jensen2018,
abstract = {Research has shown that desirable designs shape the use and experiences people have when interacting with technology. Nevertheless, how desirability influences energy consumption is often overlooked, particularly in HCI studies evaluating the sustainability benefits of smart home technology. In this paper, we present a qualitative study with 23 Australian households who reflect on their experiences of living with smart home devices. Drawing on Nelson and Stolterman's concept of desiderata we develop a typology of householders' desires for the smart home and their energy implications. We structure these desires as three smart home personas: the helper, optimiser and hedonist, which align with desiderata's three approaches to desire (reason, ethics and aesthetics). We use these insights to discuss how desirability can be used within HCI for steering design of the smart home towards sustainability.},
author = {Jensen, Rikke Hagensby and Strengers, Yolande and Kjeldskov, Jesper and Nicholls, Larissa and Skov, Mikael B},
doi = {10.1145/3173574.3173578},
file = {::},
isbn = {9781450356206},
keywords = {Author's kit,Conference Publications,Guides,instructions},
title = {{Designing the Desirable Smart Home: A Study of Household Experiences and Energy Consumption Impacts}},
url = {https://doi.org/10.1145/3173574.3173578},
year = {2018}
}
@phdthesis{Simmons2019,
author = {Simmons, Andrew J.},
file = {::},
isbn = {9781450354905},
school = {Deakin University},
title = {{Computational Pipelines for Spatio-Temporal Analysis of Team Invasion Games}},
year = {2019}
}
@article{Kanter2009,
abstract = {OBJECTIVE: To estimate whether an organized, consistent program of dietary and lifestyle counseling prevents excessive weight gain in pregnancy. METHODS: This randomized controlled trial assigned women to receive either an organized, consistent program of intensive dietary and lifestyle counseling or routine prenatal care. The primary study outcome was the proportion of patients whose gestational weight gain was within the Institute of Medicine (IOM) guidelines. Secondary outcomes included mode of delivery, rate of operative vaginal delivery, neonatal weight, and the incidence of preeclampsia, gestational diabetes mellitus (GDM), vaginal/perineal lacerations, and shoulder dystocia. RESULTS: A total of 100 women were randomized to the study (lifestyle counseling 57, routine prenatal care 43). Baseline demographic characteristics were similar between the study groups. The lifestyle counseling group gained significantly less weight than did the routine prenatal care group (28.7±12.5 lb compared with 35.6±15.5 lb, P=.01). The routine prenatal care group had significantly more cesarean deliveries due to “failure to progress” (routine prenatal care 58.3{\%} compared with lifestyle counseling 25.0{\%}, P=.02). Across groups, patients who were not adherent to the IOM guidelines had significantly heavier neonates (adherent 3,203.2±427.2 g compared with not adherent 3,517.4±572.4 g, P{\textless}.01). Nulliparous women gained significantly more weight than did parous women (36.5±14.5 lb compared with 27.7±12.7 lb, P{\textless}.01). The most predictive factor of IOM adherence was having a normal prepregnancy body mass index. No statistically significant differences were noted between the groups in adherence to IOM guidelines, rate of cesarean delivery, preeclampsia, GDM, operative vaginal delivery, or vaginal lacerations. CONCLUSION: An organized, consistent program of dietary and lifestyle counseling did reduce weight gain in pregnancy.},
author = {Kanter, James Max},
doi = {978-1-4673-8273-1},
file = {::},
isbn = {0029-7844},
issn = {00297844},
journal = {Obstetrics and Gynecology},
number = {2 PART 1},
pages = {305--312},
pmid = {19155899},
title = {{Deep Feature Synthesis: Towards Automating Data Science Endeavors James}},
volume = {113},
year = {2009}
}
@inproceedings{Simpkin2013,
abstract = {Synthetic training exercises, not unlike live exercises, require extensive planning, preparation and management in order to provide trainees with effective training experiences. These activities are performed by training facilitators, commonly known as the White Force. For many exercises, White Force personnel far outnumber the training audience, and the limited availability of White Force personnel places an upper- limit on exercise size, scope and fidelity. This paper examines the White Force activities in previous Australian synthetic training exercises, and builds the case for further development of White Force technologies and techniques. It is the premise of this work that developments leveraging White Force personnel will directly improve the cost effectiveness of synthetic training.},
address = {Vibe Hotel, North Sydney, Australia},
author = {Simpkin, Graeme and Ross, Peter and Macpherson, Bradley},
booktitle = {NATO Modelling {\&} Simulation Group (NMSG) Multi-Workshop},
doi = {10.14339/STO-MP-MSG-111},
file = {::},
title = {{On the Need for White Force Multipliers}},
url = {http://ftp.rta.nato.int/public/PubFullText/RTO/MP/STO-MP-MSG-111/MP-MSG-111-20.pdf https://www.sto.nato.int/publications/STO Meeting Proceedings/Forms/Meeting Proceedings Document Set/docsethomepage.aspx?ID=40677{\&}FolderCTID=0x0120D5200078F9E87043356C409A0},
year = {2013}
}
@misc{Oto2017,
abstract = {Purpose To present evidence from the literature on the rates, underlying causes and consequences of the misdiagnosis of epilepsy and place these meaningfully within a practical framework of risk appraisal and managed diagnostic uncertainty towards informing a clinical practice that might make misdiagnosis less likely. Method Narrative review. Results Misdiagnosis of epilepsy remains common and the consequences for the individual significant. Evidence and critical appraisal are presented as regards the absolute level of risk associated with the false positive diagnosis epilepsy, and reasons as to why those risks need to be appraised against the risks associated to false negative diagnosis. Conclusions Diagnostic error is not entirely avoidable and a degree of uncertainty, and perforce risk, is intrinsic to the diagnostic process of epilepsy. The risks of a false negative diagnosis of epilepsy must be appraised against the also significant risks of a false positive diagnosis.},
author = {Oto, Maria (Meritxell)},
booktitle = {Seizure},
doi = {10.1016/j.seizure.2016.11.029},
file = {::},
issn = {15322688},
keywords = {Diagnostic uncertainty,Epilepsy,Misdiagnosis,Risk appraisal},
month = {jan},
pages = {143--146},
pmid = {28017581},
publisher = {W.B. Saunders Ltd},
title = {{The misdiagnosis of epilepsy: Appraising risks and managing uncertainty}},
volume = {44},
year = {2017}
}
@techreport{Kalliamvakou2007,
abstract = {With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features-namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40{\%} of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub.},
author = {Kalliamvakou, Eirini and Gousios, Georgios and tudelftnl {Kelly Blincoe} and Singer, Leif and German, Daniel M and Damian, Daniela and {tudelftnl Kelly Blincoe} and Singer, Leif and German, Daniel M and Damian, Daniela},
file = {::},
keywords = {D28 [Software Engineering]: Management-Software co,GitHub,code reviews,git},
title = {{The Promises and Perils of Mining GitHub}},
url = {https://github.com/rails/rails.},
year = {2007}
}
@article{Feyisetan2020,
abstract = {Privacy-preserving data analysis has become essential in Machine Learning (ML), where access to vast amounts of data can provide large gains the in accuracies of tuned models. A large proportion of user-contributed data comes from natural language e.g., text transcriptions from voice assistants. It is therefore important for curated natural language datasets to preserve the privacy of the users whose data is collected and for the models trained on sensitive data to only retain non-identifying (i.e., generalizable) information. The workshop aims to bring together researchers and practitioners from academia and industry to discuss the challenges and approaches to designing, building, verifying, and testing privacy-preserving systems in the context of Natural Language Processing (NLP).},
author = {Feyisetan, Oluwaseyi and Ghanavati, Sepideh and Thaine, Patricia},
doi = {10.1145/3336191.3371881},
file = {::},
isbn = {9781450368223},
journal = {WSDM 2020 - Proceedings of the 13th International Conference on Web Search and Data Mining},
keywords = {Cryptography,Differential privacy,Machine learning,Natural language processing,Privacy,Security,Text processing},
number = {4417749},
pages = {903--904},
title = {{Workshop on privacy in NLP (PrivaTENLP 2020)}},
year = {2020}
}
@article{Wilson2017,
abstract = {These experiments make it possible to recommend the ultrasonic impulse technique in checking and defectoscopy of connecting joints in sectional monolithic hydraulic structures. This technique yields industrially reliable results without disturbing the continuity of the structures. Ultrasonic defectoscopy of joints was adopted as a principal mass-application checking method in construction of the Saratov Hydroelectric Station. {\textcopyright}1969 American Society of Civil Engineers.},
author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
doi = {10.1371/journal.pcbi.1005510},
editor = {Ouellette, Francis},
file = {::},
isbn = {1111111111},
issn = {1553-7358},
journal = {PLOS Computational Biology},
month = {jun},
number = {6},
pages = {e1005510},
title = {{Good enough practices in scientific computing}},
volume = {13},
year = {2017}
}
@inproceedings{Jones2005,
abstract = {A study explores the way people organize information in support of projects ("teach a course", "plan a wedding", etc.). The folder structures to organize project information - especially electronic documents and other files - frequently resembled a "divide and conquer" problem decomposition with subfolders corresponding to major components (subprojects) of the project. Folders were clearly more than simply a means to one end: Organizing for later retrieval. Folders were information in their own right - representing, for example, a person's evolving understanding of a project and its components. Unfortunately, folders are often "overloaded" with information. For example, folders sometimes included leading characters to force an ordering ("aa", "zz"). And folder hierarchies frequently reflected a tension between organizing information for current use vs. repeated re-use.},
address = {New York, New York, USA},
author = {Jones, William and Phuwanartnurak, Ammy Jiranida and Gill, Rajdeep and Bruce, Harry},
booktitle = {CHI '05 extended abstracts on Human factors in computing systems - CHI '05},
doi = {10.1145/1056808.1056952},
file = {::},
isbn = {1595930027},
pages = {1505--1508},
publisher = {ACM Press},
title = {{Don't take my folders away! Organizing Personal Information to Get Things Done}},
url = {http://portal.acm.org/citation.cfm?doid=1056808.1056952},
year = {2005}
}
@article{Bao2011,
abstract = {Computer-aided diagnosis of neural diseases from EEG signals (or other physiological signals that can be treated as time series, e.g., MEG) is an emerging field that has gained much attention in past years. Extracting features is a key component in the analysis of EEG signals. In our previous works, we have implemented many EEG feature extraction functions in the Python programming language. As Python is gaining more ground in scientific computing, an open source Python module for extracting EEG features has the potential to save much time for computational neuroscientists. In this paper, we introduce PyEEG, an open source Python module for EEG feature extraction.},
author = {Bao, Forrest Sheng and Liu, Xin and Zhang, Christina},
doi = {10.1155/2011/406391},
file = {::},
issn = {1687-5273},
journal = {Computational intelligence and neuroscience},
pages = {406391},
pmid = {21512582},
publisher = {Hindawi Limited},
title = {{PyEEG: an open source Python module for EEG/MEG feature extraction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21512582 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3070217},
volume = {2011},
year = {2011}
}
@article{Chen2018,
abstract = {This paper investigates how supply chain management (SCM) efficiency affects the value investors attach to the change in a company's inventory holdings. Based on a large number of U.S. firms from 1971 to 2013, we find that, on average, one dollar of inventory change is valued at {\$}0.507 in the stock market. Decomposition into normal and abnormal inventory changes reveals that the market value of one dollar of abnormal inventory change is 43{\%} smaller than that of normal inventory change, where a normal change moves the inventory balance to an estimated optimal level, and an abnormal change refers to the gap between actual inventory change and the estimated optimal change. We also investigate inventory turnover and gross margin and find economically and statistically significant relations between these proxies for efficient SCM and the value the market attaches to inventory changes. Finally, we find that the market attaches a higher value to inventory changes of firms with better growth prospects, higher sales predictability, and tighter financial constraints.},
author = {Chen, Jeff Zeyun and Jung, Boochun and Park, Duri and Shane, Philip B.},
doi = {10.2139/ssrn.3193750},
file = {::},
journal = {SSRN Electronic Journal},
keywords = {abnormal inventory,and comments from roger,boochun jung,corresponding author,debreceny,hawaii at manoa and,holdings,kaist-korea university joint seminar,market value of inventory,participants at university of,supply chain management,we appreciate helpful suggestions,workshop},
pages = {1--44},
title = {{The Market Value of Inventory}},
year = {2018}
}
@article{Inkpen2019,
abstract = {In recent years, AI systems have become both more powerful and increasingly promising for integration in a variety of application areas. Attention has also been called to the social challenges these systems bring, particularly in how they might fail or even actively disadvantage marginalised social groups, or how their opacity might make them difficult to oversee and challenge. In the context of these and other challenges, the roles of humans working in tandem with these systems will be important, yet the HCI community has been only a quiet voice in these debates to date. This workshop aims to catalyse and crystallise an agenda around HCI's engagement with AI systems. Topics of interest include explainable and explorable AI; documentation and review; integrating artificial and human intelligence; collaborative decision making; AI/ML in HCI Design; diverse human roles and relationships in AI systems; and critical views of AI.},
author = {Inkpen, Kori and Veale, Michael and Chancellor, Stevie and {De Choudhury}, Munmun and Baumer, Eric P.S. S},
doi = {10.1145/3290607.3299002},
file = {::},
isbn = {9781450359719},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Artificial intelligence,Human computer interaction},
pages = {1--9},
title = {{Where is the human? Bridging the gap between AI and HCI}},
year = {2019}
}
@book{IEEEComputerSociety2013,
abstract = {A recommended practice for applying the Distributed Simulation Engineering and Execution Process (DSEEP) to the development and execution of distributed simulation environments that include more than one distributed simulation architecture is described. The distributed simulation architectures to which the recommended practice applies include Distributed Interactive Simulation (DIS), High Level Architecture (HLA), and Test and Training Enabling Architecture (TENA). The DSEEP Multi-Architecture Overlay (DMAO) identifies and describes multi-architecture issues and provides recommended actions for simulation environment developers faced with those issues. The DMAO also augments the DSEEP lists of inputs, recommended tasks, and outcomes with additional inputs, recommended tasks, and outcomes that apply to multi-architecture simulation environments. This document is an overlay to the DSEEP, which is a separate recommended practice.},
author = {{IEEE Computer Society}},
booktitle = {Ieee Standard 1730.1-2013},
doi = {10.1109/IEEESTD.2013.6654219},
file = {::},
isbn = {9780738164687},
number = {January},
title = {{IEEE Recommended Practice for Distributed Simulation Engineering and Execution Process Multi-Architecture Overlay (DMAO)}},
url = {http://ieeexplore.ieee.org/servlet/opac?punumber=6654217},
volume = {2013},
year = {2013}
}
@inproceedings{Sauro2011,
abstract = {When designing questionnaires there is a tradition of including items with both positive and negative wording to minimize acquiescence and extreme response biases. Two disadvantages of this approach are respondents accidentally agreeing with negative items (mistakes) and researchers forgetting to reverse the scales (miscoding). The original System Usability Scale (SUS) and an all positively worded version were administered in two experiments (n=161 and n=213) across eleven websites. There was no evidence for differences in the response biases between the different versions. A review of 27 SUS datasets found 3 (11{\%}) were miscoded by researchers and 21 out of 158 questionnaires (13{\%}) contained mistakes from users. We found no evidence that the purported advantages of including negative and positive items in usability questionnaires outweigh the disadvantages of mistakes and miscoding. It is recommended that researchers using the standard SUS verify the proper coding of scores and include procedural steps to ensure error-free completion of the SUS by users. Researchers can use the all positive version with confidence because respondents are less likely to make mistakes when responding, researchers are less likely to make errors in coding, and the scores will be similar to the standard SUS. Copyright 2011 ACM.},
author = {Sauro, Jeff and Lewis, James R.},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/1978942.1979266},
isbn = {9781450302289},
keywords = {Acquiescent bias,Satisfaction measures,Standardized questionnaires,System Usability Scale (SUS),Usability evaluation},
pages = {2215--2223},
title = {{When designing usability questionnaires, does it hurt to be positive?}},
year = {2011}
}
@techreport{Binnie,
abstract = {Epilepsy is the area in which electroencephalography is probably of greatest potential clinical value, and yet is most abused. In general, the sensitivity of the waking interictal EEG for detecting epilepsy and its speci{\textregistered}city for distinguishing epilepsy from other episodic disorders are both very limited, and routine examination for diagnostic screening or follow up is of little value. However, as this review attempts to demonstrate, EEG is of crucial importance for answering speci{\textregistered}c, clearly de{\textregistered}ned questions which commonly arise in the management of seizure disorders, especially so, when non-routine and if necessary complex investigations are undertaken, to address the problems of individual patients. q},
author = {Binnie, Colin D and Stefan, Hermann},
file = {::},
isbn = {1441713464342},
keywords = {Electroencephalography,Epilepsy management},
title = {{Modern electroencephalography: its role in epilepsy management}},
url = {www.elsevier.nl/locate/clinph}
}
@article{Petersen2015a,
abstract = {Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.},
annote = {SMS used to structure a research area, whilst systematic reviews are focused on gathering 
and synthesizing evidence. 
Benefits of SMS are time savings for follow up studies, good overview of an area, 
ability to identify research gaps, visualisation of research trends, related work 
identification, use as a validation set based on gathered references, educational sources
Search terms follow PICO (Population, Intervention, Comparison and Outcomes)
Search Engines - IEEE Explore, ACM, Scopus, Inspec/Compendex
Backward snowball sampling to select studies
Inclusion and exclusion criteria applied to titles and abstracts
Template of Data Extraction Form is available.
Different guidelines followed for SMS, Popular ones are Peterson , Kitchenham
Seach Strategies are Database search, manual search, snowballing
Contribution Type could be process/method/model/tool/metric.
Study Type could be Evaluation Research/Validation Research/Solution Proposal/Philosophical Paper/
Experience Report/Opinion Paper.
The mapping process consists of planning, conducting, reporting and mapping.
Bubble plot and bar plot popular to produce the results.
Think aloud protocol.},
author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
doi = {10.1016/j.infsof.2015.03.007},
file = {::},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Guidelines,Software engineering,Systematic mapping studies},
pages = {1--18},
publisher = {Elsevier B.V.},
title = {{Guidelines for conducting systematic mapping studies in software engineering: An update}},
url = {http://dx.doi.org/10.1016/j.infsof.2015.03.007},
volume = {64},
year = {2015}
}
@article{DosSantos2018,
abstract = {Several conventions and standards aim to improve maintainability of software code. However, low levels of code readability perceived by developers still represent a barrier to their daily work. In this paper, we describe a survey that assessed the impact of a set of Java coding practices on the readability perceived by software developers. While some practices promoted an enhancement of readability, others did not show statistically significant effects. Interestingly, one of the practices worsened the readability. Our results may help to identify coding conventions with a positive impact on readability and, thus, guide the creation of coding standards.},
author = {{Dos Santos}, Rodrigo Magalhes and Gerosa, Marco Aur{\'{e}}lio},
doi = {10.1145/3196321.3196342},
file = {::},
isbn = {9781450357142},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {code comprehension,code readability,coding best practices,programming style,software developers' opinions survey},
pages = {277--285},
title = {{Impacts of coding practices on readability}},
year = {2018}
}
@article{Fernandez2013,
author = {Fern{\'{a}}ndez, Alberto and Palade, Vasile and Herrera, Francisco and L{\'{o}}pez, Victoria and Garc{\'{i}}a, Salvador},
doi = {10.1016/j.ins.2013.07.007},
file = {::},
issn = {00200255},
journal = {Information Sciences},
keywords = {Cost-sensitive learning,Dataset shift,Imbalanced dataset,Noisy data,Sampling,Small disjuncts},
pages = {113--141},
publisher = {Elsevier Inc.},
title = {{An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics}},
url = {http://dx.doi.org/10.1016/j.ins.2013.07.007},
volume = {250},
year = {2013}
}
@inproceedings{Skurichina1996,
abstract = {In this paper the possibilities for constructing linear classifiers are considered for very small sample sizes. We propose a stability measure and present a study on the performance and stability of the following techniques: regularization by the Ridge-estimate of the covariance matrix [12], bootstrapping followed by aggregation (‘bagging', [9]) and editing combined with pseudo- inversion [8]. It is shown that by these techniques a smooth transition can be made between the nearest mean classifier and the Fisher discriminant based on large samples sizes. Especially for highly correlated data very good results are obtained compared with the nearest mean method.},
author = {Skurichina, Marina and Duin, Robert P.W.},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.1996.547204},
file = {::},
isbn = {081867282X},
issn = {10514651},
title = {{Stabilizing classifiers for very small sample sizes}},
year = {1996}
}
@article{Myers2018a,
abstract = {Voice User Interfaces (VUIs) are growing in popularity. However , even the most current VUIs regularly cause frustration for their users. Very few studies exist on what people do to overcome VUI problems they encounter, or how VUIs can be designed to aid people when these problems occur. In this paper, we analyze empirical data on how users (n=12) interact with our VUI calendar system, DiscoverCal, over three sessions. In particular, we identify the main obstacle categories and types of tactics our participants employ to overcome them. We analyzed the patterns of how different tactics are used in each obstacle category. We found that while NLP Error obstacles occurred the most, other obstacles are more likely to frustrate or confuse the user. We also found patterns that suggest participants were more likely to employ a "guessing" approach rather than rely on visual aids or knowledge recall.},
author = {Myers, Chelsea and Furqan, Anushay and Nebolsky, Jessica and Caro, Karina and Zhu, Jichen},
doi = {10.1145/3173574.3173580},
file = {::},
isbn = {9781450356206},
keywords = {User Experience,Voice User Interfaces,Voice control},
title = {{Patterns for How Users Overcome Obstacles in Voice User Interfaces}},
url = {https://doi.org/10.1145/3173574.3173580},
year = {2018}
}
@article{Covington2016,
abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
author = {Covington, Paul and Adams, Jay and Sargin, Emre},
doi = {10.1145/2959100.2959190},
file = {::},
isbn = {9781450340359},
journal = {RecSys 2016 - Proceedings of the 10th ACM Conference on Recommender Systems},
keywords = {Deep learning,Recommender system,Scalability},
pages = {191--198},
title = {{Deep neural networks for youtube recommendations}},
year = {2016}
}
@techreport{Mukherjee2002,
abstract = {Solutions of learning problems by Empirical Risk Minimization (ERM)-and almost-ERM when the minimizer does not exist-need to be consistent , so that they may be predictive. They also need to be well-posed in the sense of being stable, so that they might be used robustly. We propose a statistical form of leave-one-out stability, called CVEEE loo stability. Our main new results are two. We prove that for bounded loss classes CVEEE loo stability is (a) sufficient for generalization, that is convergence in probability of the empirical error to the expected error, for any algorithm satisfying it and, (b) necessary and sufficient for generalization and consistency of ERM. Thus CVEEE loo stability is a weak form of stability that represents a sufficient condition for generalization for general learning algorithms while subsuming the classical conditions for consistency of ERM. We discuss alternative forms of stability. In particular, we conclude that for ERM a certain form of well-posedness is equivalent to consistency.},
author = {Mukherjee, Sayan and Niyogi, Partha and Poggio, Tomaso and Rifkin, Ryan},
file = {::},
title = {{Statistical Learning : stability is sufficient for generalization and necessary and sufficient for consistency of Empirical Risk Minimization The October 2003 version replaced a previous revision (April 2003) of a rather flawed CBCL Paper}},
year = {2002}
}
@article{Kim2019,
abstract = {People naturally bring their prior beliefs to bear on how they interpret the new information, yet few formal models exist for accounting for the influence of users' prior beliefs in interactions with data presentations like visualizations. We demonstrate a Bayesian cognitive model for understanding how people interpret visualizations in light of prior beliefs and show how this model provides a guide for improving visualization evaluation. In a first study, we show how applying a Bayesian cognition model to a simple visualization scenario indicates that people's judgments are consistent with a hypothesis that they are doing approximate Bayesian inference. In a second study, we evaluate how sensitive our observations of Bayesian behavior are to different techniques for eliciting people subjective distributions, and to different datasets. We find that people don't behave consistently with Bayesian predictions for large sample size datasets, and this difference cannot be explained by elicitation technique. In a final study, we show how normative Bayesian inference can be used as an evaluation framework for visualizations, including of uncertainty.},
archivePrefix = {arXiv},
arxivId = {1901.02949},
author = {Kim, Yea Seul and Walls, Logan A. and Krafft, Peter and Hullman, Jessica},
doi = {10.1145/3290605.3300912},
eprint = {1901.02949},
file = {::},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Bayesian cognition,Uncertainty elicitation,Visualization},
title = {{A Bayesian cognition approach to improve data visualization}},
year = {2019}
}
@article{Liu2018,
abstract = {PhD thesis of l'Universit{\'{e}}Universit´Universit{\'{e}} Paris-Saclay Prepared at T ´ e{\'{i}} ecom ParisTech Doctoral school n • 580 Sciences et technologies de l'information et de la communication (STIC)},
author = {Liu, Wanyu "Abbie"},
file = {::},
journal = {Universit{\'{e}} Paris-Saclay},
title = {{Information theory as a unified tool for understanding and designing human-computer interaction}},
year = {2018}
}
@article{Sutton2020,
abstract = {Computerized clinical decision support systems, or CDSS, represent a paradigm shift in healthcare today. CDSS are used to augment clinicians in their complex decision-making processes. Since their first use in the 1980s, CDSS have seen a rapid evolution. They are now commonly administered through electronic medical records and other computerized clinical workflows, which has been facilitated by increasing global adoption of electronic medical records with advanced capabilities. Despite these advances, there remain unknowns regarding the effect CDSS have on the providers who use them, patient outcomes, and costs. There have been numerous published examples in the past decade(s) of CDSS success stories, but notable setbacks have also shown us that CDSS are not without risks. In this paper, we provide a state-of-the-art overview on the use of clinical decision support systems in medicine, including the different types, current use cases with proven efficacy, common pitfalls, and potential harms. We conclude with evidence-based recommendations for minimizing risk in CDSS design, implementation, evaluation, and maintenance.},
author = {Sutton, Reed T and Pincock, David and Baumgart, Daniel C and Sadowski, Daniel C and Fedorak, Richard N and Kroeker, Karen I},
doi = {10.1038/s41746-020-0221-y},
issn = {2398-6352},
journal = {npj Digital Medicine},
number = {1},
pages = {17},
title = {{An overview of clinical decision support systems: benefits, risks, and strategies for success}},
url = {https://doi.org/10.1038/s41746-020-0221-y},
volume = {3},
year = {2020}
}
@article{Roelen2014a,
abstract = {Purpose: To investigate fatigue as prognostic risk marker for identifying working employees at risk of long-term sickness absence (SA). Methods: At baseline, fatigue was measured in 633 white collar employees with the checklist individual strength (CIS) including scales for fatigue severity, reduced concentration, reduced motivation, and reduced physical activity. SA was medically certified by an occupational physician in the 3rd or 4th SA week with diagnostic codes according to the 10th version of the International Classification of Diseases. Medically certified SA was retrieved at the individual level from an occupational health register after 1-year follow-up. CIS scores were investigated as prognostic risk markers predicting medically certified SA and particularly SA certified as mental SA. Results: 614 employees (N = 378 men and N = 236 women) had complete data and were eligible for analysis; 63 (10 {\%}) had medically certified SA of whom 39 (6 {\%}) had mental SA. Fatigue severity and total CIS scores were associated with medically certified SA in men, but poorly discriminated between men with and without medically certified SA. Fatigue severity, reduced concentration, reduced motivation, and total CIS scores were also associated with mental SA in men. CIS and its reduced concentration scale were valid prognostic risk markers of mental SA. Conclusion Fatigue was a prognostic risk marker of mental SA in white collar men. The CIS should be further validated as a screening tool for the risk of mental SA in white collar working populations. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Roelen, C. A.M. and Heymans, M. W. and {Van Rhenen}, W. and Groothoff, J. W. and Twisk, J. W.R. and B{\"{u}}ltmann, U.},
doi = {10.1007/s10926-013-9458-5},
file = {::},
issn = {10530487},
journal = {Journal of Occupational Rehabilitation},
keywords = {Absenteeism,Checklist individual strength,Mental health,Office workers,Risk factor screening,Sick leave},
month = {jul},
number = {2},
pages = {307--315},
publisher = {Springer New York LLC},
title = {{Fatigue as prognostic risk marker of mental sickness absence in white collar employees}},
volume = {24},
year = {2014}
}
@article{Maksimenko2018,
abstract = {Brain-computer interfaces (BCIs) attract a lot of attention because of their ability to improve the brain's efficiency in performing complex tasks using a computer. Furthermore, BCIs can increase human's performance not only due to human-machine interactions, but also thanks to an optimal distribution of cognitive load among all members of a group working on a common task, i.e., due to human-human interaction. The latter is of particular importance when sustained attention and alertness are required. In every day practice, this is a common occurrence, for example, among office workers, pilots of a military or a civil aircraft, power plant operators, etc. Their routinely work includes continuous monitoring of instrument readings and implies a heavy cognitive load due to processing large amounts of visual information. In this paper, we propose a brain-to-brain interface (BBI) which estimates brain states of every participant and distributes a cognitive load among all members of the group accomplishing together a common task. The BBI allows sharing the whole workload between all participants depending on their current cognitive performance estimated from their electrical brain activity. We show that the team efficiency can be increased due to redistribution of the work between participants so that the most difficult workload falls on the operator who exhibits maximum performance. Finally, we demonstrate that the human-to-human interaction is more efficient in the presence of a certain delay determined by brain rhythms. The obtained results are promising for the development of a new generation of communication systems based on neurophysiological brain activity of interacting people. Such BBIs will distribute a common task between all group members according to their individual physical conditions.},
author = {Maksimenko, Vladimir A. and Hramov, Alexander E. and Frolov, Nikita S. and L{\"{u}}ttjohann, Annika and Nedaivozov, Vladimir O. and Grubov, Vadim V. and Runnova, Anastasia E. and Makarov, Vladimir V. and Kurths, J{\"{u}}rgen and Pisarchik, Alexander N.},
doi = {10.3389/fnins.2018.00949},
file = {::},
issn = {1662-453X},
journal = {Frontiers in Neuroscience},
keywords = {Brain states recognition,Brain-computer interface (BCI),Brain-to-brain interface (BBI),Cognitive performance,Cognitive reserve,Human-to-human interaction,Mental fatigue,Visual attention},
month = {dec},
pages = {949},
publisher = {Frontiers Media S.A.},
title = {{Increasing Human Performance by Sharing Cognitive Load Using Brain-to-Brain Interface}},
url = {https://www.frontiersin.org/article/10.3389/fnins.2018.00949/full},
volume = {12},
year = {2018}
}
@article{Duun-Henriksen2012,
abstract = {Objective: To investigate the performance of epileptic seizure detection using only a few of the recorded EEG channels and the ability of software to select these channels compared with a neurophysiologist. Methods: Fifty-nine seizures and 1419. h of interictal EEG are used for training and testing of an automatic channel selection method. The characteristics of the seizures are extracted by the use of a wavelet analysis and classified by a support vector machine. The best channel selection method is based upon maximum variance during the seizure. Results: Using only three channels, a seizure detection sensitivity of 96{\%} and a false detection rate of 0.14/h were obtained. This corresponds to the performance obtained when channels are selected through visual inspection by a clinical neurophysiologist, and constitutes a 4{\%} improvement in sensitivity compared to seizure detection using channels recorded directly on the epileptic focus. Conclusions: Based on our dataset, automatic seizure detection can be done using only three EEG channels without loss of performance. These channels should be selected based on maximum variance and not, as often done, using the focal channels. Significance: With this simple automatic channel selection method, we have shown a computational efficient way of making automatic seizure detection. {\textcopyright}2011.},
author = {Duun-Henriksen, Jonas and Kjaer, Troels Wesenberg and Madsen, Rasmus Elsborg and Remvig, Line Sofie and Thomsen, Carsten Eckhart and Sorensen, Helge Bjarup Dissing},
doi = {10.1016/j.clinph.2011.06.001},
file = {::},
issn = {13882457},
journal = {Clinical Neurophysiology},
keywords = {Automatic seizure detection,Channel selection,EEG,Epilepsy},
month = {jan},
number = {1},
pages = {84--92},
pmid = {21752709},
title = {{Channel selection for automatic seizure detection}},
volume = {123},
year = {2012}
}
@article{Zhu2018a,
abstract = {Image reconstruction is essential for imaging applications across the physical and life sciences, including optical and radar systems, magnetic resonance imaging, X-ray computed tomography, positron emission tomography, ultrasound imaging and radio astronomy. During image acquisition, the sensor encodes an intermediate representation of an object in the sensor domain, which is subsequently reconstructed into an image by an inversion of the encoding function. Image reconstruction is challenging because analytic knowledge of the exact inverse transform may not exist a priori, especially in the presence of sensor non-idealities and noise. Thus, the standard reconstruction approach involves approximating the inverse function with multiple ad hoc stages in a signal processing chain, the composition of which depends on the details of each acquisition strategy, and often requires expert parameter tuning to optimize reconstruction performance. Here we present a unified framework for image reconstruction - automated transform by manifold approximation (AUTOMAP) - which recasts image reconstruction as a data-driven supervised learning task that allows a mapping between the sensor and the image domain to emerge from an appropriate corpus of training data. We implement AUTOMAP with a deep neural network and exhibit its flexibility in learning reconstruction transforms for various magnetic resonance imaging acquisition strategies, using the same network architecture and hyperparameters. We further demonstrate that manifold learning during training results in sparse representations of domain transforms along low-dimensional data manifolds, and observe superior immunity to noise and a reduction in reconstruction artefacts compared with conventional handcrafted reconstruction methods. In addition to improving the reconstruction performance of existing acquisition methodologies, we anticipate that AUTOMAP and other learned reconstruction approaches will accelerate the development of new acquisition strategies across imaging modalities.},
archivePrefix = {arXiv},
arxivId = {1704.08841},
author = {Zhu, Bo and Liu, Jeremiah Z. and Cauley, Stephen F. and Rosen, Bruce R. and Rosen, Matthew S.},
doi = {10.1038/nature25988},
eprint = {1704.08841},
file = {::},
issn = {14764687},
journal = {Nature},
number = {7697},
pages = {487--492},
pmid = {29565357},
publisher = {Nature Publishing Group},
title = {{Image reconstruction by domain-transform manifold learning}},
volume = {555},
year = {2018}
}
@article{Compare2016,
abstract = {Background. Low back pain (LBP) is one of the most common health problems worldwide. Purpose. To investigate the link between baseline demographic and occupational, medical, and lifestyle data with following psychological and occupational outcomes in a large sample of employees with LBP over a 3-year period. Study Design. Three-year prospective cohort study. Methods. Italian-speaking employees (N = 4492) with a diagnosis of LBP were included. Screening at Time 1 was done in order to collect information about severity and classification of LBP, demographic, lifestyle, and occupational status data. Psychological distress (PGWBI) and occupational burden were assessed after 3 years. Results. After 3 years, employees with LBP not due to organic causes had an increased risk of psychological distress. Gender appears to be an important variable for following occupational burden. Indeed, being a white-collar man with a LBP without organic causes seems to be a protective factor for following work outcomes, while being a white-collar woman with a LBP not due to organic causes appears to be a risk factor for subsequent sick leave. Moreover, LBP severity affects psychological and occupational outcomes. Conclusion. Our findings have several implications that could be considered in preventive and supportive programs for LBP employees.},
author = {Compare, Angelo and Marchettini, Paolo and Zarbo, Cristina},
doi = {10.1155/2016/3797493},
file = {::},
issn = {20901550},
journal = {Pain Research and Treatment},
publisher = {Hindawi Limited},
title = {{Risk Factors Linked to Psychological Distress, Productivity Losses, and Sick Leave in Low-Back-Pain Employees: A Three-Year Longitudinal Cohort Study}},
volume = {2016},
year = {2016}
}
@misc{Romaniuk2017,
abstract = {Attention has become an area of major interest in marketing research as a dependent or moderating variable. In this article, we argue for respondent attention as a pivotal part of any consumer psychology research protocol and highlight the risks of not incorporating realistic attention components into research design. We propose four areas where this approach can help the external validity of consumer psychology research. Our recommendations include accounting for variability in the baseline attention levels; smart use of distractions; allowing for variability in attention over the task and avoiding attention leading/assumptive questions.},
author = {Romaniuk, Jenni and Nguyen, Cathy},
booktitle = {Journal of Marketing Management},
doi = {10.1080/0267257X.2017.1305706},
issn = {14721376},
keywords = {Consumer psychology,attention,research methods},
month = {jul},
number = {11-12},
pages = {909--916},
publisher = {Routledge},
title = {{Is consumer psychology research ready for today's attention economy?}},
volume = {33},
year = {2017}
}
@article{Cavallo2020,
author = {Cavallo, Joseph J. and Donoho, Daniel A. and Forman, Howard P.},
doi = {10.1001/JAMAHEALTHFORUM.2020.0345},
journal = {JAMA Health Forum},
keywords = {covid-19,hospital bed capacity,models, health care delivery,pandemics},
month = {mar},
number = {3},
pages = {e200345--e200345},
publisher = {American Medical Association},
title = {{Hospital Capacity and Operations in the Coronavirus Disease 2019 (COVID-19) Pandemic—Planning for the Nth Patient}},
volume = {1},
year = {2020}
}
@article{Smith2005,
author = {Smith, Shelagh J.M.},
doi = {10.1136/jnnp.2005.068486},
file = {::},
journal = {Neurology in Practice},
month = {jun},
number = {2},
pmid = {15961870},
title = {{EEG in neurological conditions other than epilepsy: When does it help, what does it add?}},
volume = {76},
year = {2005}
}
@inproceedings{Bodin2018,
abstract = {The R programming language is very popular for developing statistical software and data analysis, thanks to rich libraries, concise and expressive syntax, and support for interactive programming. Yet, the semantics of R is fairly complex, contains many subtle corner cases, and is not formally specified. This makes it difficult to reason about R programs. In this work, we develop a big-step operational semantics for R in the form of an interpreter written in the Coq proof assistant. We ensure the trustworthiness of the formalization by introducing a monadic encoding that allows the Coq interpreter, CoqR, to be in direct visual correspondence with the reference R interpreter, GNU R. Additionally, we provide a testing framework that supports systematic comparison of CoqR and GNU R. In its current state, CoqR covers the nucleus of the R language as well as numerous additional features, making it pass a significant number of realistic test cases from the GNU R and FastR projects. To exercise the formal specification, we prove in Coq the preservation of memory invariants in selected parts of the interpreter. This work is an important first step towards a robust environment for formal verification of R programs.},
address = {New York, New York, USA},
author = {Bodin, Martin and Diaz, Tom{\'{a}}s and Tanter, {\'{E}}ric},
booktitle = {Proceedings of the 14th ACM SIGPLAN International Symposium on Dynamic Languages - DLS 2018},
doi = {10.1145/3276945.3276946},
file = {::},
isbn = {9781450360302},
issn = {15232867},
keywords = {Coq,R,Testing infrastructure},
number = {8},
pages = {13--24},
publisher = {ACM Press},
title = {{A trustworthy mechanized formalization of R}},
url = {http://dl.acm.org/citation.cfm?doid=3276945.3276946},
volume = {53},
year = {2018}
}
@article{Easton1995,
author = {Easton, Peter D and Harris, Trevor S},
file = {::},
journal = {Journal of Accounting Research},
number = {1},
pages = {19--36},
title = {{Earnings as an explanatory variable for returns University of Chicago Stable}},
volume = {29},
year = {1995}
}
@article{LivingEvidenceNetwork2019,
author = {{Living Evidence Network}},
file = {::},
number = {December},
title = {{Guidance for the production and publication of Cochrane living systematic reviews: Cochrane Reviews in living mode}},
url = {https://community.cochrane.org/sites/default/files/uploads/inline-files/Transform/201912{\_}LSR{\_}Revised{\_}Guidance.pdf},
year = {2019}
}
@article{Gillies,
abstract = {Machine learning is one of the most important and successful techniques in contemporary computer science. It involves the statistical inference of models (such as classi-fiers) from data. It is often conceived in a very impersonal way, with algorithms working autonomously on passively collected data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, and even deciding what should be modeled in the first place. Examining machine learning from a human-centered perspective includes explicitly recognising this human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and systems. A human-centered understanding of machine learning in human context can lead not only to more usable machine learning tools, but to new ways of framing learning computationally. This workshop will bring together researchers to discuss these issues and suggest future research questions aimed at creating a human-centered approach to machine learning.},
author = {Gillies, Marco and Fiebrink, Rebecca and Tanaka, Atau and Garcia, J{\'{e}}r{\'{e}}mie and Amershi, Saleema and Lee, Bongshin and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and D', Nicolas and Tilmanne, Alessandro Jo{\"{e}}lle and Heloir, Alexis and Nunnari, Fabrizio and Kulesza, Todd and Mackay, Wendy and Caramiaux, Baptiste},
doi = {10.1145/2851581.2856492},
file = {::},
isbn = {9781450340823},
keywords = {Authors' choice,by semicolons,include commas,of terms,required.,separated,within terms only},
title = {{Human-Centered Machine Learning Introduction and Background}},
url = {http://dx.doi.org/10.1145/2851581.2856492}
}
@article{Saerens2002,
abstract = {It sometimes happens (for instance in case control studies) that a classifier is trained on a data set that does not reflect the true a priori probabilities of the target classes on real-world data. This may have a negative effect on the classification accuracy obtained on the real-world data set, especially when the classifier's decisions are based on the a posteriori probabilities of class membership. Indeed, in this case, the trained classifier provides estimates of the a posteriori probabilities that are not valid for this real-world data set (they rely on the a priori probabilities of the training set). Applying the classifier as is (without correcting its outputs with respect to these new conditions) on this new data set may thus be suboptimal. In this note, we present a simple iterative procedure for adjusting the outputs of the trained classifier with respect to these new a priori probabilities without having to refit the model, even when these probabilities are not known in advance. As a by-product, estimates of the new a priori probabilities are also obtained. This iterative algorithm is a straightforward instance of the expectation-maximization (EM) algorithm and is shown to maximize the likelihood of the new data. Thereafter, we discuss a statistical test that can be applied to decide if the a priori class probabilities have changed from the training set to the real-world data. The procedure is illustrated on different classification problems involving a multilayer neural network, and comparisons with a standard procedure for a priori probability estimation are provided. Our original method, based on the EM algorithm, is shown to be superior to the standard one for a priori probability estimation. Experimental results also indicate that the classifier with adjusted outputs always performs better than the original one in terms of classification accuracy, when the a priori probability conditions differ from the training set to the real-world data. The gain in classification accuracy can be significant.},
author = {Saerens, Marco and Latinne, Patrice and Decaestecker, Christine},
doi = {10.1162/089976602753284446},
file = {::},
isbn = {0899766027532},
issn = {08997667},
journal = {Neural Computation},
number = {1},
pages = {21--41},
pmid = {11747533},
title = {{Adjusting the outputs of a classifier to new a Priori probabilities: A simple procedure}},
volume = {14},
year = {2002}
}
@book{sivia2006,
author = {{Sivia, D. and Skilling}, J.},
publisher = {Oxford University Press},
title = {{Data Analysis: A Bayesian Tutorial}},
year = {2006}
}
@article{Brown1964,
author = {Brown, George W. and Lu, John Y. and Wolfson, Robert J.},
doi = {10.1287/mnsc.11.1.51},
file = {::},
issn = {0025-1909},
journal = {Management Science},
month = {sep},
number = {1},
pages = {51--63},
publisher = {INFORMS},
title = {{Dynamic Modeling of Inventories Subject to Obsolescence}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.11.1.51},
volume = {11},
year = {1964}
}
@article{Blei2014,
abstract = {We survey latent variable models for solving data-analysis problems. A latent variable model is a probabilistic model that encodes hidden patterns in the data. We uncover these patterns from their conditional distribution and use them to summarize data and form predictions. Latent variable models are important in many fields, including computational biology, natural language processing, and social network analysis. Our perspective is that models are developed iteratively: We build a model, use it to analyze data, assess how it succeeds and fails, revise it, and repeat. We describe how new research has transformed these essential activities. First, we describe probabilistic graphical models, a language for formulating latent variable models. Second, we describe mean field variational inference, a generic algorithm for approximating conditional distributions. Third, we describe how to use our analyses to solve problems: exploring the data, forming predictions, and pointing us in the direction of improved models.},
author = {Blei, David M.},
doi = {10.1146/annurev-statistics-022513-115657},
file = {::},
journal = {Ssrn},
number = {2014},
title = {{Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models}},
year = {2014}
}
@article{Wang2016,
abstract = {—Deep learning has become increasingly popular in both academic and industrial areas in the past years. Various domains including pattern recognition, computer vision, and natural language processing have witnessed the great power of deep networks. However, current studies on deep learning mainly focus on data sets with balanced class labels, while its performance on imbalanced data is not well examined. Imbalanced data sets exist widely in real world and they have been providing great challenges for classification tasks. In this paper, we focus on the problem of classification using deep network on imbalanced data sets. Specifically, a novel loss function called mean false error together with its improved version mean squared false error are proposed for the training of deep networks on imbalanced data sets. The proposed method can effectively capture classification errors from both majority class and minority class equally. Experiments and comparisons demonstrate the superiority of the proposed approach compared with conventional methods in classifying imbalanced data sets on deep neural networks.},
author = {Wang, Shoujin and Liu, Wei and Wu, Jia and Cao, Longbing and Meng, Qinxue and Kennedy, Paul J.},
doi = {10.1109/IJCNN.2016.7727770},
file = {::},
isbn = {9781509006199},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {Data imbalance,Deep neural network,Loss function},
pages = {4368--4374},
title = {{Training deep neural networks on imbalanced data sets}},
volume = {2016-Octob},
year = {2016}
}
@article{Vischer2008,
abstract = {Inquiry into how people experience environmental conditions at work is a growing area of study. Until the 1980s, there was insufficient research on ‘workspaces'—and on office environments in particular—to warrant review. Since that time, the range and number of studies of workspace have burgeoned. This paper will identify and review the main themes and findings of this area of research with the objective of defining basic parameters and prevailing theories of the environmental psychology of workspace. These will generate questions and directions for future research. {\textcopyright} 2008 Taylor {\&} Francis Group, LLC.},
author = {Vischer, Jacqueline C.},
doi = {10.3763/asre.2008.5114},
issn = {17589622},
journal = {Architectural Science Review},
keywords = {Ambient environmental conditions,Ergonomics,Functional comfort,Furniture,Health and safety,Office buildings,Office layouts,Productivity,Satisfaction,Territoriality,User participation,Workspace},
number = {2},
pages = {97--108},
publisher = { Taylor {\&} Francis Group },
title = {{Towards an environmental psychology of workspace: How people are affected by environments for work}},
volume = {51},
year = {2008}
}
@article{Sivathamboo2018,
author = {Sivathamboo, Shobi and Perucca, Piero and Velakoulis, Dennis and Jones, Nigel C and Goldin, Jeremy and Kwan, Patrick and O'Brien, Terence J and O'Brien, Terence J},
doi = {10.1093/sleep/zsy015},
file = {::},
issn = {0161-8105},
journal = {Sleep},
keywords = {epilepsy,mortality,seizures,sleep,sleep apnea syndromes,sudden unexplained death in epilepsy},
month = {apr},
number = {4},
publisher = {Narnia},
title = {{Sleep-disordered breathing in epilepsy: epidemiology, mechanisms, and treatment}},
url = {https://academic.oup.com/sleep/article/doi/10.1093/sleep/zsy015/4830560},
volume = {41},
year = {2018}
}
@techreport{Breck,
abstract = {Using machine learning in real-world production systems is complicated by a host of issues not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for assessing the production-readiness of an ML system. But how much testing and monitoring is enough? We present an ML Test Score rubric based on a set of actionable tests to help quantify these issues.},
author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D},
file = {::},
title = {{What's your ML Test Score? A rubric for ML production systems}}
}
@inproceedings{Barnett2015d,
abstract = {This paper demonstrates a multi-view framework for Rapid APPlication Tool (RAPPT). RAPPT enables rapid development of mobile applications. It employs a multilevel approach to mobile application development: a Domain Specific Visual Language to define the high level structure of mobile apps, a Domain Specific Textual Language to define behavioural concepts, and concrete source code for fine grained improvements.},
author = {Barnett, Scott and Avazpour, Iman and Vasa, Rajesh and Grundy, John},
booktitle = {2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
doi = {10.1109/VLHCC.2015.7357239},
isbn = {978-1-4673-7457-6},
month = {oct},
pages = {305--306},
publisher = {IEEE},
title = {{A multi-view framework for generating mobile apps}},
url = {http://ieeexplore.ieee.org/document/7357239/},
year = {2015}
}
@article{Ghazi2019,
abstract = {Background: The need for empirical investigations in software engineering is growing. Many researchers nowadays, conduct and validate their solutions using empirical research. The Survey is an empirical method which enables researchers to collect data from a large population. The main aim of the survey is to generalize the findings. Aims: In this study, we aim to identify the problems researchers face during survey design and mitigation strategies. Method: A literature review, as well as semi-structured interviews with nine software engineering researchers, were conducted to elicit their views on problems and mitigation strategies. The researchers are all focused on empirical software engineering. Results: We identified 24 problems and 65 strategies, structured according to the survey research process. The most commonly discussed problem was sampling, in particular, the ability to obtain a sufficiently large sample. To improve survey instrument design, evaluation and execution recommendations for question formulation and survey pre-testing were given. The importance of involving multiple researchers in the analysis of survey results was stressed. Conclusions: The elicited problems and strategies may serve researchers during the design of their studies. However, it was observed that some strategies were conflicting. This shows that it is important to conduct a trade-off analysis between strategies.},
author = {Ghazi, Ahmad Nauman and Petersen, Kai and Reddy, Sri Sai Vijay Raj and Nekkanti, Harini},
doi = {10.1109/ACCESS.2018.2881041},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Empirical software engineering,surveys},
pages = {24703--24718},
publisher = {IEEE},
title = {{Survey research in software engineering: Problems and mitigation strategies}},
volume = {7},
year = {2019}
}
@article{Eltayeb2009,
abstract = {Introduction This study aims to investigate the relationship between work-related physical and psychosocial characteristics and complaints of the neck, shoulder and forearm/hands. Methods Data were used from a prospective Dutch cohort study among computer office workers with a follow-up period of 2 years. The study was conducted among 264 computer users. Physical and psychosocial risk factors were tested to predict the occurrence of neck, shoulder and forearm/hands complaints. Bivariate and multivariable logistic regression was used to identify the association between risk factors and outcome variables. Results The 2 year follow-up prevalence rates with 95{\%} CI for neck complaints were 0.31 (0.28-0.37), for shoulder complaints 0.33 (0.27-0.39) and for forearm/hands complaints 0.21 (0.14-0.28). Four main predictors for the occurrence of neck and shoulder complaints were identified: (1) Irregular head and body posture [OR: 1.1 (1.0-1.2) P = 0.04]; (2) task difficulty (job demands) [OR: 1.2 (1.0-1.5) P = 0.01]; (3) number of working hours/day with the computer [OR: 1.20 (1.0-1.4) P = 0.03]; and (4) having had a previous history of complaints [OR: 7.2 (3.8-13.2) P = 0.01]. Two predictors were identified for forearm/hands complaints: time pressure (job demands) [OR: 1.20 (1.0-1.4) P = 0.03] and having had a previous history of complaints [OR: 7.1 (3.5-14.1) P = 0.06]. Conclusion This longitudinal study suggests that risk factors of upper musculoskeletal complaints in computer workers consist of a mixture of physical and psychosocial characteristics.},
author = {Eltayeb, Shahla and Staal, J. Bart and Hassan, Amar and {De Bie}, Rob A.},
doi = {10.1007/s10926-009-9196-x},
file = {::},
issn = {10530487},
journal = {Journal of Occupational Rehabilitation},
keywords = {Complaints,Neck shoulder and forearm/hands,Risk factors},
month = {dec},
number = {4},
pages = {315--322},
publisher = {Springer US},
title = {{Work related risk factors for neck, shoulder and arms complaints: A cohort study among Dutch computer office workers}},
volume = {19},
year = {2009}
}
@article{Brown2017,
abstract = {We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20{\%} (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.},
author = {Brown, Nicholas J.L. L and Heathers, James A.J. J},
doi = {10.1177/1948550616673876},
file = {::},
issn = {19485514},
journal = {Social Psychological and Personality Science},
keywords = {advanced quantitative methods,philosophy of science,research methods},
number = {4},
pages = {363--369},
title = {{The GRIM Test: A Simple Technique Detects Numerous Anomalies in the Reporting of Results in Psychology}},
volume = {8},
year = {2017}
}
@article{Bourbonnais1996,
abstract = {[Objectives In line with Karasek's job strain model, the objective of the study was to determine whether workers submitted to high job strain, a combination of high psychological demand and low decision latitude, develop more psychological distress than workers not submitted to high job strain. A second objective was to determine whether social support at work modifies the association between job strain and psychological distress. Methods The design was cross-sectional and included white-collar workers in the Qu{\'{e}}bec city area. A selfadministered 26-item questionnaire (the Job Content Questionnaire) measured psychological demand, decision latitude, and social support at work. Psychological distress was measured by the Psychiatric Symptom Index, a 14-item self-administered instrument. Results Among the 2889 participants, the prevalence of psychological distress was 27.8{\%}. High job strain was present in 20.5{\%} of the subjects. The crude odds ratio (OR) of high job strain with psychological distress was 3.52 [95{\%} confidence interval (95{\%} CI) 2.54— 4.88]. The OR adjusted for age, gender, employment status, occupation, social support at work, nonwork social support, cynicism, hostility, domestic load, and stressful life events during the last 12 months was still significant (OR 2.45, 95{\%} CI 1.66— 3.62). Conclusions Our results support the association between job strain and psychological distress. Social support at work, although significantly associated with psychological distress, did not modify the association between job strain and psychological distress.]},
author = {Bourbonnais, Ren{\'{e}}e and Brisson, Chantal and Moisan, Jocelyne and V{\'{e}}zina, Michel},
issn = {03553140, 1795990X},
journal = {Scandinavian Journal of Work, Environment {\&} Health},
month = {feb},
number = {2},
pages = {139--145},
publisher = {Scandinavian Journal of Work, Environment {\&} Health},
title = {{Job strain and psychological distress in white-collar workers}},
url = {http://www.jstor.org/stable/40966522},
volume = {22},
year = {1996}
}
@article{McLean2019,
abstract = {Artificial Intelligent (AI) In-home Voice Assistants have seen unprecedented growth. However, we have little understanding on the factors motivating individuals to use such devices. Given the unique characteristics of the technology, in the main hands free, controlled by voice, and the presentation of a voice user interface, the current technology adoption models are not comprehensive enough to explain the adoption of this new technology. Focusing on voice interactions, this research combines the theoretical foundations of U{\&}GT with technology theories to gain a clearer understanding on the motivations for adopting and using in-home voice assistants. This research presents a conceptual model on the use of voice controlled technology and an empirical validation of the model through the use of Structural Equation Modelling with a sample of 724 in-home voice assistant users. The findings illustrate that individuals are motivated by the (1) utilitarian benefits, (2) symbolic benefits and (3) social benefits provided by voice assistants, the results found that hedonic benefits only motivate the use of in-home voice assistants in smaller households. Additionally, the research establishes a moderating role of perceived privacy risks in dampening and negatively influencing the use of in-home voice assistants.},
author = {McLean, Graeme and Osei-Frimpong, Kofi},
doi = {10.1016/j.chb.2019.05.009},
file = {::},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Artificial intelligence,Machine learning,Social presence,Technology adoption,Uses and gratification theory,Voice assistants},
number = {January},
pages = {28--37},
title = {{Hey Alexa {\ldots} examine the variables influencing the use of artificial intelligent in-home voice assistants}},
volume = {99},
year = {2019}
}
@article{Aichernig2018,
abstract = {We present a survey of the recent research efforts in integrating model learning with model-based testing. We distinguished two strands of work in this domain, namely test-based learning (also called test-based modeling) and learning-based testing. We classify the results in terms of their underlying models, their test purpose and techniques, and their target domains.},
author = {Aichernig, Bernhard K. and Mostowski, Wojciech and Mousavi, Mohammad Reza and Tappler, Martin and Taromirad, Masoumeh},
doi = {10.1007/978-3-319-96562-8_3},
file = {::},
isbn = {9783319965611},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {July},
pages = {74--100},
title = {{Model learning and model-based testing}},
url = {http://link.springer.com/10.1007/978-3-319-96562-8 http://link.springer.com/10.1007/978-3-319-96562-8{\_}3},
volume = {11026 LNCS},
year = {2018}
}
@article{Chang2016,
abstract = {More and more data nowadays exist in hierarchical formats such as JSON due to the increasing popularity of web applications and web services. While many end-user systems support getting hierarchical data from databases without programming, they provide very little support for using hierarchical data beyond turning the data into a flat string or table. In this paper, we present a spreadsheet tool for using and exploring hierarchical datasets. We introduce novel interaction techniques and algorithms to manipulate and visualize hierarchical data in a spreadsheet using the data's relative hierarchical relationships with the data in its adjacent columns. Our tool leverages the data's structural information to support selecting, grouping, joining, sorting and filtering hierarchical data in spreadsheets. Our lab study showed that our tool helped spreadsheet users complete data exploration tasks nearly two times faster than using Excel and even outperform programmers in most tasks.},
author = {Chang, Kerry Shih Ping and Myers, Brad A.},
doi = {10.1145/2858036.2858430},
file = {::},
isbn = {9781450333627},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {End-user programming,Hierarchical data,Spreadsheets},
pages = {2497--2507},
title = {{Using and exploring hierarchical data in spreadsheets}},
year = {2016}
}
@article{Palmer2009,
abstract = {In this paper we describe Ginger, a new language with first class support for literate programming. Literate programming is a philosophy that argues computer programs should be written as literature with human readability and understanding of paramount importance. While the intent of literate programming is to make understanding computer programs simpler, most literate programming systems are quite complex and consist of three different languages corresponding to 1) an implementation language, 2) a documentation language, and 3) a literate programming glue language. In Knuth's original implementation these were Pascal, TeX, and WEB respectively. Antithetical to the goals that literate programming espouses, this three language paradigm creates a truly challenging environment for new programmers. In this paper we reimagine literate programming as a core programming language feature and describe a novel system for literate programming based on G-expression transformations. We show that Ginger code can be used to naturally represent code, prose, and literate connections, which in turn unifies, simplifies and significantly extends the literate programming experience. Copyright {\textcopyright}2009 ACM.},
author = {Palmer, James Dean and Hillenbrand, Eddie},
doi = {10.1145/1639950.1640072},
file = {::},
isbn = {9781605587660},
journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {Ginger,Literate programming,Program comprehension},
pages = {1007--1014},
title = {{Reimagining literate programming}},
year = {2009}
}
@misc{Braig2018,
abstract = {Purpose We aimed to describe time trends in functional dyspepsia and the association of dyspepsia-related factors, Helicobacter pylori (H. pylori) and work-related stress with functional dyspepsia in white collar employees in 1996 and 2015. Materials and methods Repeat cross-sectional study conducted in 1996 (n = 190, response rate = 76.1) and 2015 (n = 195, response rate = 40.2) within a health insurance company in South-West Germany. Dyspeptic symptoms measured according to the Rome III criteria, effort-reward imbalance and further work- or dyspepsia-related factors were assessed by self-administered questionnaire. H. pylori infection as possible factor for dyspeptic symptoms was measured by a 13C-urea breath test or an antigen stool test. Kruskal-Wallis tests and multivariable logistic regression models were calculated comparing the upper tertile of dyspeptic symptom scale to the middle and lower tertile. Results Mean dyspepsia symptom scores and work-related stress did not differ comparing 1996 and 2015. In bivariate analyses, dyspeptic symptom scores were consistently correlated with sex, age, and using antacids. Further dyspepsia-related factors were smoking and non-leading occupational position in 1996 and non-steroidal anti-inflammatory drugs as well as high effort-reward imbalance in 2015. High intrinsic effort was positively associated with high dyspepsia symptom scores in both studies. Following multivariable adjustment, we observed a consistent association between high intrinsic effort at work and dyspeptic symptoms, although the association was only marginally statistically significant in 1996. Furthermore, a strong association of somatization, only measured in 2015, with dyspeptic symptoms was shown. Conclusions Dyspepsia-related factors may have changed throughout the last decades. Nevertheless, although occupational situations might differ, the intrinsic effort is still strongly associated with dyspeptic symptoms.},
author = {Braig, Stefanie and Berger, Simon and Rothenbacher, David and Schmid, Stefanie and Seufferlein, Thomas and Brenner, Hermann and Rothenbacher, Dietrich and G{\"{u}}ndel, Harald},
booktitle = {PLoS ONE},
doi = {10.1371/journal.pone.0199533},
file = {::},
issn = {19326203},
month = {jun},
number = {6},
publisher = {Public Library of Science},
title = {{Time trends in dyspepsia and association with H. pylori and work-related stress—An observational study in white collar employees in 1996 and 2015}},
volume = {13},
year = {2018}
}
@incollection{Dragicevic2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Dragicevic, Pierre},
booktitle = {Modern Statistical Methods for HCI},
doi = {10.1007/978-3-319-26633-6_13},
file = {::},
isbn = {9783319266336},
pages = {291--330},
publisher = {Springer},
title = {{Fair Statistical Communication in HCI}},
year = {2016}
}
@article{Herta2017,
abstract = {Objective To assess whether ICU caregivers can correctly read and interpret continuous EEG (cEEG) data displayed with the computer algorithm NeuroTrend (NT) with the main attention on seizure detection and determination of sedation depth. Methods 120 screenshots of NT (480 h of cEEG) were rated by 18 briefly trained nurses and biomedical analysts. Multirater agreements (MRA) as well as interrater agreements (IRA) compared to an expert opinion (EXO) were calculated for items such as pattern type, pattern location, interruption of recording, seizure suspicion, consistency of frequency, seizure tendency and level of sedation. Results MRA as well as IRA were almost perfect (80–100{\%}) for interruption of recording, spike-and-waves, rhythmic delta activity and burst suppression. A substantial agreement (60–80{\%}) was found for electrographic seizure patterns, periodic discharges and seizure suspicion. Except for pattern localization (70.83–92.26{\%}), items requiring a precondition and especially those who needed interpretation like consistency of frequency (47.47–79.15{\%}) or level of sedation (41.10{\%}) showed lower agreements. Conclusions The present study demonstrates that NT might be a useful bedside monitor in cases of subclinical seizures. Determination of correct sedation depth by ICU caregivers requires a more detailed training. Significance Computer algorithms may reduce the workload of cEEG analysis in ICU patients.},
author = {Herta, J. and Z{\"{o}}chmeister, A. and Hosmann, A. and Gruber, A. and Koren, J. and Baumgartner, C. and F{\"{u}}rbass, F. and Hartmann, M.},
doi = {10.1016/j.clinph.2017.04.002},
file = {::},
issn = {18728952},
journal = {Clinical Neurophysiology},
keywords = {Continuous EEG,Epileptic seizure detection,Intensive care unit,Interrater agreement,Monitoring,Nursing,Periodic discharge,Rhythmic and periodic patterns,Screening device},
month = {jun},
number = {6},
pages = {1000--1007},
publisher = {Elsevier Ireland Ltd},
title = {{Applicability of NeuroTrend as a bedside monitor in the neuro ICU}},
volume = {128},
year = {2017}
}
@article{Jedlitschka2005,
author = {Jedlitschka, Andreas and Tn, Alberta},
file = {::},
isbn = {0780395085},
journal = {Evaluation},
pages = {95--104},
title = {{Reporting Guidelines for Controlled Experiments in Software Engineering Dietmar Pfahl}},
year = {2005}
}
@article{Fan2019,
abstract = {Objective: Synchronization phenomena of epileptic electroencephalography (EEG) have long been studied. In this study, we aim at investigating the spatial-temporal synchronization pattern in epileptic human brains using the spectral graph theoretic features extracted from scalp EEG and developing an efficient multivariate approach for detecting seizure onsets in real time. Methods: A complex network model is used for representing the recurrence pattern of EEG signals, based on which the temporal synchronization patterns are quantified using the spectral graph theoretic features. Furthermore, a statistical control chart is applied to the extracted features overtime for monitoring the transits from normal to epileptic states in multivariate EEG systems. Results: Our method is tested on 23 patients from CHB-MIT Scalp EEG database. The results show that the graph theoretic feature yields a high sensitivity (∼98{\%}) and low latency (∼6 s) on average, and seizure onsets in 18 patients are 100{\%} detected. Conclusion: Our approach validates the increased temporal synchronization in epileptic EEG and achieves a comparable detection performance to previous studies. Significance: We characterize the temporal synchronization patterns of epileptic EEG using spectral network metrics. In addition, we found significant changes in temporal synchronization in epileptic EEG, which enable a patient-specific approach for real-time seizure detection for personalized diagnosis and treatment.},
author = {Fan, Miaolin and Chou, Chun An},
doi = {10.1109/TBME.2018.2850959},
file = {::},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Complex network,data fusion,epileptic seizure,nonlinear dynamics,scalp epileptic electroencephalography (EEG),spectral graph theory},
month = {mar},
number = {3},
pages = {601--608},
publisher = {IEEE Computer Society},
title = {{Detecting Abnormal Pattern of Epileptic Seizures via Temporal Synchronization of EEG Signals}},
volume = {66},
year = {2019}
}
@misc{VanCalster2019b,
abstract = {There is increasing awareness that the methodology and findings of research should be transparent. This includes studies using artificial intelligence to develop predictive algorithms that make individualized diagnostic or prognostic risk predictions. We argue that it is paramount to make the algorithm behind any prediction publicly available. This allows independent external validation, assessment of performance heterogeneity across settings and over time, and algorithm refinement or updating. Online calculators and apps may aid uptake if accompanied with sufficient information. For algorithms based on "black box" machine learning methods, software for algorithm implementation is a must. Hiding algorithms for commercial exploitation is unethical, because there is no possibility to assess whether algorithms work as advertised or to monitor when and how algorithms are updated. Journals and funders should demand maximal transparency for publications on predictive algorithms, and clinical guidelines should only recommend publicly available algorithms.},
author = {{Van Calster}, Ben and Wynants, Laure and Timmerman, Dirk and Steyerberg, Ewout W. and Collins, Gary S.},
booktitle = {Journal of the American Medical Informatics Association},
doi = {10.1093/jamia/ocz130},
file = {::},
issn = {1527974X},
keywords = {artificial intelligence,external validation,machine learning,model performance,predictive analytics},
month = {nov},
number = {12},
pages = {1651--1654},
pmid = {31373357},
publisher = {Oxford University Press},
title = {{Predictive analytics in health care: how can we know it works?}},
volume = {26},
year = {2019}
}
@article{Price2019,
abstract = {This research sought to establish which in-situ measures of cognitive fatigue, physical activity, social interaction, location, emotional state and facial landmarks, made using a smartphone application, could be used to indicate episodes of cognitive fatigue. This assessment was realised using cognitive tests (assessing memory, attention, reaction time, information processing speed and executive function), self-assessment, contextual factors and facial feature analysis. This study also investigated the use of an ensemble algorithm for the classification of cognitive fatigue utilising facial features and a Rotation Forest approach. Self-assessment of cognitive fatigue was shown to directly correlate with reaction time through a Psychomotor Vigilance Task (r =.643, p =.001), and self-reported increases in the level of social activity (r =.377, p =.001). Facial feature analysis revealed dominant emotions of sadness and anger when participants were cognitively fatigued. It also revealed underlying facial cues that indicated higher levels of cognitive fatigue including expressions of negative valence, and Facial Action Coding System units of increased brow furrow, eyelid tightening and lip suck. In addition, a Principle Component Analysis based Rotation Forest ensemble with a ternary output demonstrated a cognitive fatigue classification accuracy of 82.17{\%}. The findings presented indicate that the inclusion of data relating to surrounding cognitive, social, physical and emotional factors can improve the accuracy of mobile in-situ cognitive fatigue assessment using our previously validated smartphone-based cognitive fatigue assessment approach. The findings further suggest gross-level fatigue status may be potentially classified to a reasonable degree of accuracy using facial features, which may give rise to personalised in-situ fatigue detection.},
author = {Price, Edward and Moore, George and Galway, Leo and Linden, Mark},
doi = {10.1109/access.2019.2935540},
journal = {IEEE Access},
month = {aug},
pages = {116465--116479},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Towards Mobile Cognitive Fatigue Assessment as Indicated by Physical, Social, Environmental, and Emotional Factors}},
volume = {7},
year = {2019}
}
@article{Veldkamp2014,
abstract = {Statistical analysis is error prone. A best practice for researchers using statistics would therefore be to share data among co-authors, allowing double-checking of executed tasks just as co-pilots do in aviation. To document the extent to which this 'co-piloting' currently occurs in psychology, we surveyed the authors of 697 articles published in six top psychology journals and asked them whether they had collaborated on four aspects of analyzing data and reporting results, and whether the described data had been shared between the authors. We acquired responses for 49.6{\%} of the articles and found that co-piloting on statistical analysis and reporting results is quite uncommon among psychologists, while data sharing among co-authors seems reasonably but not completely standard. We then used an automated procedure to study the prevalence of statistical reporting errors in the articles in our sample and examined the relationship between reporting errors and co-piloting. Overall, 63{\%} of the articles contained at least one p-value that was inconsistent with the reported test statistic and the accompanying degrees of freedom, and 20{\%} of the articles contained at least one p-value that was inconsistent to such a degree that it may have affected decisions about statistical significance. Overall, the probability that a given p-value was inconsistent was over 10{\%}. Co-piloting was not found to be associated with reporting errors.},
author = {Veldkamp, Coosje L.S. S and Nuijten, Michele B. and Dominguez-Alvarez, Linda and {Van Assen}, Marcel A.L.M. L M and Wicherts, Jelte M.},
doi = {10.1371/journal.pone.0114876},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
number = {12},
pages = {1--19},
pmid = {25493918},
title = {{Statistical reporting errors and collaboration on statistical analyses in psychological science}},
volume = {9},
year = {2014}
}
@techreport{Craig,
author = {Craig, Peter},
file = {::},
title = {{Developing and evaluating complex interventions}},
url = {www.mrc.ac.uk/complexinterventionsguidance}
}
@article{Jay2015,
abstract = {Background: Chronic musculoskeletal pain is prevalent among laboratory technicians and work-related stress may aggravate the problem. Objectives: This study investigated the effect of a multifaceted worksite intervention on pain and stress among laboratory technicians with chronic musculoskeletal pain using individually tailored physical and cognitive elements. Study Design: This trial uses a single-blind randomized controlled design with allocation concealment in a 2-armed parallel group format among laboratory technicians. The trial “Implementation of physical exercise at the Workplace (IRMA09) – Laboratory technicians“ was registered at ClinicalTrials.gov prior to participant enrolment. Setting: The study was conducted at the head division of a large private pharmaceutical company's research and development department in Denmark. The study duration was March 2014 (baseline) to July 2014 (follow-up). Methods: Participants (n = 112) were allocated to receive either physical, cognitive, and mindfulness group-based training (PCMT group) or a reference group (REF) for 10 weeks at the worksite. PCMT consisted of 4 major elements: 1) resistance training individually tailored to the pain affected area, 2) motor control training, 3) mindfulness, and 4) cognitive and behavioral therapy/education. Participants of the REF group were encouraged to follow ongoing company health initiatives. The predefined primary outcome measure was pain intensity (VAS scale 0 – 10) in average of the regions: neck, shoulder, lower and upper back, elbow, and hand at 10 week follow-up. The secondary outcome measure was stress assessed by Cohen´s perceived stress questionnaire. In addition, an explorative dose-response analysis was performed on the adherence to PCMT with pain and stress, respectively, as outcome measures. Results: A significant (P {\textless} 0.0001) treatment by time interaction in pain intensity was observed with a between-group difference at follow-up of -1.0 (95{\%}CI: -1.4 to -0.6). No significant effect on stress was observed (treatment by time P = 0.16). Exploratory analyses for each body region separately showed significant pain reductions of the neck, shoulders, upper back and lower back, as well as a tendency for hand pain. Within the PCMT group, general linear models adjusted for age, baseline pain, and stress levels showed significant associations for the change in pain with the number of physical-cognitive training sessions per week (-0.60 [95{\%}CI -0.95 to -0.25]) and the number of mindfulness sessions (0.15 [95{\%}CI 0.02 to 0.18]). No such associations were found with the change in stress as outcome. Limitations: Limitations of behavioral interventions include the inability to blind participants to which intervention they receive. Self-reported outcomes are a limitation as they may be influenced by placebo effects and outcome expectations. Conclusions: We observed significant reductions in chronic musculoskeletal pain following a 10-week individually adjusted multifaceted intervention with physical training emphasizing dynamic joint mobility and mindfulness coupled with fear-avoidance and de-catastrophizing behavioral therapy compared to a reference group encouraged to follow on-going company health initiatives. A higher dose of physical-cognitive training appears to facilitate pain reduction, whereas a higher dose of mindfulness appears to increase pain. Hence, combining physical training with mindfulness may not be an optimal strategy for pain reduction. Trial registration: NCT02047669.},
author = {Jay, Kenneth and Brandt, Mikkel and Hansen, Klaus and Sundstrup, Emil and Jakobsen, Markus Due and Schraefel, M. C. and Sjogaard, Gisela and Andersen, Lars L.},
file = {::},
issn = {21501149},
journal = {Pain Physician},
keywords = {Dynamic joint mobility,Fear avoidance,Mindfulness,Motor control exercise,Pain Catastrophizing,Strength,Work environment},
month = {sep},
number = {5},
pages = {459--471},
pmid = {26431123},
publisher = {American Society of Interventional Pain Physicians},
title = {{Effect of individually tailored biopsychosocial workplace interventions on chronic musculoskeletal pain and stress among laboratory technicians: Randomized controlled trial}},
volume = {18},
year = {2015}
}
@inproceedings{Polyzotis2017,
address = {New York, New York, USA},
annote = {From Duplicate 1 (Data Management Challenges in Production Machine Learning - Polyzotis, Neoklis; Roy, Sudip; Whang, Steven Euijong; Zinkevich, Martin)

Training data - Understanding, validating, cleaning, enriching

Core expertise of database community - analyzing, modeling, enriching, validating and debugging data

Training data -{\textgreater} Training -{\textgreater} Model -{\textgreater} Serving -{\textgreater} Serving Data (Serving to Training Data Transformations)

1. Understanding - Encode their data into features, identify explicit and implicit data dependencies

Salient features of data - The range and statistical distribution of feature values, correlations between features in training data, distributions of positive and negative examples across different slices

2. Validation - Validity affects quality of model, training{\_}serving skew, time travel of features
Two factors - Scale of data and validity checks need to be done with data

3. Cleaning - three tasks
3.1. Understanding where the error occurred
3.2. Understanding the impact of the error
3.3. Fixing the error

4. Enrichment - The augementation of training and serving data 

The common forms of enrichment are joining a new data structure and using same signals with different transformations

From Duplicate 2 (Data Management Challenges in Production Machine Learning - Polyzotis, Neoklis; Roy, Sudip; Whang, Steven Euijong; Zinkevich, Martin)

Training data - Understanding, validating, cleaning, enriching

Core expertise of database community - analyzing, modeling, enriching, validating and debugging data

Training data -{\textgreater}Training -{\textgreater}Model -{\textgreater}Serving -{\textgreater}Serving Data (Serving to Training Data Transformations)

1. Understanding - Encode their data into features, identify explicit and implicit data dependencies

Salient features of data - The range and statistical distribution of feature values, correlations between features in training data, distributions of positive and negative examples across different slices

2. Validation - Validity affects quality of model, training{\_}serving skew, time travel of features
Two factors - Scale of data and validity checks need to be done with data

3. Cleaning - three tasks
3.1. Understanding where the error occurred
3.2. Understanding the impact of the error
3.3. Fixing the error

4. Enrichment - The augementation of training and serving data 

The common forms of enrichment are joining a new data structure and using same signals with different transformations},
author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data  - SIGMOD '17},
doi = {10.1145/3035918.3054782},
file = {::},
isbn = {9781450341974},
keywords = {data enrichment,data management,data understanding,data validation,machine learning,production},
pages = {1723--1726},
publisher = {ACM Press},
title = {{Data Management Challenges in Production Machine Learning}},
url = {http://dl.acm.org/citation.cfm?doid=3035918.3054782},
year = {2017}
}
@inproceedings{Poncin2011,
author = {Poncin, Wouter and Serebrenik, Alexander and van den Brand, Mark and van den Brand, Mark},
booktitle = {2011 15th European Conference on Software Maintenance and Reengineering},
doi = {10.1109/CSMR.2011.5},
file = {::},
isbn = {978-1-61284-259-2},
month = {mar},
pages = {5--14},
publisher = {IEEE},
title = {{Process Mining Software Repositories}},
url = {http://ieeexplore.ieee.org/document/5741254/},
year = {2011}
}
@article{Coleman1994,
author = {Coleman, D. and Ash, D. and Lowther, B. and Oman, P.},
doi = {10.1109/2.303623},
issn = {0018-9162},
journal = {Computer},
month = {aug},
number = {8},
pages = {44--49},
title = {{Using metrics to evaluate software system maintainability}},
url = {http://ieeexplore.ieee.org/document/303623/},
volume = {27},
year = {1994}
}
@article{Albert2012,
abstract = {We consider the problem of combining opinions from different ex- perts in an explicitly model-based way to construct a valid subjective prior in a Bayesian statistical approach. We propose a generic approach by considering a hierarchical model accounting for various sources of variation as well as account- ing for potential dependence between experts. We apply this approach to two problems. The first problem deals with a food risk assessment problem involving modelling dose-response for Listeria monocytogenes contamination of mice. Two hierarchical levels of variation are considered (between and within experts) with a complex mathematical situation due to the use of an indirect probit regression. The second concerns the time taken by PhD students to submit their thesis in a particular school. It illustrates a complex situation where three hierarchical levels of variation are modelled but with a simpler underlying probability distribution (log-Normal).},
author = {Albert, Isabelle and Donnet, Sophie and Guihenneuc-Jouyaux, Chantal and Low-Choy, Samantha and Mengersen, Kerrie and Rousseau, Judith},
doi = {10.1214/12-BA717},
file = {::},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Bayesian statistics,Hierarchical model,Random effects,Risk assessment},
number = {3},
pages = {503--532},
title = {{Combining expert opinions in prior elicitation}},
volume = {7},
year = {2012}
}
@incollection{Voelter2013,
abstract = {This book covers DSL Design, Implementation and Use of DSL in detail. It consists of four parts. Part 1 introduces DSLs in general and discusses their advantages and drawbacks. It also defines important terms and concepts and introduces the case studies used in the most of the re-mainder of the book. Part 2 discusses the design of DSLs – independent of implementation techniques. It discusses seven design dimensions, explains a number of reusable language paradigms and points out a number of process-related issues. Part 3 provides details about the implementation of DSLs with lots of code. It uses three state-of-the-art but quite different language workbenches: Jet-Brains MPS, Eclipse Xtext and TU Delft's Spoofax. Part 4 discusses the use of DSLs for requirements, architecture, implementation and product line engineering, as well as their roles as a developer utility and for implementing business logic.},
author = {Voelter, Markus},
booktitle = {DSL Engineering: Designing, Implementing and Using Domain-Specific Languages},
file = {::},
isbn = {978-1481218580},
keywords = {DSL},
mendeley-tags = {DSL},
title = {{Introduction to DSLs}},
url = {http://voelter.de/dslbook/markusvoelter-dslengineering-1.0.pdf},
year = {2013}
}
@misc{Meng2017,
abstract = {The success of an application programming interface (API) crucially depends on how well its documentation meets the information needs of software developers. Previous research suggests that these information needs have not been sufficiently understood. This article presents the results of a series of semistructured interviews and a follow-up questionnaire conducted to explore the learning goals and learning strategies of software developers, the information resources they turn to and the quality criteria they apply to API documentation. Our results show that developers initially try to form a global understanding regarding the overall purpose and main features of an API, but then adopt either a concepts-oriented or a code-oriented learning strategy that API documentation both needs to address. Our results also show that general quality criteria such as completeness and clarity are relevant to API documentation as well. Developing and maintaining API documentation therefore need to involve the expertise of communication professionals.},
author = {Meng, Michael and Steinhardt, Stephanie and Schubert, Andreas},
booktitle = {Journal of Technical Writing and Communication},
doi = {10.1177/0047281617721853},
isbn = {0047-2816, 1541-3780},
issn = {15413780},
keywords = {application programming interface documentation,audience analysis,information design,technical documentation,usability},
title = {{Application Programming Interface Documentation: What Do Software Developers Want?}},
year = {2017}
}
@article{Buckley2015,
abstract = {An international group of experts convened to provide guidance for employers to promote the avoidance of prolonged periods of sedentary work. The set of recommendations was developed from the totality of the current evidence, including long-term epidemiological studies and interventional studies of getting workers to stand and/or move more frequently. The evidence was ranked in quality using the four levels of the American College of Sports Medicine. The derived guidance is as follows: for those occupations which are predominantly desk based, workers should aim to initially progress towards accumulating 2 h/day of standing and light activity (light walking) during working hours, eventually progressing to a total accumulation of 4 h/day (prorated to part-time hours). To achieve this, seated-based work should be regularly broken up with standing-based work, the use of sit-stand desks, or the taking of short active standing breaks. Along with other health promotion goals (improved nutrition, reducing alcohol, smoking and stress), companies should also promote among their staff that prolonged sitting, aggregated from work and in leisure time, may significantly and independently increase the risk of cardiometabolic diseases and premature mortality. It is appreciated that these recommendations should be interpreted in relation to the evidence from which they were derived, largely observational and retrospective studies, or short-term interventional studies showing acute cardiometabolic changes. While longer term intervention studies are required, the level of consistent evidence accumulated to date, and the public health context of rising chronic diseases, suggest initial guidelines are justified. We hope these guidelines stimulate future research, and that greater precision will be possible within future iterations.},
author = {Buckley, John P. and Hedge, Alan and Yates, Thomas and Copeland, Robert J. and Loosemore, Michael and Hamer, Mark and Bradley, Gavin and Dunstan, David W.},
doi = {10.1136/bjsports-2015-094618},
issn = {14730480},
journal = {British Journal of Sports Medicine},
month = {nov},
number = {21},
pages = {1357--1362},
pmid = {26034192},
publisher = {BMJ Publishing Group},
title = {{The sedentary office: An expert statement on the growing case for change towards better health and productivity}},
volume = {49},
year = {2015}
}
@inproceedings{Ma2018,
abstract = {With the increasing popularity of open-source software development, there is a tremendous growth of software artifacts that provide insight into how people build software. Researchers are always looking for large-scale and representative software artifacts to produce systematic and unbiased validation of novel and existing techniques. For example, in the domain of software requirements traceability, researchers often use software applications with multiple types of artifacts, such as requirements, system elements, verifications, or tasks to develop and evaluate their traceability analysis techniques. However, the manual identification of rich software artifacts is very labor-intensive. In this work, we first conduct a large-scale study to identify which types of software artifacts are produced by a wide variety of open-source projects at different levels of granularity. Then we propose an automated approach based on Machine Learning techniques to identify various types of software artifacts. Through a set of experiments, we report and compare the performance of these algorithms when applied to software artifacts. {\textcopyright}2018 ACM.},
author = {Ma, Yuzhan and Fakhoury, Sarah and Christensen, Michael and Arnaoudova, Venera and Zogaan, Waleed and Mirakhorli, Mehdi},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/3196398.3196446},
file = {::},
isbn = {9781450357166},
issn = {02705257},
keywords = {machine learning,open-source software,software artifacts},
month = {may},
pages = {414--425},
publisher = {IEEE Computer Society},
title = {{Automatic classification of software artifacts in open-source applications}},
year = {2018}
}
@article{Ralph2019a,
abstract = {Software engineering is increasingly concerned with theory because the foundational knowledge comprising theories provides a crucial counterpoint to the practical knowledge expressed through methods and techniques. Fortunately, much guidance is available for generating and evaluating theories for explaining why things happen (variance theories). Unfortunately, little guidance is available concerning theories for explaining how things happen (process theories), or theories for analyzing and understanding situations (taxonomies). This paper therefore attempts to clarify the nature and functions of process theories and taxonomies in software engineering research, and to synthesize methodological guidelines for their generation and evaluation. It further advances the key insight that most process theories are taxonomies with additional propositions, which helps inform their evaluation. The proposed methodological guidance has many benefits: it provides a concise summary of existing guidance from reference disciplines, it adapts techniques from reference disciplines to the software engineering context, and it promotes approaches that better facilitate scientific consensus.},
author = {Ralph, Paul},
doi = {10.1109/TSE.2018.2796554},
file = {::},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Research methodology,action research,case study,experiment,framework,grounded theory,guidelines,model,process theory,questionnaire,taxonomy,theory for analysis,theory for understanding},
month = {jul},
number = {7},
pages = {712--735},
title = {{Toward Methodological Guidelines for Process Theories and Taxonomies in Software Engineering}},
url = {https://ieeexplore.ieee.org/document/8267085/},
volume = {45},
year = {2019}
}
@article{Best2007b,
author = {Best, Christopher and Hasenbosch, Sam and Skinner, Michael and Crane, Peter and Burchat, Eleanore and Gehr, Sara Elizabeth and Kam, Clinton and Shanahan, Christopher and Zamba, Mitch},
file = {::},
number = {March},
pages = {1--17},
title = {{Coalition Distributed Mission Training : Exercise Pacific Link 2 AIAC12 – Twelfth Australian International Aerospace Congress}},
url = {https://search.informit.com.au/documentSummary;dn=556840067074338;res=IELENG},
year = {2007}
}
@article{Ghani2019,
abstract = {Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.},
author = {Ghani, Norjihan Abdul and Hamid, Suraya and {Targio Hashem}, Ibrahim Abaker and Ahmed, Ejaz},
doi = {10.1016/j.chb.2018.08.039},
file = {::},
issn = {07475632},
journal = {Computers in Human Behavior},
number = {August 2018},
pages = {417--428},
title = {{Social media big data analytics: A survey}},
volume = {101},
year = {2019}
}
@article{Report2020,
annote = {Takeaways
AI/ML adoptors face data issues
AI/ML skill shortages

Risks
Unexpected outcomes/predictions
Fairness/bias
Model interpretability and transparancy},
author = {Report, Radar},
file = {::},
title = {{AI Adoption in the Enterprise 2020}},
year = {2020}
}
@article{Wong2016,
abstract = {In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting.},
archivePrefix = {arXiv},
arxivId = {1609.08764},
author = {Wong, Sebastien C. and Gatt, Adam and Stamatescu, Victor and McDonnell, Mark D.},
eprint = {1609.08764},
file = {::},
month = {sep},
title = {{Understanding data augmentation for classification: when to warp?}},
url = {http://arxiv.org/abs/1609.08764},
year = {2016}
}
@article{Burke2006,
abstract = {This endeavor provides a multidisciplinary, multilevel, and multiphasic conceptualization of team adaptation with theoretical roots in the cognitive, human factors, and industrial-organizational psychology literature. Team adaptation and the emergent nature of adaptive team performance are defined from a multilevel, theoretical standpoint. An input-throughput-output model is advanced to illustrate a series of phases unfolding over time that constitute the core processes and emergent states underlying adaptive team performance and contributing to team adaptation. The cross-level mixed-determinants model highlights team adaptation in a nomological network of lawful relations. Testable propositions, practical implications, and directions for further research in this area are also advanced. (PsycINFO Database Record (c) 2006 APA, all rights reserved).},
author = {Burke, C. Shawn and Stagl, Kevin C. and Salas, Eduardo and Pierce, Linda and Kendall, Dana},
doi = {10.1037/0021-9010.91.6.1189},
issn = {00219010},
journal = {Journal of Applied Psychology},
keywords = {Adaptability,Team adaptation,Team effectiveness,Teams,Teamwork},
month = {nov},
number = {6},
pages = {1189--1207},
pmid = {17100478},
title = {{Understanding team adaptation: A conceptual analysis and model}},
volume = {91},
year = {2006}
}
@article{Easterbrook2008,
abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering. {\textcopyright}2008 Springer-Verlag London.},
annote = {Types of research methods - Controlled Experiments, Case Studies, Survey Research, Ethnographies, Action Research

Types of research questions - Existence, Description and Classification, Descriptive-Comparative

Types of philosophical stances - positivism, constructivism, critical theory, pragmatism},
author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret Anne and Damian, Daniela},
doi = {10.1007/978-1-84800-044-5_11},
file = {::},
isbn = {9781848000438},
journal = {Guide to Advanced Empirical Software Engineering},
pages = {285--311},
title = {{Selecting empirical methods for software engineering research}},
year = {2008}
}
@inproceedings{Shepperd2019,
author = {Shepperd, Martin and Guo, Yuchen and Li, Ning and Arzoky, Mahir and Capiluppi, Andrea and Counsell, Steve and Destefanis, Giuseppe and Swift, Stephen and Tucker, Allan and Yousefi, Leila},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2019},
doi = {10.1007/978-3-030-33607-3_12},
editor = {Yin, Hujun and Camacho, David and Tino, Peter and Tall{\'{o}}n-Ballesteros, Antonio J. and Menezes, Ronaldo and Allmendinger, Richard},
file = {::},
keywords = {clustering,evolutionary clustering,evolutionary computation},
pages = {102--109},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{The Prevalence of Errors in Machine Learning Experiments}},
url = {http://link.springer.com/10.1007/978-3-030-33607-3{\_}12},
year = {2019}
}
@article{Widerberg2006,
abstract = {We argue that the way time is organized affects bodily habits and emotions. Drawing on a variety of qualitative and quantitative studies from my large-scale research project with Ulla-Britt Lilleaas, ‘The Sociality of Tiredness: The Handling of Tiredness in a Gender, Generation and Class Perspective' (presented in Lilleaas and Widerberg, 2001), we focus on class and gender aspects of bodily habits and customs generated in work life and family life (and in the combination of the two). In this article, I illustrate variations in the type of time and body habits that different work organizations and professions generate. I also stress similarities in the use of time and body across professions and gender to illuminate the driving forces of modernity. It is argued that a ‘sped-up life' and a ‘life of doing' at work and at home generate a restless body, and irritation (the emotion of late modernity?) as its emotional expression. Finally, the question is raised whether this development is not only a threat to the body, but also to the very heart of democracy. {\textcopyright} 2006, SAGE. All rights reserved.},
author = {Widerberg, Karin},
doi = {10.1177/0961463X06061348},
issn = {0961-463X},
journal = {Time {\&} Society},
keywords = {class,embodied time,gender,modernity,work},
month = {mar},
number = {1},
pages = {105--120},
publisher = {SAGELondon, Thousand Oaks, CA and New Delhi},
title = {{Embodying Modern Times}},
url = {http://journals.sagepub.com/doi/10.1177/0961463X06061348},
volume = {15},
year = {2006}
}
@inproceedings{Rani2014,
abstract = {A hybrid approach using ontology similarity and fuzzy logic for semantic question answering." In Advanced Computing, Networking and Informatics-Volume 1, pp. 601-609. Springer, Cham, 2014. Abstract. One of the challenges in information retrieval is providing accurate answers to a user's question often expressed as uncertainty words. Most answers are based on a Syntactic approach rather than a Semantic analysis of the query. In this paper our objective is to present a hybrid approach for a Semantic question answering retrieval system using Ontology Similarity and Fuzzy logic. We use a Fuzzy co-clustering algorithm to retrieve collection of documents based on Ontology Similarity. Fuzzy scale uses Fuzzy type-1 for documents and Fuzzy type-2 for words to prioritize answers. The objective of this work is to provide retrieval systems with more accurate answers than non-fuzzy Semantic Ontology approach.},
author = {Rani, Monika and Muyeba, Maybin K. and Vyas, O. P.},
booktitle = {Smart Innovation, Systems and Technologies},
doi = {10.1007/978-3-319-07353-8_69},
file = {::},
isbn = {9783319073521},
issn = {21903026},
keywords = {Fuzzy Ontology,Fuzzy type-1,Fuzzy type-2,Question and Answering,Semantic Web},
number = {VOL 1},
pages = {601--609},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{A hybrid approach using Ontology Similarity and Fuzzy logic for semantic question answering}},
volume = {27},
year = {2014}
}
@article{Brogger2018,
abstract = {Objective: Visual EEG analysis is the gold standard for clinical EEG interpretation and analysis, but there is no published data on how long it takes to review and report an EEG in clinical routine. Estimates of reporting times may inform workforce planning and automation initiatives for EEG. The SCORE standard has recently been adopted to standardize clinical EEG reporting, but concern has been expressed about the time spent reporting. Methods: Elapsed times were extracted from 5889 standard and sleep-deprived EEGs reported between 2015 and 2017 reported using the SCORE EEG software. Results: The median review time for standard EEG was 12.5 min, and for sleep deprived EEG 20.9 min. A normal standard EEG had a median review time of 8.3 min. Abnormal EEGs took longer than normal EEGs to review, and had more variable review times. 99{\%} of EEGs were reported within 24 h of end of recording. Review times declined by 25{\%} during the study period. Conclusion: Standard and sleep-deprived EEG review and reporting times with SCORE EEG are reasonable, increasing with increasing EEG complexity and decreasing with experience. EEG reports can be provided within 24 h. Significance: Clinical standard and sleep-deprived EEG reporting with SCORE EEG has acceptable reporting times.},
author = {Brogger, Jan and Eichele, Tom and Aanestad, Eivind and Olberg, Henning and Hjelland, Ina and Aurlien, Harald},
doi = {10.1016/j.cnp.2018.03.002},
file = {::},
issn = {2467981X},
journal = {Clinical Neurophysiology Practice},
keywords = {EEG reporting,EEG review,EEG review time,EEG workload,SCORE EEG},
month = {jan},
pages = {59--64},
publisher = {Elsevier B.V.},
title = {{Visual EEG reviewing times with SCORE EEG}},
volume = {3},
year = {2018}
}
@article{Smutny2020,
abstract = {With the exponential growth in the mobile device market over the last decade, chatbots are becoming an increasingly popular option to interact with users, and their popularity and adoption are rapidly spreading. These mobile devices change the way we communicate and allow ever-present learning in various environments. This study examined educational chatbots for Facebook Messenger to support learning. The independent web directory was screened to assess chatbots for this study resulting in the identification of 89 unique chatbots. Each chatbot was classified by language, subject matter and developer's platform. Finally, we evaluated 47 educational chatbots using the Facebook Messenger platform based on the analytic hierarchy process against the quality attributes of teaching, humanity, affect, and accessibility. We found that educational chatbots on the Facebook Messenger platform vary from the basic level of sending personalized messages to recommending learning content. Results show that chatbots which are part of the instant messaging application are still in its early stages to become artificial intelligence teaching assistants. The findings provide tips for teachers to integrate chatbots into classroom practice and advice what types of chatbots they can try out.},
author = {Smutny, Pavel and Schreiberova, Petra},
doi = {10.1016/j.compedu.2020.103862},
issn = {03601315},
journal = {Computers and Education},
keywords = {Chatbot,Facebook,Messenger,Mobile learning,Quality evaluation},
number = {June 2019},
pages = {103862},
publisher = {Elsevier Ltd},
title = {{Chatbots for learning: A review of educational chatbots for the Facebook Messenger}},
url = {https://doi.org/10.1016/j.compedu.2020.103862},
volume = {151},
year = {2020}
}
@article{Miller2019,
author = {Miller, Tim},
doi = {10.1016/j.artint.2018.07.007},
file = {::},
issn = {00043702},
journal = {Artificial Intelligence},
month = {feb},
pages = {1--38},
title = {{Explanation in artificial intelligence: Insights from the social sciences}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988},
volume = {267},
year = {2019}
}
@article{Briggs2012,
abstract = {A model's purpose is to inform medical decisions and health care resource allocation. Modelers employ quantitative methods to structure the clinical, epidemiological, and economic evidence base and gain qualitative insight to assist decision makers in making better decisions. From a policy perspective, the value of a model-based analysis lies not simply in its ability to generate a precise point estimate for a specific outcome but also in the systematic examination and responsible reporting of uncertainty surrounding this outcome and the ultimate decision being addressed. Different concepts relating to uncertainty in decision modeling are explored. Stochastic (first-order) uncertainty is distinguished from both parameter (second-order) uncertainty and from heterogeneity, with structural uncertainty relating to the model itself forming another level of uncertainty to consider. The article argues that the estimation of point estimates and uncertainty in parameters is part of a single process and explores the link between parameter uncertainty through to decision uncertainty and the relationship to value of information analysis. The article also makes extensive recommendations around the reporting of uncertainty, in terms of both deterministic sensitivity analysis techniques and probabilistic methods. Expected value of perfect information is argued to be the most appropriate presentational technique, alongside cost-effectiveness acceptability curves, for representing decision uncertainty from probabilistic analysis. {\textcopyright}2012 International Society for Pharmacoeconomics and Outcomes Research (ISPOR).},
author = {Briggs, Andrew H. and Weinstein, Milton C. and Fenwick, Elisabeth A.L. L and Karnon, Jonathan and Sculpher, Mark J. and Paltiel, A. David},
doi = {10.1177/0272989X12458348},
file = {::},
issn = {0272989X},
journal = {Medical Decision Making},
keywords = {guidelines,heterogeneity,sensitivity analysis,uncertainty analysi (ADP) s,value of information},
number = {5},
pages = {722--732},
publisher = {Elsevier Inc.},
title = {{Model parameter estimation and uncertainty analysis: A report of the ISPOR-SMDM modeling good research practices task force working group-6}},
url = {http://dx.doi.org/10.1016/j.jval.2012.04.014},
volume = {32},
year = {2012}
}
@article{VanEmden2002_codesmell,
abstract = {Software inspection is a known technique for improving software quality. It involves carefully examining the code, the design, and the documentation of software and checking these for aspects that are known to be potentially problematic based on past experience. Code smells are a metaphor to describe patterns that are generally associated with bad design and bad programming practices. Originally, code smells are used to find the places in software that could benefit from refactoring. In this paper we investigate how the quality of code can be automatically assessed by checking for the presence of code smells and how this approach can contribute to automatic code inspection. We present an approach for the automatic detection and visualization of code smells and discuss how this approach can be used in the design of a software inspection tool. We illustrate the feasibility of our approach with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in Java source code. Finally, we show how this tool was applied in a case study.},
author = {{Van Emden}, Eva and Moonen, Leon},
doi = {10.1109/WCRE.2002.1173068},
file = {::},
isbn = {0769517994},
issn = {10951350},
journal = {Proceedings - Working Conference on Reverse Engineering, WCRE},
keywords = {Java,Software inspection,code smells,quality assurance,refactoring},
pages = {97--106},
publisher = {IEEE},
title = {{Java quality assurance by detecting code smells}},
volume = {2002-Janua},
year = {2002}
}
@article{Verbeek2019,
abstract = {Background Faster recovery from work may help to prevent work-related ill health. Aims To provide a preliminary assessment of the range and nature of interventions that aim to improve recovery from cognitive and physical work. Methods A scoping review to examine the range and nature of the evidence, to identify gaps in the evidence base and to provide input for systematic reviews. We searched for workplace intervention studies that aimed at enhancing recovery. We used an iterative method common in qualitative research to obtain an overview of study elements, including intervention content, design, theory, measurements, effects and cost-effectiveness. Results We found 28 studies evaluating seven types of interventions mostly using a randomized controlled study design. For person-directed interventions, we found relaxation techniques, training of recovery experiences, promotion of physical activity and stress management. For work-directed interventions, there were participatory changes, work-break schedules and task variation. Most interventions were based on the conservation of resources and affect-regulation theories, none were based on the effort-recovery theory. The need for recovery (NfR) and the recovery experiences questionnaires (REQ) were used most often. Study authors reported a beneficial effect of the intervention in 14 of 26 published studies. None of the studies that used the NfR scale found a beneficial effect, whereas studies that used the REQ showed beneficial effects. Three studies indicated that interventions were not cost-effective. Conclusions Feasible and possibly effective interventions are available for improving recovery from cognitive and physical workload. Systematic reviews are needed to determine their effectiveness.},
author = {Verbeek, J and Ruotsalainen, J and Laitinen, J and Korkiakangas, E and Lusa, S and M{\"{a}}ntt{\"{a}}ri, S and Oksanen, T},
doi = {10.1093/occmed/kqy141},
issn = {0962-7480},
journal = {Occupational Medicine},
keywords = {Interventions,recovery after work,scoping review},
month = {feb},
number = {1},
pages = {54--63},
publisher = {Oxford University Press},
title = {{Interventions to enhance recovery in healthy workers; a scoping review}},
url = {https://academic.oup.com/occmed/article/69/1/54/5151255},
volume = {69},
year = {2019}
}
@article{Akl2017,
abstract = {While it is important for the evidence supporting practice guidelines to be current, that is often not the case. The advent of living systematic reviews has made the concept of “living guidelines” realistic, with the promise to provide timely, up-to-date and high-quality guidance to target users. We define living guidelines as an optimization of the guideline development process to allow updating individual recommendations as soon as new relevant evidence becomes available. A major implication of that definition is that the unit of update is the individual recommendation and not the whole guideline. We then discuss when living guidelines are appropriate, the workflows required to support them, the collaboration between living systematic reviews and living guideline teams, the thresholds for changing recommendations, and potential approaches to publication and dissemination. The success and sustainability of the concept of living guideline will depend on those of its major pillar, the living systematic review. We conclude that guideline developers should both experiment with and research the process of living guidelines.},
author = {Akl, Elie A. and Meerpohl, Joerg J. and Elliott, Julian and Kahale, Lara A. and Sch{\"{u}}nemann, Holger J. and Agoritsas, Thomas and Hilton, John and Perron, Caroline and Akl, Elie A. and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara A. and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and Kolakowsky-Hayner, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Salanti, Georgia and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch{\"{u}}nemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Shemilt, Ian and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and MacLehose, Harriet and Simmonds, Mark and Cossi, Marie Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Marshall, Iain and Soares-Weiser, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Garc{\'{i}}a, Laura Mart{\'{i}}nez and Synnot, Anneliese and Danko, Kristen and Mavergames, Chris and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Thomas, James and Egan, Cathy and McDonald, Steve and Tritton, Roger and Elliott, Julian and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Meerpohl, Joerg J. and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Turgeon, Alexis and Featherstone, Robin and Mondello, Stefania and Turner, Tari and Foxlee, Ruth and Morley, Richard and van Valkenhoef, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Wallace, Byron and Glasziou, Paul and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and Noel-Storr, Anna and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and {Yepes Nu{\~{n}}ez}, Juan Jos{\'{e}} and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
doi = {10.1016/j.jclinepi.2017.08.009},
file = {::},
issn = {18785921},
journal = {Journal of Clinical Epidemiology},
keywords = {Living guidelines,Living systematic review,Prioritizing recommendations,Updating guidelines,Updating systematic reviews},
pages = {47--53},
pmid = {28911999},
title = {{Living systematic reviews: 4. Living guideline recommendations}},
volume = {91},
year = {2017}
}
@article{Chaiklieng2015,
abstract = {A prospective cohort study was conducted to assess the incidence of shoulder pain following a survey study on baseline of health risk of shoulder pain (SP) among University office workers. A health risk assessment of SP was performed by using a risk matrix of covariation between ergonomics risk and discomfort level. The results showed that most (51.1{\%}) were found to have a moderate (21.2{\%}), high (17.3{\%}) or very high (12.6{\%}) health risk of SP. By exclusion of cases with moderate to severe level of shoulder discomfort, 149 workers were followed up periodically for identification of the SP new cases. The cumulative SP incidence for all levels increased from 24.8{\%} at the first month to 30.2{\%} at second month. This research found that a high proportion of the office workers were exposed to the ergonomics risk of work-related SP which similar to the SP incidence. The suggestion is that the SP prevention, such as improvement in work posture and ergonomic designs of workstations, are needed.},
author = {Chaiklieng, Sunisa and Krusun, Maytinee},
doi = {10.1016/j.promfg.2015.07.636},
file = {::},
issn = {23519789},
journal = {Procedia Manufacturing},
keywords = {Computer user,Discomfort,Ergonomics,Incidence,Shoulder pain},
month = {jan},
pages = {4941--4947},
publisher = {Elsevier B.V.},
title = {{Health Risk Assessment and Incidence of Shoulder Pain Among Office Workers}},
volume = {3},
year = {2015}
}
@article{Huang,
author = {Huang, Zixin and Wang, Zhenbang and Misailovic, Sasa},
file = {::},
title = {{PSense : Automatic Sensitivity Analysis for Probabilistic Programs}}
}
@article{Miller2013,
author = {Miller, John W and Henry, J Craig},
doi = {:10.1212/WNL.0b013e318279755f},
file = {::},
journal = {Neurology},
pages = {13--14},
title = {{Solving the dilemma of EEG misinterpretation}},
volume = {80(1)},
year = {2013}
}
@article{Karakolis2016,
abstract = {Sedentary office work has been shown to cause low back discomfort and potentially cause injury. Prolonged standing work has been shown to cause discomfort. The implementation of a sit–stand paradigm is hypothesised to mitigate discomfort and prevent injury induced by prolonged exposure to each posture in isolation. This study explored the potential of sit–stand to reduce discomfort and prevent injury, without adversely affecting productivity. Twenty-four participants performed simulated office work in three different conditions: sitting, standing and sit–stand. Variables measured included: perceived discomfort, L4–L5 joint loading and typing/mousing productivity. Working in a sit–stand paradigm was found to have the potential to reduce discomfort when compared to working in a sitting or standing only configuration. Sit–stand was found to be associated with reduced lumbar flexion during sitting compared to sitting only. Increasing lumbar flexion during prolonged sitting is a known injury mechanism. Therefore, sit–stand exhibited a potentially beneficial response of reduced lumbar flexion that could have the potential to prevent injury. Sit–stand had no significant effect on productivity. Practitioner Summary: This study has contributed foundational elements to guide usage recommendations for sit–stand workstations. The sit–stand paradigm can reduce discomfort; however, working in a sit–stand ratio of 15:5 min may not be the most effective ratio. More frequent posture switches may be necessary to realise the full benefit of sit–stand.},
author = {Karakolis, Thomas and Barrett, Jeff and Callaghan, Jack P.},
doi = {10.1080/00140139.2016.1146343},
issn = {13665847},
journal = {Ergonomics},
keywords = {Posture,injury,low back,pain,performance,sedentary},
month = {oct},
number = {10},
pages = {1275--1287},
pmid = {26804548},
publisher = {Taylor and Francis Ltd.},
title = {{A comparison of trunk biomechanics, musculoskeletal discomfort and productivity during simulated sit-stand office work}},
volume = {59},
year = {2016}
}
@article{Coupe2000,
abstract = { When building a Bayesian belief network, usually a large number of probabilities have to be assessed by experts in the domain of application. Experience shows that experts are often reluctant to assess all probabilities required, feeling that they are unable to give assessments with a high level of accuracy. We argue that the elicitation of probabilities from experts can be supported to a large extent by iteratively performing sensitivity analyses of the belief network in the making, starting with rough, initial assessments. Since it gives insight into which probabilities require a high level of accuracy and which do not, performing a sensitivity analysis allows for focusing further elicitation efforts. We propose an elicitation procedure in which, alternately, sensitivity analyses are performed and probability assessments refined, until satisfactory behaviour of the belief network is obtained, until the costs of further elicitation outweigh the benefits of higher accuracy or until higher accuracy can no longer be attained due to lack of knowledge. },
author = {Coup{\'{e}}, Veerle M.H. H and {Van Der Gaag}, Linda C. and Habbema, J. Dik F.},
doi = {10.1017/S0269888900003027},
file = {::},
issn = {02698889},
journal = {Knowledge Engineering Review},
number = {3},
pages = {215--232},
title = {{Sensitivity analysis: an aid for belief-network quantification}},
volume = {15},
year = {2000}
}
@article{Ernst2007,
abstract = {Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x = a), non-zero (x ≠ 0), being in a range (a ≤ x ≤ b), linear relationships (y = a x + b), ordering (x ≤ y), functions from a library (x = fn (y)), containment (x ∈ y), sortedness (x is sorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon's output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
annote = {Forth industrial revolution - fast and widespread AI, shift towards more algorithmic society
Four concerns on AI - Lack of transparency, powerful predictions, cannot be directly explained, improve trust and transparency

Monetary value of AI - Increase in global investment in AI, AI market growth

FAT (Fairness, Accountability, Transparency) Academics

Definition of Data Science - A field that unifies statistics, data analytics, machine learning and their related methods in order to understand and anlayze actual phenomena with data

Reasons for XAI - Explain to justify, explain to control, explain to improve, explain to discover
XAI Application domains - Transportation, healthcare, legal, finance, military

The technical challenge of enabling XAI - Why the use of XAI is not systematic ?
Why is not everyone using XAI?

Four basic research areas - Data Science, AI/ML, Human Science, HCI
Six major academic databases - SCOPUS, IEEExplore, ACM Digital Library, Google Scholar, Citeseer Library, ScienceDirect

Scoop related methods - Understanding the entire model behavior/understanding a single prediction},
author = {Ernst, Michael D. and Perkins, Jeff H. and Guo, Philip J. and McCamant, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
doi = {10.1016/j.scico.2007.01.015},
file = {::},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {Daikon,Dynamic analysis,Dynamic invariant detection,Inductive logic programming,Inference,Invariant,Likely invariant,Program understanding,Specification,Specification mining},
number = {1-3},
pages = {35--45},
title = {{The Daikon system for dynamic detection of likely invariants}},
volume = {69},
year = {2007}
}
@techreport{Krell,
abstract = {Motivation: For deep learning on image data, a common approach is to augment the training data by artificial new images, using techniques like moving windows, scaling, affine distortions, and elastic deformations. In contrast to image data, electroencephalographic (EEG) data suffers even more from the lack of sufficient training data. Methods: We suggest and evaluate rotational distortions similar to affine/rotational distortions of images to generate augmented data. Results: Our approach increases the performance of signal processing chains for EEG-based brain-computer interfaces when rotating only around y-and z-axis with an angle around ±18 degrees to generate new data. Conclusion: This shows that our processing efficient approach generates meaningful data and encourages to look for further new methods for EEG data augmentation.},
author = {Krell, Mario Michael and Kim, Su Kyoung},
file = {::},
title = {{Rotational Data Augmentation for Electroencephalographic Data}},
url = {http://neuroimage.usc.edu/brainstorm}
}
@article{Rhee2016,
abstract = {Recently, the popularity of smart phones has brought about changes in how people work and take breaks. This paper focuses on whether taking a break with a smart phones (e.g., browsing the internet or using social network services) has a different association with regaining vitality after 'conventional breaks' (e.g., walking or chatting face to face with friends). We surveyed a total of 450 workers in Korea with a diary questionnaire to see if there were differences in the effects of breaks via two theoretical paths of association: positively in terms of vigor and negatively in terms of emotional exhaustion. Empirical results show that psychological detachments by breaks, independent of break modes, did increase vigor and reduce emotional exhaustion, consistent with the existing literature. However, we also found that the effects, particularly in reducing emotional exhaustion, were significantly lower for the smart phone break group versus the conventional group. We discuss some theoretical and practical implications of these findings.},
author = {Rhee, Hongjai and Kim, Sudong},
doi = {10.1016/j.chb.2015.11.056},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Affects,Conventional breaks,Emotional exhaustion,Psychological detachment,Smart phone-using breaks,Vigor},
month = {apr},
pages = {160--167},
publisher = {Elsevier Ltd},
title = {{Effects of breaks on regaining vitality at work: An empirical comparison of 'conventional' and 'smart phone' breaks}},
volume = {57},
year = {2016}
}
@article{Kassiri2017,
author = {Kassiri, Hossein and Tonekaboni, Sana and Salam, M. Tariqus and Soltani, Nima and Abdelhalim, Karim and Velazquez, Jose Luis Perez and Genov, Roman},
doi = {10.1109/TBCAS.2017.2694638},
file = {::},
issn = {1932-4545},
journal = {IEEE Transactions on Biomedical Circuits and Systems},
month = {oct},
number = {5},
pages = {1026--1040},
title = {{Closed-Loop Neurostimulators: A Survey and A Seizure-Predicting Design Example for Intractable Epilepsy Treatment}},
url = {http://ieeexplore.ieee.org/document/7982670/},
volume = {11},
year = {2017}
}
@inproceedings{Beyer2019,
author = {Beyer, Dirk},
booktitle = {2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2019.00026},
isbn = {978-1-7281-3412-3},
month = {may},
pages = {111--115},
publisher = {IEEE},
title = {{A Data Set of Program Invariants and Error Paths}},
url = {https://ieeexplore.ieee.org/document/8816801/},
year = {2019}
}
@article{Phattharasupharerk2019,
abstract = {Objective: To investigate the effects of Qigong practice, Guan Yin Zi Zai Gong level 1, compared with a waiting list control group among office workers with chronic nonspecific low back pain (CNLBP). Methods: A randomized controlled trial was conducted at offices in the Bangkok Metropolitan Region. Seventy-two office workers with CNLBP were screened for inclusion/exclusion criteria (age 20–40 years; sitting period more than 4 h per day) and were allocated randomly into two groups: the Qigong and waiting list groups (n = 36 each). The participants in the Qigong group took a Qigong practice class (Guan Yin Zi Zai Gong level 1) for one hour per week for six weeks at their workstation. The participants were encouraged to conduct the Qigong exercise at home every day. The waiting list group received general advice regarding low back pain management. The primary outcomes were pain intensity, measured by the visual analog scale, and back functional disability, measured by the Roland and Morris Disability Questionnaire. The secondary outcomes were back range of motion, core stability performance index, heart rate, respiratory rate, the Srithanya Stress Scale (ST-5), and the global perceived effect (GPE) questionnaire. Results: Compared to the baseline, participants in the Qigong group experienced significantly decreased pain intensity and back functional disability. No statistically significant difference in these parameters was found in the waiting list group. Comparing the two groups, Qigong exercise significantly improved pain intensity, back functional impairment, range of motion, core muscle strength, heart rate, respiratory rate, and mental status. The Qigong group also had a significantly higher global outcome satisfaction than the waiting list group. Conclusion: Qigong practice is an option for treatment of CNLBP in office workers.},
author = {Phattharasupharerk, Suttinee and Purepong, Nithima and Eksakulkla, Sukanya and Siriphorn, Akkradate},
doi = {10.1016/j.jbmt.2018.02.004},
issn = {15329283},
journal = {Journal of Bodywork and Movement Therapies},
keywords = {Chronic low back pain,Nonspecific low back pain,Office workers,Qigong},
month = {apr},
number = {2},
pages = {375--381},
publisher = {Churchill Livingstone},
title = {{Effects of Qigong practice in office workers with chronic non-specific low back pain: A randomized control trial}},
volume = {23},
year = {2019}
}
@book{preece_book,
author = {Preece, J and Rogers, Y and Sharp, H},
publisher = {Wiley},
title = {{Interaction Design: Beyond Human Computer Interaction}},
year = {2015}
}
@techreport{Kuhlmann,
abstract = {| Epilepsy is a common disorder characterized by recurrent seizures. An overwhelming majority of people with epilepsy regard the unpredictability of seizures as a major issue. More than 30 years of international effort has been devoted to the prediction of seizures, aiming to remove the burden of unpredictability and couple novel, time-specific treatment to seizure prediction technology. A highly-influential review published in 2007 concluded that insufficient evidence indicated that seizures could be predicted. Since then, several advances have been made, including successful prospective seizure prediction using intracranial electroencephalography (EEG) in a small number of people in a trial of a real-time seizure prediction device. In this Review, we examine advances in the field, including EEG databases, seizure prediction competitions, the prospective trial, and advances in our understanding of the mechanisms of seizures. We argue that these advances, together with statistical evaluations, set the stage for a resurgence in efforts towards the development of seizure prediction methodologies. We propose new avenues of investigation involving a synergy between mechanisms, models, data, devices and algorithms and refine the existing guidelines for development of seizure prediction technology to instigate development of a solution that removes the burden of the unpredictability of seizures.},
author = {Kuhlmann, Levin and Lehnertz, Klaus and Richardson, Mark P and Schelter, Bj{\"{o}}rn and Zaveri, Hitten P},
file = {::},
title = {{Seizure prediction-ready for a new era}},
url = {www.epilepsiae.eu}
}
@misc{Elbagoury2018,
abstract = {Abstract This chapter contains sections titled: Introduction State of the Art Proposed System Design Proposed Artificial Intelligence Techniques for New AI IoT Health-Care Solutions for Stroke Monitoring Conclusion},
annote = {doi:10.1002/9781119509875.ch9},
author = {Elbagoury, Bassant M and Bakr, Ahmed A and Roushdy, Mohamed and Schrader, Thomas},
booktitle = {Emerging Technologies for Health and Medicine},
doi = {doi:10.1002/9781119509875.ch9},
file = {::},
isbn = {9781119509875},
keywords = {Artificial Intelligence,Brain Stroke,EMG signal Processing,Mobile Health,Neural Networks,Telemedicine},
month = {oct},
pages = {117--127},
series = {Wiley Online Books},
title = {{Mobile Doctor Brain AI App: Artificial Intelligence for IoT Healthcare}},
url = {https://doi.org/10.1002/9781119509875.ch9},
year = {2018}
}
@inproceedings{10.1145/3373017.3373055,
address = {New York, NY, USA},
author = {Liu, Yu and Sivathamboo, Shobi and Goodin, Peter and Bonnington, Paul and Kwan, Patrick and Kuhlmann, Levin and O'Brien, Terence and Perucca, Piero and Ge, Zongyuan and O'Brien, Terence and Perucca, Piero and Ge, Zongyuan and O'Brien, Terence and Perucca, Piero and Ge, Zongyuan},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
doi = {10.1145/3373017.3373055},
file = {::},
isbn = {9781450376976},
keywords = {CNN,Epilepsy,Multi-Biosignal,Multimodal learning,Seizure detection},
publisher = {Association for Computing Machinery},
series = {ACSW '20},
title = {{Epileptic Seizure Detection Using Convolutional Neural Network: A Multi-Biosignal Study}},
url = {https://doi.org/10.1145/3373017.3373055},
year = {2020}
}
@article{Macdonald2018,
abstract = {Teaching modern information retrieval is greatly benefited by giving students hands-on experience with an open-source search engine that they can experiment with. As such, open source platforms such as Terrier are a valuable resource upon which learning exercises can be built. However, experimentation using such systems can be a laborious process when performed by hand; queries might be rewritten, executed, and model parameters tuned. Moreover, the rise of learning-to-rank as the de-facto standard for state-ofthe- art retrieval complicates this further, with the introduction of training, validation and testing (likely over multiple folded datasets representing different query types). Currently, students resort to shell scripting to make experimentation easier, however this is far from ideal. On the other hand, the introduction of experimental pipelines in platforms like scikit-learn and Apache Spark in conjunction with notebook environments such as Jupyter have been shown to markedly reduce to barriers to non-experts setting up and running experiments. In this paper, we discuss how next generation information retrieval experimental pipelines can be combined in an agile manner using notebook-style interaction mechanisms. Building upon the Terrier IR platform, we describe how this is achieved using a recently released Terrier-Spark module and other recent changes in Terrier 5.0. Overall, this paper demonstrates the advantages of the agile nature of notebooks to experimental IR environments, from the classroom environment, through academic and industry research labs.},
author = {Macdonald, Craig and McCreadie, Richard and Ounis, Iadh},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {4},
pages = {54--61},
title = {{Agile information retrieval experimentation with Terrier notebooks}},
volume = {2167},
year = {2018}
}
@article{Wang2019,
abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development. CCS CONCEPTS • Human-centered computing {\~{}} Human computer interaction From supporting healthcare intervention decisions to informing criminal justice, artificial intelligence (AI) is now increasingly entering the mainstream and supporting high-consequence human decisions. However, the effectiveness of these systems will be limited by the machine's inability to explain its thoughts and actions to human users in these critical situations. These challenges have spurred research interest in explainable AI (XAI) [2, 12, 32, 43, 109]. To enable end users to understand, trust, and effectively manage their intelligent partners, HCI and AI researchers have produced many user-centered, innovative algorithm visualizations, interfaces and toolkits (e.g., [18, 56, 67, 86] that support users with various levels of AI literacy in diverse subject domains, from the bank customer who is refused a loan, the doctor making a diagnosis with a decision aid, to the patient who learns that he may have skin cancer from a smartphone photograph of his mole [30]. Adding on to this line of inquiry, this paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. We first conducted an extensive literature review in cognitive psychology, philosophy and decision-making theories that describe patterns of how people reason, make decisions and seek explanations, and cognitive factors that bias or compromise decision-making.},
author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y},
doi = {10.1145/3290605.3300831},
file = {::},
isbn = {9781450359702},
keywords = {Clinical decision making,Decision making,Explainable artificial intelligence,Explanations,Intelligibility},
title = {{Designing Theory-Driven User-Centric Explainable AI}},
url = {https://doi.org/10.1145/3290605.3300831},
year = {2019}
}
@article{Zhang2016,
abstract = {Uncertainty is intrinsic in most technical systems, including Cyber-Physical Systems (CPS). Therefore, handling uncertainty in a graceful manner during the real operation of CPS is critical. Since designing, developing, and testing modern and highly sophisticated CPS is an expanding field, a step to-wards dealing with uncertainty is to identify, define, and classify uncertainties at various levels of CPS. This will help develop a systematic and comprehen-sive understanding of uncertainty. To that end, we propose a conceptual model for uncertainty specifically designed for CPS. Since the study of uncertainty in CPS development and testing is still irrelatively unexplored, this conceptual model was derived in a large part by reviewing existing work on uncertainty in other fields, including philosophy, physics, statistics, and healthcare. The con-ceptual model is mapped to the three logical levels of CPS: Application, Infra-structure, and Integration. It is captured using UML class diagrams, including relevant OCL constraints. To validate the conceptual model, we identified, clas-sified, and specified uncertainties in two distinct industrial case studies.},
author = {Zhang, Man and Selic, Bran and Ali, Shaukat and Yue, Tao and Okariz, Oscar and Norgren, Roland},
doi = {10.1007/978-3-319-42061-5_16},
file = {::},
isbn = {9783319420608},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Conceptual model,Cyber-physical systems,Uncertainty},
pages = {247--264},
title = {{Understanding uncertainty in cyber-physical systems: A conceptual model}},
volume = {9764},
year = {2016}
}
@article{Millard2019,
abstract = {Background: Living systematic reviews (LSRs) offer an approach to keeping high-quality evidence synthesis continually up to date, so the most recent, relevant and reliable evidence can be used to inform policy and practice, resulting in improved quality of care and patient health outcomes. However, they require modifications to authoring and editorial processes and pose technical and publishing challenges. Several teams within Cochrane and the international Living Evidence Network have been piloting living systematic reviews. Methods: We conducted a mixed-methods evaluation with participants involved in six LSRs (three Cochrane and three non-Cochrane). Up to three semi-structured interviews were conducted with 27 participants involved with one or more of the pilot LSRs. Interviews explored participants' experiences contributing to the LSR, barriers and facilitators to their conduct and opportunities for future development. Pilot team members also completed monthly surveys capturing time for key tasks and the number of citations screened for each review. Results: Across the pilot LSRs, search frequency was monthly to three-monthly, with some using tools such as machine learning and Cochrane Crowd to screen searches. Varied approaches were used to communicate updates to readers. The number of citations screened varied widely between the reviews, from three to 300 citations per month. The amount of time spent per month by the author team on each review also varied from 5 min to 32 h. Participants were enthusiastic to be involved in the LSR pilot. They highlighted the importance of a motivated and well-organised team; the value of technology enablers to improve workflow efficiencies; the need to establish reliable and efficient processes to sustain living reviews; and the potential for saving time and effort in the long run. Participants highlighted challenges with the current publication processes, managing ongoing workload and the lack of resources to support LSRs in the long term. Conclusions: Findings to date support feasibility and acceptability of LSR production. There are challenges that need to be addressed for living systematic reviews to be sustainable and have maximum value. The findings from this study will be used in discussions with the Cochrane community, key decision makers and people more broadly concerned with LSRs to identify and develop priorities for scale-up.},
author = {Millard, Tanya and Synnot, Anneliese and Elliott, Julian and Green, Sally and McDonald, Steve and Turner, Tari},
doi = {10.1186/s13643-019-1248-5},
file = {::},
issn = {20464053},
journal = {Systematic Reviews},
keywords = {Living systematic review,Methods,Systematic review},
number = {1},
pages = {1--14},
publisher = {Systematic Reviews},
title = {{Feasibility and acceptability of living systematic reviews: Results from a mixed-methods evaluation}},
volume = {8},
year = {2019}
}
@article{Saltz2017,
abstract = {The challenge in executing a data science project is more than just identifying the best algorithm and tool set to use. Additional sociotechnical challenges include items such as how to define the project goals and how to ensure the project is effectively managed. This paper reports on a set of case studies where researchers were embedded within data science teams and where the researcher observations and analysis was focused on the attributes that can help describe data science projects and the challenges faced by the teams executing these projects, as opposed to the algorithms and technologies that were used to perform the analytics. Based on our case studies, we identified 14 characteristics that can help describe a data science project. We then used these characteristics to create a model that defines two key dimensions of the project. Finally, by clustering the projects within these two dimensions, we identified four types of data science projects, and based on the type of project, we identified some of the sociotechnical challenges that project teams should expect to encounter when executing data science projects.},
author = {Saltz, Jeffrey and Shamshurin, Ivan and Connors, Colin},
doi = {10.1002/asi.23873},
file = {::},
issn = {23301643},
journal = {Journal of the Association for Information Science and Technology},
number = {12},
pages = {2720--2728},
title = {{Predicting data science sociotechnical execution challenges by categorizing data science projects}},
volume = {68},
year = {2017}
}
@book{Russell_Norvig,
author = {Russell, Stuart and Norvig, Peter},
edition = {3rd},
publisher = {Prentice Hall Press},
title = {{Artificial Intelligence: A Modern Approach}},
year = {2009}
}
@article{He2012,
abstract = {We develop a production-inventory model for deteriorating items with demand disruption. This differs from the conventional inventory model which is affected only by either deterioration or disruption. In real-life production-inventory systems, deterioration of products and demand disruption are inevitable. The objective of this paper is to address these issues and to be able to derive the optimal production run time and replenishment policy for spot market purchases. We divide the problem into different scenarios according to disruption's time and magnitude. In each scenario the optimal production and inventory plans are provided so that the manufacturer can satisfy the new demand and decrease the loss. Then a numerical example is used to illustrate the model. We develop a production-inventory model for deteriorating items with demand disruption. This differs from the conventional inventory model which is affected only by either deterioration or disruption. In real-life production-inventory systems, deterioration of products and demand disruption are inevitable. The objective of this paper is to address these issues and to be able to derive the optimal production run time and replenishment policy for spot market purchases. We divide the problem into different scenarios according to disruption's time and magnitude. In each scenario the optimal production and inventory plans are provided so that the manufacturer can satisfy the new demand and decrease the loss. Then a numerical example is used to illustrate the model.},
author = {He, Yong and Wang, Shouyang},
doi = {10.1080/00207543.2011.615351},
issn = {00207543},
journal = {International Journal of Production Research},
keywords = {deteriorating items,disruption management,inventory,production},
title = {{Analysis of production-inventory system for deteriorating items with demand disruption}},
year = {2012}
}
@inproceedings{Greff2017,
abstract = {—We present a toolchain for computational research consisting of Sacred and two supporting tools. Sacred is an open source Python framework which aims to provide basic infrastructure for running computational experiments independent of the methods and libraries used. Instead, it focuses on solving universal everyday problems, such as managing configurations, reproducing results, and bookkeeping. Moreover, it provides an extensible basis for other tools, two of which we present here: Labwatch helps with tuning hyperparameters, and Sacredboard provides a web-dashboard for organizing and analyzing runs and results.},
author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the 16th Python in Science Conference},
doi = {10.25080/shinma-7f4c6e7-008},
file = {::},
pages = {49--56},
publisher = {SciPy},
title = {{The Sacred Infrastructure for Computational Research}},
url = {https://conference.scipy.org/proceedings/scipy2017/klaus{\_}greff.html},
year = {2017}
}
@techreport{Begley1200,
abstract = {Economic concerns are increasingly important in health system design, provider payment, and research funding decisions. Cost estimates are needed to provide insight into where the potential opportunities for cost-savings lie and to lay the groundwork for research to determine how to treat the disorder more effectively. The methods used by economists to estimate the direct and indirect costs of epilepsy are reviewed and results from studies in different countries are discussed. General patterns across patients with different types of conditions are reported. Gaps in the literature are identified and future research needs are assessed. Key Words: Cost of epilepsy Methods Findings. Knowing the cost of epilepsy increases awareness of its burden on individuals and society, who bear the burden , and the potential benefits of prevention and treatment. Calculation of the cost of epilepsy involves applying economic cost-of-illness methods to estimate the different types of cost experienced by individuals, their families, third-party payers, health providers, and society at large. Direct cost includes the cost of resources consumed when health care, social services, and patient or family member services are used to prevent, diagnose, treat, or rehabilitate persons with epilepsy. Indirect cost is the cost of unemployment, decreased productivity, and household work that is lost when people with epilepsy are less able to do their jobs or to work at home. Intangible cost is the economic value of pain and suffering that individuals experience with the disorder. Reviews of methods of estimating the cost of epilepsy have appeared elsewhere in the literature (1-4). We provide a brief description of general methods, discuss current methodo-logic issues, review the latest findings, and make recommendations for future research. GENERAL METHODS Direct cost Epilepsy-specific direct costs are defined as the monetary value of resources consumed in the prevention, treatment, or rehabilitation of people with the disorder. These costs are assumed to reflect the opportunity cost of the resources (e.g., their value if used in the next most efficient way) that are used or lost as a result of the disorder. The costs of medical services are usually estimated according to the average payment made to providers of health care. Direct costs vary depending on the perspective of the study. In countries where health care and social services are paid by the government, the cost of epilepsy treatment may be very low from the patient's perspective. Alternative perspectives include those of costs incurred by employers or health insurance plans that purchase health care. Most cost-of-illness studies adopt a societal perspective that attempts to capture full costs without regard to which individuals or entities are incurring the costs. Two broad approaches are used in generating estimates of the direct cost of epilepsy: "bottom-up" and "top-down"(5). In the former, cost estimates of the number and types of health care, social services, and family member resources used by individuals with epilepsy are derived from prospective or retrospective studies of real cases, or, alternatively, based on hypothetical information provided by expert panels and related literature. In the top-down type of study, estimates are made of the cost of services for all illnesses based on nationally representative provider surveys, and a portion is attributed to a specific illness. These studies use standard methods (disease-specific diagnosis coding) to classify health resource use for an illness The top-down approach is desirable for high-prevalence illnesses that are well represented in national surveys, and when there is a lack of data from representative samples of patients with a specific illness. In using population surveys, the top-down approach may produce estimates that better reflect the magnitude and distribution of cost in the general population. However, this approach yields rough approximations of specific cost and is incompatible with the stratification of cost by patient or disease characteristics. The bottom-up approach is used for more precise estimates and allows},
author = {Begley, Charles E and Beghi, †ettore},
file = {::},
pages = {3--9},
publisher = {Blackwell Publishing, Inc},
title = {{The Economic Cost of Epilepsy: A Review of the Literature}},
volume = {43},
year = {1200}
}
@inproceedings{Whittaker2016,
abstract = {Maintaining work focus when on a computer is a major challenge, and people often feel that they use their time ineffectively. To improve focus we designed meTime, a real-time awareness application that shows users how they allocate their time across applications. In two real-world deployments involving 118 participants, we examined whether greater awareness of time use improves focus. In our first deployment, we provided awareness information using meTime, to both office workers and students. Exposure to meTime reduced use of social media, email, browsing and total time online. However increased awareness didn't affect time spent in productivity applications. A second educational deployment largely replicated these results and showed that meTime also reduced users' perceptions of their ability to focus effectively. Changed perceptions were associated with higher class grades. We discuss practical and theoretical implications as well as design principles for use of time applications.},
author = {Whittaker, Steve and Kalnikaite, Vaiva and Hollis, Victoria and Guydish, Andrew},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2858036.2858193},
isbn = {9781450333627},
keywords = {Awareness,Behavior change,Focus,Intervention,Productivity,Task switching,Use of time},
month = {may},
pages = {1729--1738},
publisher = {Association for Computing Machinery},
title = {{'Don't waste my time': Use of time information improves focus}},
year = {2016}
}
@incollection{Amos2009,
abstract = {A number of common forms of dataset shift are introduced, and each$\backslash$nis related to a particular form of causal probabilistic model. Examples$\backslash$nare given for the different types of shift, and some corresponding$\backslash$nmodelling approaches. By characterising dataset shift in this way,$\backslash$nthere is potential for the development of models which capture the$\backslash$nspecific types of variations, combine different modes of variation,$\backslash$nor do model selection to assess whether dataset shift is an issue$\backslash$nin particular circumstances. As an example of how such models can$\backslash$nbe developed, an illustration is provided for one approach to adapting$\backslash$nGaussian process methods for a particular type of dataset shift called$\backslash$nMixture Component Shift.},
author = {Amos, Storkey},
booktitle = {Dataset Shift in Machine Learning},
file = {::},
month = {dec},
pages = {3--28},
publisher = {The MIT Press},
title = {{When Training and Test Sets Are Different: Characterizing Learning Transfer}},
year = {2009}
}
@article{Al-msiebeen2018,
abstract = {Most of open-source software systems become available on the internet today. Thus, we need automatic methods to label software code. Software code can be labeled with a set of keywords. These keywords in this paper referred as software labels. The goal of this paper is to provide a quick view of the software code vocabulary. This paper proposes an automatic approach to document the object-oriented software by labeling its code. The approach exploits all software identifiers to label software code. The paper presents the results of study conducted on the ArgoUML and drawing shapes case studies. Results showed that all code labels were correctly identiﬁed.},
archivePrefix = {arXiv},
arxivId = {1803.00048},
author = {Al-msie'been, Ra'Fat (Mutah University)},
eprint = {1803.00048},
file = {::},
journal = {Sci.Int.(Lahore)},
keywords = {code label,reverse engineering,software comprehension,software engineering,software visualization},
number = {1},
pages = {45--48},
title = {{Automatic Labelling of the Object-Oriented Source Code}},
volume = {30},
year = {2018}
}
@article{Klatt2012,
abstract = {From the very beginning the seizure prediction community faced problems concerning evaluation, standardization, and reproducibility of its studies. One of the main reasons for these shortcomings was the lack of access to high-quality long-term electroencephalography (EEG) data. In this article we present the EPILEPSIAE database, which was made publicly available in 2012. We illustrate its content and scope. The EPILEPSIAE database provides long-term EEG recordings of 275 patients as well as extensive metadata and standardized annotation of the data sets. It will adhere to the current standards in the field of prediction and facilitate reproducibility and comparison of those studies. Beyond seizure prediction, it may also be of considerable benefit for studies focusing on seizure detection, basic neurophysiology, and other fields. {\textcopyright}2012 International League Against Epilepsy.},
author = {Klatt, Juliane and Feldwisch-Drentrup, Hinnerk and Ihle, Matthias and Navarro, Vincent and Neufang, Markus and Teixeira, Cesar and Adam, Claude and Valderrama, Mario and Alvarado-Rojas, Catalina and Witon, Adrien and {Le Van Quyen}, Michel and Sales, Francisco and Dourado, Antonio and Timmer, Jens and Schulze-Bonhage, Andreas and Schelter, Bjoern},
doi = {10.1111/j.1528-1167.2012.03564.x},
file = {::;::},
issn = {00139580},
journal = {Epilepsia},
keywords = {ECoG,Electroencephalogram,Neurophysiological database,Presurgical monitoring,Seizure prediction},
month = {sep},
number = {9},
pages = {1669--1676},
pmid = {22738131},
title = {{The EPILEPSIAE database: An extensive electroencephalography database of epilepsy patients}},
volume = {53},
year = {2012}
}
@incollection{Robertazzi2017,
address = {Cham},
author = {Robertazzi, Thomas G.},
booktitle = {Introduction to Computer Networking},
doi = {10.1007/978-3-319-53103-8_4},
file = {::},
pages = {35--60},
publisher = {Springer International Publishing},
title = {{Wireless Networks}},
url = {http://link.springer.com/10.1007/978-3-319-53103-8{\_}4},
year = {2017}
}
@techreport{Schwartz,
abstract = {Scope In 1997 IEEE defined the 802.11 Wireless LAN (WLAN) standard, intended to allow wireless connection of workstations to their "base" LAN. The original standard targeted the case in which both the workstation and the LAN were owned by the same entity, providing in fact a wireless extension to an existing, wired LAN. While this WLAN application represents a growing niche in the market, the technology on which it is based started to be used also for a new application, that of providing Broadband Wireless Access (BWA) to public networks. We are still connecting workstations to "base" LAN, but this time the "base" LAN is owned by a service provider (ISP, ITSP, ILEC, CLEC), while the workstation is owned by a subscriber. This white paper explains the principles of the FHSS and DSSS radio technologies used in WLAN and BWA applications as well as the advantages and disadvantages of each one of them.},
author = {Schwartz, Sorin M},
file = {::},
title = {{sorin m. schwartz seminars sorin m. schwartz seminars Frequency Hopping Spread Spectrum (FHSS) vs. Direct Sequence Spread Spectrum (DSSS) in Broadband Wireless Access (BWA) and Wireless LAN (WLAN)}}
}
@article{Augustin2016,
abstract = {LoRa is a long-range, low-power, low-bitrate, wireless telecommunications system, promoted as an infrastructure solution for the Internet of Things: end-devices use LoRa across a single wireless hop to communicate to gateway(s), connected to the Internet and which act as transparent bridges and relay messages between these end-devices and a central network server. This paper provides an overview of LoRa and an in-depth analysis of its functional components. The physical and data link layer performance is evaluated by field tests and simulations. Based on the analysis and evaluations, some possible solutions for performance enhancements are proposed.},
author = {Augustin, Alo{\"{y}}s and Yi, Jiazi and Clausen, Thomas and Townsley, William},
doi = {10.3390/s16091466},
file = {::},
issn = {1424-8220},
journal = {Sensors},
keywords = {Internet of things,LoRa,Long range,Low power},
month = {sep},
number = {9},
pages = {1466},
publisher = {MDPI AG},
title = {{A Study of LoRa: Long Range {\&} Low Power Networks for the Internet of Things}},
url = {http://www.mdpi.com/1424-8220/16/9/1466},
volume = {16},
year = {2016}
}
@inproceedings{Khalajzadeh2020a,
author = {Khalajzadeh, Hourieh and Simmons, Andrew J. and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang},
booktitle = {Evaluation of Novel Approaches to Software Engineering (ENASE)},
doi = {10.5220/0009192900150026},
file = {::},
keywords = {big data analytics,big data modeling,big data toolkits,domain specific visual languages,end-user},
title = {{Visual Languages for Supporting Big Data Analytics Development}},
year = {2020}
}
@article{Adadi2018,
abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
annote = {Forth industrial revolution - fast {\&} widespread AI
Shift towards more algorithmic society

AI - Lack of transparency, powerful predictions, cannot be directly explained, improve trust and transparency

Monetary value of AI - Increase in global investment on AI (12 billion USD in 2017 to 52.2 trillion USD in 2021)
AI Market growth (480 billion USD in 2017 to 2.59 trillion USD in 2021)

Reasons for XAI - Explain to justify, explain to control, explain to improve, explain to discover

XAI Application domains - transportation, healthcare, legal, finance, military

The technical challenge question in enabling XAI - Why the use of XAI is not systematic ?
Why is not everyone using XAI?

Four basic research areas - Data science, AI/ML, Human science, Human Computer Interface (HCI)

Six major academic databases - SCOPUS, IEEExplore, ACM Digital Library, Google Scholar, Citeseer Library, ScienceDirect

More complex the model, it is more dificult to interpret.
Scoop related methods - understanding the entire model behavior/understanding a single prediction, broadly categorised into global interpretability and local interpretability methods

Model related methods - visualisation (surrogate models, Partial Dependence Plot (PDP), Individual Conditional Expectation (ICE)), knowledge extraction, influence methods, example based explanation

Surrogate models - a simple model used to explain complex models
Knowledge extraction - rule extraction (pedogogical, decompositional, ecelectic), model distillation
Influence methods - sensitivity analysis, layer wise relevance propagation, feature importance},
author = {Adadi, Amina and Berrada, Mohammed},
doi = {10.1109/ACCESS.2018.2870052},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Explainable artificial intelligence,black-box models,interpretable machine learning},
pages = {52138--52160},
publisher = {IEEE},
title = {{Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}},
volume = {6},
year = {2018}
}
@article{Knight2018,
abstract = {A core concern in learning is coming to understand the ways in which claims of knowledge are made. The epistemic cognition literature typically characterises this learning in terms of how learners cognitively conceptualise the source and nature of knowledge. Recent work has offered alternative accounts of epistemic cognition that recognise the discursive nature of the construct. These accounts are derived from analysis of the ways that learners talk about knowledge in tasks such as evaluating scientific claims from sources of varying qualities. In this paper we draw on this recent work to advance a novel approach to the analysis of discourse data in epistemic contexts. This approach is exemplified through its application to an existing dataset, demonstrating both the application of the approach and the particular kinds of discourse that learners engaged in. This discursive approach has the potential for broad application in the learning sciences' treatment of epistemic cognition.},
author = {Knight, Simon and Littleton, Karen},
doi = {10.1016/j.lcsi.2017.11.003},
issn = {2210657X},
journal = {Learning, Culture and Social Interaction},
keywords = {Collaborative learning,Dialogue,Discursive psychology,Epistemic cognition,Epistemological beliefs,Sociocultural discourse analysis,Sociocultural theory},
month = {mar},
pages = {55--69},
publisher = {Elsevier Ltd},
title = {{A discursive approach to the analysis of epistemic cognition}},
volume = {16},
year = {2018}
}
@misc{Siddiqui2020,
abstract = {Epilepsy is a serious chronic neurological disorder, can be detected by analyzing the brain signals produced by brain neurons. Neurons are connected to each other in a complex way to communicate with human organs and generate signals. The monitoring of these brain signals is commonly done using Electroencephalogram (EEG) and Electrocorticography (ECoG) media. These signals are complex, noisy, non-linear, non-stationary and produce a high volume of data. Hence, the detection of seizures and discovery of the brain-related knowledge is a challenging task. Machine learning classifiers are able to classify EEG data and detect seizures along with revealing relevant sensible patterns without compromising performance. As such, various researchers have developed number of approaches to seizure detection using machine learning classifiers and statistical features. The main challenges are selecting appropriate classifiers and features. The aim of this paper is to present an overview of the wide varieties of these techniques over the last few years based on the taxonomy of statistical features and machine learning classifiers—‘black-box' and ‘non-black-box'. The presented state-of-the-art methods and ideas will give a detailed understanding about seizure detection and classification, and research directions in the future.},
author = {Siddiqui, Mohammad Khubeb and Morales-Menendez, Ruben and Huang, Xiaodi and Hussain, Nasir},
booktitle = {Brain Informatics},
doi = {10.1186/s40708-020-00105-1},
file = {::},
issn = {21984026},
keywords = {Applications of machine learning on epilepsy,Black-box and non-black-box classifiers,EEG signals,Epilepsy,Seizure detection,Seizure localization,Statistical features},
month = {dec},
number = {1},
publisher = {Springer},
title = {{A review of epileptic seizure detection using machine learning classifiers}},
volume = {7},
year = {2020}
}
@book{yeung_book,
editor = {Yeung, K. and Lodge, M.},
publisher = {Oxford University Press},
title = {{Algorithmic Regulation}},
year = {2019}
}
@article{Wang2015,
abstract = {Online class imbalance learning is a new learning problem that combines the challenges of both online learning and class imbalance learning. It deals with data streams having very skewed class distributions. This type of problems commonly exists in real-world applications, such as fault diagnosis of real-time control monitoring systems and intrusion detection in computer networks. In our earlier work, we defined class imbalance online, and proposed two learning algorithms OOB and UOB that build an ensemble model overcoming class imbalance in real time through resampling and time-decayed metrics. In this paper, we further improve the resampling strategy inside OOB and UOB, and look into their performance in both static and dynamic data streams. We give the first comprehensive analysis of class imbalance in data streams, in terms of data distributions, imbalance rates and changes in class imbalance status. We find that UOB is better at recognizing minority-class examples in static data streams, and OOB is more robust against dynamic changes in class imbalance status. The data distribution is a major factor affecting their performance. Based on the insight gained, we then propose two new ensemble methods that maintain both OOB and UOB with adaptive weights for final predictions, called WEOB1 and WEOB2. They are shown to possess the strength of OOB and UOB with good accuracy and robustness.},
author = {Wang, Shuo and Minku, Leandro L. and Yao, Xin},
doi = {10.1109/TKDE.2014.2345380},
file = {::},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Bagging,Class imbalance,ensemble learning,online learning,resampling},
number = {5},
pages = {1356--1368},
publisher = {IEEE},
title = {{Resampling-based ensemble methods for online class imbalance learning}},
volume = {27},
year = {2015}
}
@article{Jahncke2017,
abstract = {The aims of this questionnaire study were to describe the occurrence and desired number of alternations between mental and physical tasks in industrial and non-industrial blue-collar work, and determine to which extent selected personal and occupational factors influence these conditions. On average, the 122 participating workers (55 females) reported to have close to four alternations per day between mental and physical tasks, and to desire more alternations than they actually had. They also expressed a general preference for performing a physical task after a mental task and vice versa. In univariate regression models, the desired change in task alternations was significantly associated with gender, age, occupation, years with current work tasks and perceived job control, while occupation was the only significant determinant in a multiple regression model including all factors. Our results suggest that alternations between productive physical and mental tasks could be a viable option in future job rotation. Practitioner Summary: We addressed attitudes among blue-collar workers to alternations between physically and mentally demanding tasks. More alternations were desired than those occurring in the job, and workers preferred performing a physical task after a mental and vice versa. Alternating physical and mental tasks could, thus, be a viable option in job rotation.},
author = {Jahncke, Helena and Hygge, Staffan and Mathiassen, Svend Erik and Hallman, David and Mixter, Susanna and Lyskov, Eugene},
doi = {10.1080/00140139.2017.1282630},
file = {::},
issn = {0014-0139},
journal = {Ergonomics},
keywords = {Cognitive task,job rotation,pause,physical variation,repetitive work},
month = {sep},
number = {9},
pages = {1218--1227},
publisher = {Taylor and Francis Ltd.},
title = {{Variation at work: alternations between physically and mentally demanding tasks in blue-collar occupations}},
url = {https://www.tandfonline.com/doi/full/10.1080/00140139.2017.1282630},
volume = {60},
year = {2017}
}
@article{tan2018introduction,
author = {Tan, Pang-Ning and Steinbach, Michael and Karpatne, Anuj and Kumar, Vipin},
publisher = {Pearson},
title = {{Introduction to Data Mining}},
year = {2018}
}
@incollection{Naidu2019,
abstract = {The 4G/5G/fast growing world wireless technology plays a vital role as wired connections nowhere using because of its installation complexity. Wireless accessing of networks, wireless services, and IoT controlling are the current trend in technology for providing quickness, easiness, mobility, feasibility, and/or flexibility. Apart from that, other merits include wireless networks that can dynamically from the network in the real world and it is easy deployment than wired. Wireless protocols such as Bluetooth (IEEE 802.15.1), ZigBee (IEEE 802.15.4), Wi-Fi (Wireless Fidelity) (IEEE 802.11), and Z-Wave (proprietary-based standard) are widely used technologies, and Wi-Fi SON (Wi-Fi Self-organizing/Optimizing network) is a newly raising technology. Wi-Fi SON is an automated intelligent network. In this research paper, we are describing the wireless technologies importance, features, working, and comparison among them and mainly focused on Self-organizing/Optimization Networks (SONs); one such network is Wi-Fi SON.},
author = {Naidu, Gollu Appala and Kumar, Jayendra},
booktitle = {Lecture Notes in Networks and Systems},
doi = {10.1007/978-981-13-3765-9_24},
file = {::},
issn = {23673389},
keywords = {Bluetooth,SON,Wi-Fi,Wi-Fi SON,Wireless protocols,Z-Wave,ZigBee},
pages = {229--239},
publisher = {Springer},
title = {{Wireless Protocols: Wi-Fi SON, Bluetooth, ZigBee, Z-Wave, and Wi-Fi}},
volume = {65},
year = {2019}
}
@book{Bourque2004,
abstract = {The purpose of the Guide to the Software Engineering Body of Knowledge is to provide a validated classification of the bounds of the software engineering discipline and topical access that will support this discipline. The Body of Knowledge is subdivided into ten software engineering Knowledge Areas (KA) that differentiate among the various important concepts, allowing readers to find their way quickly to subjects of interest. Upon finding a subject, readers are referred to key papers or book chapters. Emphases on engineering practice lead the Guide toward a strong relationship with the normative literature. The normative literature is validated by consensus formed among practitioners and is concentrated in standards and related documents. The two major standards bodies for software engineering (IEEE Computer Society Software and Systems Engineering Standards Committee and ISO/IEC JTC1/SC7) are represented in the project. The Guide is oriented toward a variety of audiences, all over the world. It aims to serve public and private organizations in need of a consistent view of software engineering for defining education and training requirements, classifying jobs, developing performance evaluation policies or specifying software development tasks. It also addresses practicing, or managing, software engineers and the officials responsible for making public policy regarding licensing and professional guidelines. In addition, professional societies and educators defining the certification rules, accreditation policies for university curricula, and guidelines for professional practice will benefit from the SWEBOK Guide, as well as the students learning the software engineering profession and educators and trainers engaged in defining curricula and course content. It is hoped that readers will find this book useful in guiding them toward the knowledge and resources they need in their lifelong career development as software engineering professionals. The current Guide marks the end of the Ironman period by providing a Guide that has achieved consensus through broad review and trial application. The overall goal of the current revision is to improve the readability, consistency, and usability of the Guide. In several cases, the topical breakdown of a KA was rearranged to make it more usable. The reference list is updated so that it will be easier to obtain the references. Trial usage resulted in the recommendation that three KAs should be rewritten. Finally, some KAs were revised to remove material duplicating that of other KAs.},
author = {Bourque, P and Dupuis, R},
booktitle = {SWEBOK 2004 Guide to the Software Engineering Body of Knowledge},
doi = {10.1109/SESS.1999.767664},
file = {::},
isbn = {0769523307},
issn = {07407459},
number = {1},
pages = {204},
pmid = {13861254},
title = {{Guide to the Software Engineering Body of Knowledge 2004 Version}},
url = {http://www.computer.org/portal/web/swebok/html/copyright},
volume = {1},
year = {2004}
}
@article{Gneiting2007,
abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distribution F if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage. {\textcopyright}2007 American Statistical Association.},
author = {Gneiting, Tilmann and Raftery, Adrian E.},
doi = {10.1198/016214506000001437},
file = {::},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Bregman divergence,Brier score,Coherent,Continuous ranked probability score,Cross-validation,Entropy,Kernel score,Loss function,Minimum contrast estimation,Negative definite function},
number = {477},
pages = {359--378},
title = {{Strictly proper scoring rules, prediction, and estimation}},
volume = {102},
year = {2007}
}
@article{Manski2019,
abstract = {A central objective of empirical research on treatment response is to inform treatment choice. Unfortunately, researchers commonly use concepts of statistical inference whose foundations are distant from the problem of treatment choice. It has been particularly common to use hypothesis tests to compare treatments. Wald's development of statistical decision theory provides a coherent frequentist framework for use of sample data on treatment response to make treatment decisions. A body of recent research applies statistical decision theory to characterize uniformly satisfactory treatment choices, in the sense of maximum loss relative to optimal decisions (also known as maximum regret). This article describes the basic ideas and findings, which provide an appealing practical alternative to use of hypothesis tests. For simplicity, the article focuses on medical treatment with evidence from classical randomized clinical trials. The ideas apply generally, encompassing use of observational data and treatment choice in nonmedical contexts.},
author = {Manski, Charles F.},
doi = {10.1080/00031305.2018.1513377},
file = {::},
issn = {15372731},
journal = {American Statistician},
keywords = {Analysis of treatment response,Medical decisions,Minimax regret,Randomized clinical trials},
number = {sup1},
pages = {296--304},
title = {{Treatment Choice With Trial Data: Statistical Decision Theory Should Supplant Hypothesis Testing}},
volume = {73},
year = {2019}
}
@article{Papandreou2009,
abstract = {The aim of this study was to examine the effect of a polyphenol-rich extract (PrB) of Vaccinium angustifolium (wild blueberries) introduced intraperitonealy (i.p.) at 30 (PrB30) and 60 (PrB60) mg/kg body weight for 7 days, on cognitive performance, brain oxidative status and acetylcholinesterase activity in adult, male, 3-4-month-old Balb-c mice. Evaluation of rodent learning and memory was assessed by a step-through test on day 6 after a double training and an initial acquisition trial on day 5. Antioxidant status was determined by ferric reducing antioxidant power (FRAP), ascorbic acid concentration (FRASC), malondialdehyde and reduced glutathione levels in whole brain homogenates. Acetylcholinesterase (AChE) activity was determined by Ellman's colorimetric method. Results showed that the PrB60-treated mice exhibited a significant improvement in learning and memory (step-through latency time of 228 ± 38 s compared to 101 ± 32 s of the control group). PrB extract administration also resulted in reduced lipid peroxidation products (38 and 79{\%}) and higher brain ascorbic acid levels (21 and 64{\%}) in both PrB30 and PrB60-treated groups, respectively, and higher glutathione levels (28{\%}) in the PrB60-treated group. Furthermore, salt- and detergent soluble AChE activity significantly decreased in both PrB-treated groups. Thus, the significant cognitive enhancement observed in adult mice after short-term i.p. supplementation with the blueberry extract concentrated in polyphenols, is closely related to higher brain antioxidant properties and inhibition of AChE activity. These findings stress the critical impact of wild blueberry bioactive components on brain function. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Papandreou, Magdalini A. and Dimakopoulou, Andriana and Linardaki, Zacharoula I. and Cordopatis, Paul and Klimis-Zacas, Dorothy and Margarity, Marigoula and Lamari, Fotini N.},
doi = {10.1016/j.bbr.2008.11.013},
issn = {01664328},
journal = {Behavioural Brain Research},
keywords = {Anthocyanins,Anti-cholinesterase activity,Antioxidants,Blueberries,Cognition,Vaccinium angustifolium},
month = {mar},
number = {2},
pages = {352--358},
publisher = {Elsevier},
title = {{Effect of a polyphenol-rich wild blueberry extract on cognitive performance of mice, brain antioxidant markers and acetylcholinesterase activity}},
volume = {198},
year = {2009}
}
@article{zou2019does,
author = {Zou, Weiqin and Xuan, Jifeng and Xie, Xiaoyuan and Chen, Zhenyu and Xu, Baowen},
journal = {Empirical Software Engineering},
number = {6},
pages = {3871--3903},
publisher = {Springer},
title = {{How does code style inconsistency affect pull request integration? An exploratory study on 117 GitHub projects}},
volume = {24},
year = {2019}
}
@article{Spasic2020,
abstract = {Background: Clinical narratives represent the main form of communication within health care, providing a personalized account of patient history and assessments, and offering rich information for clinical decision making. Natural language processing (NLP) has repeatedly demonstrated its feasibility to unlock evidence buried in clinical narratives. Machine learning can facilitate rapid development of NLP tools by leveraging large amounts of text data. Objective: The main aim of this study was to provide systematic evidence on the properties of text data used to train machine learning approaches to clinical NLP. We also investigated the types of NLP tasks that have been supported by machine learning and how they can be applied in clinical practice. Methods: Our methodology was based on the guidelines for performing systematic reviews. In August 2018, we used PubMed, a multifaceted interface, to perform a literature search against MEDLINE. We identified 110 relevant studies and extracted information about text data used to support machine learning, NLP tasks supported, and their clinical applications. The data properties considered included their size, provenance, collection methods, annotation, and any relevant statistics. Results: The majority of datasets used to train machine learning models included only hundreds or thousands of documents. Only 10 studies used tens of thousands of documents, with a handful of studies utilizing more. Relatively small datasets were utilized for training even when much larger datasets were available. The main reason for such poor data utilization is the annotation bottleneck faced by supervised machine learning algorithms. Active learning was explored to iteratively sample a subset of data for manual annotation as a strategy for minimizing the annotation effort while maximizing the predictive performance of the model. Supervised learning was successfully used where clinical codes integrated with free-text notes into electronic health records were utilized as class labels. Similarly, distant supervision was used to utilize an existing knowledge base to automatically annotate raw text. Where manual annotation was unavoidable, crowdsourcing was explored, but it remains unsuitable because of the sensitive nature of data considered. Besides the small volume, training data were typically sourced from a small number of institutions, thus offering no hard evidence about the transferability of machine learning models. The majority of studies focused on text classification. Most commonly, the classification results were used to support phenotyping, prognosis, care improvement, resource management, and surveillance. Conclusions: We identified the data annotation bottleneck as one of the key obstacles to machine learning approaches in clinical NLP. Active learning and distant supervision were explored as a way of saving the annotation efforts. Future research in this field would benefit from alternatives such as data augmentation and transfer learning, or unsupervised learning, which do not require data annotation.},
author = {Spasic, Irena and Nenadic, Goran},
doi = {10.2196/17984},
file = {::},
issn = {14388871},
journal = {Journal of Medical Internet Research},
keywords = {Machine learning,Medical informatics,Medical informatics applications,Natural language processing},
number = {3},
title = {{Clinical text data in machine learning: Systematic review}},
volume = {22},
year = {2020}
}
@article{Vaicenavicius2019,
abstract = {Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safety-critical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.},
archivePrefix = {arXiv},
arxivId = {1902.06977},
author = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"{o}}n, Thomas B.},
eprint = {1902.06977},
file = {::},
title = {{Evaluating model calibration in classification}},
url = {http://arxiv.org/abs/1902.06977},
volume = {89},
year = {2019}
}
@article{Sutton2020a,
abstract = {Computerized clinical decision support systems, or CDSS, represent a paradigm shift in healthcare today. CDSS are used to augment clinicians in their complex decision-making processes. Since their first use in the 1980s, CDSS have seen a rapid evolution. They are now commonly administered through electronic medical records and other computerized clinical workflows, which has been facilitated by increasing global adoption of electronic medical records with advanced capabilities. Despite these advances, there remain unknowns regarding the effect CDSS have on the providers who use them, patient outcomes, and costs. There have been numerous published examples in the past decade(s) of CDSS success stories, but notable setbacks have also shown us that CDSS are not without risks. In this paper, we provide a state-of-the-art overview on the use of clinical decision support systems in medicine, including the different types, current use cases with proven efficacy, common pitfalls, and potential harms. We conclude with evidence-based recommendations for minimizing risk in CDSS design, implementation, evaluation, and maintenance.},
author = {Sutton, Reed T and Pincock, David and Baumgart, Daniel C and Sadowski, Daniel C and Fedorak, Richard N and Kroeker, Karen I},
doi = {10.1038/s41746-020-0221-y},
issn = {2398-6352},
journal = {npj Digital Medicine},
number = {1},
pages = {17},
title = {{An overview of clinical decision support systems: benefits, risks, and strategies for success}},
url = {https://doi.org/10.1038/s41746-020-0221-y},
volume = {3},
year = {2020}
}
@article{Dudley2018,
author = {Dudley, John J and Kristensson, Per Ola and Kingdom, United},
file = {::},
journal = {ACM Trans. Interact. Intell. Syst.},
number = {2},
title = {{A Review of User Interface Design for Interactive Machine Learning.}},
volume = {8},
year = {2018}
}
@article{Molleri2020,
abstract = {Context: Over the past decade Software Engineering research has seen a steady increase in survey-based studies, and there are several guidelines providing support for those willing to carry out surveys. The need for auditing survey research has been raised in the literature. Checklists have been used both to conduct and to assess different types of empirical studies, such as experiments and case studies. Objective: To operationalize the assessment of survey studies by means of a checklist. To fulfill such goal, we aim to derive a checklist from standards for survey research and further evaluate the appropriateness of the checklist in the context of software engineering research. Method: We systematically aggregated knowledge from 12 methodological studies supporting survey-based research in software engineering. We identified the key stages of the survey process and its recommended practices through thematic analysis and vote counting. We evaluated the checklist by applying it to existing surveys and analyzed the results. Thereafter, we gathered the feedback of experts (the surveys' authors) on our analysis and used the feedback to improve the survey checklist. Results: The evaluation provided insights regarding limitations of the checklist in relation to its understanding and objectivity. In particular, 19 of the 38 checklist items were improved according to the feedback received from experts. Conclusion: The proposed checklist is appropriate for auditing survey reports as well as a support tool to guide ongoing research with regard to the survey design process. A discussion on how to use the checklist and what its implications are for research practice is also provided.},
archivePrefix = {arXiv},
arxivId = {1901.09850},
author = {Moll{\'{e}}ri, Jefferson Seide and Petersen, Kai and Mendes, Emilia},
doi = {10.1016/j.infsof.2019.106240},
eprint = {1901.09850},
file = {::},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Assessment,Checklist,Methodology,Survey},
pages = {1--33},
title = {{An empirically evaluated checklist for surveys in software engineering}},
volume = {119},
year = {2020}
}
@misc{Vagia2016,
abstract = {In this paper we present a literature review of the evolution of the levels of autonomy from the end of the 1950s up until now. The motivation of this study was primarily to gather and to compare the literature that exists, on taxonomies on levels of automation. Technical developments within both computer hardware and software have made it possible to introduce autonomy into virtually all aspects of human-machine systems. The current study, is focusing on how different authors treat the problem of different levels of automation. The outcome of this study is to present the differences between the proposed levels of automation and the various taxonomies, giving the potential users a number of choices in order to decide which taxonomy satisfies their needs better. In addition, this paper surveys deals with the term adaptive automation, which seems to be a new trend in the literature on autonomy.},
author = {Vagia, Marialena and Transeth, Aksel A. and Fjerdingen, Sigurd A.},
booktitle = {Applied Ergonomics},
doi = {10.1016/j.apergo.2015.09.013},
file = {::},
issn = {18729126},
keywords = {Adaptive automation,Autonomy/automation,Levels of autonomy,Taxonomies},
pmid = {26467193},
title = {{A literature review on the levels of automation during the years. What are the different taxonomies that have been proposed?}},
year = {2016}
}
@article{Nielsen2017,
abstract = {Organisations are becoming increasingly aware of the importance of employees in gaining and maintaining competitive advantage. The happy worker–productive worker thesis suggests that workers who experience high levels of well-being also perform well and vice versa; however, organisations need to know how to ensure such happy and productive workers. The present review and meta-analysis identifies workplace resources at the individual, the group, the leader, and the organisational levels that are related to both employee well-being and organisational performance. We examine which types of resources are most important in predicting both employee well-being and performance. We identified 84 quantitative studies published in print and online from 2003 to November 2015. Resources at either of the four levels were related to both employee well-being and performance. We found no significant differences in employee well-being and organisational performance between the four levels of workplace resources, suggesting that interventions may focus on any of these levels. Cross-sectional studies showed stronger relationships with well-being and performance than longitudinal studies. Studies using objective performance ratings provided weaker relationships between resources and performance than self-rated and leader/third-party-rated studies.},
author = {Nielsen, Karina and Nielsen, Morten B. and Ogbonnaya, Chidiebere and K{\"{a}}ns{\"{a}}l{\"{a}}, Marja and Saari, Eveliina and Isaksson, Kerstin},
doi = {10.1080/02678373.2017.1304463},
file = {::},
issn = {0267-8373},
journal = {Work {\&} Stress},
keywords = {Literature review,meta-analysis,performance,resources,well-being},
month = {apr},
number = {2},
pages = {101--120},
publisher = {Routledge},
title = {{Workplace resources to improve both employee well-being and performance: A systematic review and meta-analysis}},
url = {https://www.tandfonline.com/doi/full/10.1080/02678373.2017.1304463},
volume = {31},
year = {2017}
}
@phdthesis{Shoeb2009,
author = {Shoeb, Ali Hossam},
file = {::},
title = {{Application of Machine Learning to Epileptic Seizure Onset Detection and Treatment}},
year = {2009}
}
@article{Rondina2018,
abstract = {Aim of the paper is to present the methodology used to develop a new process, based on high-pressure resin transfer molding (HP-RTM), to produce full carbon reinforced plastic rims for sports car. These components are nowadays made by pre-preg autoclave processing, which is expensive and time-consuming, so that they are basically sold as aftermarket option. The proposed technology allows a manufacturing volume high enough to be suitable for series production. In this paper, the resin and fiber selection criteria and tests are shown, as regards the mechanical properties, durability and injection strategies.},
author = {Rondina, Francesco and Taddia, Sara and Mazzocchetti, Laura and Donati, Lorenzo and Minak, Giangiacomo and Rosenberg, Philipp and Bedeschi, Andrea and Dolcini, Enrico},
doi = {10.1016/j.compstruct.2018.02.083},
file = {::},
issn = {02638223},
journal = {Composite Structures},
number = {March},
pages = {368--378},
publisher = {Elsevier},
title = {{Development of full carbon wheels for sport cars with high-volume technology}},
url = {https://doi.org/10.1016/j.compstruct.2018.02.083},
volume = {192},
year = {2018}
}
@book{fenton2014software,
author = {Fenton, Norman and Bieman, James},
publisher = {CRC press},
title = {{Software metrics: a rigorous and practical approach}},
year = {2014}
}
@article{Smith2015,
abstract = {Background: There is a growing body of research into the total amount and patterns of sitting, standing and stepping in office-based workers and few studies using objectively measured sitting and standing. Understanding these patterns may identify daily times opportune for interventions to displace sitting with activity. Methods: A sample of office-based workers (n∈=∈164) residing in England were fitted with thigh-worn ActivPal accelerometers and devices were worn 24 hours a day for five consecutive days, always including Saturday and Sunday and during bathing and sleeping. Daily amounts and patterns of time spent sitting, standing, stepping and step counts and frequency of sit/stand transitions, recorded by the ActivPal accelerometer, were reported. Results: Total sitting/standing time was similar on weekdays (10.6/4.1 hrs) and weekends (10.6/4.3 hrs). Total step count was also similar over weekdays (9682∈±∈3872) and weekends (9518∈±∈4615). The highest physical activity levels during weekdays were accrued at 0700 to 0900, 1200 to 1400, and 1700 to 1900; and during the weekend at 1000 to 1700. During the weekday the greatest amount of sitting was accrued at 0900 to 1200, 1400 to 1700, and 2000 to 2300, and on the weekend between 1800 and 2300. During the weekday the greatest amount of standing was accrued between 0700 and 1000 and 1700 and 2100, and on the weekend between 1000 and 1800. On the weekday the highest number of sit/stand transitions occurred between 0800 to 0900 and remained consistently high until 1800. On the weekend, the highest number occurred between 1000 to 1400 and 1900 to 2000. Conclusion: Office based-workers demonstrate high levels of sitting during both the working week and weekend. Interventions that target the working day and the evenings (weekday and weekend) to displace sitting with activity may offer most promise for reducing population levels of sedentary behaviour and increasing physical activity levels, in office-based workers residing in England.},
author = {Smith, Lee and Hamer, Mark and Ucci, Marcella and Marmot, Alexi and Gardner, Benjamin and Sawyer, Alexia and Wardle, Jane and Fisher, Abigail},
doi = {10.1186/s12889-014-1338-1},
file = {::},
issn = {14712458},
journal = {BMC Public Health},
keywords = {Levels,Office workers,Patterns,Physical activity,Sitting,Standing},
month = {dec},
number = {1},
pages = {1--9},
pmid = {25595219},
publisher = {BioMed Central Ltd.},
title = {{Weekday and weekend patterns of objectively measured sitting, standing, and stepping in a sample of office-based workers: The active buildings study}},
volume = {15},
year = {2015}
}
@article{Siegrist2016,
abstract = {Mainstream psychological stress theory claims that it is important to include information on people's ways of coping with work stress when assessing the impact of stressful psychosocial work environments on health. Yet, some widely used respective theoretical models focus exclusively on extrinsic factors. The model of effort-reward imbalance (ERI) differs from them as it explicitly combines information on extrinsic and intrinsic factors in studying workers' health. As a growing number of studies used the ERI model in recent past, we conducted a systematic review of available evidence, with a special focus on the distinct contribution of its intrinsic component, the coping pattern “over-commitment”, towards explaining health. Moreover, we explore whether the interaction of intrinsic and extrinsic components exceeds the size of effects on health attributable to single components. Results based on 51 reports document an independent explanatory role of “over-commitment” in explaining workers' health in a majority of studies. However, support in favour of the interaction hypothesis is limited and requires further exploration. In conclusion, the findings of this review support the usefulness of a work stress model that combines extrinsic and intrinsic components in terms of scientific explanation and of designing more comprehensive worksite stress prevention programs.},
author = {Siegrist, Johannes and Li, Jian},
doi = {10.3390/ijerph13040432},
file = {::},
issn = {1660-4601},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Effort-reward imbalance,Health measures,Over-commitment,Systematic review,Work stress models},
month = {apr},
number = {4},
pages = {432},
publisher = {MDPI AG},
title = {{Associations of Extrinsic and Intrinsic Components of Work Stress with Health: A Systematic Review of Evidence on the Effort-Reward Imbalance Model}},
url = {http://www.mdpi.com/1660-4601/13/4/432},
volume = {13},
year = {2016}
}
@book{Slocum2017,
author = {Slocum, Walter L.},
booktitle = {Occupational Careers},
doi = {10.4324/9781315125459},
month = {sep},
publisher = {Routledge},
title = {{Occupational Careers}},
year = {2017}
}
@article{Kim2018,
abstract = {The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.},
author = {Kim, Miryung and Zimmermann, Thomas and Deline, Robert and Begel, Andrew},
doi = {10.1109/TSE.2017.2754374},
file = {::},
isbn = {9781450356381},
issn = {19393520},
journal = {IEEE Transactions on Software Engineering},
keywords = {Data science,Development roles,Industry,Software engineering},
month = {nov},
number = {11},
pages = {1024--1038},
title = {{Data scientists in software teams: State of the art and challenges}},
url = {https://ieeexplore.ieee.org/document/8046093/},
volume = {44},
year = {2018}
}
@article{Koenzen2020,
abstract = {Duplicating one's own code makes it faster to write software. This expediency is particularly valuable for users of computational notebooks. Duplication allows notebook users to quickly test hypotheses and iterate over data. In this paper, we explore how much, how and from where code duplication occurs in computational notebooks, and identify potential barriers to code reuse. Previous work in the area of computational notebooks describes developers' motivations for reuse and duplication but does not show how much reuse occurs or which barriers they face when reusing code. To address this gap, we first analyzed GitHub repositories for code duplicates contained in a repository's Jupyter notebooks, and then conducted an observational user study of code reuse, where participants solved specific tasks using notebooks. Our findings reveal that repositories in our sample have a mean self-duplication rate of 7.6{\%}. However, in our user study, few participants duplicated their own code, preferring to reuse code from online sources.},
annote = {Software engineering does not promote duplicating code. But, Jupyter notebook users take use of it as they do exploratory programming. There's no mention that Jupyter notebook users are data scientists. Researchers appreciate usage of modules, else unaware that where the code originates from. Research focuses on how much code is being reused and what barriers do they have.
The scope is restricted towards code cells only. (Similar to ours) Trying to find relevant code snippets is a pain point for data scientist. Levensthein distance is a metric that is used for computational analysis.},
archivePrefix = {arXiv},
arxivId = {2005.13709},
author = {Koenzen, Andreas and Ernst, Neil and Storey, Margaret-Anne},
eprint = {2005.13709},
file = {::},
isbn = {9781728169019},
title = {{Code Duplication and Reuse in Jupyter Notebooks}},
url = {http://arxiv.org/abs/2005.13709},
year = {2020}
}
@article{Charles2017,
abstract = {We establish novel generalization bounds for learning algorithms that converge to global minima. We do so by deriving black-box stability results that only depend on the convergence of a learning algorithm and the geometry around the minimizers of the loss function. The results are shown for nonconvex loss functions satisfying the Polyak-{\{}{\$}\backslash{\$}L{\}}ojasiewicz (PL) and the quadratic growth (QG) conditions. We further show that these conditions arise for some neural networks with linear activations. We use our black-box results to establish the stability of optimization algorithms such as stochastic gradient descent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and the stochastic variance reduced gradient method (SVRG), in both the PL and the strongly convex setting. Our results match or improve state-of-the-art generalization bounds and can easily be extended to similar optimization algorithms. Finally, we show that although our results imply comparable stability for SGD and GD in the PL setting, there exist simple neural networks with multiple local minima where SGD is stable but GD is not.},
archivePrefix = {arXiv},
arxivId = {1710.08402},
author = {Charles, Zachary and Papailiopoulos, Dimitris},
eprint = {1710.08402},
file = {::},
issn = {1938-7228},
title = {{Stability and Generalization of Learning Algorithms that Converge to Global Optima}},
year = {2017}
}
@inproceedings{Garg2020,
abstract = {Label shift describes the setting where although the label distribution might change between the source and target domains, the class-conditional probabilities (of data given a label) do not. There are two dominant approaches for estimating the label marginal. BBSE, a moment-matching approach based on confusion matrices, is provably consistent and provides interpretable error bounds. However, a maximum likelihood estimation approach, which we call MLLS, dominates empirically. In this paper, we present a unified view of the two methods and the first theoretical characterization of the likelihoodbased estimator. Our contributions include (i) conditions for consistency of MLLS, which include calibration of the classifier and a confusion matrix invertibility condition that BBSE also requires; (ii) a unified view of the methods, casting the confusion matrix as roughly equivalent to MLLS for a particular choice of calibration method; and (iii) a decomposition of MLLS's finite-sample error into terms reflecting the impacts of miscalibration and estimation error. Our analysis attributes BBSE's statistical inefficiency to a loss of information due to coarse calibration. We support our findings with experiments on both synthetic data and the MNIST and CIFAR10 image recognition datasets.},
archivePrefix = {arXiv},
arxivId = {2003.07554},
author = {Garg, Saurabh and Wu, Yifan and Balakrishnan, Sivaraman and Lipton, Zachary C.},
booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
eprint = {2003.07554},
file = {::},
title = {{A unified view of label shift estimation}},
year = {2020}
}
@article{Haiduc2010,
abstract = {One of the main challenges faced by today's developers is keeping up with the staggering amount of source code that needs to be read and understood. In order to help developers with this problem and reduce the costs associated with it, one solution is to use simple textual descriptions of source code entities that developers can grasp easily, while capturing the code semantics precisely. We propose an approach to automatically determine such descriptions, based on automated text summarization technology.},
author = {Haiduc, Sonia and Aponte, Jairo and Marcus, Andrian},
doi = {10.1145/1810295.1810335},
file = {::},
isbn = {9781605587196},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {program comprehension,summary,text summarization},
pages = {223--226},
title = {{Supporting program comprehension with source code summarization}},
volume = {2},
year = {2010}
}
@article{VanAllen2008,
abstract = {A Bayesian belief network models a joint distribution over variables using a DAG to represent variable dependencies and network parameters to represent the conditional probability of each variable given an assignment to its immediate parents. Existing algorithms assume each network parameter is fixed. From a Bayesian perspective, however, these network parameters can be random variables that reflect uncertainty in parameter estimates, arising because the parameters are learned from data, or because they are elicited from uncertain experts. Belief networks are commonly used to compute responses to queries—i.e., return a number for P(H=h|E=e). Parameter uncertainty induces uncertainty in query responses, which are thus themselves random variables. This paper investigates this query response distribution, and shows how to accurately model this distribution for any query and any network structure. In particular, we prove that the query response is asymptotically Gaussian and provide its mean value and asymptotic variance. Moreover, we present an algorithm for computing these quantities that has the same worst-case complexity as inference in general, and also describe straight-line code when the query includes all n variables. We provide empirical evidence that (1) our approximation of the variance is very accurate, and (2) a Beta distribution with these moments provides a very accurate model of the observed query response distribution. We also show how to use this to produce accurate error bars around these responses—i.e., to determine that the response to P(H=h|E=e) is x±y with confidence 1−{\$}\delta{\$}.},
author = {{Van Allen}, Tim and Singh, Ajit and Greiner, Russell and Hooper, Peter},
doi = {10.1016/J.ARTINT.2007.09.004},
file = {::},
issn = {0004-3702},
journal = {Artificial Intelligence},
month = {mar},
number = {4-5},
pages = {483--513},
publisher = {Elsevier},
title = {{Quantifying the uncertainty of a belief net response: Bayesian error-bars for belief net inference}},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001294},
volume = {172},
year = {2008}
}
@book{jackson2007natural,
author = {Jackson, P and Moulinier, I},
isbn = {9789027292445},
publisher = {John Benjamins Publishing Company},
series = {Natural Language Processing},
title = {{Natural Language Processing for Online Applications: Text retrieval, extraction and categorization. Second revised edition}},
url = {https://books.google.com.au/books?id=aZN05pMJTLAC},
year = {2007}
}
@inproceedings{Gelman2018,
abstract = {Code search and comprehension have become more difficult in recent years due to the rapid expansion of available source code. Current tools lack a way to label arbitrary code at scale while maintaining up-to-date representations of new programming languages, libraries, and functionalities. Comprehensive labeling of source code enables users to search for documents of interest and obtain a high-level understanding of their contents. We use Stack Overflow code snippets and their tags to train a language-agnostic, deep convolutional neural network to automatically predict semantic labels for source code documents. On Stack Overflow code snippets, we demonstrate a mean area under ROC of 0.957 over a long-tailed list of 4,508 tags. We also manually validate the model outputs on a diverse set of unlabeled source code documents retrieved from Github, and we obtain a top-1 accuracy of 86.6{\%}. This strongly indicates that the model successfully transfers its knowledge from Stack Overflow snippets to arbitrary source code documents.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1906.01032},
author = {Gelman, Ben and Hoyle, Bryan and Moore, Jessica and Saxe, Joshua and Slater, David},
booktitle = {Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis - MASES 2018},
doi = {10.1145/3243127.3243132},
eprint = {1906.01032},
file = {::},
isbn = {9781450359726},
keywords = {acm reference format,classification,crowdsourcing,deep learning,multilabel,natural language processing,semantic labeling,source code},
pages = {36--44},
publisher = {ACM Press},
title = {{A Language-Agnostic Model for Semantic Source Code Labeling}},
url = {http://arxiv.org/abs/1906.01032{\%}0Ahttp://dx.doi.org/10.1145/3243127.3243132},
year = {2018}
}
@article{Alshangiti2019,
abstract = {As smart and automated applications pervade our lives, an increasing number of software developers are required to incorporate machine learning (ML) techniques into application development. However, acquiring the ML skill set can be nontrivial for software developers owing to both the breadth and depth of the ML domain. Aims: We seek to understand the challenges developers face in the process of ML application development and offer insights to simplify the process. Despite its importance, there has been little research on this topic. A few existing studies on development challenges with ML are outdated, small scale, or they do no involve a representative set of developers. Method: We conduct an empirical study of ML-related developer posts on Stack Overflow. We perform in-depth quantitative and qualitative analyses focusing on a series of research questions related to the challenges of developing ML applications and the directions to address them. Results: Our findings include: (1) ML questions suffer from a much higher percentage of unanswered questions on Stack Overflow than other domains; (2) there is a lack of ML experts in the Stack Overflow QA community; (3) the data preprocessing and model deployment phases are where most of the challenges lay; and (4) addressing most of these challenges require more ML implementation knowledge than ML conceptual knowledge. Conclusions: Our findings suggest that most challenges are under the data preparation and model deployment phases, i.e., early and late stages. Also, the implementation aspect of ML shows much higher difficulty level among developers than the conceptual aspect.},
annote = {An empirical study on ML related SO posts 
Following are the problems in ML related Stack Overflow posts
A higher percentage of questions suffer from no accepted answer and no response compared to other domains in SO; ML questions take ten times longer to be answered than the typical SO question on average and there is a lack of ML experts in SO community},
author = {Alshangiti, Moayad and Sapkota, Hitesh and Murukannaiah, Pradeep K. and Liu, Xumin and Yu, Qi},
doi = {10.1109/ESEM.2019.8870187},
file = {::},
isbn = {9781728129686},
issn = {19493789},
journal = {International Symposium on Empirical Software Engineering and Measurement},
keywords = {Data Mining,Machine Learning,Software Development,Stack Overflow},
title = {{Why is Developing Machine Learning Applications Challenging? A Study on Stack Overflow Posts}},
volume = {2019-Septe},
year = {2019}
}
@article{Kerr2016,
abstract = {Background: Excessive sitting has been linked to poor health. It is unknown whether reducing total sitting time or increasing brief sit-to-stand transitions is more beneficial. We conducted a randomized pilot study to assess whether it is feasible for working and non-working older adults to reduce these two different behavioral targets. Methods: Thirty adults (15 workers and 15 non-workers) age 50-70 years were randomized to one of two conditions (a 2-hour reduction in daily sitting or accumulating 30 additional brief sit-tostand transitions per day). Sitting time, standing time, sit-to-stand transitions and stepping were assessed by a thigh worn inclinometer (activPAL). Participants were assessed for 7 days at baseline and followed while the intervention was delivered (2 weeks). Mixed effects regression analyses adjusted for days within participants, device wear time, and employment status. Time by condition interactions were investigated. Results: Recruitment, assessments, and intervention delivery were feasible. The 'reduce sitting' group reduced their sitting by two hours, the 'increase sit-to-stand' group had no change in sitting time (p {\textless}.001). The sit-to-stand transition group increased their sit-to-stand transitions, the sitting group did not (p {\textless}.001). Conclusions: This study was the first to demonstrate the feasibility and preliminary efficacy of specific sedentary behavioral goals.},
author = {Kerr, Jacqueline and Takemoto, Michelle and Bolling, Khalisa and Atkin, Andrew and Carlson, Jordan and Rosenberg, Dori and Crist, Katie and Godbole, Suneeta and Lewars, Brittany and Pena, Claudia and Merchant, Gina},
doi = {10.1371/journal.pone.0145427},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
month = {jan},
number = {1},
publisher = {Public Library of Science},
title = {{Two-arm randomized pilot intervention trial to decrease sitting time and increase sit-to-stand transitions in working and non-working older adults}},
volume = {11},
year = {2016}
}
@article{Kery2018,
abstract = {Literate programming tools are used by millions of programmers today, and are intended to facilitate presenting data analyses in the form of a narrative. We interviewed 21 data scientists to study coding behaviors in a literate programming environment and how data scientists kept track of variants they explored. For participants who tried to keep a detailed history of their experimentation, both informal and formal versioning attempts led to problems, such as reduced notebook readability. During iteration, participants actively curated their notebooks into narratives, although primarily through cell structure rather than markdown explanations. Next, we surveyed 45 data scientists and asked them to envision how they might use their past history in a future version control system. Based on these results, we give design guidance for future literate programming tools, such as providing history search based on how programmers recall their explorations, through contextual details including images and parameters.},
author = {Kery, Mary Beth and Radensky, Marissa and Arya, Mahima and John, Bonnie E. and Myers, Brad A.},
doi = {10.1145/3173574.3173748},
file = {::},
isbn = {9781450356206},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Data science,End-user programmers (EUP),Exploratory programming,Literate programming,end-user software engi},
title = {{The story in the notebook: Exploratory data science using a literate programming tool}},
volume = {2018-April},
year = {2018}
}
@book{Shorvon2013,
author = {Shorvon, Simon and Guerrini, Renzo and Cook, Mark and Lhatoo, Samden D.},
doi = {10.1093/med/9780199659043.001.0001},
isbn = {9780199659043},
publisher = {Oxford: Oxford University Press},
title = {{Oxford Textbook Of Epilepsy And Epileptic Seizures}},
year = {2013}
}
@article{Chen2016a,
abstract = {The sheer number of available technologies and the complex relationships among them make it challenging to choose the right technologies for software projects. Developers often turn to online resources (e.g., expert articles and community answers) to get a good understanding of the technology landscape. Such online resources are primarily opinion-based and are often out of date. Furthermore, information is often scattered in many online resources, which has to be aggregated to have a big picture of the technology landscape. In this paper, we exploit the fact that Stack Overflow users tag their questions with the main technologies that the questions revolve around, and develop association rule mining and community detection techniques to mine technology landscape from Stack Overflow question tags. The mined technology landscape is represented in a graphical Technology Associative Network (TAN). Our empirical study shows that the mined TAN captures a wide range of technologies, the complex relationships among the technologies, and the trend of the technologies in the developers' discussions on Stack Overflow. We develop a website (https://graphofknowledge.appspot.com/) for the community to access and evaluate the mined technology landscape. The website visit statistics by Google Analytics shows the developers' general interests in our technology landscape service. We also report a small-scale user study to evaluate the potential usefulness of our tool.},
author = {Chen, Chunyang and Xing, Zhenchang},
doi = {10.1145/2961111.2962588},
file = {::},
isbn = {9781450344272},
issn = {19493789},
journal = {International Symposium on Empirical Software Engineering and Measurement},
keywords = {Association Rule Mining,Community Detection,Technology Associative Network,Technology Landscape},
title = {{Mining Technology Landscape from Stack Overflow}},
volume = {08-09-Sept},
year = {2016}
}
@article{Choi2019,
abstract = {Background: This study investigated characteristics according to demographic, occupational factors of Maslach Burnout Inventory-General Survey (MBI-GS) and related scales to MBI-GS. Methods: The subjects of the study were 3,331 workers in 3 different workplaces of one electronics company. They filled in demographic factors surveys, occupational factors surveys, MBI-GS, Korean Occupational Stress Scale-Short Form (KOSS-SF), Patient Health Questionnaire-9 (PHQ-9), and World Health Organization Quality Of Life-Abbreviated version (WHOQOL-BREF). The correlations between sub-scales of MBI-GS and KOSS-SF, PHQ-9, WHOQOL-BREF were analyzed respectively. And KOSS-SF, PHQ-9, and WHOQOL-BREF were categorized; mean scores of sub-scales of MBI-GS were compared; and the quartiles of sub-scales of MBI-GS were presented. Results: A comparison of mean scores of MBI-GS according to demographic and occupational factors showed a significant difference according to age, problem drinking behavior, working time, and working duration in exhaustion regardless of sex. In professional efficacy, a significant difference was observed in age, marital status, working type, and working duration. And as a result of correlation analysis, the correlation coefficient between exhaustion and PHQ-9 was the highest regardless of sex. In addition, regardless of sex, exhaustion and cynicism scores tended to increase and professional efficacy score tended to decrease as the work stress level rose. Same tendency is shown in case of the more severe the symptom of depression and the lower quality of life. When the quartile for sub-scales' score of MBI-GS were investigated, the burnout was more pronounced in female than in male. Conclusions: Many demographic and occupational factors affect burnout were identified in one electronics company, and we investigated which sub-scales of MBI-GS were affected. Through this study, burnout characteristics were identified in a few population group of Korea, and the results are expected to be useful for burnout risk group identification, counseling, etc.},
author = {Choi, Young Gon and Choi, Byung Jin and Park, Tae Hwi and Uhm, Jun Young and Lee, Dong Bae and Chang, Seong Sil and Kim, Soo Young},
doi = {10.35371/aoem.2019.31.e29},
file = {::},
issn = {2052-4374},
journal = {Annals of Occupational and Environmental Medicine},
number = {1},
publisher = {Korean Society of Occupational and Environmental Medicine},
title = {{A study on the characteristics of Maslach Burnout Inventory-General Survey (MBI-GS) of workers in one electronics company}},
volume = {31},
year = {2019}
}
@inproceedings{Temko2015,
abstract = {A system for detection of seizures in intracranial EEG is presented that is based on a combination of generative, discriminative and hybrid approaches. We present a methodology to effectively benefit from the advantages each classifier offers. In particular, Gaussian mixture models, Support Vector Machines, hybrid likelihood ratio and Gaussian supervector approaches are developed and combined for the task. This system participated in the UPenn and Mayo Clinic's Seizure Detection Challenge, ranking in the top 5 of over 200 participants. The drawbacks of the proposed method with respect to the winning solutions are critically assessed.},
author = {Temko, Andriy and Sarkar, Achintya and Lightbody, Gordon},
booktitle = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
doi = {10.1109/EMBC.2015.7319901},
file = {::},
isbn = {9781424492718},
issn = {1557170X},
month = {nov},
pages = {6582--6585},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Detection of seizures in intracranial EEG: UPenn and Mayo Clinic's Seizure Detection Challenge}},
volume = {2015-Novem},
year = {2015}
}
@article{Buman2017,
abstract = {Background American workers spend 70–80{\%} of their time at work being sedentary. Traditional approaches to increase moderate-vigorous physical activity (MVPA) may be perceived to be harmful to productivity. Approaches that target reductions in sedentary behavior and/or increases in standing or light-intensity physical activity [LPA] may not interfere with productivity and may be more feasible to achieve through small changes accumulated throughout the workday Methods/design This group randomized trial (i.e., cluster randomized trial) will test the relative efficacy of two sedentary behavior focused interventions in 24 worksites across two states (N = 720 workers). The MOVE + intervention is a multilevel individual, social, environmental, and organizational intervention targeting increases in light-intensity physical activity in the workplace. The STAND + intervention is the MOVE + intervention with the addition of the installation and use of sit-stand workstations to reduce sedentary behavior and enhance light-intensity physical activity opportunities. Our primary outcome will be objectively-measured changes in sedentary behavior and light-intensity physical activity over 12 months, with additional process measures at 3 months and longer-term sustainability outcomes at 24 months. Our secondary outcomes will be a clustered cardiometabolic risk score (comprised of fasting glucose, insulin, triglycerides, HDL-cholesterol, and blood pressure), workplace productivity, and job satisfaction Discussion This study will determine the efficacy of a multi-level workplace intervention (including the use of a sit-stand workstation) to reduce sedentary behavior and increase LPA and concomitant impact on cardiometabolic health, workplace productivity, and satisfaction. Trial registration: ClinicalTrials.gov Identifier: NCT02566317 (date of registration: 10/1/2015).},
author = {Buman, Matthew P. and Mullane, Sarah L. and Toledo, Meynard J. and Rydell, Sarah A. and Gaesser, Glenn A. and Crespo, Noe C. and Hannan, Peter and Feltes, Linda and Vuong, Brenna and Pereira, Mark A.},
doi = {10.1016/j.cct.2016.12.008},
issn = {15592030},
journal = {Contemporary Clinical Trials},
keywords = {Cardiometabolic health,Cluster randomized trial,Physical activity,Sedentary behavior,Sit-stand workstations,Workplace},
month = {feb},
pages = {11--19},
publisher = {Elsevier Inc.},
title = {{An intervention to reduce sitting and increase light-intensity physical activity at work: Design and rationale of the ‘Stand {\&} Move at Work' group randomized trial}},
volume = {53},
year = {2017}
}
@article{Li2017,
abstract = {User-centered design (UCD) is an approach for creating human-machine interfaces that are usable and support the human operator's tasks. UCD can be challenging because designers can fail to account for human-machine interactions that occur due to the concurrency between the human and the other system elements. Formal methods are tools that enable analysts to consider all of the possible system interactions using a combination of formal modeling, specification, and proof-based verification. However, creating formal interface design models can be extremely difficult. This work describes a method that supports UCD by automatically generating formal designs of human-machine interface behavior from task-analytic models. The resulting interface design will always support the behavior captured in the task model. This paper describes the method and demonstrates its capabilities with three case studies: a light switch, a vending machine, and a patient-controlled analgesia pump. The produced designs are validated with formal verifications to prove that they support their associated tasks. Results and future research are discussed.},
author = {Li, Meng and Wei, Jiajun and Zheng, Xi and Bolton, Matthew L.},
doi = {10.1109/THMS.2017.2700630},
file = {::},
issn = {21682291},
journal = {IEEE Transactions on Human-Machine Systems},
keywords = {Formal methods,human-automation interaction,machine learning,task analysis,user-centered design (UCD)},
number = {6},
pages = {822--833},
title = {{A Formal Machine-Learning Approach to Generating Human-Machine Interfaces from Task Models}},
volume = {47},
year = {2017}
}
@article{Shen2015,
abstract = {This research aims to depict the methodological steps and tools about the combined operation of case-based reasoning (CBR) and multi-agent system (MAS) to expose the ontological application in the field of clinical decision support. The multi-agent architecture works for the consideration of the whole cycle of clinical decision-making adaptable to many medical aspects such as the diagnosis, prognosis, treatment, therapeutic monitoring of gastric cancer. In the multi-agent architecture, the ontological agent type employs the domain knowledge to ease the extraction of similar clinical cases and provide treatment suggestions to patients and physicians. Ontological agent is used for the extension of domain hierarchy and the interpretation of input requests. Case-based reasoning memorizes and restores experience data for solving similar problems, with the help of matching approach and defined interfaces of ontologies. A typical case is developed to illustrate the implementation of the knowledge acquisition and restitution of medical experts.},
author = {Shen, Ying and Colloc, Jo{\"{e}}l and Jacquet-Andrieu, Armelle and Lei, Kai},
doi = {10.1016/j.jbi.2015.06.012},
file = {::},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Case-based reasoning,Clinical decision support,Knowledge representation,Multi-agent system,Ontology application},
month = {aug},
pages = {307--317},
pmid = {26133480},
publisher = {Academic Press Inc.},
title = {{Emerging medical informatics with case-based reasoning for aiding clinical decision in multi-agent system}},
volume = {56},
year = {2015}
}
@article{Yet2013a,
abstract = {Warfarin therapy is known as a complex process because of the variation in the patients' response. Failure to deal with such variation may lead to death as a result of thrombosis or bleeding. The possible sources of variation such as concomitant illnesses and drug interactions have to be investigated by the clinician in order to deal with the variation. This paper describes a decision support system (DSS) using Bayesian networks for assisting clinicians to make better decisions in Warfarin therapy management. The DSS is developed in collaboration with a Swedish hospital group that manages Warfarin therapy for more than 3000 patients. The proposed model can assist the clinician in making dose-adjustment and follow-up interval decisions, investigating variation causes, and evaluating bleeding and thrombosis risks related to therapy. The model is built upon previous findings from medical literature, the knowledge of domain experts, and large dataset of patients. {\textcopyright} 2012 Elsevier B.V.},
annote = {Use sensitvity analysis on the outputs},
author = {Yet, Barbaros and Bastani, Kaveh and Raharjo, Hendry and Lifvergren, Svante and Marsh, William and Bergman, Bo},
doi = {10.1016/j.dss.2012.10.007},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Anticoagulant therapy,Bayesian networks,Decision support systems,Warfarin therapy},
title = {{Decision support system for Warfarin therapy management using Bayesian networks}},
year = {2013}
}
@article{Hassan2017,
abstract = {{\textcopyright}2017 IEEE. Despite the advancement in software build tools such as Maven and Gradle, human involvement is still often required in software building. To enable large-scale advanced program analysis and data mining of software artifacts, software engineering researchers need to have a large corpus of built software, so automatic software building becomes essential to improve research productivity. In this paper, we present a feasibility study on automatic software building. Particularly, we first put state-of-the-art build automation tools (Ant, Maven and Gradle) to the test by automatically executing their respective default build commands on top 200 Java projects from GitHub. Next, we focus on the 86 projects that failed this initial automated build attempt, manually examining and determining correct build sequences to build each of these projects. We present a detailed build failure taxonomy from these build results and show that at least 57{\%} build failures can be automatically resolved.},
author = {Hassan, Foyzul and Mostafa, Shaikh and Lam, Edmund S.L. L and Wang, Xiaoyin},
doi = {10.1109/ESEM.2017.11},
file = {::},
isbn = {9781509040391},
issn = {19493789},
journal = {International Symposium on Empirical Software Engineering and Measurement},
pages = {38--47},
title = {{Automatic Building of Java Projects in Software Repositories: A Study on Feasibility and Challenges}},
volume = {2017-Novem},
year = {2017}
}
@inproceedings{Braiek2018,
author = {Braiek, Houssem Ben and Khomh, Foutse and Adams, Bram},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories - MSR '18},
doi = {10.1145/3196398.3196445},
editor = {Zaidman, Andy and Kamei, Yasutaka and Hill, Emily},
file = {::},
isbn = {9781450357166},
pages = {353--363},
publisher = {ACM Press},
title = {{The open-closed principle of modern machine learning frameworks}},
url = {http://dl.acm.org/citation.cfm?doid=3196398.3196445},
year = {2018}
}
@article{Rivera-Villicana2019,
abstract = {In this paper we present an early Apprenticeship Learning approach to mimic the behaviour of different players in a short adaption of the interactive fiction Anchorhead. Our motivation is the need to understand and simulate player behaviour to create systems to aid the design and person-alisation of Interactive Narratives (INs). INs are partially observable for the players and their goals are dynamic as a result. We used Receding Horizon IRL (RHIRL) to learn players' goals in the form of reward functions, and derive policies to imitate their behaviour. Our preliminary results suggest that RHIRL is able to learn action sequences to complete a game, and provided insights towards generating behaviour more similar to specific players.},
archivePrefix = {arXiv},
arxivId = {1909.07268},
author = {Rivera-Villicana, Jessica and Zambetta, Fabio and Harland, James and Berry, Marsha},
doi = {10.1145/3341215.3356314},
eprint = {1909.07268},
file = {::},
isbn = {9781450368711},
journal = {CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play},
keywords = {Anchorhead,Apprenticeship Learning,Interactive Narratives,Inverse Reinforcement Learning,Player Modelling},
pages = {645--652},
title = {{Exploring apprenticeship learning for player modelling in interactive narratives}},
year = {2019}
}
@article{Breck2019,
annote = {Training and serving data - production asset

Data Validation System (three components) - Data Analyzer, Data Validator (through schema), Model Unit Tester

Types of Data Validation - Single batch validation, inter batch validation, model testing

Single batch validation - Are there data errors within each new batch that is ingested by the pipeline ?
Any deviation from the expected characteristics, given expert domain knowledge - anomaly

Any disagreement during schema validation is flagged as an anomaly.
Data Validator - to avoid training on bad data
- Each anomaly corresponds to a validation of some property specified in the schema

Inter batch validation - There are certain anomalies that only manifest when two or more batches of data are considered together

Training-serving skew - skew between training and serving data , reasons - different code paths to generate training(offline) and serving(online) data

Three categories of skew - Feature skew (changes to training code not reflected on serving code), Distribution skew (during sampling), scoring/serving skew(only subset of scored examples are actually served)

Time saving - by catching important data anomalies early, by helping teams to diagnose model-quality problems caused by data errors

Anomalies are categorised
Changing data type of a feature (from string to int) is a serious anomaly

Feature presence - some features are expected to be present in all examples, others present only in a fraction

Feature domains - Many features assume values only from a limited domain

Feature value count - Features can be single valued/list},
author = {Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
file = {::},
journal = {Sysml},
title = {{DATA VALIDATION FOR MACHINE LEARNING Eric}},
year = {2019}
}
@article{Nadeem2017,
abstract = {Background/Objectives: In the modern age, there is a need of wireless network protocols verification using suitable techniques and tools to meet the security challenges in Wireless Ad-Hoc sensor networks. Methods/Statistical Analysis: For verification of system like ZigBee protocol stack, formal methods are being used. The latest formal verification method called Event-B is used now a day to frame a model for verification of different wireless security protocols like IEEE 802.11 and IEEE 802.15.4. Findings: To describe specific properties in a suitably rich mathematical logic such as first order logic, we need to limit this expressiveness if we are to automatically verify a property. To verify any system properties, temporal logic are used for safety, correctness, reliability in wireless security protocols. In this paper we developed a model/ framework in Event-B for the formal verification of ZigBee protocol and simulate it using RODIN tool. In this framework, models are specified, analyzed and verified by using formal methods. Application/Improvements: This framework leads toward more secure and reliable model having no inconsistencies in ZigBee.},
author = {Nadeem, Rana Muhammad and Gill, Abdul Aziz},
file = {::},
issn = {09746846},
journal = {Indian Journal of Science and Technology},
keywords = {5-6 Words,Drawn from Title,Formal Model,Network Authentication,Secure Communication,Verification of ZigBee,Wireless Protocol.,Word Representing the Work},
month = {may},
number = {20},
title = {{A Formal Model for Verification of ZigBee Protocol for Secure Network Authentication}},
volume = {10},
year = {2017}
}
@article{Zhang2019,
abstract = {Deep neural networks (DNN) have achieved remarkable results in sentiment classification. Some ensemble methods of DNN models and traditional feature-based models are proposed recently. However, to the best of our knowledge, most of the works use traditional ensemble combination techniques, e.g. voting and stacking, which are designed for weak base classifiers. So far many base classifiers, e.g. DNN, have been able to achieve good results in sentiment classification tasks, so there should be a new ensemble combination technique designed for strong base classifiers. To address this issue, we proposed a cost-sensitive combination technique using sequential three-way decisions (3WD), which is named S3WC. In S3WC, base classifiers are arranged in a linear arrangement, and a gate mechanism is constructed in each step to divide the objects into three groups, i.e., positive region, negative region and boundary region, which respectively correspond to acceptance, rejection and deferment in sequential 3WD. Each object is grouped by minimizing its total cost consisting of misclassification cost and time cost. The objects in boundary region require more information to decrease the misclassification cost, so they are reclassified by the subsequent base classifiers in order to obtain more information, while the time cost increases. In the experiment, we apply S3WC to DNN models and traditional feature-based models on five benchmark datasets, and compare its performance with traditional ensemble combination techniques. The experimental results show that S3WC outperforms any of its base classifiers in terms of classification accuracy, and the total cost of S3WC is lower than that of the existing ensemble combination techniques (e.g. majority-voting, weighted-voting, meta-learning).},
annote = {- Contains 5 text binary classification datasets. 
- 6 baseline models 

Majority voting
Weighted voting
Meta-learning},
author = {Zhang, Yuebing and Miao, Duoqian and Wang, Jiaqi and Zhang, Zhifei},
doi = {10.1016/j.ijar.2018.10.019},
file = {::},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Ensemble learning,Sentiment classification,Three-way decisions},
pages = {85--97},
publisher = {Elsevier Inc.},
title = {{A cost-sensitive three-way combination technique for ensemble learning in sentiment classification}},
url = {https://doi.org/10.1016/j.ijar.2018.10.019},
volume = {105},
year = {2019}
}
@article{Fritz2011b,
abstract = {Recently, Pfeffer (2010) called for a better understanding of the human dimension of sustainability. Responding to this call, we explore how individuals sustain an important human resource-their own energy-at work. Specifically, we focus on strategies that employees use at work to sustain their energy. Our findings show that the most commonly used strategies (e.g., switching to another task or browsing the Internet) are not associated with higher levels of human energy at work. Rather, strategies related to learning, to the meaning of one's work, and to positive workplace relationships were most strongly related to employees' energy. Copyright by the Academy of Management; all rights reserved.},
author = {Fritz, Charlotte and Lam, Chak and Spreitzer, Gretchen},
doi = {10.5465/AMP.2011.63886528},
issn = {15589080},
journal = {Academy of Management Perspectives},
month = {aug},
number = {3},
pages = {28--39},
title = {{It's the little things that matter: An examination of knowledge workers' energy management}},
volume = {25},
year = {2011}
}
@article{TatumWO2013,
author = {{Tatum WO}},
doi = {10.1212/WNL.0b013e318279730e},
file = {::},
journal = {Neurology.},
pages = {S1--S3},
title = {{How not to read an EEG: introductory statements}},
volume = {80(1)},
year = {2013}
}
@article{Marshall2019,
abstract = {Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice.},
author = {Marshall, Iain J. and Wallace, Byron C.},
doi = {10.1186/s13643-019-1074-9},
file = {::},
issn = {20464053},
journal = {Systematic Reviews},
keywords = {Evidence synthesis,Machine learning,Natural language processing},
number = {1},
pages = {1--10},
pmid = {31296265},
publisher = {Systematic Reviews},
title = {{Toward systematic review automation: A practical guide to using machine learning tools in research synthesis}},
volume = {8},
year = {2019}
}
@article{Itti2009,
abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72{\%} of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84{\%} when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Itti, Laurent and Baldi, Pierre},
doi = {10.1016/j.visres.2008.09.007},
file = {::},
issn = {00426989},
journal = {Vision Research},
keywords = {Attention,Bayes theorem,Eye movements,Free viewing,Information theory,Natural vision,Novelty,Saliency,Surprise},
number = {10},
pages = {1295--1306},
pmid = {18834898},
publisher = {Elsevier Ltd},
title = {{Bayesian surprise attracts human attention}},
url = {http://dx.doi.org/10.1016/j.visres.2008.09.007},
volume = {49},
year = {2009}
}
@book{Stevenson2019,
abstract = {Neonatal EEG seizure detection algorithms (NS-DAs) have an upper bound of performance related to the agreement between visual interpretation of human experts. No published algorithms have reported performance that has reached this upper bound. In this paper, we combined two recently developed NSDAs in order to improve detection performance. An outlier detection stage was also added to improve robustness in the presence of unseen data. A large database of EEG from 79 term infants labeled by three independent human experts was used to develop and test the sufficiency of the hybrid NSDA. The inter-observer agreement (IOA) between experts was high ({\$}\kappa{\$} = 0.757, 95{\%}CI: 0.665-0.836, n=79). The area under the receiver operator characteristic of the NSDA compared to the consensus annotation of the human experts was 0.952 (95{\%}CI: 0.0927-0.971). The IOA of seizure detection between the NSDA and human experts was not significantly less than the IOA among human experts (∆{\$}\kappa{\$} = 0.022, 95{\%}CI:-0.020 to 0.072) and was further improved by increasing the minimum seizure duration from 10s to 30s (∆{\$}\kappa{\$} =-0.002, 95{\%}CI:-0.073 to 0.055). Automated methods of neonatal EEG seizure detection have sufficient accuracy to replace human interpretation, potentially, providing reliable interpretations for clinicians in the neonatal intensive care unit.},
author = {Stevenson, Nathan and Tapani, Karoliina and Vanhatalo, Sampsa},
doi = {10.0/Linux-x86_64},
file = {::},
isbn = {9781538613115},
title = {{Hybrid neonatal EEG seizure detection algorithms achieve the benchmark of visual interpretation of the human expert{\textless}sup{\textgreater}*{\textless}/sup{\textgreater}}},
year = {2019}
}
@incollection{Jarczyk2014,
annote = {Very important paper for methodology.
Use the in paper annotations to get the idea.},
author = {Jarczyk, Oskar and Gruszka, B{\l}a{\.{z}}ej and Jaroszewicz, Szymon and Bukowski, Leszek and Wierzbicki, Adam},
booktitle = {International Conference on Social Informatics},
doi = {10.1007/978-3-319-13734-6_6},
file = {::},
pages = {80--94},
title = {{GitHub Projects. Quality Analysis of Open-Source Software}},
url = {http://link.springer.com/10.1007/978-3-319-13734-6{\_}6},
year = {2014}
}
@article{Elliott2014,
author = {Elliott, Julian H. and Turner, Tari and Clavisi, Ornella and Thomas, James and Higgins, Julian P.T. T and Mavergames, Chris and Gruen, Russell L.},
doi = {10.1371/journal.pmed.1001603},
file = {::},
issn = {15491676},
journal = {PLoS Medicine},
number = {2},
pages = {1--6},
pmid = {24558353},
title = {{Living Systematic Reviews: An Emerging Opportunity to Narrow the Evidence-Practice Gap}},
volume = {11},
year = {2014}
}
@article{Green2012,
abstract = {The author investigates the evolution of job skill distribution using task data derived from the U.K. Skills Surveys of 1997, 2001, and 2006, and the 1992 Employment Survey in Britain. He determines the extent to which employee involvement in the workplace and computer technologies promote the use of higher order cognitive and interactive skills. He finds that literacy, other communication tasks, and self-planning skills have grown especially fast. Numerical and problem-solving skills have also become more important, but repetitive physical skills have largely remained unchanged. He finds that employee involvement and computer technologies privilege the use of greater generic skills but substitute for repetitive physical tasks. However, the classification of all tasks as either routine or non-routine is found to be problematic. Finally, the author finds a strong connection between the rising use of more academic skills and the education level required for entry into the labor market. {\textcopyright} by Cornell University.},
author = {Green, Francis},
doi = {10.1177/001979391206500103},
issn = {0019-7939},
journal = {ILR Review},
keywords = {Communication skill,Computers,Employee involvement,Literacy,Numeracy,Pay,Physical tasks,Problem-solving,Required education,Routine tasks,Task discretion,Wages},
month = {jan},
number = {1},
pages = {36--67},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Employee Involvement, Technology and Evolution in Job Skills: A Task-Based Analysis}},
url = {http://journals.sagepub.com/doi/10.1177/001979391206500103},
volume = {65},
year = {2012}
}
@inproceedings{Haiduc2010a,
abstract = {During maintenance developers cannot read the entire code of large systems. They need a way to get a quick understanding of source code entities (such as, classes, methods, packages, etc.), so they can efficiently identify and then focus on the ones related to their task at hand. Sometimes reading just a method header or a class name does not tell enough about its purpose and meaning, while reading the entire implementation takes too long. We study a solution which mitigates the two approaches, i.e., short and accurate textual descriptions that illustrate the software entities without having to read the details of the implementation. We create such descriptions using techniques from automatic text summarization. The paper presents a study that investigates the suitability of various such techniques for generating source code summaries. The results indicate that a combination of text summarization techniques is most appropriate for source code summarization and that developers generally agree with the summaries produced.},
author = {Haiduc, Sonia and Aponte, Jairo and Moreno, Laura and Marcus, Andrian},
booktitle = {Proceedings - Working Conference on Reverse Engineering, WCRE},
doi = {10.1109/WCRE.2010.13},
file = {::},
isbn = {9780769541235},
issn = {10951350},
keywords = {Program comprehension,Text summarization},
pages = {35--44},
title = {{On the use of automated text summarization techniques for summarizing source code}},
year = {2010}
}
@book{Vermeulen2013,
abstract = {Feedback and affordances are two of the most well-known principles in interaction design. Unfortunately, the related and equally important notion of feedforward has not been given as much consideration. Nevertheless, feedforward is a powerful design principle for bridging Norman's Gulf of Execution. We reframe feedforward by disambiguating it from related design principles such as feedback and perceived af-fordances, and identify new classes of feedforward. In addition , we present a reference framework that provides a means for designers to explore and recognize different opportunities for feedforward.},
author = {Vermeulen, Jo and Luyten, Kris and {Van Den Hoven}, Elise and Coninx, Karin},
file = {::},
isbn = {9781450318990},
keywords = {SIGCHI,archival format,proceedings},
title = {{Crossing the Bridge over Norman's Gulf of Execution: Revealing Feedforward's True Identity}},
url = {http://delivery.acm.org/10.1145/2470000/2466255/p1931-vermeulen-corrected.pdf?ip=139.132.188.30{\&}id=2466255{\&}acc=ACTIVE SERVICE{\&}key=65D80644F295BC0D.B242904781996EBA.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1550547772{\_}b99baa7d68743072f7bfb7101fa62c1a},
year = {2013}
}
@misc{Perucca2012,
abstract = {More than 150 years after bromide was introduced as the first antiepileptic drug, adverse effects remain a leading cause of treatment failure and a major determinant of impaired health-related quality of life in people with epilepsy. Adverse effects can develop acutely or many years after starting treatment and can affect any organ or structure. In the past two decades, many efforts have been made to reduce the burden of antiepileptic drug toxicity. Several methods to screen and quantify adverse effects have been developed. Patient profiles associated with increased risk of specific adverse effects have been uncovered through advances in the areas of epidemiology and pharmacogenomics. Several new-generation antiepileptic drugs with improved tolerability profiles and reduced potential for drug interaction have been added to the therapeutic armamentarium. Overall, these advances have expanded the opportunities to tailor treatment with antiepileptic drugs, to enhance effectiveness and minimise the risk of toxic effects. {\textcopyright}2012 Elsevier Ltd.},
author = {Perucca, Piero and Gilliam, Frank G.},
booktitle = {The Lancet Neurology},
doi = {10.1016/S1474-4422(12)70153-9},
file = {::},
issn = {14744422},
month = {sep},
number = {9},
pages = {792--802},
pmid = {22832500},
title = {{Adverse effects of antiepileptic drugs}},
volume = {11},
year = {2012}
}
@techreport{Scheepers1996,
abstract = {This paper reports the results of a population study designed to assess the standards of epilepsy care within a geographical population in relation to diagnosis, seizure management and quality of life. One of the findings was the unexpectedly high frequency of the misdiagnosis of epilepsy. Forty-nine of 214 patients with a primary diagnosis of epilepsy were subsequently found to have been misdiagnosed following a specialist review and investigations. All except two have been withdrawn from antiepileptic medication. The diagnosis of epilepsy was disputed in a further 26 patients. Of the 49 patients, 20 were found to have cardiovascular or cerebrovascular pathology. Seven had only ever experienced a single seizure and a further 10 were found to have underlying psychopathology. Such observations support the view that epilepsy is frequently misdiagnosed and this paper discusses some of the implications of misdiagnosis.},
author = {Scheepers, Bruce and Clough, Peter and Pickles, Chris},
booktitle = {Seizure},
file = {::},
keywords = {epilepsy,misdiagnosis,non-epileptic attack disorder},
pages = {403406},
title = {{The misdiagnosis of epilepsy: findings of a population study*}},
volume = {7},
year = {1996}
}
@article{KevinRange2012,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Loren, Cobb and Ashok, Krishnamurty and Jan, Mandel and Johnathan, Beezley},
doi = {10.1038/jid.2014.371},
eprint = {NIHMS150003},
file = {::},
isbn = {6176321972},
issn = {15378276},
journal = {Bone},
keywords = {epiblast,gfp fusion,histone h2b-,icm,lineage specification,live imaging,mouse blastocyst,pdgfr {\$}\alpha{\$},pdgfr $\alpha$,primitive endoderm},
number = {1},
pages = {1--7},
pmid = {1000000221},
title = {{Bayesian Tracking of Emerging Epidemics Using Ensemble Optimal Statistical Interpolation}},
volume = {23},
year = {2012}
}
@article{Osman2019,
author = {Osman, Ahmed Hamza and Alzahrani, Ahmad A.},
doi = {10.1109/ACCESS.2018.2886608},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Epilepsy disease,RBF,SOM,classification,neural network},
pages = {4741--4747},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{New Approach for Automated Epileptic Disease Diagnosis Using an Integrated Self-Organization Map and Radial Basis Function Neural Network Algorithm}},
volume = {7},
year = {2019}
}
@article{Meseguer2010,
abstract = {This paper proposes a fault diagnosis method using a timed discrete-event approach based on interval observers that improves the integration of fault detection and isolation tasks. The interface between fault detection and fault isolation considers the activation degree and the occurrence time instant of the diagnostic signals using a combination of several theoretical fault signature matrices that store the knowledge of the relationship between diagnostic signals and faults. The fault isolation module is implemented using a timed discrete-event approach that recognizes the occurrence of a fault by identifying a unique sequence of observable events (fault signals). The states and transitions that characterize such a system can directly be inferred from the relation between fault signals and faults. The proposed fault diagnosis approach has been motivated by the problem of detecting and isolating faults of the Barcelona's urban sewer system limnimeters (level meter sensors). The results obtained in this case study illustrate the benefits of using the proposed approach in comparison with the standard fault detection and isolation approach. {\textcopyright}2010 IEEE.},
author = {Meseguer, Jordi and Puig, Vicen and Escobet, Teresa},
doi = {10.1109/TSMCA.2010.2052036},
file = {::},
issn = {10834427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans},
keywords = {Discrete-event systems (DESs),fault detection,fault diagnosis,intervals,observers,robustness},
number = {5},
pages = {900--916},
title = {{Fault diagnosis using a timed discrete-event approach based on interval observers: Application to sewer networks}},
volume = {40},
year = {2010}
}
@article{Shannon1948,
author = {Shannon, C. E.},
doi = {10.1002/j.1538-7305.1948.tb00917.x},
file = {::},
issn = {15387305},
journal = {Bell System Technical Journal},
number = {4},
pages = {623--656},
title = {{A Mathematical Theory of Communication}},
volume = {27},
year = {1948}
}
@article{Praseeratasang2019,
abstract = {This article aims to resolve a particular production planning and workforce assignment problem. Many production lines may have different production capacities while producing the same product. Each production line is composed of three production stages, and each stage requires different periods of times and numbers of workers. Moreover, the workers will have different skill levels which can affect the number of workers required for production line. The number of workers required in each farm also depends on the amount of pigs that it is producing. Production planning must fulfill all the demands and can only make use of the workers available. A production plan aims to generate maximal profit for the company. A mathematical model has been developed to solve the proposed problem, when the size of problem increases, the model is unable to resolve large issues within a reasonable timeframe. A metaheuristic method called adaptive large-scale neighborhood search (ALNS) has been developed to solve the case study. Eight destroy and four repair operators (including ant colony optimization based destroy and repair methods) have been presented. Moreover, three formulas which are used to make decisions for acceptance of the newly generated solution have been proposed. The present study tested 16 data sets, including the case study. From the computational results of the small size of test instances, ALNS should be able to find optimal solutions for all the random data sets in much less computational time compared to commercial optimization software. For medium and larger test instance sizes, the findings of the heuristics were 0.48{\%} to 0.92{\%} away from the upper bound and generated within 480-620 h, in comparison to the 1 h required for the proposed method. The Ant Colony Optimization-based destroy and repair method found solutions that were 0.98 to 1.03{\%} better than the original ALNS.},
author = {Praseeratasang, Nat and Pitakaso, Rapeepan and Sethanan, Kanchana and Kaewman, Sasitorn and Golinska-Dawson, Paulina},
doi = {10.3390/joitmc5020026},
file = {::},
issn = {21998531},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
keywords = {Adaptive large neighborhood search,Ant colony optimization,Assignment problems,Scheduling problems},
number = {2},
title = {{Adaptive large neighborhood search for a production planning problem arising in pig farming}},
volume = {5},
year = {2019}
}
@article{Hussain2018,
abstract = {{\textcopyright}2018 Springer Science+Business Media B.V., part of Springer Nature Epilepsy is a neurological disorder produced due to abnormal excitability of neurons in the brain. The research reveals that brain activity is monitored through electroencephalogram (EEG) of patients suffered from seizure to detect the epileptic seizure. The performance of EEG detection based epilepsy require feature extracting strategies. In this research, we have extracted varying features extracting strategies based on time and frequency domain characteristics, nonlinear, wavelet based entropy and few statistical features. A deeper study was undertaken using novel machine learning classifiers by considering multiple factors. The support vector machine kernels are evaluated based on multiclass kernel and box constraint level. Likewise, for K-nearest neighbors (KNN), we computed the different distance metrics, Neighbor weights and Neighbors. Similarly, the decision trees we tuned the paramours based on maximum splits and split criteria and ensemble classifiers are evaluated based on different ensemble methods and learning rate. For training/testing tenfold Cross validation was employed and performance was evaluated in form of TPR, NPR, PPV, accuracy and AUC. In this research, a deeper analysis approach was performed using diverse features extracting strategies using robust machine learning classifiers with more advanced optimal options. Support Vector Machine linear kernel and KNN with City block distance metric give the overall highest accuracy of 99.5{\%} which was higher than using the default parameters for these classifiers. Moreover, highest separation (AUC = 0.9991, 0.9990) were obtained at different kernel scales using SVM. Additionally, the K-nearest neighbors with inverse squared distance weight give higher performance at different Neighbors. Moreover, to distinguish the postictal heart rate oscillations from epileptic ictal subjects, and highest performance of 100{\%} was obtained using different machine learning classifiers.},
author = {Hussain, Lal},
doi = {10.1007/s11571-018-9477-1},
file = {::},
issn = {18714099},
journal = {Cognitive Neurodynamics},
keywords = {Classification,Decision tree,Ensemble classifier,Epilepsy,K-nearest neighbors,Seizure detection,Support vector machine},
month = {jun},
number = {3},
pages = {271--294},
publisher = {Springer Netherlands},
title = {{Detecting epileptic seizure with different feature extracting strategies using robust machine learning classification techniques by applying advance parameter optimization approach}},
volume = {12},
year = {2018}
}
@article{Kramer2008,
abstract = {Epilepsy - the world's most common serious brain disorder - is defined by recurrent unprovoked seizures that result from complex interactions between distributed neural populations. We explore some macroscopic characteristics of emergent ictal networks by considering intracranial recordings from human subjects with intractable epilepsy. For each seizure, we compute a simple measure of linear coupling between all electrode pairs (more than 2400) to define networks of interdependent electrodes during preictal and ictal time intervals. We analyze these networks by applying traditional measures from network analysis and identify statistically significant global and local changes in network topology. We find at seizure onset a diffuse breakdown in global coupling, and local changes indicative of increased throughput of specific cortical and subcortical regions. We conclude that network analysis yields measures to summarize the complicated coupling topology emergent at seizure onset. Using these measures, we can identify spatially localized brain regions that may facilitate seizures and may be potential targets for focal therapies. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Kramer, Mark A. and Kolaczyk, Eric D. and Kirsch, Heidi E.},
doi = {10.1016/j.eplepsyres.2008.02.002},
file = {::},
issn = {09201211},
journal = {Epilepsy Research},
keywords = {Correlation structure,Electrocorticogram,Multivariate time series analysis,Network analysis,Oscillations,Seizures},
month = {may},
number = {2-3},
pages = {173--186},
title = {{Emergent network topology at seizure onset in humans}},
volume = {79},
year = {2008}
}
@inproceedings{Cummaudo2020a,
abstract = {Increased popularity of ‘intelligent' web services provides end-users with machine-learnt functionality at little effort to developers. However, these services require a decision threshold to be set which is dependent on problem-specific data. Developers lack a systematic approach for evaluating intelligent services and existing evaluation tools are predominantly targeted at data scientists for pre-development evaluation. This paper presents a workflow and supporting tool, Threshy, to help software developers select a decision threshold suited to their problem domain. Unlike existing tools, Threshy is designed to operate in multiple workflows including pre-development, pre-release, and support. Threshy is designed for tuning the confidence scores returned by intelligent web services and does not deal with hyper-parameter optimisation used in ML models. Additionally, it considers the financial impacts of false positives. Threshold configuration files exported by Threshy can be integrated into client applications and monitoring infrastructure. Demo: https://bit.ly/2YKeYhE.},
archivePrefix = {arXiv},
arxivId = {2008.08252},
author = {Cummaudo, Alex and Barnett, Scott and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3417919},
eprint = {2008.08252},
file = {::},
isbn = {9781450370431},
keywords = {Decision theory,Intelligent services,Thresholding,Tooling},
pages = {1645--1649},
publisher = {ACM},
title = {{Threshy: supporting safe usage of intelligent web services}},
year = {2020}
}
@inproceedings{shoeb2010application,
abstract = {We present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp EEG, a non-invasive measure of the brain's electrical activity. This problem is challenging because the brain's electrical activity is composed of numerous classes with overlapping characteristics. The key steps involved in realizing a high performance algorithm included shaping the problem into an appropriate machine learning framework, and identifying the features critical to separating seizure from other types of brain activity. When trained on 2 or more seizures per patient and tested on 916 hours of continuous EEG from 24 patients, our algorithm detected 96{\%} of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period. We also provide information about how to download the CHB-MIT database, which contains the data used in this study.},
author = {Shoeb, Ali H and Guttag, John V},
booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
file = {::},
isbn = {9781605589077},
pages = {975--982},
title = {{Application of machine learning to epileptic seizure detection}},
year = {2010}
}
@article{Olsen2019,
author = {Olsen, Megan and Raunak, Mohammad},
doi = {10.1109/TR.2018.2850315},
file = {::},
issn = {00189529},
journal = {IEEE Transactions on Reliability},
keywords = {Agent-based models (ABM),Discrete-event simulation (DES),Metamorphic testing (MT),Simulation validation},
number = {1},
pages = {91--108},
publisher = {IEEE},
title = {{Increasing validity of simulation models through metamorphic testing}},
volume = {68},
year = {2019}
}
@article{Furbass2017,
abstract = {Objective This study investigated sensitivity and false detection rate of a multimodal automatic seizure detection algorithm and the applicability to reduced electrode montages for long-term seizure documentation in epilepsy patients. Methods An automatic seizure detection algorithm based on EEG, EMG, and ECG signals was developed. EEG/ECG recordings of 92 patients from two epilepsy monitoring units including 494 seizures were used to assess detection performance. EMG data were extracted by bandpass filtering of EEG signals. Sensitivity and false detection rate were evaluated for each signal modality and for reduced electrode montages. Results All focal seizures evolving to bilateral tonic-clonic (BTCS, n = 50) and 89{\%} of focal seizures (FS, n = 139) were detected. Average sensitivity in temporal lobe epilepsy (TLE) patients was 94{\%} and 74{\%} in extratemporal lobe epilepsy (XTLE) patients. Overall detection sensitivity was 86{\%}. Average false detection rate was 12.8 false detections in 24 h (FD/24 h) for TLE and 22 FD/24 h in XTLE patients. Utilization of 8 frontal and temporal electrodes reduced average sensitivity from 86{\%} to 81{\%}. Conclusion Our automatic multimodal seizure detection algorithm shows high sensitivity with full and reduced electrode montages. Significance Evaluation of different signal modalities and electrode montages paces the way for semi-automatic seizure documentation systems.},
author = {F{\"{u}}rbass, F. and Kampusch, S. and Kaniusas, E. and Koren, J. and Pirker, S. and Hopfeng{\"{a}}rtner, R. and Stefan, H. and Kluge, T. and Baumgartner, C.},
doi = {10.1016/j.clinph.2017.05.013},
file = {::},
issn = {18728952},
journal = {Clinical Neurophysiology},
keywords = {Algorithm,Automatic,ECG,EEG,EMG,Multimodal,Seizure detection},
month = {aug},
number = {8},
pages = {1466--1472},
publisher = {Elsevier Ireland Ltd},
title = {{Automatic multimodal detection for long-term seizure documentation in epilepsy}},
volume = {128},
year = {2017}
}
@article{Smit,
abstract = {Maintainability is a desirable property of software, and a variety of metrics have been proposed for measuring it, all based on different notions of complexity. Although these metrics are useful, complexity is only one factor influencing maintainability. Practical experience in software development has led to a set of best practices and coding conventions that are believed to make source code easier to read, understand and maintain. Based on a survey of software engineers, we identify the relative importance of 71 coding conventions to maintainability. We propose a metric that offers a different perspective on maintenance, namely a "convention adherence" metric based on the number and severity of violations of these coding conventions. We examine the code repositories of four open-source Java projects to measure their adherence to coding conventions over the life of the project, based on both their self-identified conventions and those of the convention-adherence metric. Through our analysis, we discovered several interesting phenomena, including pre-release effort to bring new code in line with desirable conventions, effective usage of automated code convention checkers as part of the build process to improve adherence, variations in adherence over the software lifecycle, and a class of conventions consistently ignored in open source projects.},
author = {Smit, Michael and Gergel, Barry and Hoover, HJ J and Stroulia, Eleni},
file = {::},
journal = {Cs.Ualberta.Ca},
pages = {1--10},
title = {{Maintainability and Source Code Conventions: An Analysis of Open Source Projects}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Maintainability+and+Source+Code+Conventions+:+An+Analysis+of+Open+Source+Projects{\#}0}
}
@article{Chikkerur2010,
abstract = {In the theoretical framework of this paper, attention is part of the inference process that solves the visual recognition problem of what is where. The theory proposes a computational role for attention and leads to a model that predicts some of its main properties at the level of psychophysics and physiology. In our approach, the main goal of the visual system is to infer the identity and the position of objects in visual scenes: spatial attention emerges as a strategy to reduce the uncertainty in shape information while feature-based attention reduces the uncertainty in spatial information. Featural and spatial attention represent two distinct modes of a computational process solving the problem of recognizing and localizing objects, especially in difficult recognition tasks such as in cluttered natural scenes. We describe a specific computational model and relate it to the known functional anatomy of attention. We show that several well-known attentional phenomena - including bottom-up pop-out effects, multiplicative modulation of neuronal tuning curves and shift in contrast responses - all emerge naturally as predictions of the model. We also show that the Bayesian model predicts well human eye fixations (considered as a proxy for shifts of attention) in natural scenes. {\textcopyright} 2010 Elsevier Ltd.},
author = {Chikkerur, Sharat and Serre, Thomas and Tan, Cheston and Poggio, Tomaso},
doi = {10.1016/j.visres.2010.05.013},
file = {::},
issn = {00426989},
journal = {Vision Research},
keywords = {Attention,Bayesian inference,Computational model,Object recognition},
number = {22},
pages = {2233--2247},
publisher = {Elsevier Ltd},
title = {{What and where: A Bayesian inference theory of attention}},
url = {http://dx.doi.org/10.1016/j.visres.2010.05.013},
volume = {50},
year = {2010}
}
@article{Hao2012,
abstract = {Many questions submitted to Collaborative Question Answering (CQA) sites have similar questions answered before. We propose a precise approach of automatically finding an answer to such questions by automatically identifying "equivalent" questions submitted and answered, in the past. Our method is based on automatically generating equivalent question patterns by grouping together questions that have previously obtained the same answers. The generated patterns are used as seed patterns to match more questions to extract large number of equivalent patterns by a new bootstrapping-based learning method. The resulting patterns can be applied to match a new question to an equivalent one that has already been answered, and thus suggest potential answers automatically. We experimented with this approach over a large collection of more than 200,000 real questions drawn from the Yahoo! Answers archive, automatically acquiring over 16,991 groups of equivalent question patterns. These patterns allow our method to obtain over 57{\%} recall and over 54{\%} precision on suggesting an answer automatically to new questions, significantly improving over baseline methods.},
author = {Hao, Tianyong and Agichtein, Eugene},
doi = {10.1007/s10791-012-9188-x},
file = {::},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Bootstrapping,Collaborative question answering,Equivalent pattern,Pattern extension},
month = {jun},
number = {3-4},
pages = {332--353},
title = {{Finding similar questions in collaborative question answering archives: Toward bootstrapping-based equivalent pattern learning}},
volume = {15},
year = {2012}
}
@article{Yuan2012,
abstract = {Automatic seizure detection is significant in both diagnosis of epilepsy and relieving the heavy workload of inspecting prolonged EEG. This paper presents a new seizure detection method for multi-channel long-term EEG. The fractal intercept derived from fractal geometry is extracted as a novel nonlinear feature of EEG signals, and the relative fluctuation index is calculated as a linear feature. The feature vector, consisting of the two EEG descriptors, is fed into a single-layer neural network for classification. Extreme learning machine (ELM) algorithm is adopted to train the neural network. Finally, post-processing including smoothing, channel fusion, and collar technique is employed to obtain more accurate and stable results. Both the segment-based and event-based assessments are used for the performance evaluation of this method on the 21-patient Freiburg dataset. The segment-based sensitivity of 91.72{\%} and specificity of 94.89{\%} were achieved. For the event-based assessment, this method yielded a sensitivity of 93.85{\%} with a false detection rate of 0.35/h. {\textcopyright}2012 Elsevier Inc.},
author = {Yuan, Qi and Zhou, Weidong and Liu, Yinxia and Wang, Jiwen},
doi = {10.1016/j.yebeh.2012.05.009},
file = {::},
issn = {15255050},
journal = {Epilepsy and Behavior},
keywords = {Automatic seizure detection,Extreme learning machine,Fluctuation index,Fractal intercept},
month = {aug},
number = {4},
pages = {415--421},
pmid = {22687388},
title = {{Epileptic seizure detection with linear and nonlinear features}},
volume = {24},
year = {2012}
}
@techreport{Saha2015,
author = {Saha, Debasmita and Mandal, Ardhendu},
booktitle = {Article in INTERNATIONAL JOURNAL OF COMPUTER SCIENCES AND ENGINEERING},
file = {::},
title = {{User Interface Design Issues for Easy and Efficient Human Computer Interaction: An Explanatory Approach Design and Development of an Artificial Neural Network (ANN) based expert system to diagnose human brain Tumor from CT scan and MRI images View project}},
url = {https://www.researchgate.net/publication/294428623},
year = {2015}
}
@inproceedings{Khan2012,
abstract = {The proposed research work designs a detector algorithm for automatic detection of epileptic seizures. In this work a wavelet based feature extraction technique has been adopted. Epochs of EEG are decomposed using discrete wavelet transform (DWT) up to 5 level of wavelet decomposition. Relative values of energy and a normalized coefficient of variation (NCOV) based measure, ({\$}\sigma{\$} 2/{\$}\mu{\$} a) are computed on the wavelet coefficients acquired in the frequency range of 0-32 Hz from both seizure and non-seizure segments. The performance of NCOV over the traditionally used coefficient of variation, COV ({\$}\sigma{\$} 2/{\$}\mu{\$} 2) was studied. The feature NCOV yielded better performance than the commonly used COV, {\$}\sigma{\$} 2/ {\$}\mu{\$} 2. The algorithm was evaluated on 5 subjects from CHB-MIT scalp EEG database. {\textcopyright}2012 IEEE.},
author = {Khan, Yusuf Uzzaman and Rafiuddin, Nidal and Farooq, Omar},
booktitle = {2012 IEEE International Conference on Signal Processing, Computing and Control, ISPCC 2012},
doi = {10.1109/ISPCC.2012.6224361},
file = {::},
isbn = {9781467313179},
keywords = {Discrete Wavelet Transform,EEG,Epilepsy,Feature Space,Seizure},
title = {{Automated seizure detection in scalp EEG using multiple wavelet scales}},
year = {2012}
}
@article{Vercoulen1994,
abstract = {The absence of laboratory tests and clear criteria to identify homogeneous (sub)groups in patients presenting with unexplained fatigue, and to assess clinical status and disability in these patients, calls for further assessment methods. In the present study, a multi-dimensional approach to the assessment of chronic fatigue syndrome (CFS) is evaluated. Two-hundred and ninety-eight patients with CFS completed a set of postal questionnaires that assessed the behavioural, emotional, social, and cognitive aspects of CFS. By means of statistical analyses nine relatively independent dimensions of CFS were identified along which CFS-assessment and CFS-research can be directed. These dimensions were named: psychological well-being, functional impairment in daily life, sleep disturbances, avoidance of physical activity, neuropsychological impairment, causal attributions related to the complaints, social functioning, self-efficacy expectations, and subjective experience of the personal situation. A description of the study sample on these dimensions is presented. {\textcopyright} 1994.},
author = {Vercoulen, Jan H.M.M. and Swanink, Caroline M.A. and Fennis, Jan F.M. and Galama, Joep M.D. and van der Meer, Jos W.M. and Bleijenberg, Gijs},
doi = {10.1016/0022-3999(94)90099-X},
issn = {00223999},
journal = {Journal of Psychosomatic Research},
month = {jul},
number = {5},
pages = {383--392},
pmid = {7965927},
publisher = {Elsevier},
title = {{Dimensional assessment of chronic fatigue syndrome}},
url = {https://linkinghub.elsevier.com/retrieve/pii/002239999490099X},
volume = {38},
year = {1994}
}
@article{Shang2016,
annote = {Overview of imbalanced learning},
author = {Shang, Jennifer and Mingyun, Gu and Yijing, Li and Bing, Gong and Yuanyue, Huang and Haixiang, Guo},
doi = {10.1016/j.eswa.2016.12.035},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Data,Imbalanced data,Machine learning,Rare events},
number = {December},
pages = {220--239},
publisher = {Elsevier Ltd},
title = {{Learning from class-imbalanced data: Review of methods and applications}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.12.035},
volume = {73},
year = {2016}
}
@article{Alexandru2019,
abstract = {Researchers often analyze several revisions of a software project to obtain historical data about its evolution. For example, they statically analyze the source code and monitor the evolution of certain metrics over multiple revisions. The time and resource requirements for running these analyses often make it necessary to limit the number of analyzed revisions, e.g., by only selecting major revisions or by using a coarse-grained sampling strategy, which could remove significant details of the evolution. Most existing analysis techniques are not designed for the analysis of multi-revision artifacts and they treat each revision individually. However, the actual difference between two subsequent revisions is typically very small. Thus, tools tailored for the analysis of multiple revisions should only analyze these differences, thereby preventing re-computation and storage of redundant data, improving scalability and enabling the study of a larger number of revisions. In this work, we propose the Lean Language-Independent Software Analyzer (LISA), a generic framework for representing and analyzing multi-revisioned software artifacts. It employs a redundancy-free, multi-revision representation for artifacts and avoids re-computation by only analyzing changed artifact fragments across thousands of revisions. The evaluation of our approach consists of measuring the effect of each individual technique incorporated, an in-depth study of LISA resource requirements and a large-scale analysis over 7 million program revisions of 4,000 software projects written in four languages. We show that the time and space requirements for multi-revision analyses can be reduced by multiple orders of magnitude, when compared to traditional, sequential approaches.},
author = {Alexandru, Carol V and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C},
doi = {10.1007/s10664-018-9630-9},
file = {::},
issn = {1573-7616},
journal = {Empirical Software Engineering},
month = {feb},
number = {1},
pages = {332--380},
title = {{Redundancy-free analysis of multi-revision software artifacts}},
url = {https://doi.org/10.1007/s10664-018-9630-9},
volume = {24},
year = {2019}
}
@article{Rasheed2020,
abstract = {With the advancement in artificial intelligence (AI) and machine learning (ML) techniques, researchers are striving towards employing these techniques for advancing clinical practice. One of the key objectives in healthcare is the early detection and prediction of disease to timely provide preventive interventions. This is especially the case for epilepsy, which is characterized by recurrent and unpredictable seizures. Patients can be relieved from the adverse consequences of epileptic seizures if it could somehow be predicted in advance. Despite decades of research, seizure prediction remains an unsolved problem. This is likely to remain at least partly because of the inadequate amount of data to resolve the problem. There have been exciting new developments in ML-based algorithms that have the potential to deliver a paradigm shift in the early and accurate prediction of epileptic seizures. Here we provide a comprehensive review of state-of-the-art ML techniques in early prediction of seizures using EEG signals. We will identify the gaps, challenges, and pitfalls in the current research and recommend future directions.},
archivePrefix = {arXiv},
arxivId = {2002.01925},
author = {Rasheed, Khansa and Qayyum, Adnan and Qadir, Junaid and Sivathamboo, Shobi and Kawn, Patrick and Kuhlmann, Levin and O'Brien, Terence and Razi, Adeel},
doi = {10.1109/RBME.2020.3008792},
eprint = {2002.01925},
issn = {19411189},
journal = {IEEE Reviews in Biomedical Engineering},
keywords = {EEG,Epileptic Seizure,Machine Learning},
pmid = {32746369},
publisher = {Institute of Electrical and Electronics Engineers},
title = {{Machine Learning for Predicting Epileptic Seizures Using EEG Signals: A Review}},
year = {2020}
}
@article{Randolph2016,
abstract = {The occupational and environmental health nurse should encourage workers to take work breaks. Breaks enable all workers to perform at their best.},
author = {Randolph, Susan A},
doi = {10.1177/2165079916653416},
file = {::},
issn = {2165-0969},
journal = {Workplace health {\&} safety},
keywords = {breaks,exercise,health promotion,mental health,stress},
month = {jul},
number = {7},
pages = {344},
pmid = {27282978},
publisher = {SAGE Publications Inc.},
title = {{The Importance of Employee Breaks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27282978},
volume = {64},
year = {2016}
}
@article{Gheorghiu2018,
abstract = {As road traffic conditions worsen due to the constantly increasing number of cars, traffic management systems are struggling to provide a suitable environment, by gathering all the relevant information from the road network. However, in most cases these are obtained via traffic detectors placed near road junctions, thus providing no information on the conditions in between. A large-scale sensor network using detectors on the majority of vehicles would certainly be capable of providing useful data, but has two major impediments: the equipment installed on the vehicles should be cheap enough (assuming the willingness of private car owners to be a part of the network) and be capable of transferring the required amount of data in due time, as the vehicle passes by the road side unit that acts as interface with the traffic management system. These restrictions reduce the number of technologies that can be used. In this article a series of comprehensive tests have been performed to evaluate the Bluetooth and ZigBee protocols for this purpose from many points of view: handshake time, static and dynamic data transfer (in laboratory conditions and in real traffic conditions). An assessment of the environmental conditions (during tests and probable to be encountered in real conditions) was also provided.},
author = {Gheorghiu, Razvan and Iordache, Valentin},
doi = {10.3390/s18061801},
file = {::},
issn = {1424-8220},
journal = {Sensors},
keywords = {Bluetooth,Data acquisition system,Sensor networks,Sensors,Vehicular communications,Wi-Fi,ZigBee},
month = {jun},
number = {6},
pages = {1801},
publisher = {MDPI AG},
title = {{Use of Energy Efficient Sensor Networks to Enhance Dynamic Data Gathering Systems: A Comparative Study between Bluetooth and ZigBee}},
url = {http://www.mdpi.com/1424-8220/18/6/1801},
volume = {18},
year = {2018}
}
@article{Wang2019,
abstract = {By bringing together code, text, and examples, Jupyter notebooks have become one of the most popular means to produce scientific results in a productive and reproducible way. As many of the notebook authors are experts in their scientific fields, but laymen with respect to software engineering, one may ask questions on the quality of notebooks and their code. In a preliminary study, we experimentally demonstrate that Jupyter notebooks are inundated with poor quality code, e.g., not respecting recommended coding practices, or containing unused variables and deprecated functions. Considering the education nature of Jupyter notebooks, these poor coding practices as well as the lacks of quality control might be propagated into the next generation of developers. Hence, we argue that there is a strong need to programmatically analyze Jupyter notebooks, calling on our community to pay more attention to the reliability of Jupyter notebooks.},
archivePrefix = {arXiv},
arxivId = {1906.05234},
author = {Wang, Jiawei and Li, Li and Zeller, Andreas},
eprint = {1906.05234},
file = {::},
number = {3},
title = {{Better Code, Better Sharing:On the Need of Analyzing Jupyter Notebooks}},
url = {http://arxiv.org/abs/1906.05234},
year = {2019}
}
@article{West2009,
abstract = {The current productivity crisis in drug discovery has prompted the pharmaceutical industry to decentralize R{\&}D, which is now more responsive, more flexible and better connected to research in academia and biotechnology firms. Organizational changes are also under way in academia. Universities are expanding their technology transfer offices and research funders are investing more in translational research. This article explains how organizational changes in industry and academia can complement each other. Successful translation of research into innovative drugs needs to take account of the increasing organizational complexity of drug discovery as the knowledge to be integrated becomes more diffuse, specialized and valuable. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {West, Will and Nightingale, Paul},
doi = {10.1016/j.tibtech.2009.06.007},
issn = {01677799},
journal = {Trends in Biotechnology},
month = {oct},
number = {10},
pages = {558--561},
pmid = {19683820},
publisher = {Elsevier Current Trends},
title = {{Organizing for innovation: towards successful translational research}},
volume = {27},
year = {2009}
}
@article{Lu2018,
abstract = {This paper presents a systematic survey on recent development of neural text generation models. Specifically, we start from recurrent neural network language models with the traditional maximum likelihood estimation training scheme and point out its shortcoming for text generation. We thus introduce the recently proposed methods for text generation based on reinforcement learning, re-parametrization tricks and generative adversarial nets (GAN) techniques. We compare different properties of these models and the corresponding techniques to handle their common problems such as gradient vanishing and generation diversity. Finally, we conduct a benchmarking experiment with different types of neural text generation models on two well-known datasets and discuss the empirical results along with the aforementioned model properties.},
archivePrefix = {arXiv},
arxivId = {1803.07133},
author = {Lu, Sidi and Zhu, Yaoming and Zhang, Weinan and Wang, Jun and Yu, Yong},
eprint = {1803.07133},
file = {::},
month = {mar},
title = {{Neural Text Generation: Past, Present and Beyond}},
url = {http://arxiv.org/abs/1803.07133},
year = {2018}
}
@article{Dove2012,
abstract = {This paper is a reflection on the emerging genre of narrative visualization, a creative response to the need to share complex data engagingly with the public. In it, we explain how narrative visualization offers authors the opportunity to communicate more effectively with their audience by reproducing and sharing an experience of insight similar to their own. To do so, we propose a two part model, derived from previous literature, in which insight is understood as both an experience and also the product of that experience. We then discuss how the design of narrative visualization should be informed by attempts elsewhere to track the provenance of insights and share them in a collaborative setting. Finally, we present a future direction for research that includes using EEG technology to record neurological patterns during episodes of insight experience as the basis for evaluation.},
author = {Dove, G. and Jones, S.},
keywords = {Z665 Library Science. Information Science},
title = {{Narrative Visualization: Sharing Insights into Complex Data}},
year = {2012}
}
@techreport{Simard,
abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple "do-it-yourself" implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
author = {Simard, Patrice Y and Steinkraus, Dave and Platt, John C},
file = {::},
title = {{Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis}}
}
@article{Tristan2014,
abstract = {Implementing inference procedures for each new probabilistic model is time-consuming and error-prone. Probabilistic programming addresses this problem by allowing a user to specify the model and then automatically generating the inference procedure. To make this practical it is important to generate high per-formance inference code. In turn, on modern architectures, high performance re-quires parallel execution. In this paper we present Augur, a probabilistic modeling language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network.},
author = {Tristan, JB B and Huang, Daniel},
file = {::},
journal = {Advances in Neural {\ldots}},
pages = {1--9},
title = {{Augur: Data-Parallel Probabilistic Modeling}},
url = {http://papers.nips.cc/paper/5531-augur-data-parallel-probabilistic-modeling},
year = {2014}
}
@inproceedings{Kery2017,
abstract = {How do people ideate through code? Using semi-structured interviews and a survey, we studied data scientists who program, often with small scripts, to experiment with data. These studies show that data scientists frequently code new analysis ideas by building off of their code from a previous idea. They often rely on informal versioning interactions like copying code, keeping unused code, and commenting out code to repurpose older analysis code while attempting to keep those older analyses intact. Unlike conventional version control, these informal practices allow for fast versioning of any size code snippet, and quick comparisons by interchanging which versions are run. However, data scientists must maintain a strong mental map of their code in order to distinguish versions, leading to errors and confusion. We explore the needs for improving version control tools for exploratory tasks, and demonstrate a tool for lightweight local versioning, called Variolite, which programmers found usable and desirable in a preliminary usability study.},
author = {Kery, Mary Beth and Horvath, Amber and Myers, Brad},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3025453.3025626},
file = {::},
isbn = {9781450346559},
keywords = {End-user programming,Exploratory data analysis,Variants,Variations,Version control systems (VCS)},
pages = {1265--1276},
publisher = {ACM},
title = {{Variolite: Supporting Exploratory Programming by Data Scientists}},
url = {https://dl.acm.org/doi/10.1145/3025453.3025626},
year = {2017}
}
@article{Wang2018d,
abstract = {As an emerging research topic, online class imbalance learning often combines the challenges of both class imbalance and concept drift. It deals with data streams having very skewed class distributions, where concept drift may occur. It has recently received increased research attention; however, very little work addresses the combined problem where both class imbalance and concept drift coexist. As the first systematic study of handling concept drift in class-imbalanced data streams, this paper first provides a comprehensive review of current research progress in this field, including current research focuses and open challenges. Then, an in-depth experimental study is performed, with the goal of understanding how to best overcome concept drift in online learning with class imbalance. Based on the analysis, a general guideline is proposed for the development of an effective algorithm.},
annote = {From Duplicate 1 (A Systematic Study of Online Class Imbalance Learning with Concept Drift - Wang, Shuo; Minku, Leandro L.; Yao, Xin)
And Duplicate 3 (A Systematic Study of Online Class Imbalance Learning with Concept Drift - Wang, Shuo; Minku, Leandro L; Yao, Xin)

Precision is a measure of exactness - the proportion of positive class examples that are classified correctly to the examples predicted as positive by the classifier

Concept drift is mainly categorized into two groups - active vs passive approaches, depending on whether an explicit drift mechanism is employed or not.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06683v1},
author = {Wang, Shuo and Minku, Leandro L. and Yao, Xin},
doi = {10.1109/TNNLS.2017.2771290},
eprint = {arXiv:1703.06683v1},
file = {::},
isbn = {2017030317},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Class imbalance,concept drift,online learning,resampling},
month = {oct},
number = {10},
pages = {4802--4821},
pmid = {29993955},
publisher = {IEEE},
title = {{A Systematic Study of Online Class Imbalance Learning with Concept Drift}},
volume = {29},
year = {2018}
}
@inproceedings{smit2011code,
abstract = {Maintainability is a desired property of software, and a variety of metrics have been proposed for measuring it, focusing on different notions of complexity and code readability. Many practices have been proposed to improve maintainability through code refactorings: improving the cohesion, simplification of interfaces, renamings to improve understandability. Code conventions are a body of advice on lexical and syntactic aspects of code, aiming to standardize low-level code design under the assumption that such a systematic approach will make code easier to read, understand, and maintain. We present the first stage in our examination of code-convention adherence practices as a proxy measurement for maintainability. Based on a preliminary survey of software engineers, we identify a set of coding conventions that most relate to maintainability. Then we devise a {\&}{\#}x201C;convention adherence{\&}{\#}x201D; metric, based on the number and severity of violations of a defined coding convention. Finally, we analyze several open-source projects according to this metric to better understand how consistent different teams are with respect to adopting and conforming to code conventions.},
author = {Smit, Michael and Gergel, Barry and Hoover, H. James and Stroulia, Eleni},
booktitle = {2011 27th IEEE International Conference on Software Maintenance (ICSM)},
doi = {10.1109/ICSM.2011.6080819},
file = {::},
isbn = {9781457706646},
organization = {IEEE},
pages = {504--507},
title = {{Code convention adherence in evolving software}},
year = {2011}
}
@article{Nelesen2008,
abstract = {Background: Although characteristics such as heart rate (HR) and blood pressure (BP) are commonly reported in studies of the relationship between fatigue and cardiac functioning, few reports examine how cardiac function parameters such as cardiac output (CO) and stroke volume (SV) relate to fatigue. This study examined the relationship between self-reported fatigue and hemodynamic functioning at rest and in response to a public speaking stressor in healthy individuals. Methods: A total of 142 individuals participated in this study. Subjects were placed in low-, moderate-, or high-fatigue groups based on their Profile of Moods State fatigue scale. Heart rate, SV, and CO were determined using impedance cardiography at rest and during a speaking stressor. Stroke volume and CO values were converted to stroke index (SI) and cardiac index (CI) by adjusting for body surface area. Data were analyzed with hierarchical regression analysis and a 3 (group) x 3 (stress period) mixed model analysis of variance. Results: At rest, fatigue was not associated with BP or HR but was significantly associated with decreased CI (P{\textless}.001; 95{\%} confidence interval, -0.046 to -0.014) and stroke index (SI) (P=.002; 95{\%} confidence interval -0.664 to -0.151), even after controlling for demographic variables and depressive symptoms. Heart rate and BP increased, as expected, from baseline to preparation to speaking stressor (F1,124=118.6 and F 1,122=46.450, respectively) (P{\textless}.001 for both). More interestingly, there were effects on SI and CI of fatigue (P{\textless}.03 for both) and stress (P{\textless}.03 for both); high-fatigue individuals had lower SI and CI levels than moderate- and low-fatigue individuals both at rest and in response to the stressor. Conclusion: This study demonstrates that fatigue complaints may have hemodynamic correlates even in ostensibly healthy individuals. {\textcopyright}2008 American Medical Association. All rights reserved.},
author = {Nelesen, Richard and Dar, Yasmin and Thomas, Kamala and Dimsdale, Joel E.},
doi = {10.1001/archinte.168.9.943},
file = {::},
issn = {00039926},
journal = {Archives of Internal Medicine},
month = {may},
number = {9},
pages = {943--949},
title = {{The relationship between fatigue and cardiac functioning}},
volume = {168},
year = {2008}
}
@inproceedings{Mirowski2008,
abstract = {Recent research suggests that electrophysiological changes develop minutes to hours before the actual clinical onset in focal epileptic seizures. Seizure prediction is a major field of neurological research, enabled by statistical analysis methods applied to features derived from intracranial Electroencephalographic (EEG) recordings of brain activity. However, no reliable seizure prediction method is ready for clinical applications. In this study, we use modern machine learning techniques to predict seizures from a number of features proposed in the literature. We concentrate on aggregated features that encode the relationship between pairs of EEG channels, such as cross-correlation, nonlinear interdependence, difference of Lyapunov exponents and wavelet analysis-based synchrony such as phase locking. We compare L1-regularized logistic regression, convolutional networks, and support vector machines. Results are reported on the standard Freiburg EEG dataset which contains data from 21 patients suffering from medically intractable focal epilepsy. For each patient, at least one method predicts 100{\%} of the seizures on average 60 minutes before the onset, with no false alarm. Possible future applications include implantable devices capable of warning the patient of an upcoming seizure as well as implanted drug-delivery devices. {\textcopyright}2008 IEEE.},
author = {Mirowski, Piotr W. and LeCun, Yann and Madhavan, Deepak and Kuzniecky, Ruben},
booktitle = {Proceedings of the 2008 IEEE Workshop on Machine Learning for Signal Processing, MLSP 2008},
doi = {10.1109/MLSP.2008.4685487},
file = {::},
isbn = {9781424423767},
pages = {244--249},
title = {{Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG}},
year = {2008}
}
@article{Chavaillaz2016,
abstract = {Little is known about the long-term effects of system reliability when operators do not use a system during an extended lay-off period. To examine threats to skill maintenance, 28 participants operated twice a simulation of a complex process control system for 2.5 h, with an 8-month retention interval between sessions. Operators were provided with an adaptable support system, which operated at one of the following reliability levels: 60{\%}, 80{\%} or 100{\%}. Results showed that performance, workload, and trust remained stable at the second testing session, but operators lost self-confidence in their system management abilities. Finally, the effects of system reliability observed at the first testing session were largely found again at the second session. The findings overall suggest that adaptable automation may be a promising means to support operators in maintaining their performance at the second testing session.},
author = {Chavaillaz, Alain and Wastell, David and Sauer, J??rgen},
doi = {10.1016/j.apergo.2015.10.006},
file = {::},
issn = {18729126},
journal = {Applied Ergonomics},
keywords = {Adaptable automation,Lay-off period,Skill retention,System reliability,Trust},
pmid = {26603139},
title = {{Effects of extended lay-off periods on performance and operator trust under adaptable automation}},
year = {2016}
}
@article{Voulodimos2018,
abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
doi = {10.1155/2018/7068349},
file = {::},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
title = {{Deep Learning for Computer Vision: A Brief Review}},
volume = {2018},
year = {2018}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
publisher = {Springer Verlag},
title = {{Pattern Recognition and Machine Learning (Information Science and Statistics)}},
year = {2006}
}
@misc{Radon2018,
author = {Radon},
title = {{Radon 2.4.0 documentation}},
url = {https://radon.readthedocs.io/en/latest/},
urldate = {2019-12-16},
year = {2018}
}
@inproceedings{Vartak2016,
abstract = {{\textcopyright} 2016 Copyright held by the owner/author(s). Building a machine learning model is an iterative process. A data scientist will build many tens to hundreds of models before arriving at one that meets some acceptance criteria (e.g. AUC cutoff, accuracy threshold). However, the current style of model building is ad-hoc and there is no practical way for a data scientist to manage models that are built over time. As a result, the data scientist must attempt to "remember" previously constructed models and insights obtained from them. This task is challenging for more than a handful of models and can hamper the process of sensemaking. Without a means to manage models, there is no easy way for a data scientist to answer questions such as "Which models were built using an incorrect feature?", "Which model performed best on American customers?" or "How did the two top models compare?" In this paper, we describe our ongoing work on ModelDB, a novel end-to-end system for the management of machine learning models. ModelDB clients automatically track machine learning models in their native environments (e.g. scikit-learn, spark.ml), the ModelDB backend introduces a common layer of abstractions to represent models and pipelines, and the ModelDB frontend allows visual exploration and analyses of models via a web-based interface.},
author = {Vartak, Manasi and Subramanyam, Harihar and Lee, Wei En and Viswanathan, Srinidhi and Husnoo, Saadiyah and Madden, Samuel and Zaharia, Matei},
booktitle = {HILDA 2016 - Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
doi = {10.1145/2939502.2939516},
isbn = {9781450342070},
title = {{ModelDB: A system for machine learning model management}},
url = {https://dl.acm.org/citation.cfm?id=2939516},
year = {2016}
}
@article{THAUT2005,
abstract = {In a series of experiments, we have begun to investigate the effect of music as a mnemonic device on learning and memory and the underlying plasticity of oscillatory neural networks. We used verbal learning and memory tests (standardized word lists, AVLT) in conjunction with electroencephalographic analysis to determine differences between verbal learning in either a spoken or musical (verbal materials as song lyrics) modality. In healthy adults, learning in both the spoken and music condition was associated with significant increases in oscillatory synchrony across all frequency bands. A significant difference between the spoken and music condition emerged in the cortical topography of the learning-related synchronization. When using EEG measures as predictors during learning for subsequent successful memory recall, significantly increased coherence (phase-locked synchronization) within and between oscillatory brain networks emerged for music in alpha and gamma bands. In a similar study with multiple sclerosis patients, superior learning and memory was shown in the music condition when controlled for word order recall, and subjects were instructed to sing back the word lists. Also, the music condition was associated with a significant power increase in the low-alpha band in bilateral frontal networks, indicating increased neuronal synchronization. Musical learning may access compensatory pathways for memory functions during compromised PFC functions associated with learning and recall. Music learning may also confer a neurophysiological advantage through the stronger synchronization of the neuronal cell assemblies underlying verbal learning and memory. Collectively our data provide evidence that melodic-rhythmic templates as temporal structures in music may drive internal rhythm formation in recurrent cortical networks involved in learning and memory.},
author = {THAUT, M. H.},
doi = {10.1196/annals.1360.017},
issn = {0077-8923},
journal = {Annals of the New York Academy of Sciences},
keywords = {EEG,brain plasticity,memory,multiple sclerosis,music,temporal entrainment},
month = {dec},
number = {1},
pages = {243--254},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Temporal Entrainment of Cognitive Functions: Musical Mnemonics Induce Brain Plasticity and Oscillatory Synchrony in Neural Networks Underlying Memory}},
url = {http://doi.wiley.com/10.1196/annals.1360.017},
volume = {1060},
year = {2005}
}
@article{GeorgeW.Brown2016,
author = {{George W . Brown}, John Y . Lu and Robert J . Wolfson},
file = {::},
number = {1},
pages = {51--63},
title = {{Dynamic Modelling of Inventories Subject to Obsolescence Author ( s ): George W . Brown , John Y . Lu and Robert J . Wolfson Published by : INFORMS Stable URL : http://www.jstor.org/stable/2627992}},
volume = {11},
year = {2016}
}
@article{DeHaan2006,
abstract = {This paper describes a case study for constructing the yearly schedule of a secondary school in the Netherlands. This construction is divided in three steps. In the first step we create cluster schemes containing the optional subjects. A cluster scheme consists of cluster lines, and a cluster line contains classes which will be taught simultaneously. Part of the problem is that the students are not yet assigned to the classes. Once the cluster schemes are fixed, it remains to schedule the lessons to time slots and rooms. We first schedule the lessons to day-parts, and once this is completed we schedule the lessons to time slots within the day-parts. Thanks to consistency checks in the day-part phase, going from day-parts to time slots is possible. Finally, in the third step, we improve the previously found schedule by a tabu search using ejection chains. Compared to hand-made schedules, the results are very promising. {\textcopyright}Springer-Verlag Berlin Heidelberg 2007.},
author = {{De Haan}, Peter and Landman, Ronald and Post, Gerhard and Ruizenaar, Henri},
doi = {10.1007/978-3-540-77345-0_17},
file = {::},
isbn = {3540773444},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {267--279},
title = {{A case study for timetabling in a dutch secondary school}},
volume = {3867 LNCS},
year = {2006}
}
@inproceedings{Halford2017,
abstract = {Purpose: The goal of the project is to determine characteristics of academic neurophysiologist EEG interpreters (EEGers), which predict good interrater agreement (IRA) and to determine the number of EEGers needed to develop an ideal standardized testing and training data set for epileptiform transient (ET) detection algorithms. Methods: A three-phase scoring method was used. In phase 1, 19 EEGers marked the location of ETs in two hundred 30-second segments of EEG from 200 different patients. In phase 2, EEG events marked by at least 2 EEGers were annotated by 18 EEGers on a 5-point scale to indicate whether they were ETs. In phase 3, a third opinion was obtained from EEGers on any inconsistencies between phase 1 and phase 2 scoring. Results: The IRA for the 18 EEGers was only fair. A select group of the EEGers had good IRA and the other EEGers had low IRA. Board certification by the American Board of Clinical Neurophysiology was associated with better IRA performance but other board certifications, years of fellowship training, and years of practice were not. As the number of EEGers used for scoring is increased, the amount of change in the consensus opinion decreases steadily and is quite low as the group size approaches 10. Conclusions: The IRA among EEGers varies considerably. The EEGers must be tested before use as scorers for ET annotation research projects. The American Board of Clinical Neurophysiology certification is associated with improved performance. The optimal size for a group of experts scoring ETs in EEG is probably in the 6 to 10 range.},
author = {Halford, Jonathan J. and Arain, Amir and Kalamangalam, Giridhar P. and Laroche, Suzette M. and Leonardo, Bonilha and Basha, Maysaa and Azar, Nabil J. and Kutluay, Ekrem and Martz, Gabriel U. and Bethany, Wolf J. and Waters, Chad G. and Dean, Brian C.},
booktitle = {Journal of Clinical Neurophysiology},
doi = {10.1097/WNP.0000000000000344},
file = {::},
issn = {15371603},
keywords = {EEG,Electroencephalography,Epileptiform transient,Interrater agreement,Spike detection},
month = {mar},
number = {2},
pages = {168--173},
publisher = {Lippincott Williams and Wilkins},
title = {{Characteristics of EEG Interpreters Associated with Higher Interrater Agreement}},
volume = {34},
year = {2017}
}
@article{Seo2014,
abstract = {Building is an integral part of the software development process. However, little is known about the compiler errors that occur in this process. In this paper, we present an empirical study of 26.6 million builds produced during a period of nine months by thousands of developers. We describe the workflow through which those builds are generated, and we analyze failure frequency, compiler error types, and resolution efforts to fix those compiler errors. The results provide insights on how a largeorganization build process works, and pinpoints errors for which further developer support would be most effective.},
author = {Seo, Hyunmin and Sadowski, Caitlin and Elbaum, Sebastian and Aftandilian, Edward and Bowdidge, Robert},
doi = {10.1145/2568225.2568255},
file = {::},
isbn = {9781450327565},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Software builds,build errors,empirical analysis},
number = {1},
pages = {724--734},
title = {{Programmers' build errors: A case study (at google)}},
year = {2014}
}
@article{Porcheron2018,
abstract = {Voice User Interfaces (VUIs) are becoming ubiquitously available, being embedded both into everyday mobility via smartphones, and into the life of the home via 'assistant' devices. Yet, exactly how users of such devices practically thread that use into their everyday social interactions remains underexplored. By collecting and studying audio data from month-long deployments of the Amazon Echo in participants' homes-informed by ethnomethodology and conversation analysis-our study documents the methodical practices of VUI users, and how that use is accomplished in the complex social life of the home. Data we present shows how the device is made accountable to and embedded into conversational settings like family dinners where various simultaneous activities are being achieved. We discuss how the VUI is finely coordinated with the sequential organisation of talk. Finally, we locate implications for the accountability of VUI interaction, request and response design, and raise conceptual challenges to the notion of designing 'conversational' interfaces.},
author = {Porcheron, Martin and Fischer, Joel E and Reeves, Stuart and Sharples, Sarah},
doi = {10.1145/3173574.3174214},
file = {::},
isbn = {9781450356206},
keywords = {Amazon Echo,collocated interaction,conversation analysis,conversational agent,conversational user interface,ethnomethodology,in-the-wild study,intelligent personal assistant,voice user interface},
title = {{Voice Interfaces in Everyday Life}},
url = {https://doi.org/10.1145/3173574.3174214},
year = {2018}
}
@article{Randles2017,
abstract = {As scientific work becomes more computational and data-intensive, research processes and results become more difficult to interpret and reproduce. In this poster, we show how the Jupyter notebook, a tool originally designed as a free version of Mathematica notebooks, has evolved to become a robust tool for scientists to share code, associated computation, and documentation.},
archivePrefix = {arXiv},
arxivId = {1804.05492},
author = {Randles, Bernadette M. and Pasquetto, Irene V. and Golshan, Milena S. and Borgman, Christine L.},
doi = {10.1109/JCDL.2017.7991618},
eprint = {1804.05492},
file = {::},
isbn = {9781538638613},
issn = {15525996},
journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
keywords = {Jupyter notebooks,data sharing and reuse,open data,open science,reproducibility},
pages = {31--32},
publisher = {IEEE},
title = {{Using the Jupyter Notebook as a Tool for Open Science: An Empirical Study}},
year = {2017}
}
@misc{Noachtar2009,
abstract = {Electroencephalography (EEG) is the most specific method to define epileptogenic cortex. Its sensitivity and specificity depend on several factors such as age and recording procedures, for example, sleep recordings and activation procedures (hyperventilation, photic stimulation). EEG reveals characteristic findings in several epilepsy syndromes. Rarely, epileptiform discharges are recorded in healthy, particularly young individuals. Ictal video/EEG recording is considered to be critical in localizing the epileptogenic zone. A careful analysis of the first clinical signs and symptoms of a seizure and of the evolution of the seizure symptomatology can provide important localizing clues. Although surface EEG recordings are less sensitive than invasive studies, they provide the best overview and, therefore, the most efficient way to define the approximate localization of the epileptogenic zone. Invasive recordings are used in patients in whom the epileptogenic zone either cannot be located with noninvasive diagnostic methods or is adjacent to eloquent cortex. The most commonly used invasive electrodes are stereotactically implanted depth electrodes and subdural strip or grid electrodes. Foramen ovale and epidural electrodes are of intermediate invasiveness, but less sensitive. Invasive electrodes are subject to sampling errors if misplaced and should be used only after exhaustive noninvasive evaluations have (1) failed to localize the epileptogenic zone and (2) led to a testable hypothesis regarding this localization. Invasive EEG studies are associated with additional risks that are justifiable only if there is a good chance of obtaining essential localizing information and on a potentially resectable area. {\textcopyright}2009 Elsevier Inc. All rights reserved.},
author = {Noachtar, Soheyl and R{\'{e}}mi, Jan},
booktitle = {Epilepsy and Behavior},
doi = {10.1016/j.yebeh.2009.02.035},
file = {::},
issn = {15255050},
keywords = {EEG,EEG-video-monitoring,Epilepsy,Epileptiform discharges,Invasive EEG},
month = {may},
number = {1},
pages = {22--33},
title = {{The role of EEG in epilepsy: A critical review}},
volume = {15},
year = {2009}
}
@techreport{Liaut2020,
author = {Liaut, Alexandre},
file = {::},
keywords = {Key Words},
title = {{Sigfox connected objects: Radio specifications}},
year = {2020}
}
@article{Gneiting2007a,
abstract = {Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection. {\textcopyright}2007 Royal Statistical Society.},
author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
doi = {10.1111/j.1467-9868.2007.00587.x},
file = {::},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
number = {2},
pages = {243--268},
title = {{Probabilistic forecasts, calibration and sharpness}},
volume = {69},
year = {2007}
}
@article{Jun2019,
abstract = {BACKGROUNDWork-related psychosocial factors such as job strain are thought to contribute to elevated psychological stress in office workers. One factor that may impact the relationship between job strain and psychological distress is the individual's coping resources.OBJECTIVEThe purpose of this study is to examine the interaction effect of coping resources on the relationship between job strain and psychological distress in office workers.METHODS220 office workers in Australia and Korea completed the Job Content Questionnaire (to evaluate job strain and social support at work), the Depression, Anxiety, and Stress Scale, (DASS-21, a measure of psychological distress), and the Coping with Job Stress Scale to assess control and escape coping. Hierarchical regression analyses were executed to examine the interaction and moderating effect of coping resources.RESULTSJob strain had a direct positive relationship with all three domains of psychological distress. The relationship between job strain and depression was positively moderated by escape coping, but negatively moderated by social support. Use of higher levels of escape coping predicted higher levels of depression and anxiety symptoms when higher levels of job strain were perceived.CONCLUSIONSFindings suggest there may be a direct relationship between job strain and psychological distress in office workers. This relationship, however, may be moderated by the office workers coping resources (coping strategies and social support). It is suggested that the evaluation of coping might be a key consideration in the elements of the assessment of psychological distress in office workers.},
author = {Jun, Deokhoon and O'Leary, Shaun and McPhail, Steven M. and Johnston, Venerina},
doi = {10.3233/pwor-192968},
issn = {10519815},
journal = {Work},
month = {aug},
number = {1},
pages = {55--65},
pmid = {31450530},
publisher = {IOS Press},
title = {{Job strain and psychological distress in office workers: The role of coping}},
volume = {64},
year = {2019}
}
@techreport{Langan2005,
abstract = {Objective: To examine the influence of various factors on the risk of sudden unexpected death in epilepsy (SUDEP). Methods: The authors investigated 154 cases in which a postmortem examination was performed. Each case had four controls with epilepsy from the community, matched for age and geographic location. Backward stepwise conditional logistic regression analysis was performed and odds ratios for risk and protection were determined. Results: The risk of SUDEP was increased with a history of generalized tonic-clonic seizures in the previous 3 months (odds ratio [OR]: 13.8, 95{\%} CI: 6.6 to 29.1). The presence of supervision at night was found to be protective (OR: 0.4, 95{\%} CI: 0.2 to 0.8) when a supervising individual shared the same bedroom or when special precautions such as a listening device were employed (OR: 0.1, 95{\%} CI: 0.0 to 0.3). Conclusion: This work lends support to the view that SUDEP is a seizure-related phenomenon and that control of tonic-clonic seizures is important in its prevention. Nocturnal supervision seems to protect against SUDEP. Cohort studies have reported risk factors for SUDEP including youth, male sex, remote symptomatic epilepsy , structural findings on neuropathology, severe epilepsy, alcohol abuse, abnormal EEGs with epilep-tiform changes, mental handicap, use of psychotropic medication, nonadherence to treatment, abrupt medication changes, low antiepileptic drug (AED) levels, and unwitnessed nocturnal seizures. 1 Case-control studies have identified a number of risk factors for sudden unexpected death in epilepsy (SUDEP). 2-5 A study from Stockholm 3 found that a higher risk of SUDEP was observed in those with more frequent seizures, polytherapy, frequent medication changes, longer disease duration, and idio-pathic epilepsy. Another study from the United States 4 reported increased risk of SUDEP by tonic-clonic seizures, polytherapy, and the presence of a learning disability. Another US study of mortality in AED development programs found that disease severity was significantly related to the risk of SUDEP. 5 Limitations of these studies include small numbers, 4 low postmortem rates, 4 and the exclusion of patients off AED therapy. 3 Some risk factors identified by cohort studies were not found to be significant in case-control studies. We undertook a large case-control study to confirm previous results and to look at factors not previously examined. Methods. People with epilepsy who died suddenly between the ages of 16 and 50 years were identified by coroners and neurolo-gists and by interviews with bereaved families. Deaths occurred between 1989 and 1998. Coroners in England and Wales were invited to notify neurolo-gists of cases considered to be SUDEP. Neurologists were contacted via the British Neurologic Surveillance Unit, 6 an organization that coordinates and improves the ascertainment of rare neurologic disorders in the United Kingdom. Cases were also identified through interviews with self-referred parents and partners of the deceased through Epilepsy Bereaved?, a UK support charity. Interviews involved a semistructured questionnaire that examined aspects of the patients' epilepsy, medical and social background , and the circumstances of death. Written informed consent was obtained before the interview. Subjects were individuals with a history of active epilepsy (at least one seizure in the past 5 years or taking an AED if in remission) whose death fulfilled the following definition: sudden, unexpected, witnessed, or unwitnessed, nontraumatic, and non-drowning death in an individual with epilepsy, with or without evidence of a seizure and excluding documented status epilepticus in which the postmortem examination does not reveal a cause for death. 7 Background information was obtained from general practitioners , hospital consultants and, where appropriate, coroners. Where necessary, information was traced through the Office for National Statistics. Each case had four controls matched for age (5 years) and geographic location. Practices in the appropriate geographic areas were identified from the MRC General Practice Research Framework , a network of approximately 900 groups of family practitioners (general practitioners) throughout the United Kingdom and includes practices in urban and rural areas. The groups allow access to 11{\%} of the UK population. Individuals with epilepsy suitable to act as controls were identified using a diagnostic index or prescription database. Controls were randomly chosen from this eligible population, and, once a diagnosis of epilepsy was confirmed, data were extracted from the patients' medical records. The factors under examination included duration of epilepsy, seizure type and control including changes in seizure severity, treatment history and compliance, recent AED withdrawal, concomitant use of psychotropic medication, family history of sudden death, learning disability, EEG changes, history of drug or alcohol abuse, presence of other medical conditions, level of attendance at doctor or hospital appointments, and supervision at night. Supervision at night was defined as the presence in the bedroom of an individual of normal intelligence and at least 10 years old or the use of special precautions. Special precautions involved regular checks throughout the night or the use of a listening device. For both cases and controls, 10{\%} of entries were randomly chosen and the accuracy of data retrieval from notes and data entry into the database checked. The error rate was less than 5{\%}. Backward stepwise conditional logistic regression analysis 8 From the},
author = {Langan, Y and Nashef, ; L and Sander, J W},
file = {::},
title = {{Case-control study of SUDEP}},
year = {2005}
}
@article{Spinney2015,
abstract = {Little is known of the patterns of physical activity, standing and sitting by office workers. However, insight into these behaviours is of growing interest, notably in regard to public health priorities to reduce non-communicable disease risk factors associated with high levels of sitting time and low levels of physical activity. With the advent and increasing availability of indoor tracking systems it is now becoming possible to build detailed pictures of the usage of indoor spaces. This paper reports initial results of indoor tracking used in conjunction with the Activ-PAL activity monitoring device. In this paper we give an overview of the usage of the tracking system and its installation and illustrate some of the resultant data. We also provide preliminary results that investigate the relationship between location, light physical activity and sitting in a small sample of office workers (n=33) from two separate office environments in order to demonstrate the relevance and explanatory power of the technique.},
author = {Spinney, Richard and Smith, Lee and Ucci, Marcella and Fisher, Abigail and Konstantatou, Marina and Sawyer, Alexia and Wardle, Jane and Marmot, Alexi},
doi = {10.1371/journal.pone.0127688},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
month = {may},
number = {5},
publisher = {Public Library of Science},
title = {{Indoor tracking to understand physical activity and sedentary behaviour: Exploratory study in UK office buildings}},
volume = {10},
year = {2015}
}
@article{Baker2016,
abstract = {Mich{\`{e}}le Nuijten and her colleagues found rampant inconsistencies when they unleased statcheck on the psychological literature. The program scans articles for statistical results, redoes the calculations and checks that the numbers match. It went through 30,717 papers to identify 16,695 that tested hypotheses using statistics. In half of those, it found at least one potential error (M. B. Nuijten et al. Behav. Res. Methods 48, 1205–1226; 2016).},
author = {Baker, Monya},
doi = {10.1038/540151a},
file = {::},
issn = {0028-0836},
journal = {Nature},
month = {dec},
number = {7631},
pages = {151--152},
title = {{Stat-checking software stirs up psychology}},
url = {http://www.nature.com/articles/540151a},
volume = {540},
year = {2016}
}
@misc{SunMicrosystems1999,
author = {{Sun Microsystems}},
title = {{Code Conventions for the Java Programming Language: Contents}},
url = {https://www.oracle.com/technetwork/java/javase/documentation/codeconvtoc-136057.html},
urldate = {2019-12-13},
year = {1999}
}
@inproceedings{Ratcliff2013,
abstract = {Epistemology is a branch of Philosophy where the debates focus on the nature of knowledge and ways of knowing. Epistemological questions and debates are often reflected in other disciplines having a strong relationship with knowledge, such as the sciences. Furthermore, these debates may also be reflected in the practice of engineering due to its relationship with the sciences. For example: one epistemological debate is concerned with whether different cultures know things in the same way. The psychological sciences of Cultural Psychology and Cross-cultural Psychology reflect this debate by offering differing answers to questions such as "is the concept of depression valid across cultures?" Another reflection of this debate may be observed in system engineering during requirements elicitation if the several clients of a system represent multiple cultures. Here, fundamental concepts such as "proper operation" may be understood differently among the cultures, with dire consequences ifthe differences in knowing are unrecognized. This paper provides an illustration of how re-formulating a problem from one discipline (system engineering) into a common root discipline (epistemology) and then to another derivative discipline (psychology) may provide new beneficial insights. {\textcopyright}2013 The authors. Published by Elsevier B.V.},
author = {Ratcliff, Reginald},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2013.01.041},
file = {::},
issn = {18770509},
keywords = {Architecture framework,Cross-cultural psychology,Cultural understanding,Epistemology,Requirements,System engineering,Sytem architecture,Viewpoints},
month = {jan},
pages = {393--402},
publisher = {Elsevier B.V.},
title = {{Applying epistemology to system engineering: An illustration}},
volume = {16},
year = {2013}
}
@inproceedings{Curumsing:2020semotion,
annote = {Unpublished},
archivePrefix = {arXiv},
arxivId = {2004.03120},
author = {Curumsing, Maheshwaree Kissoon and Cummaudo, Alex and Graetsch, Ulrike Maria and Barnett, Scott and Vasa, Rajesh},
eprint = {2004.03120},
keywords = {InReview},
mendeley-tags = {InReview},
title = {{Ranking Computer Vision Service Issues using Emotion}},
year = {2020}
}
@article{Zaharia2018,
abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
pages = {39--45},
title = {{Accelerating the Machine Learning Lifecycle with MLflow}},
url = {https://people.eecs.berkeley.edu/{~}alig/papers/mlflow.pdf},
year = {2018}
}
@article{Bridger2013,
abstract = {The Cognitive Failures Questionnaire (CFQ) is used in ergonomics research to measure behavioural problems associated with attentiveness and memory in everyday life. CFQ scores have been related to constructs such as accident proneness and outcomes such as human error and psychological strain. The two-year test-retest reliability of the CFQ is reported together with the findings of factor analyses of CFQ data from 535 respondents. Evidence for the predictive and criterion validity and internal reliability of the CFQ is provided. Psychological strain was measured concurrently with CFQ on both testing occasions, two years apart. The test-retest reliability of the summated CFQ score was found to be 0.71, while for the General Health Questionnaire (GHQ-12) strain measure it was 0.32.The relative variance stability was five times greater for the CFQ than the GHQ, indicating that scores on these questionnaires are not covariates. The use of the CFQ as a measure of cognitive control capacity is also discussed.Practitioner Summary: Ergonomists have long been interested in human error and the role of high work demands due to poor equipment design and excessive workload. The CFQ measures attentiveness in daily life and is shown to have excellent psychometric properties that make it suitable for use in both laboratory and field studies as a trait measure of attentiveness in daily life. {\textcopyright} 2013 Crown copyright.},
author = {Bridger, Robert S. and Johnsen, Svein {\AA}ge Kj{\o}s and Brasher, Kate},
doi = {10.1080/00140139.2013.821172},
issn = {00140139},
journal = {Ergonomics},
keywords = {attention and vigilance,cognitive impairment,human performance modelling,individual differences},
month = {oct},
number = {10},
pages = {1515--1524},
publisher = {Taylor {\&} Francis},
title = {{Psychometric properties of the Cognitive Failures Questionnaire†}},
volume = {56},
year = {2013}
}
@article{Kaul2017,
abstract = {In recent years, the importance of feature engineering has been confirmed by the exceptional performance of deep learning techniques, that automate this task for some applications. For others, feature engineering requires substantial manual effort in designing and selecting features and is often tedious and non-scalable. We present AutoLearn, a regression-based feature learning algorithm. Being data-driven, it requires no domain knowledge and is hence generic. Such a representation is learnt by mining pairwise feature associations, identifying the linear or non-linear relationship between each pair, applying regression and selecting those relationships that are stable and improve the prediction performance. Our experimental evaluation on 18 UC Irvine and 7 Gene expression datasets, across different domains, provides evidence that the features learnt through our model can improve the overall prediction accuracy by 13.28{\%}, compared to original feature space and 5.87{\%} over other top performing models, across 8 different classifiers without using any domain knowledge.},
author = {Kaul, Ambika and Maheshwary, Saket and Pudi, Vikram},
doi = {10.1109/ICDM.2017.31},
file = {::},
isbn = {9781538638347},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Classification,Feature Generation,Feature Selection},
pages = {217--226},
title = {{Autolearn - automated feature generation and selection}},
volume = {2017-Novem},
year = {2017}
}
@article{Dittner,
abstract = {Objectives: Fatigue is a common feature of physical and neurological disease as well as psychiatric disorders, often reported amongst patients' most severe and distressing symptoms. A large number of scales have been developed attempting to measure the nature, severity and impact of fatigue in a range of clinical populations. The aim of the present review is to guide the clinician and researcher in choosing a scale to suit their needs. Methods: Database searches of Medline, PsycINFO and EMBASE were undertaken to find published scales. Results: Details of 30 scales are reported. These vary greatly in how widely they have been used and how well they have been evaluated. The present review describes the scales and their properties and provides illustrations of their use in published studies. Conclusions: Recommendations are made for the selection of a scale and for the development and validation of new and existing scales.},
author = {Dittner, A J and Wessely, S C and Brown, R G},
doi = {10.1016/S0022-3999(03)00371-4},
file = {::},
keywords = {Impact,Instrument,Measurement,Psychometric,Scale,Severity},
title = {{The assessment of fatigue A practical guide for clinicians and researchers}}
}
@article{Sheahan2016,
abstract = {A significant portion of the population (25-50{\%}) is known to develop acute low back pain (LBP) within a bout of prolonged sitting. Previous research has supported the use of frequent rest breaks, from seated office work, in order to reduce self-reported LBP, however, there is limited consensus about the recommended frequency and duration of rest breaks. This may be due to the limited consideration of individual differences in acute LBP development. The purpose of this study was to examine the effect of three different standing rest-break conditions on a group of pain developers (PD) and non-pain developers (NPD) engaged in prolonged seated work. Twenty participants completed four one-hour-long bouts of seated typing: Condition A - no rest; Condition B - 5 min of standing rest every 30 min; Condition C - 2.5 min of standing rest every 15 min; Condition D - 50 s of standing rest every 5 min. Self-reported LBP, self-reported mental fatigue and 30-s samples of EMG were collected every 10 min throughout each session. Eight out of 20 participants (40{\%}) reported LBP during Condition A (classified as PD). Only PD demonstrated clinically relevant increases in LBP across conditions where Conditions B, C, or D provided some relief, but did not restore pain scores to their original level, prior to sitting. PD and NPD developed mental fatigue equally, with Conditions B and D helping to reduce fatigue. No differences in productivity were observed between conditions or groups and no main effects were observed for muscle activity, median power frequency or co-contraction. These data suggests that frequent, short, standing rest breaks may help to reduce symptoms of LBP, however they are only a temporary solution as PD still developed clinically important LBP, even with frequent rest breaks.},
author = {Sheahan, Peter J. and Diesbourg, Tara L. and Fischer, Steven L.},
doi = {10.1016/j.apergo.2015.08.013},
issn = {18729126},
journal = {Applied Ergonomics},
keywords = {Low back pain,Prolonged sitting,Standing rest breaks},
month = {mar},
pages = {64--70},
publisher = {Elsevier Ltd},
title = {{The effect of rest break schedule on acute low back pain development in pain and non-pain developers during seated work}},
volume = {53},
year = {2016}
}
@article{Karoly2017,
abstract = {{\textcopyright} The Author (2017). Published by Oxford University Press on behalf of the Guarantors of Brain. All rights reserved. For Permissions, please email: journals.permissions@oup.com. It is now established that epilepsy is characterized by periodic dynamics that increase seizure likelihood at certain times of day, and which are highly patient-specific. However, these dynamics are not typically incorporated into seizure prediction algorithms due to the difficulty of estimating patient-specific rhythms from relatively short-term or unreliable data sources. This work outlines a novel framework to develop and assess seizure forecasts, and demonstrates that the predictive power of forecasting models is improved by circadian information. The analyses used long-term, continuous electrocorticography from nine subjects, recorded for an average of 320 days each. We used a large amount of out-of-sample data (a total of 900 days for algorithm training, and 2879 days for testing), enabling the most extensive post hoc investigation into seizure forecasting. We compared the results of an electrocorticography-based logistic regression model, a circadian probability, and a combined electrocorticography and circadian model. For all subjects, clinically relevant seizure prediction results were significant, and the addition of circadian information (combined model) maximized performance across a range of outcome measures. These results represent a proof-of-concept for implementing a circadian forecasting framework, and provide insight into new approaches for improving seizure prediction algorithms. The circadian framework adds very little computational complexity to existing prediction algorithms, and can be implemented using current-generation implant devices, or even non-invasively via surface electrodes using a wearable application. The ability to improve seizure prediction algorithms through straightforward, patient-specific modifications provides promise for increased quality of life and improved safety for patients with epilepsy.},
author = {Karoly, Philippa J. and Ung, Hoameng and Grayden, David B. and Kuhlmann, Levin and Leyde, Kent and Cook, Mark J. and Freestone, Dean R.},
doi = {10.1093/brain/awx173},
file = {::},
issn = {14602156},
journal = {Brain},
keywords = {circadian rhythms,epilepsy,forecasting,seizure prediction},
month = {aug},
number = {8},
pages = {2169--2182},
publisher = {Oxford University Press},
title = {{The circadian profile of epilepsy improves seizure forecasting}},
volume = {140},
year = {2017}
}
@misc{Pillai2006,
abstract = {The interictal EEG provides information that aids in diagnosis and management of epilepsy. One must remember that the EEG is merely a tool, and its usefulness depends largely upon the skill of the individual who wields it. Like all diagnostic tests, it has significant limitations and cannot substitute for a careful history and exercise of good judgment. Nonetheless, in skilled hands, it provides unique and vital information in many patients, and enhances our understanding of their condition. {\textcopyright}2006 International League Against Epilepsy.},
author = {Pillai, Jyoti and Sperling, Michael R.},
booktitle = {Epilepsia},
doi = {10.1111/j.1528-1167.2006.00654.x},
file = {::},
issn = {00139580},
keywords = {EEG,Epilepsy,Generalized Spike and Wave,Interictal Spike},
month = {oct},
number = {SUPPL. 1},
pages = {14--22},
title = {{Interictal EEG and the diagnosis of epilepsy}},
volume = {47},
year = {2006}
}
@article{Camilli2017,
abstract = {{\textcopyright}Springer International Publishing AG 2017. With the purpose of delivering more robust systems, this paper revisits the problem of Inverse Uncertainty Quantification that is related to the discrepancy between the measured data at runtime (while the system executes) and the formal specification (i.e., a mathematical model) of the system under consideration, and the value calibration of unknown parameters in the model. We foster an approach to quantify and mitigate system uncertainty during the development cycle by combining Bayesian reasoning and online Model-based testing.},
author = {Camilli, Matteo and Gargantini, Angelo and Scandurra, Patrizia and Bellettini, Carlo},
doi = {10.1007/978-3-319-66197-1_24},
file = {::},
isbn = {9783319661964},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {375--381},
title = {{Towards inverse uncertainty quantification in software development}},
volume = {10469 LNCS},
year = {2017}
}
@misc{Schmittdiel2018,
abstract = {T he U.S. health care system faces a daunting set of challenges , including high costs, disparities in access, and significant gaps between evidence and practice. 1 But despite these concerns, less than 0.1{\%} of the over {\$}3.2 trillion spent on health care in the U.S. goes toward research to improve how we deliver care. 1,2 Delivery science research seeks to overcome barriers, leverage facilitators, and implement innovations to improve care. 2 While partnerships between clinical and operations leaders and delivery science researchers can play a significant role in creating new evidence and quickly closing gaps between evidence and practice, 1,3 these collaborations are often limited in both number and scope. To achieve the goal of creating a learning health system, 4 researchers and health system leaders must work together to develop and evaluate new programs, learn from real-world experiments in health care delivery and policy, disseminate knowledge, and implement more effective and efficient care strategies (Fig. 1). However, successful cross-cutting collaborations can be challenging due to differing timelines, sources of financial support, career goals, and measures of success. 3,5 Here we propose a BCollaboration Check-list{\^{}} to help researchers and clinical leaders more successfully navigate and address key strategic issues when contemplating a partnership between research and operations. UNDERSTANDING THE STAKEHOLDER NEEDS The first step is for potential collaborators to gain a clear understanding of exactly what questions need to be answered and why they are being asked. Before collaborating on a new or existing operational program, potential research partners should ask: {\&} What specific outcome is the program trying to achieve, and for which patients?},
author = {Schmittdiel, Julie A. and Grant, Richard W.},
booktitle = {Journal of General Internal Medicine},
doi = {10.1007/s11606-017-4189-5},
file = {::},
issn = {15251497},
keywords = {Internal Medicine},
month = {jan},
number = {1},
pages = {9--10},
publisher = {Springer New York LLC},
title = {{Crossing the Research to Quality Chasm: A Checklist for Researchers and Clinical Leadership Partners}},
url = {https://doi.org/10.1001/jama.2017.1964.},
volume = {33},
year = {2018}
}
@article{Sharma2018,
abstract = {Human-Computer Speech is gaining momentum as a technique of computer interaction. There has been a recent upsurge in speech based search engines and assistants such as Siri, Google Chrome and Cortana. Natural Language Processing (NLP) techniques such as NLTK for Python can be applied to analyse speech, and intelligent responses can be found by designing an engine to provide appropriate human like responses. This type of programme is called a Chatbot, which is the focus of this study. This paper presents a survey on the techniques used to design Chatbots and a comparison is made between different design techniques from nine carefully selected papers according to the main methods adopted. These papers are representative of the significant improvements in Chatbots in the last decade. The paper discusses the similarities and differences in the techniques and examines in particular the Loebner prize-winning Chatbots.},
author = {Sharma, Revati and Patel, Meetkumar},
doi = {10.17148/iarjset.2018.596},
issn = {23941588},
journal = {Iarjset},
keywords = {aiml,chatbot,loebner prize,nlp,nltk,sql},
number = {9},
pages = {37--46},
title = {{Review on Chatbot Design Techniques in Speech Conversation Systems}},
volume = {5},
year = {2018}
}
@article{Saez2016,
abstract = {Canonical machine learning algorithms assume that the number of objects in the considered classes are roughly similar. However, in many real-life situations the distribution of examples is skewed since the examples of some of the classes appear much more frequently. This poses a difficulty to learning algorithms, as they will be biased towards the majority classes. In recent years many solutions have been proposed to tackle imbalanced classification, yet they mainly concentrate on binary scenarios. Multi-class imbalanced problems are far more difficult as the relationships between the classes are no longer straightforward. Additionally, one should analyze not only the imbalance ratio but also the characteristics of the objects within each class. In this paper we present a study on oversampling for multi-class imbalanced datasets that focuses on the analysis of the class characteristics. We detect subsets of specific examples in each class and fix the oversampling for each of them independently. Thus, we are able to use information about the class structure and boost the more difficult and important objects. We carry an extensive experimental analysis, which is backed-up with statistical analysis, in order to check when the preprocessing of some types of examples within a class may improve the indiscriminate preprocessing of all the examples in all the classes. The results obtained show that oversampling concrete types of examples may lead to a significant improvement over standard multi-class preprocessing that do not consider the importance of example types.},
annote = {Defines the following challenges for class imbalance:
- Safe examples
- Borderline examples
- Rare examples
- Outliers (sometimes Noisy)


One idea from this paper is to extend the work based on the types of false classifiers in the data.},
author = {S{\'{a}}ez, Jos{\'{e}} A. and Krawczyk, Bartosz and Wo{\'{z}}niak, Micha{\l}},
doi = {10.1016/j.patcog.2016.03.012},
file = {::},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Imbalanced classification,Machine learning,Minority class types,Multi-class imbalance,Oversampling},
pages = {164--178},
title = {{Analyzing the oversampling of different classes and types of examples in multi-class imbalanced datasets}},
volume = {57},
year = {2016}
}
@article{Wang2020,
abstract = {Importance: In December 2019, novel coronavirus (2019-nCoV)-infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited. Objective: To describe the epidemiological and clinical characteristics of NCIP. Design, Setting, and Participants: Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020. Exposures: Documented NCIP. Main Outcomes and Measures: Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked. Results: Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3{\%}) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29{\%}]) and hospitalized patients (17 [12.3{\%}]). Common symptoms included fever (136 [98.6{\%}]), fatigue (96 [69.6{\%}]), and dry cough (82 [59.4{\%}]). Lymphopenia (lymphocyte count, 0.8 × 109/L [interquartile range {\{}IQR{\}}, 0.6-1.1]) occurred in 97 patients (70.3{\%}), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58{\%}), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9{\%}). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9{\%}]), and many received antibacterial therapy (moxifloxacin, 89 [64.4{\%}]; ceftriaxone, 34 [24.6{\%}]; azithromycin, 25 [18.1{\%}]) and glucocorticoid therapy (62 [44.9{\%}]). Thirty-six patients (26.1{\%}) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1{\%}]), arrhythmia (16 [44.4{\%}]), and shock (11 [30.6{\%}]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n = 36), compared with patients not treated in the ICU (n = 102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2{\%}] vs 38 [37.3{\%}]), and were more likely to have dyspnea (23 [63.9{\%}] vs 20 [19.6{\%}]), and anorexia (24 [66.7{\%}] vs 31 [30.4{\%}]). Of the 36 cases in the ICU, 4 (11.1{\%}) received high-flow oxygen therapy, 15 (41.7{\%}) received noninvasive ventilation, and 17 (47.2{\%}) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1{\%}) were discharged and 6 died (overall mortality, 4.3{\%}), but the remaining patients are still hospitalized. Among those discharged alive (n = 47), the median hospital stay was 10 days (IQR, 7.0-14.0). Conclusions and Relevance: In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41{\%} of patients, 26{\%} of patients received ICU care, and mortality was 4.3{\%}..},
author = {Wang, Dawei and Hu, Bo and Hu, Chang and Zhu, Fangfang and Liu, Xing and Zhang, Jing and Wang, Binbin and Xiang, Hui and Cheng, Zhenshun and Xiong, Yong and Zhao, Yan and Li, Yirong and Wang, Xinghuan and Peng, Zhiyong},
doi = {10.1001/jama.2020.1585},
file = {::},
issn = {15383598},
journal = {JAMA - Journal of the American Medical Association},
keywords = {adult,china,dyspnea,hospital admission,infections,intensive care unit,pneumonia,respiratory distress syndrome,sars-cov-2},
month = {mar},
number = {11},
pages = {1061--1069},
pmid = {32031570},
publisher = {American Medical Association},
title = {{Clinical Characteristics of 138 Hospitalized Patients with 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China}},
volume = {323},
year = {2020}
}
@inproceedings{Markovtsev2019,
author = {Markovtsev, Vadim and Long, Waren and Mougard, Hugo and Slavnov, Konstantin and Bulychev, Egor},
booktitle = {2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2019.00073},
file = {::},
isbn = {978-1-7281-3412-3},
month = {may},
pages = {468--478},
publisher = {IEEE},
title = {{Style-Analyzer: Fixing Code Style Inconsistencies with Interpretable Unsupervised Algorithms}},
url = {https://2019.msrconf.org/details/msr-2019-papers/45/style-analyzer-fixing-code-style-inconsistencies-with-interpretable-unsupervised-alg},
year = {2019}
}
@article{Chen2020,
abstract = {Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software. We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q{\$}\backslash{\$}{\&}A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers. We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors.},
archivePrefix = {arXiv},
arxivId = {2005.00760},
author = {Chen, Zhenpeng and Cao, Yanbin and Liu, Yuanqiang and Wang, Haoyu and Xie, Tao and Liu, Xuanzhe},
eprint = {2005.00760},
file = {::},
keywords = {deep learning,software deployment,stack overflow},
pages = {72--74},
title = {{Understanding Challenges in Deploying Deep Learning Based Software: An Empirical Study}},
url = {http://arxiv.org/abs/2005.00760},
year = {2020}
}
@book{Grandjean1987,
author = {Grandjean, E. (Etienne)},
isbn = {9780850663501},
pages = {227},
publisher = {Taylor {\&} Francis},
title = {{Ergonomics in computerized offices}},
year = {1987}
}
@article{Pradhan2018,
abstract = {From an accessibility perspective, voice-controlled, home-based intelligent personal assistants (IPAs) have the potential to greatly expand speech interaction beyond dictation and screen reader output. To examine the accessibility of off-the-shelf IPAs (e.g., Amazon Echo) and to understand how users with disabilities are making use of these devices, we conducted two exploratory studies. The first, broader study is a content analysis of 346 Amazon Echo reviews that include users with disabilities, while the second study more specifically focuses on users with visual impairments, through interviews with 16 current users of home-based IPAs. Findings show that, although some accessibility challenges exist, users with a range of disabilities are using the Amazon Echo, including for unexpected cases such as speech therapy and support for caregivers. Richer voice-based applications and solutions to support discoverability would be particularly useful to users with visual impairments. These findings should inform future work on accessible voice-based IPAs.},
author = {Pradhan, Alisha and Mehta, Kanika and Findlater, Leah},
doi = {10.1145/3173574.3174033},
file = {::},
isbn = {9781450356206},
keywords = {Author Keywords Intelligent personal assistants,accessibility,conversational interfaces,disability ACM Classification Keywords H5m Informa,speech},
title = {{"Accessibility Came by Accident": Use of Voice-Controlled Intelligent Personal Assistants by People with Disabilities}},
url = {https://doi.org/10.1145/3173574.3174033},
year = {2018}
}
@article{Beniczky2018,
abstract = {OBJECTIVE To determine the accuracy of automated detection of generalized tonic-clonic seizures (GTCS) using a wearable surface EMG device. METHODS We prospectively tested the technical performance and diagnostic accuracy of real-time seizure detection using a wearable surface EMG device. The seizure detection algorithm and the cutoff values were prespecified. A total of 71 patients, referred to long-term video-EEG monitoring, on suspicion of GTCS, were recruited in 3 centers. Seizure detection was real-time and fully automated. The reference standard was the evaluation of video-EEG recordings by trained experts, who were blinded to data from the device. Reading the seizure logs from the device was done blinded to all other data. RESULTS The mean recording time per patient was 53.18 hours. Total recording time was 3735.5 hours, and device deficiency time was 193 hours (4.9{\%} of the total time the device was turned on). No adverse events occurred. The sensitivity of the wearable device was 93.8{\%} (30 out of 32 GTCS were detected). Median seizure detection latency was 9 seconds (range -4 to 48 seconds). False alarm rate was 0.67/d. CONCLUSIONS The performance of the wearable EMG device fulfilled the requirements of patients: it detected GTCS with a sensitivity exceeding 90{\%} and detection latency within 30 seconds. CLASSIFICATION OF EVIDENCE This study provides Class II evidence that for people with a history of GTCS, a wearable EMG device accurately detects GTCS (sensitivity 93.8{\%}, false alarm rate 0.67/d).},
author = {Beniczky, S{\'{a}}ndor and Conradsen, Isa and Henning, Oliver and Fabricius, Martin and Wolf, Peter},
doi = {10.1212/WNL.0000000000004893},
file = {::},
issn = {1526-632X},
journal = {Neurology},
month = {jan},
number = {5},
pages = {e428----e434},
pmid = {29305441},
publisher = {American Academy of Neurology},
title = {{Automated real-time detection of tonic-clonic seizures using a wearable EMG device.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29305441 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5791791},
volume = {90},
year = {2018}
}
@article{Andrzejak2012,
abstract = {To derive tests for randomness, nonlinear-independence, and stationarity, we combine surrogates with a nonlinear prediction error, a nonlinear interdependence measure, and linear variability measures, respectively. We apply these tests to intracranial electroencephalographic recordings (EEG) from patients suffering from pharmacoresistant focal-onset epilepsy. These recordings had been performed prior to and independent from our study as part of the epilepsy diagnostics. The clinical purpose of these recordings was to delineate the brain areas to be surgically removed in each individual patient in order to achieve seizure control. This allowed us to define two distinct sets of signals: One set of signals recorded from brain areas where the first ictal EEG signal changes were detected as judged by expert visual inspection ("focal signals") and one set of signals recorded from brain areas that were not involved at seizure onset ("nonfocal signals"). We find more rejections for both the randomness and the nonlinear-independence test for focal versus nonfocal signals. In contrast more rejections of the stationarity test are found for nonfocal signals. Furthermore, while for nonfocal signals the rejection of the stationarity test increases the rejection probability of the randomness and nonlinear-independence test substantially, we find a much weaker influence for the focal signals. In consequence, the contrast between the focal and nonfocal signals obtained from the randomness and nonlinear-independence test is further enhanced when we exclude signals for which the stationarity test is rejected. To study the dependence between the randomness and nonlinear-independence test we include only focal signals for which the stationarity test is not rejected. We show that the rejection of these two tests correlates across signals. The rejection of either test is, however, neither necessary nor sufficient for the rejection of the other test. Thus, our results suggest that EEG signals from epileptogenic brain areas are less random, more nonlinear-dependent, and more stationary compared to signals recorded from nonepileptogenic brain areas. We provide the data, source code, and detailed results in the public domain. {\textcopyright}2012 American Physical Society.},
author = {Andrzejak, Ralph G. and Schindler, Kaspar and Rummel, Christian},
doi = {10.1103/PhysRevE.86.046206},
file = {::},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
month = {oct},
number = {4},
title = {{Nonrandomness, nonlinear dependence, and nonstationarity of electroencephalographic recordings from epilepsy patients}},
volume = {86},
year = {2012}
}
@article{Wennberg2016,
abstract = {Objectives: To compare the acute effects of uninterrupted sitting with sitting interrupted by brief bouts of light-intensity walking on self-reported fatigue, cognition, neuroendocrine biomarkers and cardiometabolic risk markers in overweight/obese adults. Design: Randomised two-condition crossover trial. Setting: Laboratory study conducted in Melbourne, Australia. Participants: 19 overweight/obese adults (45-75 years). Interventions: After an initial 2 h period seated, participants consumed a meal-replacement beverage and completed (on 2 days separated by a 6-day washout period) each condition over the next 5 h: uninterrupted sitting (sedentary condition) or sitting with 3 min bouts of light-intensity walking every 30 min (active condition). Primary outcome measures: Self-reported fatigue, executive function and episodic memory at 0 h, 4 h and 7 h. Secondary outcome measures: Neuroendocrine biomarkers and cardiometabolic risk markers (blood collections at 0 h, 4 h and 7 h, blood pressure and heart rate measured hourly and interstitial glucose measured using a continuous glucose monitoring system). Results: During the active condition, fatigue levels were lower at 4 h (-13.32 (95{\%} CI -23.48 to -3.16)) and at 7 h (-10.73 (95{\%} CI -20.89 to -0.58)) compared to the sedentary condition. Heart rate was higher at 4 h (4.47 (95{\%} CI 8.37 to 0.58)) and at 7 h (4.32 (95{\%} CI 8.21 to 0.42)) during the active condition compared to the sedentary condition. There were no significant differences between conditions by time for other variables. In the sedentary condition, changes in fatigue scores over time correlated with a decrease in heart rate and plasma dihydroxyphenylalanine (DOPA) and an increase in plasma dihydroxyphenylglycol (DHPG). Conclusions: Interrupting prolonged sitting with lightintensity walking breaks may be an effective fatigue countermeasure acutely. Fatigue levels corresponded with the heart rate and neuroendocrine biomarker changes in uninterrupted sitting in this pilot study. Further research is needed to identify potential implications, particularly for the occupational health context.},
author = {Wennberg, Patrik and Boraxbekk, Carl Johan and Wheeler, Michael and Howard, Bethany and Dempsey, Paddy C. and Lambert, Gavin and Eikelis, Nina and Larsen, Robyn and Sethi, Parneet and Occleston, Jessica and Hernest{\aa}l-Boman, Jenny and Ellis, Kathryn A. and Owen, Neville and Dunstan, David W.},
doi = {10.1136/bmjopen-2015-009630},
file = {::},
issn = {20446055},
journal = {BMJ Open},
month = {feb},
number = {2},
pmid = {26920441},
publisher = {BMJ Publishing Group},
title = {{Acute effects of breaking up prolonged sitting on fatigue and cognition: A pilot study}},
volume = {6},
year = {2016}
}
@article{Juarez-Garcia2006,
abstract = {Background: The management of epilepsy incurs significant costs to the United Kingdom (UK) National Health Service (NHS). Making a diagnosis of epilepsy can, however, be difficult and misdiagnosis frequently occurs when patients are seen by non-specialists. This study estimates the financial costs of epilepsy misdiagnosis in the NHS in England and Wales. Methods: Standard costing methods were applied to estimate the costs attributable to epilepsy misdiagnosis. The primary data were published in UK studies on the prevalence of epilepsy, epilepsy misdiagnosis and costs identified from Medline, Cinahl and Embase (1996-May 2006). Results: An estimated total of 92,000 people were misdiagnosed with epilepsy in England and Wales in 2002. The average medical cost per patient per year of misdiagnosis was {\pounds}316, with the chief economic burdens being inpatient admissions (45{\%}), inappropriate prescribing of antiepileptic drugs (AEDs) (26{\%}), outpatient attendances (16{\%}) and general practitioner (GP) care (8{\%}). The estimated annual medical costs in England and Wales were {\pounds}29,000,000, while total costs could reach up to {\pounds}138,000,000 a year. Conclusions: Allowing for uncertainty, and considering the analysis exclusively from the NHS/CBS (community based services) perspective the opportunity costs of misdiagnosis are substantial. There is a need for health care commissioners to ensure that misdiagnosis is kept to a minimum by ensuring that individuals with a recent onset suspected seizure are seen as soon as possible by a specialist medical practitioner with training and expertise in epilepsy. {\textcopyright}2006 British Epilepsy Association.},
author = {Juarez-Garcia, Ariadna and Stokes, Tim and Shaw, Beth and Camosso-Stefinovic, Janette and Baker, Richard},
doi = {10.1016/j.seizure.2006.08.005},
file = {::},
issn = {10591311},
journal = {Seizure},
keywords = {England and Wales,Epilepsy,Financial costs,Misdiagnosis},
month = {dec},
number = {8},
pages = {598--605},
title = {{The costs of epilepsy misdiagnosis in England and Wales}},
volume = {15},
year = {2006}
}
@techreport{Sokolova2006,
abstract = {Different evaluation measures assess different characteristics of machine learning algorithms. The empirical evaluation of algorithms and classifiers is a matter of ongoing debate between researchers. Although most measures in use today focus on a classifier's ability to identify classes correctly, we suggest that, in certain cases, other properties, such as failure avoidance or class discrimination may also be useful. We suggest the application of measures which evaluate such properties. These measures-Youden's index, likelihood , Discriminant power-are used in medical diagnosis. We show that these measures are interrelated, and we apply them to a case study from the field of electronic negotiations. We also list other learning problems which may benefit from the application of the proposed measures.},
author = {Sokolova, Marina and Japkowicz, Nathalie and Szpakowicz, Stan},
file = {::},
keywords = {American Association for Artificial Intelligence.,Copyright {\textcopyright}2006},
title = {{Beyond Accuracy, F-score, and ROC: A Family of Discriminant Measures for Performance Evaluation}},
url = {www.aaai.org},
year = {2006}
}
@article{Smith2013,
abstract = {Introduction: Health benefits of regular participation in physical activity are well documented but population levels are low. Office layout, and in particular the number and location of office building destinations (eg, print and meeting rooms), may influence both walking time and characteristics of sitting time. No research to date has focused on the role that the layout of the indoor office environment plays in facilitating or inhibiting step counts and characteristics of sitting time. The primary aim of this study was to investigate associations between office layout and physical activity, as well as sitting time using objective measures. Methods and analysis: Active buildings is a unique collaboration between public health, built environment and computer science researchers. The study involves objective monitoring complemented by a larger questionnaire arm. UK office buildings will be selected based on a variety of features, including office floor area and number of occupants. Questionnaires will include items on standard demographics, well-being, physical activity behaviour and putative socioecological correlates of workplace physical activity. Based on survey responses, approximately 30 participants will be recruited from each building into the objective monitoring arm. Participants will wear accelerometers (to monitor physical activity and sitting inside and outside the office) and a novel tracking device will be placed in the office (to record participant location) for five consecutive days. Data will be analysed using regression analyses, as well as novel agent-based modelling techniques. Ethics and dissemination: The results of this study will be disseminated through peer-reviewed publications and scientific presentations. Ethical approval was obtained through the University College London Research Ethics Committee (Reference number 4400/001).},
author = {Smith, Lee and Ucci, Marcella and Marmot, Alexi and Spinney, Richard and Laskowski, Marek and Sawyer, Alexia and Konstantatou, Marina and Hamer, Mark and Ambler, Gareth and Wardle, Jane and Fisher, Abigail},
doi = {10.1136/bmjopen-2013-004103},
file = {::},
issn = {20446055},
journal = {BMJ Open},
month = {nov},
number = {11},
publisher = {British Medical Journal Publishing Group},
title = {{Active buildings: Modelling physical activity and movement in office buildings. An observational study protocol}},
volume = {3},
year = {2013}
}
@article{Beniczky2013,
abstract = {Summary The electroencephalography (EEG) signal has a high complexity, and the process of extracting clinically relevant features is achieved by visual analysis of the recordings. The interobserver agreement in EEG interpretation is only moderate. This is partly due to the method of reporting the findings in free-text format. The purpose of our endeavor was to create a computer-based system for EEG assessment and reporting, where the physicians would construct the reports by choosing from predefined elements for each relevant EEG feature, as well as the clinical phenomena (for video-EEG recordings). A working group of EEG experts took part in consensus workshops in Dianalund, Denmark, in 2010 and 2011. The faculty was approved by the Commission on European Affairs of the International League Against Epilepsy (ILAE). The working group produced a consensus proposal that went through a pan-European review process, organized by the European Chapter of the International Federation of Clinical Neurophysiology. The Standardised Computer-based Organised Reporting of EEG (SCORE) software was constructed based on the terms and features of the consensus statement and it was tested in the clinical practice. The main elements of SCORE are the following: personal data of the patient, referral data, recording conditions, modulators, background activity, drowsiness and sleep, interictal findings, "episodes" (clinical or subclinical events), physiologic patterns, patterns of uncertain significance, artifacts, polygraphic channels, and diagnostic significance. The following specific aspects of the neonatal EEGs are scored: alertness, temporal organization, and spatial organization. For each EEG finding, relevant features are scored using predefined terms. Definitions are provided for all EEG terms and features. SCORE can potentially improve the quality of EEG assessment and reporting; it will help incorporate the results of computer-assisted analysis into the report, it will make possible the build-up of a multinational database, and it will help in training young neurophysiologists. {\textcopyright} Wiley Periodicals, Inc. {\textcopyright} 2013 International League Against Epilepsy.},
author = {Beniczky, S{\'{a}}ndor and Aurlien, Harald and Br{\o}gger, Jan C. and Fuglsang-Frederiksen, Anders and Martins-Da-Silva, Ant{\~{o}}nio and Trinka, Eugen and Visser, Gerhard and Rubboli, Guido and Hjalgrim, Helle and Stefan, Hermann and Ros{\'{e}}n, Ingmar and Zarubova, Jana and Dobesberger, Judith and Alving, J{\o}rgen and Andersen, Kjeld V. and Fabricius, Martin and Atkins, Mary D. and Neufeld, Miri and Plouin, Perrine and Marusic, Petr and Pressler, Ronit and Mameniskiene, Ruta and Hopfeng{\"{a}}rtner, R{\"{u}}diger and {Van Emde Boas}, Walter and Wolf, Peter},
doi = {10.1111/epi.12135},
file = {::},
issn = {00139580},
journal = {Epilepsia},
keywords = {Assessment,Database,Definitions,EEG,Semiology,Terms},
month = {jun},
number = {6},
pages = {1112--1124},
title = {{Standardized Computer-based Organized Reporting of EEG: SCORE}},
volume = {54},
year = {2013}
}
@article{Taylor2016,
abstract = {INTRODUCTION: The 15-minute work break provides an opportunity to promote health, yet few studies have examined this part of the workday. We studied physical activity and sedentary behavior among office workers and compared the results of the Booster Break program with those of a second intervention and a control group to determine whether the Booster Break program improved physical and behavioral health outcomes.
METHODS: We conducted a 3-arm, cluster-randomized controlled trial at 4 worksites in Texas from 2010 through 2013 to compare a group-based, structured Booster Break program to an individual-based computer-prompt intervention and a usual-break control group; we analyzed physiologic, behavioral, and employee measures such as work social support, quality of life, and perceived stress. We also identified consistent and inconsistent attendees of the Booster Break sessions.
RESULTS: We obtained data from 175 participants (mean age, 43 y; 67{\%} racial/ethnic minority). Compared with the other groups, the consistent Booster Break attendees had greater weekly pedometer counts (P {\textless} .001), significant decreases in sedentary behavior and self-reported leisure-time physical activity (P {\textless} .001), and a significant increase in triglyceride concentrations (P = .02) (levels remained within the normal range). Usual-break participants significantly increased their body mass index, whereas Booster Break participants maintained body mass index status during the 6 months. Overall, Booster Break participants were 6.8 and 4.3 times more likely to have decreases in BMI and weekend sedentary time, respectively, than usual-break participants.
CONCLUSION: Findings varied among the 3 study groups; however, results indicate the potential for consistent attendees of the Booster Break intervention to achieve significant, positive changes related to physical activity, sedentary behavior, and body mass index.},
author = {Taylor, Wendell C. and Paxton, Raheem J. and Shegog, Ross and Coan, Sharon P. and Dubin, Allison and Page, Timothy F. and Rempel, David M.},
doi = {10.5888/pcd13.160231},
file = {::},
issn = {15451151},
journal = {Preventing chronic disease},
month = {nov},
pages = {E155},
publisher = {Centers for Disease Control and Prevention},
title = {{Impact of Booster Breaks and Computer Prompts on Physical Activity and Sedentary Behavior Among Desk-Based Workers: A Cluster-Randomized Controlled Trial}},
volume = {13},
year = {2016}
}
@article{Meyman,
author = {Meyman, T and Psycholoog, WB Schaufeli - De and undefined 1996},
title = {{Psychische vermoeidheid en arbeid.(Fatigue at work)}}
}
@book{SiddiquiM,
abstract = {"IEEE Part Number: CFP16TEN-USB." Annotation All aspects of electrical, electronic, computer and information technologies.},
author = {{Siddiqui M}, Islam M},
file = {::},
isbn = {9781509025978},
title = {{Data mining approach in seizure detection}}
}
@article{Zhu2017,
abstract = {Multiclass imbalance data learning has attracted increasing interests from the research community. Unfortunately, existing oversampling solutions, when facing this more challenging problem as compared to two-class imbalance case, have shown their respective deficiencies such as causing serious over generalization or not actively improving the class imbalance in data space. We propose a k-nearest neighbors (k-NN)-based synthetic minority oversampling algorithm, termed SMOM, to handle multiclass imbalance problems. Different from previous k-NN-based oversampling algorithms, where for any original minority instance the synthetic instances are randomly generated in the directions of its k-nearest neighbors, SMOM assigns a selection weight to each neighbor direction. The neighbor directions that can produce serious over generalization will be given small selection weights. This way, SMOM forms a mechanism of avoiding over generalization as the safer neighbor directions are more likely to be selected to yield the synthetic instances. Owing to this, SMOM can aggressively explore the regions of minority classes by configuring a high value for parameter k, but do not result in severe over generalization. Extensive experiments using 27 real-world data sets demonstrate the effectiveness of our algorithm.},
author = {Zhu, Tuanfei and Lin, Yaping and Liu, Yonghe},
doi = {10.1016/j.patcog.2017.07.024},
file = {::},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Multiclass imbalance problems,Neighbor directions,Over generalization,Synthetic minority oversampling},
pages = {327--340},
publisher = {Elsevier Ltd},
title = {{Synthetic minority oversampling technique for multiclass imbalance problems}},
volume = {72},
year = {2017}
}
@article{Kovalenko2020,
archivePrefix = {arXiv},
arxivId = {2002.03997},
author = {Kovalenko, Vladimir and Bogomolov, Egor and Bryksin, Timofey and Bacchelli, Alberto},
eprint = {2002.03997},
file = {::},
keywords = {Interpretability,Point-wise explanations,Ranking},
title = {{Building Implicit Vector Representations of Individual Coding Style}},
year = {2020}
}
@article{Ghazi2019,
abstract = {Background: The need for empirical investigations in software engineering is growing. Many researchers nowadays, conduct and validate their solutions using empirical research. The Survey is an empirical method which enables researchers to collect data from a large population. The main aim of the survey is to generalize the findings. Aims: In this study, we aim to identify the problems researchers face during survey design and mitigation strategies. Method: A literature review, as well as semi-structured interviews with nine software engineering researchers, were conducted to elicit their views on problems and mitigation strategies. The researchers are all focused on empirical software engineering. Results: We identified 24 problems and 65 strategies, structured according to the survey research process. The most commonly discussed problem was sampling, in particular, the ability to obtain a sufficiently large sample. To improve survey instrument design, evaluation and execution recommendations for question formulation and survey pre-testing were given. The importance of involving multiple researchers in the analysis of survey results was stressed. Conclusions: The elicited problems and strategies may serve researchers during the design of their studies. However, it was observed that some strategies were conflicting. This shows that it is important to conduct a trade-off analysis between strategies.},
author = {Ghazi, Ahmad Nauman and Petersen, Kai and Reddy, Sri Sai Vijay Raj and Nekkanti, Harini},
doi = {10.1109/ACCESS.2018.2881041},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Empirical software engineering,surveys},
pages = {24703--24718},
publisher = {IEEE},
title = {{Survey research in software engineering: Problems and mitigation strategies}},
volume = {7},
year = {2019}
}
@misc{Woolf2008b,
author = {Woolf, Steven H.},
booktitle = {JAMA - Journal of the American Medical Association},
doi = {10.1001/jama.2007.26},
issn = {00987484},
month = {jan},
number = {2},
pages = {211--213},
publisher = {American Medical Association},
title = {{The meaning of translational research and why it matters}},
url = {https://jamanetwork.com/journals/jama/fullarticle/1149350},
volume = {299},
year = {2008}
}
@article{Reeves2015,
abstract = {{\textless}div class="page" title="Page 1"{\textgreater}{\textless}div class="layoutArea"{\textgreater}{\textless}div class="column"{\textgreater}{\textless}p{\textgreater}{\textless}span{\textgreater}The human-computer interaction (HCI) has had a long and troublesome relationship to the role of ‘science'. HCI's status as an academic object in terms of coherence and adequacy is often in question—leading to desires for establishing a true scientific discipline. In this paper I explore formative cognitive science influences on HCI, through the impact of early work on the design of input devices. The paper discusses a core idea that I argue has animated much HCI research since: the notion of scientific design spaces. In evaluating this concept, I disassemble the broader ‘picture of science' in HCI and its role in constructing a disciplinary order for the increasingly diverse and overlapping research communities that contribute in some way to what we call ‘HCI'. In concluding I explore notions of rigour and debates around how we might reassess HCI's disciplinarity.{\textless}/span{\textgreater}{\textless}/p{\textgreater}{\textless}/div{\textgreater}{\textless}/div{\textgreater}{\textless}/div{\textgreater}},
author = {Reeves, Stuart},
doi = {10.7146/aahcc.v1i1.21296},
file = {::},
journal = {Aarhus Series on Human Centered Computing},
number = {1},
pages = {12},
title = {{Human-computer interaction as science}},
volume = {1},
year = {2015}
}
@misc{Mormann2007,
abstract = {The sudden and apparently unpredictable nature of seizures is one of the most disabling aspects of the disease epilepsy. A method capable of predicting the occurrence of seizures from the electroencephalogram (EEG) of epilepsy patients would open new therapeutic possibilities. Since the 1970s investigations on the predictability of seizures have advanced from preliminary descriptions of seizure precursors to controlled studies applying prediction algorithms to continuous multi-day EEG recordings. While most of the studies published in the 1990s and around the turn of the millennium yielded rather promising results, more recent evaluations could not reproduce these optimistic findings, thus raising a debate about the validity and reliability of previous investigations. In this review, we will critically discuss the literature on seizure prediction and address some of the problems and pitfalls involved in the designing and testing of seizure-prediction algorithms. We will give an account of the current state of this research field, point towards possible future developments and propose methodological guidelines for future studies on seizure prediction. {\textcopyright}The Author (2006). Published by Oxford University Press on behalf of the Guarantors of Brain. All rights reserved.},
author = {Mormann, Florian and Andrzejak, Ralph G. and Elger, Christian E. and Lehnertz, Klaus},
booktitle = {Brain},
doi = {10.1093/brain/awl241},
file = {::},
keywords = {Algorithm,Guidelines,Methodology,Performance,Seizure anticipation,Statistical validation},
number = {2},
pages = {314--333},
pmid = {17008335},
publisher = {Oxford University Press},
title = {{Seizure prediction: The long and winding road}},
volume = {130},
year = {2007}
}
@techreport{Fisk,
abstract = {The fatigue impact scale (FIS) was developed previously as a symptom-specific profile measure of health-related quality of life (HRQoL) for use in medical conditions in which fatigue is a prominent chronic symptom. Thus, it was not developed to be a responsive measure of daily changes in fatigue. This study describes the development and initial validation of an adaptation of the FIS for daily use. Items for the daily fatigue impact scale (D-FIS) were selected from the pool of original FIS items through Rasch analyses of existing data. The reduced-item FIS was administered to a sample of 93 subjects with flu-like illness, 25 of whom were followed for a 21-day period. Rasch analyses were used to further reduce the scale to a minimum number of items that represented a unidimensional measure of self-reported fatigue impact. This 8-item D-FIS demonstrated good relations to flu symptom ratings and to other general health ratings. It also proved to be a responsive measure of change in reported fatigue impact for subjects who were followed longitudinally. This initial validation study indicates that the D-FIS has considerable promise as a valid measure of the subjective daily experience of fatigue.},
author = {Fisk, John D and Doble, Susan E},
file = {::},
keywords = {Fatigue,Health-related quality of life,Influenza,Outcomes},
title = {{Construction and validation of a fatigue impact scale for daily administration (D-FIS)}}
}
@article{Regnesentral2017,
author = {Regnesentral, Norsk},
file = {::},
keywords = {Calibration,Forecast accuracy,Forecast evaluation,Probabilistic forecasting,Proper scoring rules,Reliability},
number = {October},
title = {{Verification : assessment of calibration and Note}},
year = {2017}
}
@inproceedings{Stol2016,
abstract = {Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.},
annote = {Grounded Theory (GT) is useful in medical sociology, nursing, education and management theory. Analysis of 98 articles that mention GT, 52 explicitly claim to use GT, 46 use techniques of GT. The goal of GT is to generate theory rather than test or validate existing theory.To see "What's going on here?"GT is a method of inductively generating theory from data.Key components of GT - Limit exposure to literature, Treat everything as data, Immediate and continuous data analysis, Theoretical sampling, Theoretical sensitivity, Coding, Memoing, Constant comparison, Memor sorting/Theoretical sorting, Coheisve theory, Theoretical saturationThe positivist approach depends on five pillars : Unity of the scientific method, Search for causal relationships, Belief in empiricism, Science is value-free, Science is founded upon logic and mathematics. List of well known peer reviewed SE journals is presented. Searched databases are Scopus, Science Direct, IEEE Explore, ACM DL. Grounded Theory use is ambiguious, Many studies present little to no detail, Many studies ignore GT Variants, Few GT Studies Generate TheoryChallenges in Doing GT - Managing large amounts of heterogenous data, Coding unconventional texts, Cross-referencing participant statements with records},
author = {Stol, Klaas Jan and Ralph, Paul and Fitzgerald, Brian},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/2884781.2884833},
file = {::},
isbn = {9781450339001},
issn = {02705257},
keywords = {Grounded theory,Guidelines,Review,Software engineering},
month = {may},
pages = {120--131},
publisher = {IEEE Computer Society},
title = {{Grounded theory in software engineering research: A critical review and guidelines}},
volume = {14-22-May-},
year = {2016}
}
@article{He2004c,
abstract = {The study was carried out to examine whether acupuncture treatment can reduce chronic pain in the neck and shoulders and related headache, and also to examine whether possible effects are long-lasting. Therefore, 24 female office workers (47±9 years old, mean±SD) who had had neck and shoulder pain for 12±9 years were randomly assigned to a test group (TG) or a control group (CG). Acupuncture was applied 10 times during 3-4 weeks either at presumed anti-pain acupoints (TG) or at placebo-points (CG). A physician measured the pain threshold (PPT) in the neck and shoulder regions with algometry before the first treatment, and after the last one and six months after the treatments. Questionnaires on muscle pain and headache were answered at the same occasions and again 3 years after the last treatment. The intensity and frequency of pain fell more for TG than for CG (Pb≤0.04) during the treatment period. Three years after the treatments TG still reported less pain than before the treatments (Pw{\textless}0.001), contrary to what CG did (Pb{\textless}0.04). The degree of headache fell during the treatment period for both groups, but more for TG than for CG (Pb=0.02). Three years after the treatments the effect still lasted for TG (P w{\textless}0.001) while the degree of headache for CG was back to the pre-treatment level (Pb{\textless}0.001). PPT of some muscles rose during the treatments for TG and remained higher 6 months after the treatments (P w{\textless}0.05), which contrasts the situation for CG. Adequate acupuncture treatment may reduce chronic pain in the neck and shoulders and related headache. The effect lasted for 3 years. {\textcopyright} 2004 International Association for the Study of Pain. Published by Elsevier B.V. All rights reserved.},
author = {He, Dong and Veiersted, Kaj Bo and H{\o}stmark, Arne T. and Medb{\o}, Jon Ingulf},
doi = {10.1016/j.pain.2004.01.018},
issn = {03043959},
journal = {Pain},
keywords = {Acupuncture,Algometry,Chronic pain,Neck,Placebo or sham acupuncture,Shoulder},
month = {jun},
number = {3},
pages = {299--307},
title = {{Effect of acupuncture treatment on chronic neck and shoulder pain in sedentary female workers: A 6-month and 3-year follow-up study}},
volume = {109},
year = {2004}
}
@article{Schelter2017,
abstract = {We present a lightweight system to extract, store and manage metadata and prove-nance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workflow, and provides a basis for comparability and repeata-bility of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
author = {Schelter, Sebastian and B{\"{o}}se, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
file = {::},
journal = {Machine Learning Systems Workshop at NIPS},
title = {{Automatically Tracking Metadata and Provenance of Machine Learning Experiments}},
url = {http://learningsys.org/nips17/assets/papers/paper{\_}13.pdf},
year = {2017}
}
@incollection{Barile2016,
abstract = {Innovation implies collaboration among business actors and is fundamental in value cocreation networks. A significant context is the healthcare, where Translational Medicine (T-MED) aims at decreasing barriers between clinical research and medical treatments. The Translational Medicine is the integration and optimization of inputs in basic research improving the patient care, advancing and speeding up the process of offering new protocols, therapies, and practices. This conceptual contribution analyzes the Service Innovation in Translational Medicine using the paradigms of the Service Science (SS), Complex Service Systems, and Viable Systems Approach (VSA). It aims to increase the comprehension of relationships and complexity reduction processes in Translational Medicine and in Service Innovation in healthcare and improving of understanding the constraints to the innovation supporting Translational Medicine approach, seeking strategic solutions with a holistic view of healthcare networks.},
author = {Barile, Sergio and Polese, Francesco and Saviano, Marialuisa and Carrubbo, Luca},
booktitle = {Innovating in Practice: Perspectives and Experiences},
doi = {10.1007/978-3-319-43380-6_18},
isbn = {9783319433806},
month = {jan},
pages = {417--438},
publisher = {Springer International Publishing},
title = {{Service innovation in translational medicine}},
url = {https://link-springer-com.ezproxy-b.deakin.edu.au/chapter/10.1007/978-3-319-43380-6{\_}18},
year = {2016}
}
@article{Bakker2015,
abstract = {Although many studies in the field of recovery from work utilize a quantitative diary design, little is known about the validity of the daily measures used in such studies. The present study analyses the factor structure of the state version of the Recovery Experience Questionnaire (REQ) on the between-person (trait) and within-person (state) levels. A total of 127 employees participated in the study. Most of them filled out the questionnaire on three consecutive workdays (N = 375 observations). Results of multilevel confirmatory factor analyses (MCFA) showed that a four-factor model fit the data better than alternative models at both levels of analysis (between and within). In addition, some factor loadings of the four recovery experience dimensions (particularly for relaxation and control) were lower on the day level as compared to the general level. Nevertheless, we conclude that both state and trait versions of the REQ show good psychometric properties. Implications for future research on recovery are discussed.},
author = {Bakker, Arnold B. and Sanz-Vergel, Ana I. and Rodr{\'{i}}guez-Mu{\~{n}}oz, Alfredo and Oerlemans, Wido G.M.},
doi = {10.1080/1359432X.2014.903242},
issn = {14640643},
journal = {European Journal of Work and Organizational Psychology},
keywords = {Fatigue,Multilevel confirmatory factor analysis,Psychological detachment,Recovery,Relaxation},
month = {may},
number = {3},
pages = {350--359},
publisher = {Routledge},
title = {{The state version of the recovery experience questionnaire: A multilevel confirmatory factor analysis}},
volume = {24},
year = {2015}
}
@article{Greis,
abstract = {End-users are often exposed to uncertain data in interactive systems such as personal health apps, intelligent navigation systems, and systems driven by machine learning. On one hand, communicating uncertainty may improve the understanding of data and predictions. On the other hand, communicating uncertainty can greatly confuse users and decrease trust. While some specialized guidelines for dealing with uncertainty exist within particular fields such as information visualization or context-aware computing, HCI lacks general design guidelines around the more basic question of "will communicating uncertainty rather help or confuse my users?" The goal of this workshop is to bring together researchers and practitioners from across HCI and related fields to establish a better understanding of when and how to design for uncertainty. The outcome of the workshop will be a set of real-world application scenarios with descriptions of the impact of presenting uncertainty in that scenario. Additionally, we will create a set of design guidelines that supports designers and researchers in this emerging space in evaluating whether and how to present uncertainty.},
author = {Greis, Miriam and Hullman, Jessica and Correll, Michael and Kay, Matthew and Shaer, Orit},
doi = {10.1145/3027063.3027091},
file = {::},
isbn = {9781450346566},
keywords = {Data Modeling {\&} Analysis,Design,Multidisciplinary,Uncertainty},
title = {{Designing for Uncertainty in HCI: When Does Uncertainty Help?}},
url = {http://dx.doi.org/10.1145/3027063.3027091}
}
@inproceedings{Yuan2017,
abstract = {{\textcopyright}2017 ACM. With the advances in pervasive sensor technologies, physiological signals can be captured continuously to prevent the serious outcomes caused by epilepsy. Detection of epileptic seizure onset on collected multi-channel electroencephalogram (EEG) has attracted lots of attention recently. Deep learning is a promising method to analyze large-scale unlabeled data. In this paper, we propose a multi-view deep learning model to capture brain abnormality from multi-channel epileptic EEG signals for seizure detection. Specifically, we first generate EEG spectrograms using short-time Fourier transform (STFT) to represent the time-frequency information after signal segmentation. Second, we adopt stacked sparse denoising autoencoders (SSDA) to unsupervisedly learn multiple features by considering both intra and inter correlation of EEG channels, denoted as intra-channel and cross-channel features, respectively. Third, we add an SSDA-based channel selection procedure using proposed response rate to reduce the dimension of intra-channel feature. Finally, we concatenate the learned multi-features and apply a fully-connected SSDA model with softmax classifier to jointly learn the cross-patient seizure detector in a supervised fashion. To evaluate the performance of the proposed model, we carry out experiments on a real world benchmark EEG dataset and compare it with six baselines. Extensive experimental results demonstrate that the proposed learning model is able to extract latent features with meaningful interpretation, and hence is effective in detecting epileptic seizure.},
author = {Yuan, Ye and Jia, Kebin and Xun, Guangxu and Zhang, Aidong},
booktitle = {ACM-BCB 2017 - Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
doi = {10.1145/3107411.3107419},
file = {::},
isbn = {9781450347228},
keywords = {Deep learning,Electroencephalogram,Epileptic seizure,Feature extraction,Time-frequency analysis},
month = {aug},
pages = {213--222},
publisher = {Association for Computing Machinery, Inc},
title = {{A multi-view deep learning method for epileptic seizure Detection using short-time Fourier transform}},
year = {2017}
}
@techreport{Sun2015,
author = {Sun, Wei},
file = {::},
title = {{Purdue e-Pubs Stability of machine learning algorithms}},
url = {https://docs.lib.purdue.edu/open{\_}access{\_}dissertations/563},
year = {2015}
}
@misc{Bell2014,
abstract = {Background: Missing outcome data is a threat to the validity of treatment effect estimates in randomized controlled trials. We aimed to evaluate the extent, handling, and sensitivity analysis of missing data and intention-to-treat (ITT) analysis of randomized controlled trials (RCTs) in top tier medical journals, and compare our findings with previous reviews related to missing data and ITT in RCTs. Methods: Review of RCTs published between July and December 2013 in the BMJ, JAMA, Lancet, and New England Journal of Medicine, excluding cluster randomized trials and trials whose primary outcome was survival. Results: Of the 77 identified eligible articles, 73 (95{\%}) reported some missing outcome data. The median percentage of participants with a missing outcome was 9{\%} (range 0 - 70{\%}). The most commonly used method to handle missing data in the primary analysis was complete case analysis (33, 45{\%}), while 20 (27{\%}) performed simple imputation, 15 (19{\%}) used model based methods, and 6 (8{\%}) used multiple imputation. 27 (35{\%}) trials with missing data reported a sensitivity analysis. However, most did not alter the assumptions of missing data from the primary analysis. Reports of ITT or modified ITT were found in 52 (85{\%}) trials, with 21 (40{\%}) of them including all randomized participants. A comparison to a review of trials reported in 2001 showed that missing data rates and approaches are similar, but the use of the term ITT has increased, as has the report of sensitivity analysis. Conclusions: Missing outcome data continues to be a common problem in RCTs. Definitions of the ITT approach remain inconsistent across trials. A large gap is apparent between statistical methods research related to missing data and use of these methods in application settings, including RCTs in top medical journals.},
author = {Bell, Melanie L. and Fiero, Mallorie and Horton, Nicholas J. and Hsu, Chiu Hsieh},
booktitle = {BMC Medical Research Methodology},
doi = {10.1186/1471-2288-14-118},
file = {::},
issn = {14712288},
keywords = {Intention-to-treat,Missing data,Sensitivity analysis},
month = {dec},
number = {1},
pages = {118},
publisher = {BioMed Central Ltd.},
title = {{Handling missing data in RCTs; A review of the top medical journals}},
url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-118},
volume = {14},
year = {2014}
}
@article{Fuller-Tyszkiewicz2020,
abstract = {Recent work using naturalistic, repeated, ambulatory assessment approaches have uncovered a range of within-person mood- and body image-related dynamics (such as fluctuation of mood and body dissatisfaction) that can prospectively predict eating disorder behaviors (e.g., a binge episode following an increase in negative mood). The prognostic significance of these state-based dynamics for predicting trait-level eating disorder severity, however, remains largely unexplored. The present study uses within-person relationships among state levels of negative mood, body image, and dieting as predictors of baseline, trait-level eating pathology, captured prior to a period of state-based data capture. Two-hundred and sixty women from the general population completed baseline measures of trait eating pathology and demographics, followed by a 7 to 10-day ecological momentary assessment phase comprising items measuring state body dissatisfaction, negative mood, upward appearance comparisons, and dietary restraint administered 6 times daily. Regression-based analyses showed that, in combination, state-based dynamics accounted for 34–43{\%} variance explained in trait eating pathology, contingent on eating disorder symptom severity. Present findings highlight the viability of within-person, state-based dynamics as predictors of baseline trait-level disordered eating severity. Longitudinal testing is needed to determine whether these dynamics account for changes in disordered eating over time.},
author = {Fuller-Tyszkiewicz, Matthew and Krug, Isabel and Smyth, Joshua M and Fernandez-Aranda, Fernando and Treasure, Janet and Linardon, Jake and Vasa, Rajesh and Shatte, Adrian},
doi = {10.3390/jcm9061948},
file = {::},
issn = {2077-0383},
journal = {Journal of Clinical Medicine},
month = {jun},
number = {6},
pages = {1948},
title = {{State-Based Markers of Disordered Eating Symptom Severity}},
url = {https://www.mdpi.com/2077-0383/9/6/1948},
volume = {9},
year = {2020}
}
@article{Manteuffel2014,
abstract = {Architecture decisions are often not explicitly documented in practice but reside in the architect's mind as tacit knowledge, even though explicit capturing and documentation of architecture decisions has been associated with a multitude of benefits. As part of a research collaboration with ABB, we developed a tool to document architecture decisions. This tool is an add-in for Enterprise Architect and is an implementation of a viewpoint-based decision documentation framework. To validate the add-in, we conducted an exploratory case study with ABB architects. In the study, we assessed the status quo of architecture decision documentation, identified architects' expectations of the ideal decision documentation tool, and evaluated the new add-in. We found that although awareness of decision documentation is increasing at ABB, several barriers exist that limit the use of decisions in practice. Regarding their ideal tool, architects want a descriptive and efficient approach. Supplemental features like reporting or decision sharing are requested. The new add-in, was well-perceived by the architects. As a result of the study, we propose a clearer separation of problem, outcomes, and alternatives for the decision documentation framework. {\textcopyright} 2014 IEEE.},
author = {Manteuffel, Christian and Tofan, Dan and Koziolek, Heiko and Goldschmidt, Thomas and Avgeriou, Paris},
doi = {10.1109/WICSA.2014.32},
file = {::},
isbn = {9781479934126},
journal = {Proceedings - Working IEEE/IFIP Conference on Software Architecture 2014, WICSA 2014},
keywords = {architectural viewpoints,architecture decisions,industrial case study,software architecture,tool-support},
pages = {225--234},
publisher = {IEEE},
title = {{Industrial implementation of a documentation framework for architectural decisions}},
year = {2014}
}
@article{VanJaarsveld2011,
abstract = {In this paper obsolescence of service parts is analyzed in a practical environment. Based on the analysis, we propose a method that can be used to estimate the risk of obsolescence of service parts, which is subsequently used to enhance inventory control for those parts. The method distinguishes groups of service parts. For these groups, the risk of obsolescence is estimated using the behavior of similar groups of service parts in the past. The method uses demand data as main information source, and can therefore be applied without the use of an expert's opinion. We will give numerical values for the risk of obsolescence obtained with the method, and the effects of these values on inventory control will be examined. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
annote = {Assumptions:
- high price
- long life-cycle
- consist of many parts

i.e. automobiles, aircrafts, etc.},
author = {{Van Jaarsveld}, Willem and Dekker, Rommert},
doi = {10.1016/j.ijpe.2010.06.014},
file = {::},
issn = {09255273},
journal = {International Journal of Production Economics},
keywords = {Case study,Inventory control,Obsolescence,Spare parts,Sudden death},
number = {1},
pages = {423--431},
title = {{Estimating obsolescence risk from demand data to enhance inventory control - A case study}},
volume = {133},
year = {2011}
}
@inproceedings{Alexandru2018,
author = {Alexandru, Carol V. and Merchante, Jos{\'{e}} J. and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C. and Robles, Gregorio},
booktitle = {2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
doi = {10.1145/3276954.3276960},
file = {::},
pages = {1--11},
publisher = {Association for Computing Machinery (ACM)},
title = {{On the usage of pythonic idioms}},
year = {2018}
}
@misc{Karol2015,
abstract = {BACKGROUND: Sedentary work is associated with many adverse health outcomes, and sit-stand workstations in offices have emerged as a way to counteract sedentary work. OBJECTIVE: This paper reviews the existing knowledge on sit-stand workstations, treadmill workstations and bicycle workstations. METHODS: The inclusion/exclusion criteria were: 1) empirical research examining the effectiveness of sit-stand workstations in lab or field studies, 2)working adult population, 3) sit-standworkstation interventions whereworkers performed the same task from a seated or standing position, 4) outcomes measures of discomfort (comfort), performance, sit-stand behaviors, user satisfaction, kinematic and physiological measures. Search terms were: sit-stand, treadmill, bicycle, workstations, sedentary behavior, office ergonomics, and comfort. RESULTS: Many studies considered productivity, comfort and physiological measures as important outcomes to assess the efficacy of sit-stand workstations and the experimental design was variable. Preliminary data suggests that some amount of standing during an 8-hour workday could be beneficial without compromising user comfort or productivity; however, there is very little data on the efficacy of treadmill and bicycle workstations. CONCLUSIONS: Based on these preliminary data from 26 studies, conducting large scale randomized controlled trials with ergonomic training as their essential component is recommended to understand the benefits of sit-standworkstations for prevention of sedentary work.},
author = {Karol, Sohit and Robertson, Michelle M.},
booktitle = {Work},
doi = {10.3233/WOR-152168},
issn = {10519815},
keywords = {Musculoskeletal symptoms,comfort and health,ergonomics training,productivity},
month = {aug},
number = {2},
pages = {255--267},
publisher = {IOS Press},
title = {{Implications of sit-stand and active workstations to counteract the adverse effects of sedentary work: A comprehensive review}},
volume = {52},
year = {2015}
}
@article{Manteuffel2016,
abstract = {Architecture decisions are often not explicitly documented in practice, even though explicit capturing and documentation of architecture decisions has been associated with a multitude of benefits. Several decision documentation tools have been proposed but they often do not meet the expectations and needs of the industry. As part of a research collaboration with ABB, we developed a decision documentation tool that aims to ensure industrial applicability. This tool is an add-in for Enterprise Architect and is an implementation of a viewpoint-based decision documentation framework. To validate the add-in, we conducted a case study with architects from ABB. In this study, we assessed to what extent and why software architects intend to use our approach. We therefore investigated the perceived usefulness, perceived ease-of-use, and contextual factors threat influence the architect's intention to use the tool. We found that the tool has a high relevance for software architects, that it increases the quality of decision documentation and that it improves productivity of architects in various ways. With respect to ease-of-use, the study identified the need for better guidelines. With respect to contextual factors, we found several factors that influence the intention to use the add-in in an industrial environment.},
author = {Manteuffel, Christian and Tofan, Dan and Avgeriou, Paris and Koziolek, Heiko and Goldschmidt, Thomas},
doi = {10.1016/j.jss.2015.10.034},
file = {::},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Architectural viewpoints,Architecture decisions,Tool-support},
pages = {181--198},
publisher = {Elsevier Inc.},
title = {{Decision architect - A decision documentation tool for industry}},
url = {http://dx.doi.org/10.1016/j.jss.2015.10.034},
volume = {112},
year = {2016}
}
@techreport{Stenetorp2012,
abstract = {We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multi-category entity mention annotation task, showing a 15{\%} decrease in total annotation time. BRAT is available under an open-source license from: http://brat.nlplab.org},
author = {Stenetorp, Pontus and Pyysalo, Sampo and Topi, Goran and Ohta, Tomoko and Ananiadou, Sophia and ichi Tsujii and ichi Tsujii},
file = {::},
pages = {102--107},
title = {{BRAT: a Web-based Tool for NLP-Assisted Text Annotation}},
url = {http://brat.nlplab.org},
year = {2012}
}
@inproceedings{Netoff2009,
abstract = {Approximately 300,000 Americans suffer from epilepsy but no treatment currently exists. A device that could predict a seizure and notify the patient of the impending event or trigger an antiepileptic device would dramatically increase the quality of life for those patients. A patient-specific classification algorithm is proposed to distinguish between preictal and interictal features extracted from EEG recordings. It demonstrates that the classifier based on a Cost-Sensitive Support Vector Machine (CSVM) can distinguish preictal from interictal with a high degree of sensitivity and specificity, when applied to linear features of power spectrum in 9 different frequency bands. The proposed algorithm was applied to EEG recordings of 9 patients in the Freiburg EEG database, totaling 45 seizures and 219-hour-long interictal, and it produced sensitivity of 77.8{\%} (35 of 45 seizures) and the zero false positive rate using 5-minute-long window of preictal via double-cross validation. This approach is advantageous, for it can help an implantable device for seizure prediction consume less power by real-time analysis based on extraction of linear features and by offline optimization, which may be computationally intensive and by real-time analysis. {\textcopyright}2009 IEEE.},
author = {Netoff, Theoden and Park, Yun and Parhi, Keshab},
booktitle = {Proceedings of the 31st Annual International Conference of the IEEE Engineering in Medicine and Biology Society: Engineering the Future of Biomedicine, EMBC 2009},
doi = {10.1109/IEMBS.2009.5333711},
file = {::},
isbn = {9781424432967},
pages = {3322--3325},
publisher = {IEEE Computer Society},
title = {{Seizure prediction using cost-sensitive support vector machine}},
year = {2009}
}
@article{Zhu2018,
abstract = {We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and facilitate the sharing of fine-tuned open-source implementations among researchers for their work. As a consequence, this would help in improving the reproductivity and reliability of future research work in text generation.},
archivePrefix = {arXiv},
arxivId = {1802.01886},
author = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
eprint = {1802.01886},
file = {::},
month = {feb},
title = {{Texygen: A Benchmarking Platform for Text Generation Models}},
url = {http://arxiv.org/abs/1802.01886},
year = {2018}
}
@article{Yang2016,
abstract = {Enriched by natural language texts, Stack Overflow code snippets are an invaluable code-centric knowledge base of small units of source code. Besides being useful for software developers, these annotated snippets can potentially serve as the basis for automated tools that provide working code solutions to specific natural language queries. With the goal of developing automated tools with the Stack Overflow snippets and surrounding text, this paper investigates the following questions: (1) How usable are the Stack Overflow code snippets? and (2) When using text search engines for matching on the natural language questions and answers around the snippets, what percentage of the top results contain usable code snippets? A total of 3M code snippets are analyzed across four languages: C{\#}, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable. Conversely, Java and C{\#} proved to be the languages with the lowest usability rate. Further qualitative analysis on usable Python snippets shows the characteristics of the answers that solve the original question. Finally, we use Google search to investigate the alignment of usability and the natural language annotations around code snippets, and explore how to make snippets in Stack Overflow an adequate base for future automatic program generation.},
archivePrefix = {arXiv},
arxivId = {1605.04464},
author = {Yang, Di and Hussain, Aftab and Lopes, Cristina Videira},
doi = {10.1145/2901739.2901767},
eprint = {1605.04464},
file = {::},
isbn = {9781450341868},
journal = {Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
keywords = {Automatic program generation,Code mining},
pages = {391--401},
title = {{From query to usable code: An analysis of Stack Overflow code snippets}},
year = {2016}
}
@inproceedings{Omari2019,
abstract = {The Python programming language has been picking up traction in Industry for the past few years in virtually all application domains. Python is known for its high calibre and passionate community of developers. Empirical research on Python systems has potential to promote a healthy environment, where claims and beliefs held by the community are supported by data. To facilitate such research, a corpus of 132 open source python projects have been identified, basic information, quality as well as complexity metrics has been collected and organized into CSV files. Collectively, the list consists of 36, 635 python modules, 59, 532 classes, 253, 954 methods and 84, 892 functions. Projects in the selected list span various application domains including Web/APIs, Scientific Computing, Security and more.},
author = {Omari, Safwan and Martinez, Gina},
booktitle = {Proceedings of the Future Technologies Conference (FTC) 2019},
doi = {10.1007/978-3-030-32523-7_49},
editor = {Arai, Kohei and Bhatia, Rahul and Kapoor, Supriya},
file = {::},
isbn = {978-3-030-32523-7},
pages = {661--669},
publisher = {Springer International Publishing},
title = {{Enabling Empirical Research: A Corpus of Large-Scale Python Systems}},
year = {2020}
}
@inproceedings{Cummaudo:2019icsme,
abstract = {Recent advances in artificial intelligence (AI) and machine learning (ML), such as computer vision, are now available as intelligent services and their accessibility and simplicity is compelling. Multiple vendors now offer this technology as cloud services and developers want to leverage these advances to provide value to end-users. However, there is no firm investigation into the maintenance and evolution risks arising from use of these intelligent services; in particular, their behavioural consistency and transparency of their functionality. We evaluated the responses of three different intelligent services (specifically computer vision) over 11 months using 3 different data sets, verifying responses against the respective documentation and assessing evolution risk. We found that there are: (1) inconsistencies in how these services behave; (2) evolution risk in the responses; and (3) a lack of clear communication that documents these risks and inconsistencies. We propose a set of recommendations to both developers and intelligent service providers to inform risk and assist maintainability.},
address = {Cleveland, OH, USA},
author = {Cummaudo, Alex and Vasa, Rajesh and Grundy, John and Abdelrazek, Mohamed and Cain, Andrew},
booktitle = {Proceedings of the 35th IEEE International Conference on Software Maintenance and Evolution},
doi = {10.1109/ICSME.2019.00051},
isbn = {978-1-72-813094-1},
month = {dec},
pages = {333--342},
publisher = {IEEE},
title = {{Losing Confidence in Quality: Unspoken Evolution of Computer Vision Services}},
year = {2019}
}
@article{Sørmo2005,
abstract = {We present an overview of different theories of explanation from the philosophy and cognitive science communities. Based on these theories, as well as models of explanation from the knowledge-based systems area, we present a framework for explanation in case-based reasoning (CBR) based on explanation goals. We propose ways that the goals of the user and system designer should be taken into account when deciding what is a good explanation for a given CBR system. Some general types of goals relevant to many CBR systems are identified, and used to survey existing methods of explanation in CBR. Finally, we identify some future challenges.},
author = {S{\o}rmo, Frode and Cassens, Org and Aamodt, Agnar},
doi = {10.1007/s10462-005-4607-7},
file = {::},
isbn = {1046200546077},
journal = {Artificial Intelligence Review},
keywords = {case-based reasoning,explanation},
pages = {109--143},
title = {{Explanation in Case-Based Reasoning-Perspectives and Goals}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10462-005-4607-7.pdf},
volume = {24},
year = {2005}
}
@article{Alshomrani2015,
abstract = {In a general scenario of classification, one of the main drawbacks for the achievement of accurate models is the presence of high overlapping among the concepts to be learnt. This drawback becomes more severe when we are addressing problems with an imbalanced class distribution. In such cases, the minority class usually represents the most important target of the classification. The failure to correctly identify the minority class instances is often related to those boundary areas in which they are outnumbered by the majority class examples. Throughout the learning stage of the most common rule learning methodologies, the process is often biased to obtain rules that cover the largest areas of the problem. The reason for this behavior is that these types of algorithms aim to maximize the confidence, measured as a ratio of positive and negative covered examples. Rules that identify small areas, in which minority class examples are poorly represented and overlap with majority class examples, will be discarded in favor of more general rules whose consequent will be unequivocally associated with the majority class. Among all types of rule systems, linguistic Fuzzy Rule Based Systems have shown good behavior in the context of classification with imbalanced datasets. Accordingly, we propose a feature weighting approach which aims at analyzing the significance of the problem's variables by weighting the membership degree within the inference process. This is done by applying a different degree of significance to the variables that represent the dataset, enabling to smooth the problem boundaries. These parameters are learnt by means of an optimization process in the framework of evolutionary fuzzy systems. Experimental results using a large number of benchmark problems with different degrees of imbalance and overlapping, show the goodness of our proposal.},
author = {Alshomrani, Saleh and Bawakid, Abdullah and Shim, Seong O. and Fern{\'{a}}ndez, Alberto and Herrera, Francisco},
doi = {10.1016/j.knosys.2014.09.002},
file = {::},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Evolutionary fuzzy systems,Feature weighting,Fuzzy rule based classification systems,Imbalanced datasets,Overlapping},
pages = {1--17},
publisher = {Elsevier B.V.},
title = {{A proposal for evolutionary fuzzy systems using feature weighting: Dealing with overlapping in imbalanced datasets}},
url = {http://dx.doi.org/10.1016/j.knosys.2014.09.002},
volume = {73},
year = {2015}
}
@article{Rasmussen2019,
abstract = {Purpose: Blue-collar workers spend much leisure time sedentary, which is associated with numerous health impairments. The extensive sedentary leisure time among blue-collar workers could be caused by fatigue from physically demanding work, like stationary standing. Occupational stationary standing is prevalent in many blue-collar jobs and has been shown to induce fatigue. The objective of this study was to investigate the association between occupational standing and sedentary leisure time over several workdays among blue-collar workers. Methods: This study used data from 925 workers from Danish workplaces within cleaning, transportation, manufacturing, construction, road maintenance, garbage disposal, and health service. Eligible workers wore accelerometers for 2–5 consecutive workdays. A linear regression was used to investigate the association between percent of work time spent standing and leisure time spent sedentary. A multilevel growth model was used to assess the association between standing during work and sedentary leisure time over consecutive workdays. Results: We found no association between percent of work hours spent standing and percent of leisure time spent sedentary (coef. = 0.01, p = 0.84). The results showed an increase in the workers' sedentary leisure time over a week (coef. = 0.70, p {\textless}0.01). However, this increase was not associated with consecutive workdays exposed to occupational standing (coef. = 0.02, p = 0.42). Conclusion: In this study, we found no support of a positive association between occupational standing and sedentary leisure time. This lack of association could be attributable to a low variation in sedentary leisure time or the chosen definition and measurement of occupational standing.},
author = {Rasmussen, Charlotte Lund and Nabe-Nielsen, Kirsten and J{\o}rgensen, Marie Birk and Holtermann, Andreas},
doi = {10.1007/s00420-018-1378-4},
file = {::},
issn = {14321246},
journal = {International Archives of Occupational and Environmental Health},
keywords = {Accelerometer,Cleaning,Manufacturing,Physical activity,Physical work demand,Transportation},
month = {may},
number = {4},
pages = {481--490},
publisher = {Springer Verlag},
title = {{The association between occupational standing and sedentary leisure time over consecutive workdays among blue-collar workers in manual jobs}},
volume = {92},
year = {2019}
}
@article{Mulville2016,
abstract = {Purpose: This paper sets out to understand the impact of the ambient environment on perceived comfort, health, wellbeing and by extension productivity in the workplace. Design/methodology/approach: The research combined an occupant survey considering satisfaction with the ambient environment, health and wellbeing and workplace behaviour with the monitoring of ambient environmental conditions. Findings: The paper demonstrates that the ambient environment can have a significant impact on occupant comfort, health and wellbeing, which in turn has implications for built asset performance. Within the ambient environmental factors considered, a hierarchy may exist with noise being of particular importance. Occupant behaviour within the workplace was also found to be influential. Research limitations/implications: The research was limited to a single commercial office building, and a wider range of case studies would therefore be of benefit. The research was also limited to the summer months. Practical implications: The findings show that an active approach to asset management is required, by continuously monitoring internal environment and engaging with occupants. This must carefully consider how ambient environmental factors and workplace behaviour impact upon occupants' comfort, health and wellbeing to ensure the performance of the built asset is maximised. Originality/value: This paper demonstrates that both occupiers' workplace behaviour and ambient environmental conditions can have an impact on occupant comfort, health, wellbeing and productivity. The paper strengthens the case for the active management of the workplace environment through environmental monitoring and behaviour change campaigns supported by corresponding changes to workplace culture.},
author = {Mulville, Mark and Callaghan, Nicola and Isaac, David},
doi = {10.1108/JCRE-11-2015-0038},
issn = {14791048},
journal = {Journal of Corporate Real Estate},
keywords = {Asset management,Behaviour,Comfort,Health,Productivity,Wellbeing},
month = {sep},
number = {3},
pages = {180--193},
publisher = {Emerald Group Publishing Ltd.},
title = {{The impact of the ambient environment and building configuration on occupant productivity in open-plan commercial offices}},
volume = {18},
year = {2016}
}
@article{Schulz2017,
abstract = {Occupational health researchers and practitioners have mainly focused on the individual and organizational levels, whereas the team level has been largely neglected. In this study, we define team health climate as employees' shared perceptions of the extent to which their team is concerned, cares, and communicates about health issues. Based on climate, signaling, and social exchange theories, we examined a multilevel model of team health climate and its relationships with five well-established health-related outcomes (i.e., subjective general health, psychosomatic complaints, mental health, work ability, and presenteeism). Results of multilevel analyses of data provided by 6,449 employees in 621 teams of a large organization showed that team health climate is positively related to subjective general health, mental health, and work ability, and negatively related to presenteeism, above and beyond the effects of team size, age, job tenure, job demands, job control, and employees' individual perceptions of health climate. Moreover, additional analyses showed that a positive team health climate buffered the negative relationship between employee age and work ability. Implications for future research on team health climate and suggestions for occupational health interventions in teams are discussed.},
author = {Schulz, Heiko and Zacher, Hannes and Lippke, Sonia},
doi = {10.3389/fpsyg.2017.00074},
file = {::},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Health,Organizational climate,Presenteeism,Teams,Work ability},
month = {jan},
number = {JAN},
pages = {74},
publisher = {Frontiers Media S.A.},
title = {{The Importance of Team Health Climate for Health-Related Outcomes of White-Collar Workers}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2017.00074/full},
volume = {08},
year = {2017}
}
@inproceedings{Cummaudo2020b,
abstract = {Intelligent services provide the power of AI to developers via simple RESTful API endpoints, abstracting away many complexities of machine learning. However, most of these intelligent services-such as computer vision-continually learn with time. When the internals within the abstracted 'black box' become hidden and evolve, pitfalls emerge in the robustness of applications that depend on these evolving services. Without adapting the way developers plan and construct projects reliant on intelligent services, significant gaps and risks result in both project planning and development. Therefore, how can software engineers best mitigate software evolution risk moving forward, thereby ensuring that their own applications maintain quality? Our proposal is an architectural tactic designed to improve intelligent service-dependent software robustness. The tactic involves creating an application-specific benchmark dataset baselined against an intelligent service, enabling evolutionary behaviour changes to be mitigated. A technical evaluation of our implementation of this architecture demonstrates how the tactic can identify 1,054 cases of substantial confidence evolution and 2,461 cases of substantial changes to response label sets using a dataset consisting of 331 images that evolve when sent to a service.},
archivePrefix = {arXiv},
arxivId = {2005.13186},
author = {Cummaudo, Alex and Barnett, Scott and Vasa, Rajesh and Grundy, John and Abdelrazek, Mohamed},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3409688},
eprint = {2005.13186},
file = {::},
isbn = {9781450370431},
keywords = {Intelligent web services,Software architecture,Software evolution},
pages = {269--280},
publisher = {ACM},
title = {{Beware the Evolving ‘Intelligent' Web Service! An Integration Architecture Tactic to Guard AI-First Components}},
year = {2020}
}
@article{Vassallo2018a,
abstract = {Continuous Integration (CI) is a software engineering practice where developers constantly integrate their changes to a project through an automated build process. The goal of CI is to provide developers with prompt feedback on several quality dimensions after each change. Indeed, previous studies provided empirical evidence on a positive association between properly following CI principles and source code quality. A core principle behind CI is Continuous Code Quality (also known as CCQ, which includes automated testing and automated code inspection) may appear simple and effective, yet we know little about its practical adoption. In this paper, we propose a preliminary empirical investigation aimed at understanding how rigorously practitioners follow CCQ. Our study reveals a strong dichotomy between theory and practice: developers do not perform continuous inspection but rather control for quality only at the end of a sprint and most of the times only on the release branch. Preprint [https://doi.org/10.5281/zenodo.1341036]. Data and Materials [http://doi.org/10.5281/zenodo.1341015].},
author = {Vassallo, Carmine and Bacchelli, Alberto and Palomba, Fabio and Gall, Harald C.},
doi = {10.1145/3238147.3240729},
file = {::},
isbn = {9781450359375},
journal = {ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
keywords = {Code Quality,Continuous Integration,Empirical Studies},
pages = {790--795},
title = {{Continuous code quality: Are we (really) doing that?}},
year = {2018}
}
@techreport{VanVeldhoven,
abstract = {Occup Environ Med 2003;60(Suppl I):i3-i9 The "need for recovery scale" is suggested as an operationalisation for the measurement of (early symptoms of) fatigue at work. Definition of and background on the concept of need for recovery are briefly discussed. Details about scale construction are summarised. Correlations with other relevant measurement scales on fatigue at work are presented to validate the operationalisation claim, as are early results on predictive validity. A study is presented that further investigates the measurement quality and validity of the scale. The data used in this study were collected by Occupational Health Services for 68 775 workers during the period 1996-2000. Comparing the measurement quality of subgroups (Cronbach's alpha) differing in terms of age class, sex, and education level, the general applicability of the scale was shown. The validity of the scale was studied by analysing its association with psychosocial risk factors. Multiple regression analyses of need for recovery were performed on individual and department level data, using 10 psychosocial job characteristics as independent variables. The two most important factors in the explanation of variance at the individual level were also dominant at the department level: pace and amount of work, and emotional workload. The percentage of explained variance was higher at the department level than at the individual level, and increased with department size. Results suggest that the need for recovery scale is an adequate scale, both for applications at the individual and at the group (department/organisation) level.},
author = {{Van Veldhoven}, M},
file = {::},
title = {{Measurement quality and validity of the "need for recovery scale"}},
url = {http://oem.bmj.com/}
}
@article{Zhou2018,
abstract = {Epilepsy is a neurological disorder that affects approximately fifty million people according to the World Health Organization. While electroencephalography (EEG) plays important roles in monitoring the brain activity of patients with epilepsy and diagnosing epilepsy, an expert is needed to analyze all EEG recordings to detect epileptic activity. This method is obviously time-consuming and tedious, and a timely and accurate diagnosis of epilepsy is essential to initiate antiepileptic drug therapy and subsequently reduce the risk of future seizures and seizure-related complications. In this study, a convolutional neural network (CNN) based on raw EEG signals instead of manual feature extraction was used to distinguish ictal, preictal, and interictal segments for epileptic seizure detection. We compared the performances of time and frequency domain signals in the detection of epileptic signals based on the intracranial Freiburg and scalp CHB-MIT databases to explore the potential of these parameters. Three types of experiments involving two binary classification problems (interictal vs. preictal and interictal vs. ictal) and one three-class problem (interictal vs. preictal vs. ictal) were conducted to explore the feasibility of this method. Using frequency domain signals in the Freiburg database, average accuracies of 96.7, 95.4, and 92.3{\%} were obtained for the three experiments, while the average accuracies for detection in the CHB-MIT database were 95.6, 97.5, and 93{\%} in the three experiments. Using time domain signals in the Freiburg database, the average accuracies were 91.1, 83.8, and 85.1{\%} in the three experiments, while the signal detection accuracies in the CHB-MIT database were only 59.5, 62.3, and 47.9{\%} in the three experiments. Based on these results, the three cases are effectively detected using frequency domain signals. However, the effective identification of the three cases using time domain signals as input samples is achieved for only some patients. Overall, the classification accuracies of frequency domain signals are significantly increased compared to time domain signals. In addition, frequency domain signals have greater potential than time domain signals for CNN applications.},
author = {Zhou, Mengni and Tian, Cheng and Cao, Rui and Wang, Bin and Niu, Yan and Hu, Ting and Guo, Hao and Xiang, Jie},
doi = {10.3389/fninf.2018.00095},
file = {::},
issn = {16625196},
journal = {Frontiers in Neuroinformatics},
keywords = {Convolutional neural networks,Electroencephalogram,Epilepsy,Frequency domain signals,Time domain signals},
month = {dec},
publisher = {Frontiers Media S.A.},
title = {{Epileptic seizure detection based on EEG signals and CNN}},
volume = {12},
year = {2018}
}
@incollection{ManchesterSchoolofManagements2018,
author = {{Manchester School of Managements}, Cary L. and Cooper, Cary L.},
booktitle = {Managerial, Occupational and Organizational Stress Research},
doi = {10.4324/9781315196244-1},
month = {nov},
pages = {3--20},
publisher = {Routledge},
title = {{Occupational sources of stress: a review of the literature relating to coronary heart disease and mental ill health}},
year = {2018}
}
@article{Yusop2020a,
author = {Yusop, Nor Shahida Mohamad and Grundy, John and Schneider, Jean-Guy and Vasa, Rajesh},
doi = {10.1016/j.infsof.2020.106396},
file = {::},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Empirical evaluation,Open source software development,Usability defect reporting,Usability defect taxonomy},
month = {aug},
pages = {106396},
publisher = {Elsevier B.V.},
title = {{A Revised Open Source Usability Defect Classification Taxonomy}},
year = {2020}
}
@inproceedings{Khalajzadeh2020b,
author = {Khalajzadeh, Hourieh and Simmons, Andrew J and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang},
booktitle = {2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
doi = {10.1109/VL/HCC50065.2020.9127196},
file = {::},
isbn = {978-1-7281-6901-9},
month = {aug},
pages = {1--4},
publisher = {IEEE},
title = {{End-User-Oriented Tool Support for Modeling Data Analytics Requirements}},
url = {https://ieeexplore.ieee.org/document/9127196/},
year = {2020}
}
@article{Mack2018,
abstract = {In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue—challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.},
archivePrefix = {arXiv},
arxivId = {1801.03829},
author = {Mack, Kelly and Lee, John and Chang, Kevin and Karahalios, Karrie and Parameswaran, Aditya},
doi = {10.1145/3170427.3174359},
eprint = {1801.03829},
file = {::;::},
isbn = {9781450356206},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Excel,Reddit,Scalability,Spreadsheets},
pages = {1--9},
title = {{Characterizing scalability issues in spreadsheet software using online forums}},
volume = {2018-April},
year = {2018}
}
@inproceedings{Best2014,
address = {Sydney, Australia},
author = {Best, Christopher and Jia, Dawei and Simpkin, Graeme},
booktitle = {NATO STO MSG 111 Multi-Workshop, Sydney, Australia},
file = {::},
title = {{Air Force Synthetic Training Effectiveness Research in the Australian Context}},
url = {https://www.sto.nato.int/publications/STO Meeting Proceedings/STO-MP-MSG-111/MP-MSG-111-16.pdf},
year = {2014}
}
@misc{Paksaichol2012,
abstract = {The purpose of this study was to systematically review prospective cohort studies to gain insights into risk factors for the development of non-specific neck pain in office workers as well as to assess the strength of evidence. Publications were systematically searched from 1980 - March 2011 in several databases. The following key words were used: neck pain paired with risk or prognostic factors and office or computer or visual display unit or visual display terminal. Relevant studies were retrieved and assessed for methodological quality by two independent reviewers. The strength of the evidence was based on methodological quality and consistency of the results. Five high-quality and two low-quality prospective cohort studies investigating the predictive value of 47 individual, work-related physical and work-related psychosocial factors for the onset of non-specific neck pain in office workers were included in this review. Strong evidence was found for female gender and previous history of neck complaints to be predictors of the onset of neck pain. Interestingly, for a large number of factors that have been mentioned in the literature as risk factors for neck pain, such as high physical leisure activity, low social support, and high psychosocial stress, we found no predictive value for future neck pain in office workers. Literature with respect to the development of non-specific neck pain in office workers is scant. Only female gender and previous history of neck complaints have been identified as risk factors that predict the onset of neck pain.},
author = {Paksaichol, Arpalak and Janwantanakul, Prawit and Purepong, Nithima and Pensri, Praneet and {Van Der Beek}, Allard J.},
booktitle = {Occupational and Environmental Medicine},
doi = {10.1136/oemed-2011-100459},
issn = {13510711},
month = {sep},
number = {9},
pages = {610--618},
publisher = {BMJ Publishing Group Ltd},
title = {{Office workers' risk factors for the development of non-specific neck pain: A systematic review of prospective cohort studies}},
volume = {69},
year = {2012}
}
@inproceedings{Liu2020,
abstract = {Drawing reliable inferences from data involves many, sometimes arbitrary, decisions across phases of data collection, wrangling, and modeling. As different choices can lead to diverging conclusions, understanding how researchers make analytic decisions is important for supporting robust and replicable analysis. In this study, we pore over nine published research studies and conduct semi-structured interviews with their authors. We observe that researchers often base their decisions on methodological or theoretical concerns, but subject to constraints arising from the data, expertise, or perceived interpretability. We confirm that researchers may experiment with choices in search of desirable results, but also identify other reasons why researchers explore alternatives yet omit findings. In concert with our interviews, we also contribute visualizations for communicating decision processes throughout an analysis. Based on our results, we identify design opportunities for strengthening end-to-end analysis, for instance via tracking and meta-analysis of multiple decision paths.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1910.13602},
author = {Liu, Yang and Althoff, Tim and Heer, Jeffrey},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3313831.3376533},
eprint = {1910.13602},
file = {::},
isbn = {9781450367080},
keywords = {Analytic decision making,Data analysis,Garden of forking paths,Interview Study,Multiverse analysis,Reproducibility},
month = {apr},
pages = {1--14},
publisher = {ACM},
title = {{Paths Explored, Paths Omitted, Paths Obscured: Decision Points {\&} Selective Reporting in End-to-End Data Analysis}},
year = {2020}
}
@article{Hoekstra2012,
abstract = {A valid interpretation of most statistical techniques requires that one or more assumptions be met. In published articles, however, little information tends to be reported on whether the data satisfy the assumptions underlying the statistical techniques used. This could be due to self-selection: Only manuscripts with data fulfilling the assumptions are submitted. Another explanation could be that violations of assumptions are rarely checked for in the first place. We studied whether and how 30 researchers checked fictitious data for violations of assumptions in their own working environment. Participants were asked to analyze the data as they would their own data, for which often used and well-known techniques such as the t-procedure, ANOVA and regression (or non-parametric alternatives) were required. It was found that the assumptions of the techniques were rarely checked, and that if they were, it was regularly by means of a statistical test. Interviews afterward revealed a general lack of knowledge about assumptions, the robustness of the techniques with regards to the assumptions, and how (or whether) assumptions should be checked. These data suggest that checking for violations of assumptions is not a well-considered choice, and that the use of statistics can be described as opportunistic. {\textcopyright} 2012 Hoekstra, Kiers and Johnson.},
author = {Hoekstra, Rink and Kiers, Henk A.L. L and Johnson, Addie},
doi = {10.3389/fpsyg.2012.00137},
file = {::},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Analyzing data,Assumptions,Homogeneity,Normality,Robustness},
number = {MAY},
pages = {1--9},
title = {{Are assumptions of well-known statistical techniques checked, and why (not)?}},
volume = {3},
year = {2012}
}
@techreport{Othman2018,
abstract = {This paper focuses on question retrieval which is a crucial and tricky task in Community Question Answering (cQA). Question retrieval aims at finding historical questions that are semantically equivalent to the queried ones, assuming that the answers to the similar questions should also answer the new ones. The major challenges are the lexical gap problem as well as the verboseness in natural language. Most existing methods measure the similarity between questions based on the bag-of-words (BOWs) representation capturing no semantics between words. In this paper, we rely on word embeddings and TF-IDF for a meaningful vector representation of the questions. The similarity between questions is measured using cosine similarity based on their vector-based word representations. Experiments carried out on a real world data set from Yahoo! Answers show that our method is competetive.},
author = {Othman, Nouha and Faiz, Rim and Sma{\"{i}}li, Kamel},
booktitle = {Journal of the International Science and General Applications},
file = {::},
number = {1},
title = {{Using Word Embeddings to Retrieve Semantically Similar Questions in Community Question Answering}},
url = {http://www.mathoverflow.net},
volume = {1},
year = {2018}
}
@article{Casalnuovo2015,
abstract = {{\textcopyright}2015 IEEE. Asserts have long been a strongly recommended (if non-functional) adjunct to programs. They certainly don't add any user-evident feature value; and it can take quite some skill and effort to devise and add useful asserts. However, they are believed to add considerable value to the developer. Certainly, they can help with automated verification; but even in the absence of that, claimed advantages include improved understandability, maintainability, easier fault localization and diagnosis, all eventually leading to better software quality. We focus on this latter claim, and use a large dataset of asserts in C and C++ programs to explore the connection between asserts and defect occurrence. Our data suggests a connection: functions with asserts do have significantly fewer defects. This indicates that asserts do play an important role in software quality; we therefore explored further the factors that play a role in assertion placement: specifically, process factors (such as developer experience and ownership) and product factors, particularly interprocedural factors, exploring how the placement of assertions in functions are influenced by local and global network properties of the callgraph. Finally, we also conduct a differential analysis of assertion use across different application domains.},
author = {Casalnuovo, Casey and Devanbu, Prem and Oliveira, Abilio and Filkov, Vladimir and Ray, Baishakhi},
doi = {10.1109/ICSE.2015.88},
file = {::},
isbn = {9781479919345},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {755--766},
title = {{Assert use in GitHub projects}},
volume = {1},
year = {2015}
}
@article{Cook2013,
abstract = {Background: Seizure prediction would be clinically useful in patients with epilepsy and could improve safety, increase independence, and allow acute treatment. We did a multicentre clinical feasibility study to assess the safety and efficacy of a long-term implanted seizure advisory system designed to predict seizure likelihood and quantify seizures in adults with drug-resistant focal seizures. Methods: We enrolled patients at three centres in Melbourne, Australia, between March 24, 2010, and June 21, 2011. Eligible patients had between two and 12 disabling partial-onset seizures per month, a lateralised epileptogenic zone, and no history of psychogenic seizures. After devices were surgically implanted, patients entered a data collection phase, during which an algorithm for identification of periods of high, moderate, and low seizure likelihood was established. If the algorithm met performance criteria (ie, sensitivity of high-likelihood warnings greater than 65{\%} and performance better than expected through chance prediction of randomly occurring events), patients then entered an advisory phase and received information about seizure likelihood. The primary endpoint was the number of device-related adverse events at 4 months after implantation. Our secondary endpoints were algorithm performance at the end of the data collection phase, clinical effectiveness (measures of anxiety, depression, seizure severity, and quality of life) 4 months after iniation of the advisory phase, and longer-term adverse events. This trial is registered with ClinicalTrials.gov, number NCT01043406. Findings: We implanted 15 patients with the advisory system. 11 device-related adverse events were noted within four months of implantation, two of which were serious (device migration, seroma); an additional two serious adverse events occurred during the first year after implantation (device-related infection, device site reaction), but were resolved without further complication. The device met enabling criteria in 11 patients upon completion of the data collection phase, with high likelihood performance estimate sensitivities ranging from 65{\%} to 100{\%}. Three patients' algorithms did not meet performance criteria and one patient required device removal because of an adverse event before sufficient training data were acquired. We detected no significant changes in clinical effectiveness measures between baseline and 4 months after implantation. Interpretation: This study showed that intracranial electroencephalographic monitoring is feasible in ambulatory patients with drug-resistant epilepsy. If these findings are replicated in larger, longer studies, accurate definition of preictal electrical activity might improve understanding of seizure generation and eventually lead to new management strategies. Funding: NeuroVista. {\textcopyright} 2013 Elsevier Ltd.},
author = {Cook, Mark J. and O'Brien, Terence J. and Berkovic, Samuel F. and Murphy, Michael and Morokoff, Andrew and Fabinyi, Gavin and D'Souza, Wendyl and Yerra, Raju and Archer, John and Litewka, Lucas and Hosking, Sean and Lightfoot, Paul and Ruedebusch, Vanessa and Sheffield, W. Douglas and Snyder, David and Leyde, Kent and Himes, David},
doi = {10.1016/S1474-4422(13)70075-9},
issn = {14744422},
journal = {The Lancet Neurology},
month = {jun},
number = {6},
pages = {563--571},
publisher = {Elsevier},
title = {{Prediction of seizure likelihood with a long-term, implanted seizure advisory system in patients with drug-resistant epilepsy: A first-in-man study}},
volume = {12},
year = {2013}
}
@article{Kristensson2019,
abstract = {This course introduces computational methods in human-computer interaction. Computational interaction methods use computational thinking-abstraction, automation, and analysis-to explain and enhance interaction. This course introduces the theory of practice of computational interaction by teaching Bayesian methods for interaction across four wide areas of interest when designing computationally-driven user interfaces: decoding, adaptation, learning and optimization. The lectures center on hands-on Python programming interleaved with theory and practical examples grounded in problems of wide interest in human-computer interaction.},
author = {Kristensson, Per Ola and Oulasvirta, Antti and Banovic, Nikola and Williamson, John},
doi = {10.1145/3290607.3298820},
file = {::},
isbn = {9781450359719},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Computational interaction,Inference,Machine learning,Optimization},
pages = {1--6},
title = {{Computational interaction with Bayesian methods}},
year = {2019}
}
@article{Lonzetta2018,
abstract = {{\textless}p{\textgreater}Bluetooth technology is a key component of wireless communications. It provides a low-energy and low-cost solution for short-range radio transmissions. Bluetooth, more specifically Bluetooth Low Energy (BLE) has become the predominant technology for connecting IoT (Internet of Things). It can be found in cell phones, headsets, speakers, printers, keyboards, automobiles, children's toys, and medical devices, as well as many other devices. The technology can also be found in automated smart homes, to provide monitors and controls for lights, thermostats, door locks, appliances, security systems, and cameras. Bluetooth offers convenience and ease of use, but it lacks a centralized security infrastructure. As a result, it has serious security vulnerabilities, and the need for awareness of the security risks are increasing as the technology becomes more widespread. This paper presents an overview of Bluetooth technology in IoT including its security, vulnerabilities, threats, and risk mitigation solutions, as well as real-life examples of exploits. Our study highlights the importance of understanding attack risks and mitigation techniques involved with using Bluetooth technology on our devices. Real-life examples of recent Bluetooth exploits are presented. Several recommended security measures are discussed to secure Bluetooth communication.{\textless}/p{\textgreater}},
author = {Lonzetta, Angela and Cope, Peter and Campbell, Joseph and Mohd, Bassam and Hayajneh, Thaier},
doi = {10.3390/jsan7030028},
file = {::},
issn = {2224-2708},
journal = {Journal of Sensor and Actuator Networks},
keywords = {BlueBorne,BlueBugging,BlueJacking,Bluetooth,Bluetooth attacks,Bluetooth countermeasures,Bluetooth hacking,Bluetooth security,Bluetooth vulnerabilities,IoT},
month = {jul},
number = {3},
pages = {28},
publisher = {MDPI AG},
title = {{Security Vulnerabilities in Bluetooth Technology as Used in IoT}},
url = {http://www.mdpi.com/2224-2708/7/3/28},
volume = {7},
year = {2018}
}
@article{Martins2018,
abstract = {We provide a repository of 50,000 compilable Java projects. Each project in this dataset comes with references to all the dependencies required to compile it, the resulting bytecode, and the scripts with which the projects were built.

The dependencies and the build scripts provide a mechanism to re-create compilation of the projects, if needed (to instruct source code for bytecode analysis, for example). The bytecode is ready for testing, execution, and dynamic analysis tools.},
author = {Martins, Pedro and Achar, Rohan and Lopes, Cristina V.},
doi = {10.1145/3196398.3196450},
file = {::},
isbn = {9781450357166},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {large scale compilation,runnable software repositories,software mining},
pages = {1--5},
title = {{50K-C: A dataset of compilable, and compiled, Java projects}},
year = {2018}
}
@article{Dix2017,
abstract = {Many find statistics confusing, and perhaps more so given recent publicity of problems with traditional pvalues and alternative statistical techniques including confidence intervals and Bayesian statistics. This course aims to help attendees navigate this morass: to understand the debates and more importantly make appropriate choices when designing and analysing experiments, empirical studies and other forms of quantitative data.},
author = {Dix, Alan},
doi = {10.1145/3027063.3027109},
file = {::},
isbn = {9781450346566},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Bayesian statistics,Evaluation,Human-computer interaction,Statistics,hypothesis testing},
pages = {1236--1239},
title = {{Making sense of statistics in HCI: From P to Bayes and beyond}},
volume = {Part F1276},
year = {2017}
}
@techreport{Ling2008,
abstract = {Synonyms Learning with different classification costs, cost-sensitive classification Definition Cost-Sensitive Learning is a type of learning in data mining that takes the misclassification costs (and possibly other types of cost) into consideration. The goal of this type of learning is to minimize the total cost. The key difference between cost-sensitive learning and cost-insensitive learning is that cost-sensitive learning treats the different misclassifications differently. Cost-insensitive learning does not take the misclassification costs into consideration. The goal of this type of learning is to pursue a high accuracy of classifying examples into a set of known classes. The class imbalanced datasets occurs in many real-world applications where the class distributions of data are highly imbalanced. Cost-sensitive learning is a common approach to solve this problem.},
author = {Ling, Charles X and Sheng, Victor S},
booktitle = {Encyclopedia of Machine Learning},
file = {::},
publisher = {Springer},
title = {{Cost-Sensitive Learning and the Class Imbalance Problem Motivation and Background}},
year = {2008}
}
@article{Classen2012,
abstract = {This study investigates differences in the diversity of cooperation partners used for innovation-related activities (i.e., search breadth) between family and nonfamily small and medium-sized enterprises (SMEs), as well as within the group of family SMEs. The results generally confirm our hypotheses derived from the behavioral theory of the firm. Specifically, we show that family SMEs have a lower search breadth than their nonfamily counterparts. Our findings further illustrate how attributes of the CEO (level of education) and the top management team (nonfamily management involvement and educational background diversity) relate to the search breadth of family SMEs. {\textcopyright} 2012 International Council for Small Business.},
author = {Classen, Nicolas and {Van Gils}, Anita and Bammens, Yannick and Carree, Martin},
doi = {10.1111/j.1540-627X.2012.00350.x},
issn = {00472778},
journal = {Journal of Small Business Management},
month = {apr},
number = {2},
pages = {191--215},
title = {{Accessing Resources from Innovation Partners: The Search Breadth of Family SMEs}},
url = {http://doi.wiley.com/10.1111/j.1540-627X.2012.00350.x},
volume = {50},
year = {2012}
}
@article{Dulac-arnold,
archivePrefix = {arXiv},
arxivId = {arXiv:2003.11881v1},
author = {Dulac-arnold, Gabriel and Mar, L G and Li, Jerry},
eprint = {arXiv:2003.11881v1},
file = {::},
title = {{An empirical investigation of the challenges of real-world reinforcement learning}}
}
@inproceedings{zhu2014patterns,
author = {Zhu, Jiaxin and Zhou, Minghui and Mockus, Audris},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
file = {::},
organization = {ACM},
pages = {30},
title = {{Patterns of folder use and project popularity: A case study of GitHub repositories}},
year = {2014}
}
@article{Malloy2019,
abstract = {Python is one of the most popular and widely adopted programming languages in use today. In 2008 the Python developers introduced a new version of the language, Python 3.0, that was not backward compatible with Python 2, initiating a transitional phase for Python software developers. In this paper, we describe a study that investigates the degree to which Python software developers are making the transition from Python 2 to Python 3. We have developed a Python compliance analyser, PyComply, and have analysed a previously studied corpus of Python applications called Qualitas. We use PyComply to measure and quantify the degree to which Python 3 features are being used, as well as the rate and context of their adoption in the Qualitas corpus. Our results indicate that Python software developers are not exploiting the new features and advantages of Python 3, but rather are choosing to retain backward compatibility with Python 2. Moreover, Python developers are confining themselves to a language subset, governed by the diminishing intersection of Python 2, which is not under development, and Python 3, which is under development with new features being introduced as the language continues to evolve.},
author = {Malloy, Brian A. and Power, James F.},
doi = {10.1007/s10664-018-9637-2},
file = {::},
isbn = {1066401896},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Compliance,Programming language evolution,Python programming},
number = {2},
pages = {751--778},
publisher = {Empirical Software Engineering},
title = {{An empirical analysis of the transition from Python 2 to Python 3}},
volume = {24},
year = {2019}
}
@article{Jing2020,
abstract = {Importance: The validity of using electroencephalograms (EEGs) to diagnose epilepsy requires reliable detection of interictal epileptiform discharges (IEDs). Prior interrater reliability (IRR) studies are limited by small samples and selection bias. Objective: To assess the reliability of experts in detecting IEDs in routine EEGs. Design, Setting, and Participants: This prospective analysis conducted in 2 phases included as participants physicians with at least 1 year of subspecialty training in clinical neurophysiology. In phase 1, 9 experts independently identified candidate IEDs in 991 EEGs (1 expert per EEG) reported in the medical record to contain at least 1 IED, yielding 87636 candidate IEDs. In phase 2, the candidate IEDs were clustered into groups with distinct morphological features, yielding 12602 clusters, and a representative candidate IED was selected from each cluster. We added 660 waveforms (11 random samples each from 60 randomly selected EEGs reported as being free of IEDs) as negative controls. Eight experts independently scored all 13262 candidates as IEDs or non-IEDs. The 1051 EEGs in the study were recorded at the Massachusetts General Hospital between 2012 and 2016. Main Outcomes and Measures: Primary outcome measures were percentage of agreement (PA) and beyond-chance agreement (Gwet $\kappa$) for individual IEDs (IED-wise IRR) and for whether an EEG contained any IEDs (EEG-wise IRR). Secondary outcomes were the correlations between numbers of IEDs marked by experts across cases, calibration of expert scoring to group consensus, and receiver operating characteristic analysis of how well multivariate logistic regression models may account for differences in the IED scoring behavior between experts. Results: Among the 1051 EEGs assessed in the study, 540 (51.4{\%}) were those of females and 511 (48.6{\%}) were those of males. In phase 1, 9 experts each marked potential IEDs in a median of 65 (interquartile range [IQR], 28-332) EEGs. The total number of IED candidates marked was 87636. Expert IRR for the 13262 individually annotated IED candidates was fair, with the mean PA being 72.4{\%} (95{\%} CI, 67.0{\%}-77.8{\%}) and mean $\kappa$ being 48.7{\%} (95{\%} CI, 37.3{\%}-60.1{\%}). The EEG-wise IRR was substantial, with the mean PA being 80.9{\%} (95{\%} CI, 76.2{\%}-85.7{\%}) and mean $\kappa$ being 69.4{\%} (95{\%} CI, 60.3{\%}-78.5{\%}). A statistical model based on waveform morphological features, when provided with individualized thresholds, explained the median binary scores of all experts with a high degree of accuracy of 80{\%} (range, 73{\%}-88{\%}). Conclusions and Relevance: This study's findings suggest that experts can identify whether EEGs contain IEDs with substantial reliability. Lower reliability regarding individual IEDs may be largely explained by various experts applying different thresholds to a common underlying statistical model.},
author = {Jing, Jin and Herlopian, Aline and Karakis, Ioannis and Ng, Marcus and Halford, Jonathan J. and Lam, Alice and Maus, Douglas and Chan, Fonda and Dolatshahi, Marjan and Muniz, Carlos F. and Chu, Catherine and Sacca, Valeria and Pathmanathan, Jay and Ge, Wendong and Sun, Haoqi and Dauwels, Justin and Cole, Andrew J. and Hoch, Daniel B. and Cash, Sydney S. and Westover, M. Brandon},
doi = {10.1001/jamaneurol.2019.3531},
file = {::},
issn = {21686157},
journal = {JAMA Neurology},
month = {jan},
number = {1},
pages = {49--57},
pmid = {31633742},
publisher = {American Medical Association},
title = {{Interrater Reliability of Experts in Identifying Interictal Epileptiform Discharges in Electroencephalograms}},
volume = {77},
year = {2020}
}
@techreport{Hahn,
author = {Hahn, U and Lab, Julie and Ogren, P V and Wetzler, P G and Bethard, S J and Buyko, E and Landefeld, R and M{\"{u}}hlhausen, M and Poprat, M and Tomanek, K and Wermter, J and Nyberg, E and Riebling, E and Wang, R C and Frederking, R and Kunze, M and R{\"{o}}sner, D and Savova, G K and Kipper-Schuler, K and Buntrock, J D and Chute, C G},
file = {::},
title = {{Workshop Programme Session Time Title Opening 09:00-09:15 Welcome and Opening Session ClearTK: A UIMA Toolkit for Statistical Natural Language Processing Integrating a Natural Language Message Pre-Processor with UIMA 11:55-12:15 UIMA for NLP based Researc}}
}
@techreport{Tracey2009,
address = {Melbourne, Vic},
author = {Tracey, Eleanore and Hasenbosch, Sam and Vince, Julian and Pope, Daniel and Stott, Aaron and Best, Christopher and Shanahan, Christopher and Finch, Melanie},
file = {::},
institution = {Defence Science and Technology Organisation (Australia)},
keywords = {Team training,distributed mission training (DMT),simulation systems,team performance assessment and review,virtual preparation},
title = {{Exercise Black Skies 2008 : Enhancing Live Training Through Virtual Preparation Part Two : An Evaluation of Tools and Techniques}},
url = {http://digext6.defence.gov.au/dspace/handle/1947/10024},
year = {2009}
}
@article{Phelan2019,
abstract = {Bayesian statistical analysis has gained attention in recent years, including in HCI. The Bayesian approach has several advantages over traditional statistics, including producing results with more intuitive interpretations. Despite growing interest, few papers in CHI use Bayesian analysis. Existing tools to learn Bayesian statistics require significant time investment, making it difficult to casually explore Bayesian methods. Here, we present a tool that lowers the barrier to exploration: a set of R code templates that guide Bayesian novices through their first analysis. The templates are tailored to CHI, supporting analyses found to be most common in recent CHI papers. In a user study, we found that the templates were easy to understand and use. However, we found that participants without a statistical background were not confident in their use. Together our contributions provide a concise analysis tool and empirical results for understanding and addressing barriers to using Bayesian analysis in HCI.},
author = {Phelan, Chanda and Hullman, Jessica and Kay, Matthew and Resnick, Paul},
doi = {10.1145/3290605.3300709},
file = {::},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Bayesian statistics,Code templates,Evaluation,Hypothesis testing,Statistics,Tutorials},
pages = {1--12},
title = {{Some prior(s) experience necessary templates for getting started with Bayesian analysis}},
year = {2019}
}
@inproceedings{Norheim2019,
abstract = {Blue-collar workers with physically demanding occupations have a high prevalence of musculoskeletal disorders accompanied by pain. Previous research has suggested that reduced motor variability may increase the risk of developing musculoskeletal disorders. Here we present preliminary data from an ongoing cross-sectional examination of physical performance in elderly manual workers. This paper includes data from 20 male workers (age 52–70 years). Handgrip force variability was measured using a digital hand dynamometer during an endurance trial where the workers exerted 30{\%} of their maximal isometric contraction force until task failure. Absolute variability (standard deviation), relative variability (coefficient of variation), and the complexity of the force signal (sample entropy) were computed. The workers were dichotomized into two groups: no to mild pain/discomfort (No pain) and moderate to severe pain/discomfort (Pain) to investigate the effects of musculoskeletal pain on force variability. This dichotomy was done based on the rating of pain/discomfort within the last seven days, where workers reporting ≥3 score in the upper extremities were allocated to the pain group (Pain, n = 9, No pain, n = 11). No significant between-group differences in force variability during the endurance trial were found. Both absolute and relative variability increased significantly over time. These preliminary data do not support a difference in force variability between blue-collar workers with or without musculoskeletal pain or discomfort in the upper extremities.},
author = {Norheim, Kristoffer Larsen and B{\o}nl{\o}kke, Jakob Hjort and Omland, {\O}yvind and Samani, Afshin and Madeleine, Pascal},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-96065-4_9},
isbn = {9783319960647},
issn = {21945357},
keywords = {Discomfort,Endurance,Handgrip strength,Motor control},
month = {aug},
pages = {59--67},
publisher = {Springer Verlag},
title = {{Force variability and musculoskeletal pain in blue-collar workers}},
volume = {826},
year = {2019}
}
@inproceedings{Gebru2018,
abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
archivePrefix = {arXiv},
arxivId = {1803.09010},
author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and {Daum{\'{e}} III}, Hal and Crawford, Kate},
booktitle = {Proceedings of the 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2018)},
eprint = {1803.09010},
file = {::},
title = {{Datasheets for Datasets}},
url = {http://arxiv.org/abs/1803.09010},
year = {2018}
}
@article{Fauci2020,
abstract = {Covid-19 — Navigating the Uncharted Anthony Fauci comments on the early clinical features and epidemiology of cases reported in Wuhan, China, along with current mortality data, noting that the outb...},
author = {Fauci, Anthony S and Lane, H Clifford and Redfield, Robert R},
doi = {10.1056/NEJMe2002387},
file = {::},
issn = {1533-4406},
journal = {The New England journal of medicine},
month = {feb},
number = {13},
pages = {1268--1269},
pmid = {32109011},
publisher = {Massachusetts Medical Society},
title = {{Covid-19 - Navigating the Uncharted.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/32109011},
volume = {382},
year = {2020}
}
@article{Usman2017,
abstract = {Epileptic seizures occur due to disorder in brain functionality which can affect patient's health. Prediction of epileptic seizures before the beginning of the onset is quite useful for preventing the seizure by medication. Machine learning techniques and computational methods are used for predicting epileptic seizures from Electroencephalograms (EEG) signals. However, preprocessing of EEG signals for noise removal and features extraction are two major issues that have an adverse effect on both anticipation time and true positive prediction rate. Therefore, we propose a model that provides reliable methods of both preprocessing and feature extraction. Our model predicts epileptic seizures' sufficient time before the onset of seizure starts and provides a better true positive rate. We have applied empirical mode decomposition (EMD) for preprocessing and have extracted time and frequency domain features for training a prediction model. The proposed model detects the start of the preictal state, which is the state that starts few minutes before the onset of the seizure, with a higher true positive rate compared to traditional methods, 92.23{\%}, and maximum anticipation time of 33 minutes and average prediction time of 23.6 minutes on scalp EEG CHB-MIT dataset of 22 subjects.},
author = {Usman, Syed Muhammad and Usman, Muhammad and Fong, Simon},
doi = {10.1155/2017/9074759},
file = {::},
issn = {1748-670X},
journal = {Computational and Mathematical Methods in Medicine},
pages = {1--10},
publisher = {Hindawi Limited},
title = {{Epileptic Seizures Prediction Using Machine Learning Methods}},
url = {https://www.hindawi.com/journals/cmmm/2017/9074759/},
volume = {2017},
year = {2017}
}
@article{DeBloom2017,
abstract = {Lunch breaks constitute the longest within-workday rest period, but it is unclear how they affect recovery from job stress. We conducted two randomized controlled trials with 153 Finnish knowledge workers who engaged for 15 min daily in prescribed lunch break activities for ten consecutive working days. Participants were randomly assigned to a: 1) park walking group (N = 51), 2) relaxation exercises group (N = 46) and 3) control group (N = 56). The study was divided into two parts scheduled in spring (N = 83) and fall (N = 70). Recovery experiences (detachment, relaxation, enjoyment) and recovery outcomes (restoration, fatigue, job satisfaction) were assessed with SMS and paper-and-pencil questionnaires several times per day before, during and after the intervention period. A manipulation check revealed that both intervention groups reported less tension after lunch breaks during the intervention than before. In spring, the interventions did hardly affect recovery experiences and outcomes. In fall, restoration increased and fatigue decreased markedly immediately after lunch breaks and in the afternoon in both intervention groups (d = 0.22–0.58) and most consistent positive effects across the day were reported by the park walking group. Park walks and relaxation exercises during lunch breaks can enhance knowledge workers' recovery from work, but effects seem weak, short-lived and dependent on the season.},
author = {de Bloom, Jessica and Sianoja, Marjaana and Korpela, Kalevi and Tuomisto, Martti and Lilja, Ansa and Geurts, Sabine and Kinnunen, Ulla},
doi = {10.1016/j.jenvp.2017.03.006},
issn = {15229610},
journal = {Journal of Environmental Psychology},
keywords = {Detachment,Fatigue,Job satisfaction,Nature,Recovery,Vitality},
month = {aug},
pages = {14--30},
publisher = {Academic Press},
title = {{Effects of park walks and relaxation exercises during lunch breaks on recovery from job stress: Two randomized controlled trials}},
volume = {51},
year = {2017}
}
@article{Khurana2017,
abstract = {Feature engineering involves constructing novel features from given data with the goal of improving predictive learning performance. Feature engineering is predominantly a human-intensive and time consuming step that is central to the data science workflow. In this paper, we present a novel system called 'Cognito', that performs automatic feature engineering on a given dataset for supervised learning. The system explores various feature construction choices in a hierarchical and non-exhaustive manner, while progressively maximizing the accuracy of the model through a greedy exploration strategy. Additionally, the system allows users to specify domain or data specific choices to prioritize the exploration. Cognito is capable of handling large datasets through sampling and built-in parallelism, and integrates well with a state-of-The-Art model selection strategy. We present the design and operation of Cognito, along with experimental results on eight real datasets to demonstrate its efficacy. {\textcopyright}2016 IEEE.},
author = {Khurana, Udayan and Turaga, Deepak and Samulowitz, Horst and Parthasrathy, Srinivasan},
doi = {10.1109/ICDMW.2016.0190},
file = {::},
isbn = {9781509054725},
issn = {23759259},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Data Science,Feature Construction,Feature Engineering,Machine Learning},
pages = {1304--1307},
publisher = {IEEE},
title = {{Cognito: Automated Feature Engineering for Supervised Learning}},
year = {2017}
}
@article{Braiek2018a,
abstract = {Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.},
archivePrefix = {arXiv},
arxivId = {1812.02257},
author = {Braiek, Houssem Ben and Khomh, Foutse},
eprint = {1812.02257},
keywords = {data cleaning,feature engineering testing,implementation testing,machine learning,model testing},
title = {{On Testing Machine Learning Programs}},
url = {http://arxiv.org/abs/1812.02257},
year = {2018}
}
@article{Juul-Kristensen2005,
abstract = {Aims: To identify prognostic ergonomic and work technique factors for musculoskeletal symptoms among office workers and in a subgroup with highly monotonous repetitive computer work. Methods: A baseline questionnaire was delivered to 5033 office workers in 11 Danish companies in the first months of 1999, and a follow up questionnaire was mailed in the last months of 2000 to 3361 respondents. A subgroup with highly monotonous repetitive computer work was formed including those that were repeating the same movements and/or tasks for at least 75{\%} of the work time. The questionnaire contained questions on ergonomic factors and factors related to work technique. The outcome variables were based on the frequency of musculoskeletal symptoms during the last 12 months. Logistic regression analyses were used to identify prognostic factors for symptoms in the three body regions. Results: In total, 39{\%}, 47{\%}, and 51{\%} of the symptomatic subjects had a reduced frequency of symptom days in the neck/shoulder, low back, or elbow/hand region, respectively. In all regions more men than women had reduced symptoms. In the multivariate logistic regression analyses, working no more than 75{\%} of the work time with the computer was a prognostic factor for musculoskeletal symptoms in the neck/ shoulder and elbow/hand, and a high influence on the speed of work was a prognostic factor for symptoms in the low back. In the subgroup with highly monotonous repetitive computer work, the odds ratios of the prognostic factors were similar to those for the whole group of office workers. Conclusion: When organising computer work it is important to allow for physical variation with other work tasks, thereby avoiding working with the computer during all the work time, and further to consider the worker's own influence on the speed of work.},
author = {Juul-Kristensen, B. and Jensen, C.},
doi = {10.1136/oem.2004.013920},
file = {::},
issn = {13510711},
journal = {Occupational and Environmental Medicine},
month = {mar},
number = {3},
pages = {188--194},
publisher = {BMJ Publishing Group Ltd},
title = {{Self-reported workplace related ergonomic conditions as prognostic factors for musculoskeletal symptoms: The "BIT" follow up study on office workers}},
volume = {62},
year = {2005}
}
@article{Logeswaran2019,
abstract = {We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https://github.com/lajanugen/zeshel.},
archivePrefix = {arXiv},
arxivId = {1906.07348},
author = {Logeswaran, Lajanugen and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina and Devlin, Jacob and Lee, Honglak},
eprint = {1906.07348},
file = {::},
title = {{Zero-Shot Entity Linking by Reading Entity Descriptions}},
url = {http://arxiv.org/abs/1906.07348},
year = {2019}
}
@phdthesis{Rosch2016,
author = {R{\"{o}}sch, Manuel and de Boer, Patrick and Bernstein, Abraham},
file = {::},
school = {University of Zurich},
title = {{PaperValidator - Towards the Automated Validation of Statistics in Publications}},
type = {Master's Thesis},
url = {http://manuelroesch.divunity.ch/MA.pdf},
year = {2016}
}
@inproceedings{Barnett2015b,
abstract = {Modern IDEs provide limited support for developers when starting a new data-driven mobile app. App developers are currently required to write copious amounts of boilerplate code, scripts, organise complex directories, and author actual functionality. Although this scenario is ripe for automation, current tools are yet to address it adequately. In this paper we present RAPPT, a tool that generates the scaffolding of a mobile app based on a high level description specified in a Domain Specific Language (DSL). We demonstrate the feasibility of our approach by an example case study and feedback from a professional development team. Demo at: https://www.youtube.com/watch?v=ffquVgBYpLM.},
author = {Barnett, Scott and Vasa, Rajesh and Grundy, John},
booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.216},
isbn = {978-1-4799-1934-5},
month = {may},
pages = {657--660},
publisher = {IEEE},
title = {{Bootstrapping Mobile App Development}},
url = {http://ieeexplore.ieee.org/document/7203036/},
year = {2015}
}
@article{Li2018,
abstract = {Entropy measures that assess signals' complexity have drawn increasing attention recently in biomedical field, as they have shown the ability of capturing unique features that are intrinsic and physiologically meaningful. In this study, we applied entropy analysis to electroencephalogram (EEG) data to examine its performance in epilepsy detection based on short-term EEG, aiming at establishing a short-term analysis protocol with optimal seizure detection performance. Two classification problems were considered, i.e., 1) classifying interictal and ictal EEGs (epileptic group) from normal EEGs; and 2) classifying ictal from interictal EEGs. For each problem, we explored two protocols to analyze the entropy of EEG: i) using a single analytical window with different window lengths, and ii) using an average of multiple windows for each window length. Two entropy methods—fuzzy entropy (FuzzyEn) and distribution entropy (DistEn)–were used that have valid outputs for any given data lengths. We performed feature selection and trained classifiers based on a cross-validation process. The results show that performance of FuzzyEn and DistEn may complement each other and the best performance can be achieved by combining: 1) FuzzyEn of one 5-s window and the averaged DistEn of five 1-s windows for classifying normal from epileptic group (accuracy: 0.93, sensitivity: 0.91, specificity: 0.96); and 2) the averaged FuzzyEn of five 1-s windows and DistEn of one 5-s window for classifying ictal from interictal EEGs (accuracy: 0.91, sensitivity: 0.93, specificity: 0.90). Further studies are warranted to examine whether this proposed short-term analysis procedure can help track the epileptic activities in real time and provide prompt feedback for clinical practices.},
author = {Li, Peng and Karmakar, Chandan and Yearwood, John and Venkatesh, Svetha and Palaniswami, Marimuthu and Liu, Changchun},
doi = {10.1371/journal.pone.0193691},
editor = {Bazhenov, Maxim},
file = {::},
issn = {1932-6203},
journal = {PLOS ONE},
month = {mar},
number = {3},
pages = {e0193691},
publisher = {Public Library of Science},
title = {{Detection of epileptic seizure based on entropy analysis of short-term EEG}},
url = {https://dx.plos.org/10.1371/journal.pone.0193691},
volume = {13},
year = {2018}
}
@article{Hong2019,
abstract = {This study employs an experiment to test subjects' perceptions of an artificial intelligence (AI) crime-predicting agent that produces clearly racist predictions. It used a 2 (human crime predictor/AI crime predictor) x 2 (high/low seriousness of crime) design to test the relationship between the level of autonomy and responsibility for the unjust results. The seriousness of crime was manipulated to examine the relationship between the perceived threat and trust in the authority's decisions. Participants (N = 334) responded to an online questionnaire after reading one of four scenarios with the same story depicting a crime predictor unjustly reporting a higher likelihood of subsequent crimes for a black defendant than for a white defendant for similar crimes. The results indicate that people think that an AI crime predictor has significantly less autonomy than a human crime predictor. However, both the identity of the crime predictor and the seriousness of the crime showed insignificant results on the level of responsibility assigned to the predictor. Also, a clear positive relationship between autonomy and responsibility was found in both human and AI crime predictor scenarios. The implications of the findings for applications and theory are discussed.},
author = {Hong, Joo Wha and Williams, Dmitri},
doi = {10.1016/j.chb.2019.06.012},
file = {::},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Artificial intelligence,Attribution theory,CASA,Human-AI Communication,Predictive policing,Racism},
number = {February},
pages = {79--84},
publisher = {Elsevier},
title = {{Racism, responsibility and autonomy in HCI: Testing perceptions of an AI agent}},
url = {https://doi.org/10.1016/j.chb.2019.06.012},
volume = {100},
year = {2019}
}
@article{Shah2018,
author = {Shah, Vinit and von Weltin, Eva and Lopez, Silvia and McHugh, James Riley and Veloso, Lillian and Golmohammadi, Meysam and Obeid, Iyad and Picone, Joseph},
doi = {10.3389/fninf.2018.00083},
file = {::},
issn = {16625196},
journal = {Frontiers in Neuroinformatics},
keywords = {Annotated data,Automatic seizure detection,EEG,Electroencephalogram,Machine learning,Seizure detection,Temporal-Spatial sequence data},
month = {nov},
publisher = {Frontiers Media S.A.},
title = {{The temple university hospital seizure detection corpus}},
volume = {12},
year = {2018}
}
@inproceedings{Gousios2013,
abstract = {During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it. {\textcopyright} 2013 IEEE.},
author = {Gousios, Georgios},
booktitle = {2013 10th Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2013.6624034},
file = {::},
isbn = {978-1-4673-2936-1},
issn = {21601852},
keywords = {Dataset,GitHub,Repository},
month = {may},
pages = {233--236},
publisher = {IEEE},
title = {{The GHTorent dataset and tool suite}},
url = {http://ieeexplore.ieee.org/document/6624034/},
year = {2013}
}
@article{Wasserstein2019,
author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
doi = {10.1080/00031305.2019.1583913},
file = {::},
issn = {15372731},
journal = {American Statistician},
number = {sup1},
pages = {1--19},
title = {{Moving to a World Beyond “p {\textless}0.05”}},
volume = {73},
year = {2019}
}
@article{Stevens1946,
abstract = {2d ed. rev. and enl. "Unabridged republication of the second revised and enlarged edition of 1894"--Title page verso.},
author = {Stevens, S. S.},
doi = {10.1126/science.103.2684.677},
file = {::},
issn = {0036-8075},
journal = {Science},
month = {jun},
number = {2684},
pages = {677--680},
title = {{On the Theory of Scales of Measurement}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0167715289900989 https://www.sciencemag.org/lookup/doi/10.1126/science.103.2684.677},
volume = {103},
year = {1946}
}
@article{Chen2018b,
abstract = {Background. At present, there is no consolidated evidence for workplace-based interventions for the prevention and reduction of neck pain in office workers. Purpose. The purpose of this review was to investigate the effectiveness of workplace- based interventions for neck pain in office workers. Data Sources. MEDLINE, PEDro, CINAHL, and CENTRAL were searched for trials published since inception and before May 31, 2016. Study Selection. Randomized controlled trials (RCTs) were considered when they met the following criteria: population consisted of office workers, intervention(s) was performed at the workplace, outcome measures included neck and/or neck/shoulder pain intensity and incidence/prevalence, and comparator groups included no/other intervention. Data Extraction. Data were extracted by 1 reviewer using predefined data fields and checked by a second reviewer. Risk of bias was assessed by 2 independent reviewers using the 2015 Cochrane Back and Neck Group guidelines. Evidence quality was evaluated using the Grading of Recommendations Assessment, Development, and Evaluation system. Data Synthesis. Twenty-seven RCTs were included. There was moderate-quality evidence that neck/shoulder strengthening exercises and general fitness training were effective in reducing neck pain in office workers who were symptomatic, although the effect size was larger for strengthening exercises. Greater effects were observed with greater participation in exercise. Ergonomic interventions were supported by low-quality evidence. Limitations. Data could not be obtained from some studies for meta-analysis and assessment of risk of bias. Reporting bias might have been present because only studies in the English language were included. Conclusions. Workplace-based strengthening exercises were effective in reducing neck pain in office workers who were symptomatic, and the effect size was larger when the exercises were targeted to the neck/shoulder. Future RCTs of ergonomic interventions targeted at office workers who are symptomatic are required. More research on neck pain prevention is warranted.},
author = {Chen, Xiaoqi and Coombes, Brooke K and Sj{\o}gaard, Gisela and Jun, Deokhoon and O'Leary, Shaun and Johnston, Venerina},
doi = {10.1093/ptj/pzx101},
file = {::},
issn = {0031-9023},
journal = {Physical Therapy},
month = {jan},
number = {1},
pages = {40--62},
publisher = {Oxford University Press},
title = {{Workplace-Based Interventions for Neck Pain in Office Workers: Systematic Review and Meta-Analysis}},
url = {http://academic.oup.com/ptj/article/98/1/40/4562646},
volume = {98},
year = {2018}
}
@article{Wang2018c,
abstract = {The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models – in short model management – is a critical task in virtually all production ML use
cases. Wrong model management decisions can lead to poor performance of a ML system and can result
in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving,
there is a lack of understanding of challenges and best practices for ML model management. Therefore,
this field is receiving increased attention in recent years, both from the data management as well as
from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview
over conceptual, engineering, and data-processing related challenges arising in the management of the
corresponding ML models, and point out future research directions.},
annote = {From Duplicate 1 (On Challenges in Machine Learning Model Management - Wang, Haixun; Gonzalez, Joseph; Li, Guoliang; Meliou, Alexandra)

ML Lifecycle is model development, training (training pipelines, trained models and live data), inference (prediction service and logic) and feedback to training, query and prediction from end user application

Model development stages are offline training data, data collection, cleaning and visualisation, feature engineering and model design and training and validation

Each stage (neural networks design, data managment, cluster management) demands different skills set.

Pervasive problems in ML are feature management, data provenance, pipeline reproducibility, low-latency serving, prediction monitoring
Facebook - FB Learner flow, Uber - Michelangelo, Google - TFX
Classical time series forecasting technique - ARIMA
Content moderation - offline training of models using manually labeled data
Automating model metadata tracking 
1. Managing the meta data - Who created the model at what time ? Which hyperparameters were used ? What feature transformations have been applied ?
2. Lineage - Which dataset was the model derived from ?
Which dataset was used for computing the evalution data ?

Model management challenges are conceptual challenges, data management challenges and engineering challenges

Conceptual challenges are ML model definition (To define the actual model to manage), model validation (To backtest the accuray of models, improvement might cause longer prediction times), decisions on model retraining, adversarial settings (understand the boundary conditions of the classifier)

Data management challenges are lack of declarative abstraction for the whole ML pipeline, querying model meta data

Engineering challenges are multi language code bases, heterogenous skill level of users, backward compatibility of trained models

Provenance - place of origin, prevasive - unwelcoming
ProvDB - a unified provenance, metadata management system, flexible and extensible ingestion mechanism, novel querying

Data Engineers, Data Scientists, Domain Experts

Evolution of Jupyter Notebook - targeting Data Scientists 
A holistic approach of conducting, auditing and continuously monitoring

ProvDB - developed under the umbrella of Datahub project, 'Schema later' approach
Caffe - a deep learning framework
Metadata - about accuracy and loss metrics for learned models
Cypher and Gremlin are query languages 
A version, identified by id is immutable in ProvDB
ProvDB two subsets - graph segmentation, graph summarization

Version -{\textgreater} artifact -{\textgreater} snapshot -{\textgreater} record
Artifacts - scripts, datasets, dervied results
Entities - Project artefacts (files, datasets, scripts)
Activities - System/user actions (train, git commit, cron jobs)
Agents - parties who are responsible for some activity (team member, system component)

Storing, querying and analyzing provenance graphs - versioned artifacts, evolving workflows, partial knowledge in collaboration


Raw Data -{\textgreater} Data Prep -{\textgreater} Training -{\textgreater} Deployment - cyclic steps of ML

ML Flow facilitates local execution and remote cluster deployment
ML Flow - Open source ML Platform, Works with any ML Library (Keras, Tensorflow, PyTorch), Programming Language, deploy tool

Three key components - MLflow Tracking (Experiment tracking), MLflow Projects(reproducible runs), MLflow models (model packaging)

MLflow tracking API supported in Azure Machine Learning
Leverage throughout all products, logging real time
Facebook(FBLearner), Uber(Michaelangelo), Google(Tensorflow)

ML application challenges - data versions, codes, tuning parameters

MLflow covers three key challenges - experimentation, reproducibility, model deployment
ML optimizes specific metric (eg - prediction accuracy)

From Duplicate 2 (On Challenges in Machine Learning Model Management - Wang, Haixun; Gonzalez, Joseph; Li, Guoliang; Meliou, Alexandra)

ML Lifecycle is model development, training (training pipelines, trained models and live data), inference (prediction service and logic) and feedback to training, query and prediction from end user application

Model development stages are offline training data, data collection, cleaning and visualisation, feature engineering and model design and training and validation

Each stage (neural networks design, data managment, cluster management) demands different skills set.

Pervasive problems in ML are feature management, data provenance, pipeline reproducibility, low-latency serving, prediction monitoring
Facebook - FB Learner flow, Uber - Michelangelo, Google - TFX
Classical time series forecasting technique - ARIMA
Content moderation - offline training of models using manually labeled data
Automating model metadata tracking 
1. Managing the meta data - Who created the model at what time ? Which hyperparameters were used ? What feature transformations have been applied ?
2. Lineage - Which dataset was the model derived from ?
Which dataset was used for computing the evalution data ?

Model management challenges are conceptual challenges, data management challenges and engineering challenges

Conceptual challenges are ML model definition (To define the actual model to manage), model validation (To backtest the accuray of models, improvement might cause longer prediction times), decisions on model retraining, adversarial settings (understand the boundary conditions of the classifier)

Data management challenges are lack of declarative abstraction for the whole ML pipeline, querying model meta data

Engineering challenges are multi language code bases, heterogenous skill level of users, backward compatibility of trained models

Provenance - place of origin, prevasive - unwelcoming
ProvDB - a unified provenance, metadata management system, flexible and extensible ingestion mechanism, novel querying

Data Engineers, Data Scientists, Domain Experts

Evolution of Jupyter Notebook - targeting Data Scientists 
A holistic approach of conducting, auditing and continuously monitoring

ProvDB - developed under the umbrella of Datahub project, 'Schema later' approach
Caffe - a deep learning framework
Metadata - about accuracy and loss metrics for learned models
Cypher and Gremlin are query languages 
A version, identified by id is immutable in ProvDB
ProvDB two subsets - graph segmentation, graph summarization

Version -{\textgreater}artifact -{\textgreater}snapshot -{\textgreater}record
Artifacts - scripts, datasets, dervied results
Entities - Project artefacts (files, datasets, scripts)
Activities - System/user actions (train, git commit, cron jobs)
Agents - parties who are responsible for some activity (team member, system component)

Storing, querying and analyzing provenance graphs - versioned artifacts, evolving workflows, partial knowledge in collaboration


Raw Data -{\textgreater}Data Prep -{\textgreater}Training -{\textgreater}Deployment - cyclic steps of ML

ML Flow facilitates local execution and remote cluster deployment
ML Flow - Open source ML Platform, Works with any ML Library (Keras, Tensorflow, PyTorch), Programming Language, deploy tool

Three key components - MLflow Tracking (Experiment tracking), MLflow Projects(reproducible runs), MLflow models (model packaging)

MLflow tracking API supported in Azure Machine Learning
Leverage throughout all products, logging real time
Facebook(FBLearner), Uber(Michaelangelo), Google(Tensorflow)

ML application challenges - data versions, codes, tuning parameters

MLflow covers three key challenges - experimentation, reproducibility, model deployment
ML optimizes specific metric (eg - prediction accuracy)},
author = {Wang, Haixun and Gonzalez, Joseph and Li, Guoliang and Meliou, Alexandra},
file = {::},
number = {4},
pages = {5--13},
title = {{On Challenges in Machine Learning Model Management}},
url = {http://tab.computer.org/tcde/bull{\_}about.html.},
volume = {41},
year = {2018}
}
@inproceedings{Shamim2016,
abstract = {{\textcopyright} 2016 IEEE. Epileptic seizures are recurring brief episodes of abnormal excessive or synchronous neuronal activity in the brain, and are often accompanied by changes in various autonomic functions like heart rate (HR). A better approach for detecting epileptic seizures is by using electrocardiogram (ECG) signals because ECG acquisition is relatively easier as compared to EEG. In this paper a new technique is proposed for detection of seizures in epileptic patients using the electrocardiogram (ECG) signal. Feature sets for analysis of HRV (heart rate variability) comprises of parameters from multiple domains. For temporal analysis activity, mobility and complexity features are identified and for spectral analysis mean of absolute deviation of Fast Fourier Transform coefficients and spectral entropy are identified for seizure detection. These features are classified by using two different approaches i.e. by setting threshold and by using linear support vector machine where average latency by threshold approach was found to be better than linear SVM. The performance parameters for the proposed technique using threshold approach for classification are accuracy (94.2{\%}), sensitivity (84.1{\%}) and specificity (94.5{\%}) which shows that the proposed algorithm detects epileptic seizures efficiently. Comparison of performance of this model was done with those proposed earlier using ECG signal and this model was found to be better.},
author = {Shamim, Gulezar and Khan, Yusuf Uzzaman and Sarfraz, Mohammad and Farooq, Omar},
booktitle = {2016 International Conference on Signal Processing and Communication, ICSC 2016},
doi = {10.1109/ICSPCom.2016.7980585},
isbn = {9781509026845},
keywords = {Classification,Feature extraction,Heart rate variability,Performance evaluation,Seizure},
pages = {250--254},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Epileptic seizure detection using heart rate variability}},
year = {2016}
}
@inproceedings{DeLucia2012,
abstract = {Information Retrieval (IR) techniques have been used for various software engineering tasks, including the labeling of software artifacts by extracting "keywords" from them. Such techniques include Vector Space Models, Latent Semantic Indexing, Latent Dirichlet Allocation, as well as customized heuristics extracting words from specific source code elements. This paper investigates how source code artifact labeling performed by IR techniques would overlap (and differ) from labeling performed by humans. This has been done by asking a group of subjects to label 20 classes from two Java software systems, JHotDraw and eXVantage. Results indicate that, in most cases, automatic labeling would be more similar to human-based labeling if using simpler techniques - e.g., using words from class and method names - that better reflect how humans behave. Instead, clustering-based approaches (LSI and LDA) are much more worthwhile to be used on source code artifacts having a high verbosity, as well as for artifacts requiring more effort to be manually labeled. {\textcopyright}2012 IEEE.},
author = {{De Lucia}, Andrea and {Di Penta}, Massimiliano and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
booktitle = {IEEE International Conference on Program Comprehension},
doi = {10.1109/ICPC.2012.6240488},
file = {::},
isbn = {9781467312165},
keywords = {Empirical Studies,Information Retrieval,Latent Dirichlet Allocation,Topic Extraction},
pages = {193--202},
publisher = {IEEE},
title = {{Using IR methods for labeling source code artifacts: Is it worthwhile?}},
year = {2012}
}
@book{spolsky2008more,
author = {Spolsky, Avram Joel},
publisher = {Apress},
title = {{More Joel on software: further thoughts on diverse and occasionally related matters that will prove of interest to software developers, designers, and managers, and to those who, whether by good fortune or ill luck, work with them in some capacity}},
year = {2008}
}
@article{Song2004,
abstract = {In this paper we consider a stochastic-demand periodic-review inventory model with sudden obsolescence. We characterize the structure of the optimal policy and propose a dynamic programming algorithm for computing its parameters. We then utilize this algorithm to approximate the solution to the continuous-review sudden obsolescence problem with general obsolescence distribution. We prove convergence of our approximation scheme, and demonstrate it numerically against known closed-form solutions of special cases.},
author = {Song, Yuyue and Lau, Hoong Chuin},
doi = {10.1016/S0377-2217(03)00399-0},
file = {::},
issn = {0377-2217},
journal = {European Journal of Operational Research},
month = {nov},
number = {1},
pages = {110--120},
publisher = {North-Holland},
title = {{A periodic-review inventory model with application to the continuous-review obsolescence problem}},
url = {https://www.sciencedirect.com/science/article/pii/S0377221703003990?via{\%}3Dihub},
volume = {159},
year = {2004}
}
@article{VandeVel2016,
abstract = {Purpose Detection of, and alarming for epileptic seizures is increasingly demanded and researched. Our previous review article provided an overview of non-invasive, non-EEG (electro-encephalography) body signals that can be measured, along with corresponding methods, state of the art research, and commercially available systems. Three years later, many more studies and devices have emerged. Moreover, the boom of smart phones and tablets created a new market for seizure detection applications. Method We performed a thorough literature review and had contact with manufacturers of commercially available devices. Results This review article gives an updated overview of body signals and methods for seizure detection, international research and (commercially) available systems and applications. Reported results of non-EEG based detection devices vary between 2.2{\%} and 100{\%} sensitivity and between 0 and 3.23 false detections per hour compared to the gold standard video-EEG, for seizures ranging from generalized to convulsive or non-convulsive focal seizures with or without loss of consciousness. It is particularly interesting to include monitoring of autonomic dysfunction, as this may be an important pathophysiological mechanism of SUDEP (sudden unexpected death in epilepsy), and of movement, as many seizures have a motor component. Conclusion Comparison of research results is difficult as studies focus on different seizure types, timing (night versus day) and patients (adult versus pediatric patients). Nevertheless, we are convinced that the most effective seizure detection systems are multimodal, combining for example detection methods for movement and heart rate, and that devices should especially take into account the user's seizure types and personal preferences.},
author = {{Van de Vel}, Anouk and Cuppens, Kris and Bonroy, Bert and Milosevic, Milica and Jansen, Katrien and {Van Huffel}, Sabine and Vanrumste, Bart and Cras, Patrick and Lagae, Lieven and Ceulemans, Berten},
doi = {10.1016/j.seizure.2016.07.012},
file = {::},
issn = {15322688},
journal = {Seizure},
keywords = {Alarm system,Epilepsy,Non-EEG based seizure detection,SUDEP,Sudden unexpected death},
pages = {141--153},
publisher = {BEA Trading Ltd},
title = {{Non-EEG seizure detection systems and potential SUDEP prevention: State of the art: Review and update}},
url = {http://dx.doi.org/10.1016/j.seizure.2016.07.012},
volume = {41},
year = {2016}
}
@article{LeRoy2016,
abstract = {Innovation strategies are increasingly inter-organizational, and yet firms may find it difficult to choose the appropriate type of cooperation: with competitors (coopetition) or with non-competitors (suppliers, customers, universities, etc.). Coopetition is frequently considered to be a riskier venture, which may lead to the conclusion that this strategy is not the most appropriate for increasing a firms innovation. The literature on the topic is inconclusive so we try to clarify this issue by introducing a new dimension to the nexus of inter-organizational cooperation and innovation, namely, the geographical location of the cooperation partner. We analyze cooperation strategies with different types of partners and we test our hypotheses on 3,933 firms sampled from the French CIS 04 database. We find that the choice of cooperation strategy depends on the type of cooperation partner (non-rival or competitor), on the type of innovation (radical or incremental); and on the geographical location of the competitor.},
author = {{Le Roy}, Fr{\'{e}}d{\'{e}}ric and Robert, Marc and Lasch, Frank},
doi = {10.1080/00208825.2016.1112148},
file = {::},
issn = {0020-8825},
journal = {International Studies of Management {\&} Organization},
keywords = {Cooperation,coopetition,incremental innovation,radical innovation},
month = {apr},
number = {2-3},
pages = {136--158},
publisher = {Routledge},
title = {{Choosing the Best Partner for Product Innovation}},
url = {http://www.tandfonline.com/doi/full/10.1080/00208825.2016.1112148},
volume = {46},
year = {2016}
}
@article{Patnaik2008,
abstract = {Electroencephalogram (EEG) has established itself as an important means of identifying and analyzing epileptic seizure activity in humans. In most cases, identification of the epileptic EEG signal is done manually by skilled professionals, who are small in number. In this paper, we try to automate the detection process. We use wavelet transform for feature extraction and obtain statistical parameters from the decomposed wavelet co-efficients. A feed-forward backpropagating artificial neural network (ANN) is used for the classification. We use genetic algorithm for choosing the training set and also implement a post-classification stage using harmonic weights to increase the accuracy. Average specificity of 99.19{\%}, sensitivity of 91.29{\%} and selectivity of 91.14{\%} are obtained. {\textcopyright}2008 Elsevier Ireland Ltd. All rights reserved.},
author = {Patnaik, L. M. and Manyam, Ohil K.},
doi = {10.1016/j.cmpb.2008.02.005},
file = {::},
issn = {01692607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Artificial neural network (ANN),Discrete wavelet transform (DWT),Electroencephalogram (EEG),Genetic algorithm,Resilient backpropagation},
month = {aug},
number = {2},
pages = {100--109},
title = {{Epileptic EEG detection using neural networks and post-classification}},
volume = {91},
year = {2008}
}
@article{Beurskens2000,
abstract = {Objectives - To evaluate the validity of the checklist individual strength questionnaire (CIS) in the working population. This 20 item self reported questionnaire has often been used in patients with chronic fatigue. To date, no research has focused on the validity of the CIS in occupational groups. Methods - To evaluate the discriminant validity the CIS was filled out by five groups of employees with expected differences in fatigue. The convergent validity was evaluated by comparing the results of the CIS with the results of three related measures: measured unidimensional fatigue, burnout, and need for recovery. Results - The CIS was able to discriminate between fatigued and non-fatigued employees in occupational groups. The expected agreement between the results of the CIS and related measures was confirmed. Conclusions - The CIS seems to be an appropriate instrument for measuring fatigue in the working population.},
author = {Beurskens, Anna J.H.M. and B{\"{u}}ltmann, Ute and Kant, IJmert and Vercoulen, Jan H.M.M. and Bleijenberg, Gijs and Swaen, Gerard M.H.},
doi = {10.1136/oem.57.5.353},
file = {::},
issn = {13510711},
journal = {Occupational and Environmental Medicine},
keywords = {Fatigue,Measurement,Occupational groups},
month = {may},
number = {5},
pages = {353--357},
pmid = {10769302},
publisher = {BMJ Publishing Group Ltd},
title = {{Fatigue among working people: Validity of a questionnaire measure}},
volume = {57},
year = {2000}
}
@article{Rodrigues2017,
abstract = {BACKGROUND: Some studies have suggested a causal relationship between computerwork and the development of musculoskeletal disorders. However, studies considering the use of specific tools to assess workplace ergonomics and psychosocial factors in computer office workers with and without reported musculoskeletal pain are scarce. OBJECTIVE: The aim of this study was to compare the ergonomic, physical, and psychosocial factors in computer office workers with and without reported musculoskeletal pain (MSP). METHODS:Thirty-five computer officeworkers (aged 18 - 55 years) participated in the study. The following evaluations were completed: Rapid Upper Limb Assessment (RULA), Rapid Office Strain Assessment (ROSA), and Maastricht Upper Extremity Questionnaire revised Brazilian Portuguese version (MUEQ-Br revised). Student t-tests were used to make comparisons between groups. RESULTS: The computer office workers were divided into two groups: workers with reported MSP (WMSP, n = 17) and workers without positive report (WOMSP, n = 18). Those in the WMSP group showed significantly greater mean values in the total ROSA score (WMSP: 6.71 [CI 95{\%}:6.20 - 7.21] and WOMSP: 5.88 [CI 95{\%}:5.37 - 6.39], p = 0.01). The WMSP group also showed higher scores in the chair section of the ROSA, workstation of MUEQ-Br revised, and in the upper limb RULA score. The chair height and armrest sections from ROSA showed the higher mean values in workers WMSP compared to workersWOMSP. A positive moderate correlation was observed between ROSA and RULA total scores (R = 0.63, p {\textless} 0.001). CONCLUSION: Our results demonstrated that computer office workers who reported MSP had worse ergonomics indexes for chair workstation and worse physical risk related to upper limb (RULA upper limb section) than workers without pain. However, there were no observed differences in workers with and without MSP regarding work-related psychosocial factors. The results suggest that inadequate workstation conditions, specifically the chair height, arm and back rest, are linked to improper upper limb postures and that these factors are contributing to MSP in computer office workers.},
author = {Rodrigues, Mirela Sant Ana and Leite, Raquel Descie Veraldi and Lelis, Cheila Maira and Chaves, Tha{\'{i}}s Cristina},
doi = {10.3233/WOR-172582},
file = {::},
issn = {10519815},
journal = {Work},
keywords = {Ergonomics,checklist,questionnaire},
month = {jan},
number = {4},
pages = {563--572},
publisher = {IOS Press},
title = {{Differences in ergonomic and workstation factors between computer office workers with and without reported musculoskeletal pain}},
volume = {57},
year = {2017}
}
@phdthesis{Horbach2019,
author = {Horbach, Serge P.J.M. J M},
file = {::},
school = {Radboud University},
title = {{To Spill, Filter and Clean}},
url = {https://www.nrin.nl/ri-collection/library/tospillfilterandclean/},
year = {2019}
}
@book{martin2009clean,
author = {Martin, Robert C},
publisher = {Pearson Education},
title = {{Clean code: a handbook of agile software craftsmanship}},
year = {2009}
}
@article{Yamashita2018,
abstract = {Previous research has shown that tracking technologies have the potential to help family caregivers optimize their coping strategies and improve their relationships with care recipients. In this paper, we explore how sharing the tracked data (i.e., caregiving journals and patient's conditions) with other family caregivers affects home care and family communication. Although previous works suggested that family caregivers may benefit from reading the records of others, sharing patients' private information might fuel negative feelings of surveillance and violation of trust for care recipients. To address this research question, we added a sharing feature to the previously developed tracking tool and deployed it for six weeks in the homes of 15 family caregivers who were caring for a depressed family member. Our findings show how the sharing feature attracted the attention of care recipients and helped the family caregivers discuss sensitive issues with care recipients.},
author = {Yamashita, Naomi and Kuzuoka, Hideaki and Kudo, Takashi and Hirata, Keiji and Aramaki, Eiji and Hattori, Kazuki},
doi = {10.1145/3173574.3173796},
file = {::},
isbn = {9781450356206},
keywords = {Author's kit,Conference Publications,Guides,instructions},
title = {{How Information Sharing about Care Recipients by Family Caregivers Impacts Family Communication}},
url = {https://doi.org/10.1145/3173574.3173796},
year = {2018}
}
@article{Chandola2009,
abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
doi = {10.1145/1541880.1541882},
file = {::},
issn = {1557-7341},
journal = {ACM Computing Surveys},
keywords = {Anomaly detection,Outlier detection},
number = {3},
pages = {1--72},
title = {{Anomaly detection: A survey}},
volume = {41},
year = {2009}
}
@article{Khomh2018,
annote = {AI is not intended to achieve 100{\%} accuracy as humans aren't 100{\%} accurate in their tasks. This creates the question what is the correct measure.

Also mentions the idea of putting AI into context. 

Sociological studyies are needed to determine how AI changes human behaviour.},
author = {Khomh, Foutse and Adams, Bram and Cheng, Jinghui and Fokaefs, Marios and Antoniol, Giuliano},
doi = {10.1109/MS.2018.3571224},
file = {::},
issn = {0740-7459},
journal = {IEEE Software},
number = {5},
pages = {81--84},
publisher = {IEEE},
title = {{Software Engineering for Machine-Learning Applications: The Road Ahead}},
url = {https://ieeexplore.ieee.org/document/8474484/},
volume = {35},
year = {2018}
}
@article{Perloff2017,
abstract = {Introduction: A core challenge of a multidisciplinary and multi-organizational translational research enterprise such as a Clinical and Translational Research Award (CTSA) is coordinating and integrating the work of individuals, workgroups, and organizations accustomed to working independently and autonomously. Tufts Clinical and Translational Science Institute (CTSI) undertook and studied a multifacted intervention to address this challenge and to create a culture of systems thinking, process awareness, responsive to others' needs, and shared decision-making. Intervention: The intervention, based on relational coordination, included 1) relational interventions, in three staff retreats and a diagnostic survey to provide feedback on the current quality of relational coordination, and 2) structural interventions, in the launching of five new cross-functional teams with regular meeting structures. Methods: A mixed-methods evaluation yielded quantitative data via two types of team surveys and qualitative data via interviews and meeting observations. Results: The findings suggest that interventions to improve relational coordination are feasible for CTSAs, including good fidelity to the model and staff/physician engagement. Survey and interview data suggest model improvements in coordination and alignment. Further research about their optimal design is warranted.},
author = {Perloff, Jennifer and Rushforth, Alice and Welch, Lisa C. and Daudelin, Denise and Suchman, Anthony L. and {Hoffer Gittell}, Jody and Santos, Hannah and Beswick, Joanne and Moore, Saleema and Selker, Harry P.},
doi = {10.1017/cts.2017.10},
issn = {2059-8661},
journal = {Journal of Clinical and Translational Science},
keywords = {Relational coordination,collaboration,culture,formative evaluation},
month = {aug},
number = {4},
pages = {218--225},
publisher = {Cambridge University Press (CUP)},
title = {{Intervening to enhance collaboration in translational research: A relational coordination approach}},
url = {https://doi.org/10.1017/cts.2017.10},
volume = {1},
year = {2017}
}
@article{Koren2018,
abstract = {Background: Ongoing or recurrent seizure activity without prominent motor features is a common burden in neurological critical care patients and people with epilepsy during ICU stays. Continuous EEG (CEEG) is the gold standard for detecting ongoing ictal EEG patterns and monitoring functional brain activity. However CEEG review is very demanding and time consuming. The purpose of the present multirater, EEG expert reviewer study, is to test and assess the clinical feasibility of an automatic EEG pattern detection method (Neurotrend). Methods: Four board certified EEG reviewers used Neurotrend to annotate 76 CEEG datasets {\`{a}} 6 h (in total 456 h of EEG) for rhythmic and periodic EEG patterns (RPP), unequivocal ictal EEG patterns and burst suppression. All reviewers had a predefined time limit of 5 min (± 2 min) per CEEG dataset and were compared to a predefined gold standard (conventional EEG review with unlimited time). Subanalysis of specific features of RPP was conducted as well. We used Gwet's AC1 and AC2 coefficients to calculate interrater agreement (IRA) and multirater agreement (MRA). Also, we determined individual performance measures for unequivocal ictal EEG patterns and burst suppression. Bonferroni-Holmes correction for multiple testing was applied to all statistical tests. Results: Mean review time was 3.3 min (± 1.9 min) per CEEG dataset. We found substantial IRA for unequivocal ictal EEG patterns (0.61-0.79; mean sensitivity 86.8{\%}; mean specificity 82.2{\%}, p {\textless} 0.001) and burst suppression (0.68-0.71; mean sensitivity 96.7{\%}; mean specificity 76.9{\%} p {\textless} 0.001). Two reviewers showed substantial IRA for RPP (0.68-0.72), whereas the other two showed moderate agreement (0.45-0.54), compared to the gold standard (p {\textless} 0.001). MRA showed almost perfect agreement for burst suppression (0.86) and moderate agreement for RPP (0.54) and unequivocal ictal EEG patterns (0.57). Conclusions: We demonstrated the clinical feasibility of an automatic critical care EEG pattern detection method on two levels: (1) reasonable high agreement compared to the gold standard, (2) reasonable short review times compared to previously reported EEG review times with conventional EEG analysis.},
author = {Koren, Johannes P. and Herta, Johannes and F{\"{u}}rbass, Franz and Pirker, Susanne and Reiner-Deitemyer, Veronika and Riederer, Franz and Flechsenhar, Julia and Hartmann, Manfred and Kluge, Tilmann and Baumgartner, Christoph},
doi = {10.3389/fneur.2018.00454},
file = {::},
issn = {16642295},
journal = {Frontiers in Neurology},
keywords = {Continuous EEG,Intensive care unit,Neurotrend,Non-convulsive seizures,Standardized critical care EEG terminology,Status epilepticus},
month = {jun},
number = {JUN},
publisher = {Frontiers Media S.A.},
title = {{Automated long-term EEG review: Fast and precise analysis in critical care patients}},
volume = {9},
year = {2018}
}
@misc{Pylint,
author = {Pylint},
title = {{Pylint - code analysis for Python | www.pylint.org}},
url = {https://www.pylint.org/},
urldate = {2019-12-02}
}
@article{Fernandes2019,
annote = {Introduces an ensemble approach that is adaptive

Use mAUC and G-mean

Journal, 20 datasets

References for G-mean and MAUC, a multiclass version of the AUC

Back up all decisions from other papers, don't make up anything at all!!!},
author = {Fernandes, Everlandio and {De Carvalho}, Andre Carlos Ponce de Leon Ferreira and Yao, Xin},
doi = {10.1109/TKDE.2019.2898861},
file = {::},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
number = {8},
pages = {1},
title = {{Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Data}},
url = {https://ieeexplore.ieee.org/document/8640265/},
volume = {14},
year = {2019}
}
@inproceedings{Yuan2018,
abstract = {Epileptic seizure detection using multi-channel scalp electroencephalogram (EEG) signals has gained increasing attention in clinical therapy. Recently, researchers attempt to employ deep learning techniques with channel selection to determine critical channels. However, existing models with such hard selection procedure do not take dynamic constraints into account, since the irrelevant channels vary significantly across different situations. To address these issues, we propose ChannelAtt, an end-to-end multi-view deep learning model with channel-aware attention mechanism, to express multi-channel EEG signals in a high-level space with interpretable meanings. ChannelAtt jointly learns both multi-view representation and its contribution scores. We propose two attention mechanisms to learn the attentional representations of multi-channel EEG signals in time-frequency domain. Experimental results show that the proposed ChannelAtt model outperforms the baselines in detecting epileptic seizures. Analytical results of a case study demonstrate that the learned attentional representations are meaningful.},
author = {Yuan, Ye and Xun, Guangxu and Ma, Fenglong and Suo, Qiuling and Xue, Hongfei and Jia, Kebin and Zhang, Aidong},
booktitle = {2018 IEEE EMBS International Conference on Biomedical and Health Informatics, BHI 2018},
doi = {10.1109/BHI.2018.8333405},
file = {::},
isbn = {9781538624050},
month = {apr},
pages = {206--209},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A novel channel-aware attention framework for multi-channel EEG seizure detection via multi-view deep learning}},
volume = {2018-Janua},
year = {2018}
}
@article{Lombrozo2006,
abstract = {Teleological explanations (TEs) account for the existence or properties of an entity in terms of a function: we have hearts because they pump blood, and telephones for communication. While many teleological explanations seem appropriate, others are clearly not warranted-for example, that rain exists for plants to grow. Five experiments explore the theoretical commitments that underlie teleological explanations. With the analysis of [Wright, L. (1976). Teleological Explanations. Berkeley, CA: University of California Press] from philosophy as a point of departure, we examine in Experiment 1 whether teleological explanations are interpreted causally, and confirm that TEs are only accepted when the function invoked in the explanation played a causal role in bringing about what is being explained. However, we also find that playing a causal role is not sufficient for all participants to accept TEs. Experiment 2 shows that this is not because participants fail to appreciate the causal structure of the scenarios used as stimuli. In Experiments 3-5 we show that the additional requirement for TE acceptance is that the process by which the function played a causal role must be general in the sense of conforming to a predictable pattern. These findings motivate a proposal, Explanation for Export, which suggests that a psychological function of explanation is to highlight information likely to subserve future prediction and intervention. We relate our proposal to normative accounts of explanation from philosophy of science, as well as to claims from psychology and artificial intelligence.},
author = {Lombrozo, Tania and Carey, Susan},
doi = {10.1016/j.cognition.2004.12.009},
file = {::},
journal = {Cognition},
keywords = {Causal reasoning,Explanation,Function,Philosophy of science,Teleology},
pages = {167--204},
title = {{Functional explanation and the function of explanation}},
url = {www.elsevier.com/locate/COGNIT},
volume = {99},
year = {2006}
}
@article{Zhao2018,
author = {Zhao, Xiuhe and Lhatoo, Samden D.},
doi = {10.1007/s11910-018-0849-z},
file = {::},
issn = {1528-4042},
journal = {Current Neurology and Neuroscience Reports},
month = {jul},
number = {7},
pages = {40},
publisher = {Springer US},
title = {{Seizure detection: do current devices work? And when can they be useful?}},
url = {http://link.springer.com/10.1007/s11910-018-0849-z},
volume = {18},
year = {2018}
}
@phdthesis{Barnett2017,
author = {Barnett, Scott},
file = {::},
school = {Swinburne University of Technology},
title = {{Extracting Technical Domain Knowledge to Improve Software Architecture}},
year = {2017}
}
@article{Hadgraft2016,
abstract = {Background: Office workers spend a large proportion of their working hours sitting. This may contribute to an increased risk of chronic disease and premature mortality. While there is growing interest in workplace interventions targeting prolonged sitting, few qualitative studies have explored workers' perceptions of reducing occupational sitting outside of an intervention context. This study explored barriers to reducing office workplace sitting, and the feasibility and acceptability of strategies targeting prolonged sitting in this context. Methods: Semi-structured interviews were conducted with a convenience sample of 20 office workers (50 {\%} women), including employees and managers, in Melbourne, Australia. The three organisations (two large, and one small organisation) were from retail, health and IT industries and had not implemented any formalised approaches to sitting reduction. Questions covered barriers to reducing sitting, the feasibility of potential strategies aimed at reducing sitting, and perceived effects on productivity. Interviews were audiotaped and transcribed verbatim. Data were analysed using thematic analysis. Results: Participants reported spending most (median: 7.2 h) of their working hours sitting. The nature of computer-based work and exposure to furniture designed for a seated posture were considered to be the main factors influencing sitting time. Low cost strategies, such as standing meetings and in-person communication, were identified as feasible ways to reduce sitting time and were also perceived to have potential productivity benefits. However, social norms around appropriate workplace behaviour and workload pressures were perceived to be barriers to uptake of these strategies. The cost implications of height-adjustable workstations influenced perceptions of feasibility. Managers noted the need for an evidence-based business case supporting action on prolonged sitting, particularly in the context of limited resources and competing workplace health priorities. Conclusions: While a number of low-cost approaches to reduce workplace sitting are perceived to be feasible and acceptable in the office workplace, factors such as work demands and the organisational social context may still act as barriers to greater uptake. Building a supportive organisational culture and raising awareness of the adverse health effects of prolonged sitting may be important for improving individual-level and organisational-level motivation for change.},
author = {Hadgraft, Nyssa T. and Brakenridge, Charlotte L. and Lamontagne, Anthony D. and Fjeldsoe, Brianna S. and Lynch, Brigid M. and Dunstan, David W. and Owen, Neville and Healy, Genevieve N. and Lawler, Sheleigh P.},
doi = {10.1186/s12889-016-3611-y},
file = {::},
issn = {14712458},
journal = {BMC Public Health},
keywords = {Occupational health,Qualitative,Sedentary behaviour,Workplace},
month = {sep},
number = {1},
pages = {933},
publisher = {BioMed Central Ltd.},
title = {{Feasibility and acceptability of reducing workplace sitting time: A qualitative study with Australian office workers}},
volume = {16},
year = {2016}
}
@article{Khalajzadeh2020,
author = {Khalajzadeh, Hourieh and Simmons, Andrew J. and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang},
doi = {10.1016/j.cola.2020.100964},
file = {::},
issn = {2590-1184},
journal = {Journal of Computer Languages},
keywords = {big data analytics,big data modeling,big data toolkits,domain-specific visual languages,end-user tools,multidisciplinary teams},
publisher = {Elsevier Ltd},
title = {{An End-to-End Model-based Approach to Support Big Data Analytics Development}},
url = {https://doi.org/10.1016/j.cola.2020.100964},
year = {2020}
}
@article{Kalid2018,
abstract = {The growing worldwide population has increased the need for technologies, computerised software algorithms and smart devices that can monitor and assist patients anytime and anywhere and thus enable them to lead independent lives. The real-time remote monitoring of patients is an important issue in telemedicine. In the provision of healthcare services, patient prioritisation poses a significant challenge because of the complex decision-making process it involves when patients are considered ‘big data'. To our knowledge, no study has highlighted the link between ‘big data' characteristics and real-time remote healthcare monitoring in the patient prioritisation process, as well as the inherent challenges involved. Thus, we present comprehensive insights into the elements of big data characteristics according to the six ‘Vs': volume, velocity, variety, veracity, value and variability. Each of these elements is presented and connected to a related part in the study of the connection between patient prioritisation and real-time remote healthcare monitoring systems. Then, we determine the weak points and recommend solutions as potential future work. This study makes the following contributions. (1) The link between big data characteristics and real-time remote healthcare monitoring in the patient prioritisation process is described. (2) The open issues and challenges for big data used in the patient prioritisation process are emphasised. (3) As a recommended solution, decision making using multiple criteria, such as vital signs and chief complaints, is utilised to prioritise the big data of patients with chronic diseases on the basis of the most urgent cases.},
author = {Kalid, Naser and Zaidan, A. A. and Zaidan, B. B. and Salman, Omar H. and Hashim, M. and Muzammil, H.},
doi = {10.1007/s10916-017-0883-4},
file = {::},
issn = {1573689X},
journal = {Journal of Medical Systems},
keywords = {Big data,Multi-criterion decision making,Patient prioritisation,Real-time remote monitoring,Telemedicine},
month = {feb},
number = {2},
pages = {1--30},
publisher = {Springer New York LLC},
title = {{Based Real Time Remote Health Monitoring Systems: A Review on Patients Prioritization and Related "Big Data" Using Body Sensors information and Communication Technology}},
url = {https://link.springer.com/article/10.1007/s10916-017-0883-4},
volume = {42},
year = {2018}
}
@inproceedings{Renzella2020,
abstract = {Introductory programming is challenging for many students, requiring them to engage with a deep approach to learning concepts in order to succeed. These challenges compound for online students who do not have direct face-to-face interactions with teaching staff. With the growing demand for online education, we need to examine approaches that assist in building supportive learning environments for these students. A growing body of work from other education disciplines indicates that audio feedback provides an opportunity for developing stronger relationships with students. Further studies recommend an integrated implementation of audio recording into the virtual learning environment. To evaluate audio feedback for use in programming education, we developed an integrated, cross-browser audio feedback feature into the open-source Doubt-fire learning management system. Doubtfire is used to support and scale a task-oriented teaching and learning system built upon the principles of constructive alignment and has been shown to help students engage with programming concepts in campus-only units. Our findings from experimental and observational activities indicate that programming tutors can use a blended approach of audio and text feedback via the learning management system to better support student learning. The richer, more nuanced feedback delivery communicates personality to students while retaining the benefits of written feedback for code-specific issues. CCS CONCEPTS • Applied computing → Learning management systems; Distance learning; E-learning; • Human-centered computing → Human computer interaction (HCI).},
address = {Seoul, Republic of Korea},
author = {Renzella, Jake and Cain, Andrew},
booktitle = {Proceedings of the 42nd International Conference on Software Engineering},
doi = {10.1145/3377814.3381712},
file = {::},
isbn = {9781450371247},
keywords = {Learning management system,audio feedback,forma-tive feedback,introductory programming,online students ACM Reference Format: Jake Renzell},
title = {{Enriching Programming Student Feedback with Audio Comments}},
year = {2020}
}
@article{Brakenridge2016,
abstract = {BACKGROUND The office workplace is a key setting in which to address excessive sitting time and inadequate physical activity. One major influence on workplace sitting is the organizational environment. However, the impact of organizational-level strategies on individual level activity change is unknown. Further, the emergence of sophisticated, consumer-targeted wearable activity trackers that facilitate real-time self-monitoring of activity, may be a useful adjunct to support organizational-level strategies, but to date have received little evaluation in this workplace setting. OBJECTIVE The aim of this study is to evaluate the feasibility, acceptability, and effectiveness of organizational-level strategies with or without an activity tracker on sitting, standing, and stepping in office workers in the short (3 months, primary aim) and long-term (12 months, secondary aim). METHODS This study is a pilot, cluster-randomized trial (with work teams as the unit of clustering) of two interventions in office workers: organizational-level support strategies (eg, visible management support, emails) or organizational-level strategies plus the use of a waist-worn activity tracker (the LUMOback) that enables self-monitoring of sitting, standing, and stepping time and enables users to set sitting and posture alerts. The key intervention message is to 'Stand Up, Sit Less, and Move More.' Intervention elements will be implemented from within the organization by the Head of Workplace Wellbeing. Participants will be recruited via email and enrolled face-to-face. Assessments will occur at baseline, 3, and 12 months. Time spent sitting, sitting in prolonged (≥30 minute) bouts, standing, and stepping during work hours and across the day will be measured with activPAL3 activity monitors (7 days, 24 hours/day protocol), with total sitting time and sitting time during work hours the primary outcomes. Web-based questionnaires, LUMOback recorded data, telephone interviews, and focus groups will measure the feasibility and acceptability of both interventions and potential predictors of behavior change. RESULTS Baseline and follow-up data collection has finished. Results are expected in 2016. CONCLUSIONS This pilot, cluster-randomized trial will evaluate the feasibility, acceptability, and effectiveness of two interventions targeting reductions in sitting and increases in standing and stepping in office workers. Few studies have evaluated these intervention strategies and this study has the potential to contribute both short and long-term findings.},
author = {Brakenridge, Charlotte L and Fjeldsoe, Brianna S and Young, Duncan C and Winkler, Elisabeth A H and Dunstan, David W and Straker, Leon M and Brakenridge, Christian J and Healy, Genevieve N},
doi = {10.2196/resprot.5438},
file = {::},
issn = {1929-0748},
journal = {JMIR research protocols},
keywords = {activity monitor,ecological model,light intensity activity,objective,office workers,sedentary lifestyle,self-monitoring,trial,wearable device,workplace},
month = {may},
number = {2},
pages = {e73},
pmid = {27226457},
publisher = {JMIR Publications Inc.},
title = {{Organizational-Level Strategies With or Without an Activity Tracker to Reduce Office Workers' Sitting Time: Rationale and Study Design of a Pilot Cluster-Randomized Trial.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27226457 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4909392},
volume = {5},
year = {2016}
}
@misc{Elger2008,
abstract = {The epilepsies are among the most common serious brain disorders, can occur at all ages, and are characterized by a variety of presentations and causes. Diagnosis of epilepsy remains clinical, and neurophysiological investigations support the diagnosis of the syndrome. Brain imaging is able to identify many of the structural causes of the epilepsies. Current antiepileptic drugs (AEDs) block seizures without influencing the underlying tendency to generate seizures, and are effective in 60-70{\%} of individuals. Several modern drugs are as efficacious as the older medications, but have important advantages including the absence of adverse drug interactions and hypersensitivity reactions. Epilepsy is associated with an increased prevalence of mental health disorders including anxiety, depression, and suicidal thoughts. An understanding of the psychiatric correlates of epilepsy is important to the adequate management of people with epilepsy. Anticipation of common errors in the diagnosis and management of epilepsy is important. Frequent early diagnostic errors include nonepileptic psychogenic seizures, syncope with myoclonus, restless legs syndrome, and REM behavioral disorders, the last mostly in elderly men. Overtreatment with too rapid titration and too high doses or too many AEDs should be avoided. For people with refractory focal epilepsy, vagus nerve stimulation offers palliative treatment with possible mood improvement and neurosurgical resection offers the possibility of a life-changing cure. Potential advances in the management of epilepsy are briefly discussed. This short review summarizes the authors' how-to-do approach to the modern management of people with epilepsy. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Elger, Christian E. and Schmidt, Dieter},
booktitle = {Epilepsy and Behavior},
doi = {10.1016/j.yebeh.2008.01.003},
file = {::},
issn = {15255050},
keywords = {Antiepileptic drugs,Drug treatment,Epilepsy,Epilepsy management,Epilepsy surgery,Nonepileptic seizures},
month = {may},
number = {4},
pages = {501--539},
pmid = {18314396},
title = {{Modern management of epilepsy: A practical approach}},
volume = {12},
year = {2008}
}
@article{Ye2020,
author = {Ye, Katherine and Ni, Wode and Krieger, Max and Ma'ayan, Dor and Wise, Jenna and Aldrich, Jonathan and Sunshine, Joshua and Crane, Keenan},
file = {::},
title = {{Penrose: From Mathematical Notation to Beautiful Diagrams}},
year = {2020}
}
@article{Moinuddin2017,
abstract = {The Internet of Things (IoT) integrates a large number of physical objects that are uniquely identified, ubiquitously interconnected and accessible through the Internet. IoT aims to transform any object in the real-world into a computing device that has sensing, communication and control capabilities. There is a growing number of IoT devices and applications and this leads to an increase in the number and complexity of malicious attacks. It is important to protect IoT systems against malicious attacks, especially to prevent attackers from obtaining control over the devices. A large number of security research solutions for IoT have been proposed in the last years, but most of them are not standardized or interoperable. In this paper, we investigate the security capabilities of existing protocols and networking stacks for IoT. We focus on solutions specified by well-known standardization bodies such as IEEE and IETF, and industry alliances, such as NFC Forum, ZigBee Alliance, Thread Group and LoRa Allianc.},
author = {Moinuddin, Khaja and Srikantha, Nalavadi and S, Lokesh K and Narayana, Aswatha},
doi = {10.18535/ijecs/v6i6.41},
file = {::},
journal = {International Journal Of Engineering And Computer Science},
keywords = {Internet of Things,authenti-cation,confidentiality,integrity,security,standard},
pages = {2319--7242},
title = {{A Survey on Secure Communication Protocols for IoT Systems}},
url = {www.ijecs.in},
volume = {6},
year = {2017}
}
@article{Gilson2016,
abstract = {Objectives This efficacy study assessed the added impact real time computer prompts had on a participatory approach to reduce occupational sedentary exposure and increase physical activity. Design Quasi-experimental. Methods 57 Australian office workers (mean [SD]; age = 47 [11] years; BMI = 28 [5] kg/m2; 46 men) generated a menu of 20 occupational ‘sit less and move more' strategies through participatory workshops, and were then tasked with implementing strategies for five months (July–November 2014). During implementation, a sub-sample of workers (n = 24) used a chair sensor/software package (Sitting Pad) that gave real time prompts to interrupt desk sitting. Baseline and intervention sedentary behaviour and physical activity (GENEActiv accelerometer; mean work time percentages), and minutes spent sitting at desks (Sitting Pad; mean total time and longest bout) were compared between non-prompt and prompt workers using a two-way ANOVA. Results Workers spent close to three quarters of their work time sedentary, mostly sitting at desks (mean [SD]; total desk sitting time = 371 [71] min/day; longest bout spent desk sitting = 104 [43] min/day). Intervention effects were four times greater in workers who used real time computer prompts (8{\%} decrease in work time sedentary behaviour and increase in light intensity physical activity; p {\textless} 0.01). Respective mean differences between baseline and intervention total time spent sitting at desks, and the longest bout spent desk sitting, were 23 and 32 min/day lower in prompt than in non-prompt workers (p {\textless} 0.01). Conclusions In this sample of office workers, real time computer prompts facilitated the impact of a participatory approach on reductions in occupational sedentary exposure, and increases in physical activity.},
author = {Gilson, Nicholas D. and Ng, Norman and Pavey, Toby G. and Ryde, Gemma C. and Straker, Leon and Brown, Wendy J.},
doi = {10.1016/j.jsams.2016.01.009},
issn = {18781861},
journal = {Journal of Science and Medicine in Sport},
keywords = {Computer prompts,Occupational sitting,Office workers,Physical activity},
month = {nov},
number = {11},
pages = {926--930},
publisher = {Elsevier Ltd},
title = {{Project Energise: Using participatory approaches and real time computer prompts to reduce occupational sitting and increase work time physical activity in office workers}},
volume = {19},
year = {2016}
}
@inproceedings{manning-etal-2014-stanford,
address = {Baltimore, Maryland},
author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
doi = {10.3115/v1/P14-5010},
month = {jun},
pages = {55--60},
publisher = {Association for Computational Linguistics},
title = {{The Stanford Core NLP Natural Language Processing Toolkit}},
url = {https://www.aclweb.org/anthology/P14-5010},
year = {2014}
}
@inproceedings{Cummaudo:2020icse,
address = {Seoul, Republic of Korea},
annote = {In Press},
author = {Cummaudo, Alex and Vasa, Rajesh and Barnett, Scott and Grundy, John and Abdelrazek, Mohamed},
booktitle = {Proceedings of the 42nd International Conference on Software Engineering},
keywords = {InPress},
mendeley-tags = {InPress},
month = {oct},
publisher = {IEEE},
title = {{Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow}},
year = {2020}
}
@misc{Jun2017,
abstract = {Introduction: Identifying risk factors associated with the development of work-related neck pain in office workers is necessary to facilitate the development of prevention strategies that aim to minimise this prevalent and costly health problem. The aim of this systematic review is to identify individual worker (e.g., lifestyle activity, muscular strength, and posture) and workplace (e.g., ergonomics and work environment) physical factors associated with the development of non-specific neck pain in office workers. Methods: Studies from 1980 to 2016 were identified by an electronic search of Pubmed, CINAHL, EMBASE, Psychlnfo and Proquest databases. Two authors independently screened search results, extracted data, and assessed risk of bias using the epidemiological appraisal instrument (EAI). A random effect model was used to estimate the risk of physical factors for neck pain. Results: Twenty papers described the findings of ten prospective cohort studies and two randomized controlled trials. Low satisfaction with the workplace environment (pooled RR 1.28; CI 1.07–1.55), keyboard position close to the body [pooled RR 1.46; (CI 1.07–1.99)], low work task variation [RR 1.27; CI (1.08–1.50)] and self-perceived medium/high muscular tension (pooled RR 2.75/1.82; CI 1.60 /1.14–4.72/2.90) were found to be risk factors for the development of neck pain. Conclusions: This review found evidence for a few number of physical risk factors for the development of neck pain, however, there was also either limited or conflicting factors. Recommendations for future studies evaluating risk factors are reported and how these may contribute to the prevention of neck pain in office workers.},
author = {Jun, Deokhoon and Zoe, Michaleff and Johnston, Venerina and O'Leary, Shaun},
booktitle = {International Archives of Occupational and Environmental Health},
doi = {10.1007/s00420-017-1205-3},
file = {::},
issn = {03400131},
keywords = {Ergonomics,Individual factors,Meta-analysis,Neck pain,Office worker,Physical factors,Systematic review,Work environment},
month = {jul},
number = {5},
pages = {373--410},
publisher = {Springer Verlag},
title = {{Physical risk factors for developing non-specific neck pain in office workers: a systematic review and meta-analysis}},
volume = {90},
year = {2017}
}
@article{Hand1984,
author = {Hand, D. J.},
doi = {10.2307/2987739},
file = {::},
issn = {00390526},
journal = {The Statistician},
month = {dec},
number = {4},
pages = {351--369},
title = {{Statistical Expert Systems: Design}},
url = {https://www.jstor.org/stable/2987739?origin=crossref},
volume = {33},
year = {1984}
}
@article{Danquah2016,
abstract = {Background: Prolonged sitting time has been associated with adverse health outcomes. Interventions at work may contribute to reduced sitting. The objective was to test if a multicomponent work-based intervention can reduce sitting time and the number of prolonged sitting periods ({\textgreater} 30 min), increase the number of sit-to-stand transitions and decrease waist circumference and body fat percentage among office workers. Primary outcomes were: change in sitting time, prolonged sitting periods and sit-to-stand transitions at followup 1 month later. Methods: At four workplaces, 19 offices (317 workers in total) were cluster randomized for intervention or control. The intervention included the appointment of local ambassadors, management support, environmental changes, a lecture and a workshop. Sitting time was measured using an ActiGraph GT3X+ fixed on the thigh. Data were processed using Acti4 software providing data on time spent sitting, standing and doing other activities. Control participants were instructed to behave as usual. Follow-up measurements were obtained after 1 and 3 months. Results: At 1 and 3 months, total sitting time was 71 (P {\textless} 0.001) and 48min (P {\textless} 0.001) lower per 8-h workday in the intervention group compared with the control group. At 1 month, the number of prolonged sitting periods was lower (-0.79/8-h workday, P {\textless} 0.001) and sit-to-stand transitions were higher (+14{\%}/sitting hour, P = 0.001) in the intervention compared with the control group. After 3 months, trends persisted. The body fat percentage was lower by 0.61 percentage points (P = 0.011) in the intervention group compared with the control group after 3 months. Conclusions: The multicomponent workplace-based intervention was effective in reducing sitting time, prolonged sitting periods and body fat percentage, and in increasing the number of sit-to-stand transitions.},
author = {Danquah, I.H. and Kloster, S. and Holtermann, A. and Aadahl, M. and Bauman, A. and Ersb{\o}ll, A.K. and Tolstrup, J.S.},
doi = {10.1093/ije/dyw009},
file = {::},
issn = {0300-5771},
journal = {International Journal of Epidemiology},
keywords = {Cluster randomized controlled trial,Prolonged sitting,Sit-to-stand transitions,Sitting time,Workplace},
month = {apr},
number = {1},
pages = {dyw009},
publisher = {Oxford University Press},
title = {{Take a Stand!–a multi-component intervention aimed at reducing sitting time among office workers–a cluster randomized trial}},
url = {https://academic.oup.com/ije/article-lookup/doi/10.1093/ije/dyw009},
volume = {46},
year = {2016}
}
@book{GhoshSohom2019,
author = {{Ghosh Sohom}, Author. and Gunning, Dwight},
publisher = {Packt Publishing},
title = {{Natural language processing fundamentals.}},
url = {http://ezproxy.deakin.edu.au/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=cat00097a{\&}AN=deakin.b4158080{\&}authtype=sso{\&}custid=deakin{\&}site=eds-live{\&}scope=site},
year = {2019}
}
@article{Liao,
author = {Liao, Q Vera and Muller, Michael},
file = {::},
journal = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
keywords = {a specialized,agents,artificial intelligence,extensive training nor mastering,human in,human-ai collaboration,machine learning,so that model development,the loop,will no longer require},
title = {{Human-AI Collaboration : Towards Socially-Guided Machine Learning}},
year = {2019}
}
@article{Angela2014,
author = {Angela, J Yu},
file = {::},
journal = {The Oxford Handbook of Attention},
title = {{Bayesian Models of Attention}},
year = {2014}
}
@article{Rodger2014,
abstract = {Because supply chains are complex systems prone to uncertainty, statistical analysis is a useful tool for capturing their dynamics. Using data on acquisition history and data from case study reports, we used regression analysis to predict backorder aging using National Item Identification Numbers (NIINs) as unique identifiers. More than 56,000 NIINs were identified and used in the analysis. Bayesian analysis was then used to further investigate the NIIN component variables. The results indicated that it is statistically feasible to predict whether an individual NIIN has the propensity to become a backordered item. This paper describes the structure of a Bayesian network from a real-world supply chain data set and then determines a posterior probability distribution for backorders using a stochastic simulation based on Markov blankets. Fuzzy clustering was used to produce a funnel diagram that demonstrates that the Acquisition Advice Code, Acquisition Method Suffix Code, Acquisition Method Code, and Controlled Inventory Item Code backorder performance metric of a trigger group dimension may change dramatically with variations in administrative lead time, production lead time, unit price, quantity ordered, and stock. Triggers must be updated regularly and smoothly to keep up with the changing state of the supply chain backorder trigger clusters of market sensitiveness, collaborative process integration, information drivers, and flexibility.},
author = {Rodger, James A.},
doi = {10.1016/J.ESWA.2014.05.012},
file = {::},
issn = {0957-4174},
journal = {Expert Systems with Applications},
month = {nov},
number = {16},
pages = {7005--7022},
publisher = {Pergamon},
title = {{Application of a Fuzzy Feasibility Bayesian Probabilistic Estimation of supply chain backorder aging, unfilled backorders, and customer wait time using stochastic simulation with Markov blankets}},
url = {https://www.sciencedirect.com/science/article/pii/S0957417414002929},
volume = {41},
year = {2014}
}
@article{Watanabe2019,
abstract = {Previous machine learning (ML) system development research suggests that emerging software quality attributes are a concern due to the probabilistic behavior of ML systems. Assuming that detailed development processes depend on individual developers and are not discussed in detail. To help developers to standardize their ML system development processes, we conduct a preliminary systematic literature review on ML system development processes. A search query of 2358 papers identified 7 papers as well as two other papers determined in an ad-hoc review. Our findings include emphasized phases in ML system developments, frequently described practices and tailored traditional software development practices.},
archivePrefix = {arXiv},
arxivId = {1910.05528},
author = {Watanabe, Yasuhiro and Washizaki, Hironori and Sakamoto, Kazunori and Saito, Daisuke and Honda, Kiyoshi and Tsuda, Naohiko and Fukazawa, Yoshiaki and Yoshioka, Nobukazu},
eprint = {1910.05528},
file = {::},
month = {oct},
title = {{Preliminary Systematic Literature Review of Machine Learning System Development Process}},
url = {http://arxiv.org/abs/1910.05528},
year = {2019}
}
@article{Baniecki2020,
abstract = {When analysing a complex system, very often an answer for one question raises new questions. The same law applies to the analysis of Machine Learning (ML) models. One method to explain the model is not enough because different questions and different stakeholders need different approaches. Most of the proposed methods for eXplainable Artificial Intelligence (XAI) focus on a single aspect of model behaviour. However, we cannot sufficiently explain a complex model using a single method that gives only one perspective. Isolated explanations are prone to misunderstanding, which inevitably leads to wrong reasoning. In this paper, we present the problem of model explainability as an interactive and sequential explanatory analysis of a model (IEMA). We introduce the grammar of such interactive explanations. We show how different XAI methods complement each other and why it is essential to juxtapose them together. We argue that without multi-faceted interactive explanation, there will be no understanding nor trust for models. The proposed process derives from the theoretical, algorithmic side of the model explanation and aims to embrace ideas learned through research in cognitive sciences. Its grammar is implemented in the modelStudio framework that adopts interactivity, automation and customisablity as its main traits. This thoughtful design addresses the needs of multiple diverse stakeholders, not only ML practitioners.},
archivePrefix = {arXiv},
arxivId = {2005.00497},
author = {Baniecki, Hubert and Biecek, Przemyslaw},
eprint = {2005.00497},
file = {::},
keywords = {Analysis {\textperiodcentered},Artificial,Black-,Box,Decision-making,Explanations {\textperiodcentered},Explanatory,Human-,Intelligence {\textperiodcentered},Interactive,Model,Models {\textperiodcentered},Oriented XAI {\textperiodcentered},Xplainable,e},
month = {may},
title = {{The Grammar of Interactive Explanatory Model Analysis}},
url = {http://arxiv.org/abs/2005.00497},
year = {2020}
}
@article{Beelders2016,
abstract = {Syntax highlighting or syntax colouring, plays a vital role in programming development environments by colour-coding various code elements differently. The supposition is that this syntax highlighting assists programmers when reading and analysing code. However, academic text books are largely only available in black-and-white which could influence the comprehension of novice and beginner programmers. This study investigated whether student programmers experience more difficulty in reading and comprehending source code when it is presented without syntax highlighting. Number of fixations, fixation durations and regressions were all higher for black-and-white code than for colour code but not significantly so. Subjectively students indicated that the colour code snippets were easier to read and more aesthetically pleasing. Based on the analysis it could be concluded that students do not experience significantly more difficulty when reading code in black-and-white as printed in text books.},
author = {Beelders, T. R. and {Du Plessis}, Jean Pierre L.},
doi = {10.16910/jemr.9.1.1},
file = {::},
issn = {19958692},
journal = {Journal of Eye Movement Research},
keywords = {Code comprehension,Eye-tracking,Reading behaviour,Syntax highlighting},
number = {1},
pages = {2207--2219},
title = {{Syntax highlighting as an influencing factor when reading and comprehending source code}},
volume = {9},
year = {2016}
}
@article{Srivastava2018,
abstract = {A machine learning (ML)-based text classification system has several classifiers. The performance evaluation (PE) of the ML system is typically driven by the training data size and the partition protocols used. Such systems lead to low accuracy because the text classification systems lack the ability to model the input text data in terms of noise characteristics. This research study proposes a concept of misrepresentation ratio (MRR) on input healthcare text data and models the PE criteria for validating the hypothesis. Further, such a novel system provides a platform to amalgamate several attributes of the ML system such as: data size, classifier type, partitioning protocol and percentage MRR. Our comprehensive data analysis consisted of five types of text data sets (TwitterA, WebKB4, Disease, Reuters (R8), and SMS); five kinds of classifiers (support vector machine with linear kernel (SVM-L), MLP-based neural network, AdaBoost, stochastic gradient descent and decision tree); and five types of training protocols ({\textless}italic{\textgreater}K2, K4, K5, K10{\textless}/italic{\textgreater} and {\textless}italic{\textgreater}JK{\textless}/italic{\textgreater}). Using the decreasing order of MRR, our ML system demonstrates the mean classification accuracies as: 70.13 ± 0.15{\%}, 87.34 ± 0.06{\%}, 93.73 ± 0.03{\%}, 94.45 ± 0.03{\%} and 97.83 ± 0.01{\%}, respectively, using all the classifiers and protocols. The corresponding AUC is 0.98 for SMS data using Multi-Layer Perceptron (MLP) based neural network. All the classifiers, the best accuracy of 91.84 ± 0.04{\%} is shown to be of MLP-based neural network and this is 6{\%} better over previously published. Further we observed that as MRR decreases, the system robustness increases and validated by standard deviations. The overall text system accuracy using all data types, classifiers, protocols is 89{\%}, thereby showing the entire ML system to be novel, robust and unique. The system is also tested for stability and reliability.},
author = {Srivastava, Saurabh Kumar and Singh, Sandeep Kumar and Suri, Jasjit S.},
doi = {10.1007/s10916-018-0941-6},
file = {::},
issn = {1573689X},
journal = {Journal of Medical Systems},
keywords = {Classifiers,Healthcare text classification,Machine learning,Misrepresentation ratio,Reliability,Stability},
title = {{Healthcare Text Classification System and its Performance Evaluation: A Source of Better Intelligence by Characterizing Healthcare Text}},
year = {2018}
}
@article{Nundhapana2018,
abstract = {Understandability is a software characteristic that helps ease software maintenance and evolution. When modifying or reusing software that is written by someone else, software developers often have difficulties in trying to understand what the existing software does and how. Such an issue is commonly found in software-developing organizations. This paper discusses an approach taken by an IT organization in Thailand which attempts to enforce coding standards within its iOS development team in order to promote software understandability and maintainability. Among coding standards, naming conventions are important but are most often violated. This paper presents the development of a naming convention checking framework that consists of tools to automatically detect naming convention violations in Objective C programs. The framework facilitates iOS developers in modifying the programs so that they adhere to the naming conventions. An experiment showed that the developers' understanding in the programs that had been modified, as suggested by the naming convention checking framework, did improve at a statistical significance level of 0.05. This approach can enhance program understandability and can be applied to other software-developing organizations.},
author = {Nundhapana, Ruchuta and Senivongse, Twittie},
file = {::},
isbn = {9789881404817},
issn = {20780958},
journal = {Lecture Notes in Engineering and Computer Science},
keywords = {Maintainability,Naming convention,Objective C,Understandability},
pages = {314--319},
title = {{Enhancing understandability of objective C programs using naming convention checking framework}},
volume = {2237},
year = {2018}
}
@article{Kristiansen2012,
author = {Kristiansen, Simon and Stidsen, Thomas R},
file = {::},
journal = {Proceedings of the Ninth International Conference on the Practice and Theory of Automated Timetabling (PATAT 2012)},
keywords = {2000,90-08,90b35,90c10,90c59,adaptive large neighborhood,educational timetabling,elective course planning,high school planning,mathematics subject classi cation,metaheuristics,search,student sectioning},
number = {August},
pages = {29--31},
title = {{Adaptive large neighborhood search for student sectioning at Danish high schools}},
year = {2012}
}
@article{Chen2014,
abstract = {Epilepsy is one of the most common neurological disorders with 0.8{\%} of the world population. The epilepsy is unpredictable and recurrent, so it is very difficult to treat. In this paper, we propose a new Electroencephalography (EEG) seizure detection method by using the dual-tree complex wavelet (DTCWT) - Fourier features. These features achieve perfect classification rates (100{\%}) for the EEG database from the University of Bonn. These classification rates outperform a number of existing EEG seizure detection methods published in the literature. However, it should be mentioned that several recent works also achieved this perfect classification rate (100{\%}). Our proposed method should be as good as these works since our method only performs the DTCWT transform for up to 5 scales and our method only conducts the FFT to the 4th and 5th scales of the DTCWT decomposition. In addition, we could replace the conventional FFT in our method by sparse FFT so that our method could be even faster. {\textcopyright}2013 Elsevier Ltd. All rights reserved.},
author = {Chen, Guangyi},
doi = {10.1016/j.eswa.2013.09.037},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Dual-tree complex wavelet transform (DTCWT),Electroencephalography (EEG),Fourier transform,Seizure detection},
month = {apr},
number = {5},
pages = {2391--2394},
title = {{Automatic EEG seizure detection using dual-tree complex wavelet-Fourier features}},
volume = {41},
year = {2014}
}
@article{Agrawal2018,
abstract = {Context: Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeling technique is Latent Dirichlet allocation. When running on different datasets, LDA suffers from “order effects”, i.e., different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results; specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results. Objective: To provide a method in which distributions generated by LDA are more stable and can be used for further analysis. Method: We use LDADE, a search-based software engineering tool which uses Differential Evolution (DE) to tune the LDA's parameters. LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of Software Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark) across Linux platform and for different kinds of LDAs (VEM, Gibbs sampling). Results were scored via topic stability and text mining classification accuracy. Results: In all treatments: (i) standard LDA exhibits very large topic instability; (ii) LDADE's tunings dramatically reduce cluster instability; (iii) LDADE also leads to improved performances for supervised as well as unsupervised learning. Conclusion: Due to topic instability, using standard LDA with its “off-the-shelf” settings should now be depreciated. Also, in future, we should require SE papers that use LDA to test and (if needed) mitigate LDA topic instability. Finally, LDADE is a candidate technology for effectively and efficiently reducing that instability.},
author = {Agrawal, Amritanshu and Fu, Wei and Menzies, Tim},
doi = {10.1016/j.infsof.2018.02.005},
file = {::},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Differential evolution,LDA,Stability,Topic modeling,Tuning},
number = {February},
pages = {74--88},
publisher = {Elsevier},
title = {{What is wrong with topic modeling? And how to fix it using search-based software engineering}},
url = {https://doi.org/10.1016/j.infsof.2018.02.005},
volume = {98},
year = {2018}
}
@article{Metamodeling2003,
abstract = { Metamodeling is an essential foundation for MDD, but there's little consensus on the precise form it should take and role it should play. The authors analyze the underlying motivation for MDD and then derive a concrete set of requirements that a supporting infrastructure should satisfy. They discuss why the traditional "language definition" interpretation of metamodeling isn't a sufficient foundation and explain how it can be extended to unlock MDD's full potential.},
author = {Atkinson, Colin and K{\"{u}}hne, Thomas},
doi = {10.1109/MS.2003.1231149},
file = {::},
isbn = {0740-7459},
issn = {07407459},
journal = {IEEE Software},
keywords = {domain meta concepts,language definition,ling,metamode-,model driven development requirements},
number = {5},
pages = {36--41},
title = {{Model-driven development: A metamodeling foundation}},
volume = {20},
year = {2003}
}
@article{Liu2019,
author = {Liu, Wanyu and Oulasvirta, Antti and Rioul, Olivier and Beaudouin-lafon, Michel and Liu, Wanyu and Oulasvirta, Antti and Rioul, Olivier and Beaudouin-lafon, Michel and Information, Yves Guiard},
file = {::},
title = {{Information theory : An analysis and design tool for HCI To cite this version : HAL Id : hal-02300784 Information Theory : An Analysis and Design Tool for HCI}},
year = {2019}
}
@article{Stevenson2019,
abstract = {Neonatal seizures are a common emergency in the neonatal intensive care unit (NICU). There are many questions yet to be answered regarding the temporal/spatial characteristics of seizures from different pathologies, response to medication, effects on neurodevelopment and optimal detection. The dataset presented in this descriptor contains EEG recordings from human neonates, the visual interpretation of the EEG by the human experts, supporting clinical data and codes to assist access. Multi-channel EEG was recorded from 79 term neonates admitted to the NICU at the Helsinki University Hospital. The median recording duration was 74 min (IQR: 64 to 96 min). The presence of seizures in the EEGs was annotated independently by three experts. An average of 460 seizures were annotated per expert in the dataset; 39 neonates had seizures and 22 were seizure free, by consensus. The dataset can be used as a reference set of neonatal seizures, in studies of inter-observer agreement and for the development of automated methods of seizure detection and other EEG analyses.},
author = {Stevenson, N. J. and Tapani, K. and Lauronen, L. and Vanhatalo, S.},
doi = {10.1038/sdata.2019.39},
file = {::},
issn = {20524463},
journal = {Scientific Data},
pmid = {30835259},
publisher = {Nature Publishing Groups},
title = {{A dataset of neonatal eeg recordings with seizure annotations}},
volume = {6},
year = {2019}
}
@inproceedings{Renzella2020a,
abstract = {With the global increase in demand for online tertiary education, teachers are facing unique challenges in scaling assessment activities and meaningful student engagement. One such aspect is contract cheating behaviours exhibited in the modern online environment posing a threat to the academic integrity of tertiary education. These obstacles amplify when applied to traditionally difficult domains like introductory programming education. Prior research on contract cheating identification proposes that while challenging, techniques such as developing strong teacher-student relationships, and real-time discussions may lead to instances of identifying contract cheating behaviours. The proposition, then, is to scale real-time, student-teacher discussions with large, online co-horts-similar to those discussions which traditionally took place in the classroom. This poster paper presents Intelligent Discussion Comments (IDCs): A scalable, teacher-asynchronous system which engages students in real-time discussions to extract authentic student understanding. Artificial intelligence services such as voice identification and transcription enrich the discussion process, supporting the teaching team in their decision-making process. CCS CONCEPTS • Applied computing → Learning management systems; Distance learning; E-learning; • Human-centered computing → Human computer interaction (HCI).},
address = {Seoul, Republic of Korea},
author = {Renzella, Jake and Cain, Andrew and Schneider, Jean-Guy},
booktitle = {Proceedings of the 42nd International Conference on Software Engineering},
doi = {10.1145/3377812.3390795},
file = {::},
isbn = {9781450371223},
keywords = {Learning management system,computing assessment,computing education,contract cheating detection,human computer interaction,online education,programming education},
title = {{An Intelligent Tool for Combatting Contract Cheating Behaviour by Facilitating Scalable Student-Tutor Discussions}},
year = {2020}
}
@article{Callaghan2015,
abstract = {There has been a major shift toward office workstations that accommodate standing postures. This shift is attributable to negative health and musculoskeletal issues from sedentary exposures. However, changing exposures from sitting to standing does not eliminate these issues, as evidence indicates prolonged standing also induces problems. Reducing seated exposure and rotating frequently between sitting and standing has been shown to result in positive health outcomes, reduced discomfort, and increased work performance. Implementing sit-stand workstations has promise to mitigate work-related health issues, if the users are provided with training that includes accommodations for individual work patterns and preferences.},
author = {Callaghan, Jack P. and {De Carvalho}, Diana and Gallagher, Kaitlin and Karakolis, Thomas and Nelson-Wong, Erika},
doi = {10.1177/1064804615585412},
issn = {1064-8046},
journal = {Ergonomics in Design: The Quarterly of Human Factors Applications},
keywords = {occupational standing,office ergonomics,sit-stand,sit-stand desks,working posture},
month = {jul},
number = {3},
pages = {20--24},
publisher = {SAGE Publications Inc.},
title = {{Is Standing the Solution to Sedentary Office Work?}},
url = {http://journals.sagepub.com/doi/10.1177/1064804615585412},
volume = {23},
year = {2015}
}
@article{Fowler2002_refactoring,
abstract = {Almost every expert in Object-Oriented Development stresses the importance of iterative development. As you proceed with the iterative development, you need to add function to the existing code base. If you are really lucky that code base is structured just right to support the new function while still preserving its design integrity. Of course most of the time we are not lucky, the code does not quite fit what we want to do. You could just add the function on top of the code base. But soon this leads to applying patch upon patch making your system more complex than it needs to be. This complexity leads to bugs, and cripples your productivity.},
author = {Fowler, Martin},
doi = {10.1007/3-540-45672-4_31},
file = {::},
isbn = {3540440240},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
publisher = {Addison-Wesley Professional},
title = {{Refactoring: Improving the design of existing code}},
volume = {2418},
year = {2002}
}
@book{osgood1957measurement,
author = {Osgood, Charles Egerton and Suci, George J and Tannenbaum, Percy H},
number = {47},
publisher = {University of Illinois press},
title = {{The measurement of meaning}},
year = {1957}
}
@article{Japkowicz1998,
annote = {Introduces concept complexity, data set size and degree of class imbalance.},
author = {Japkowicz, Nathalie and ju Stephen, Sha and ju Stephen, Sha},
file = {::},
title = {{The Class Imbalance Problem: A Systematic Study}}
}
@article{Kane2017,
author = {Kane, Nick and Acharya, Jayant and Benickzy, Sandor and Caboclo, Luis and Finnigan, Simon and Kaplan, Peter W. and Shibasaki, Hiroshi and Pressler, Ronit and van Putten, Michel J.A.M.},
doi = {10.1016/j.cnp.2017.07.002},
file = {::},
issn = {2467981X},
journal = {Clinical Neurophysiology Practice},
month = {jan},
pages = {170--185},
publisher = {Elsevier B.V.},
title = {{A revised glossary of terms most commonly used by clinical electroencephalographers and updated proposal for the report format of the EEG findings. Revision 2017}},
volume = {2},
year = {2017}
}
@article{Nargesian2017,
abstract = {Feature engineering is the task of improving predictive modelling performance on a dataset by transforming its feature space. Existing approaches to automate this process rely on either transformed feature space exploration through evaluation-guided search, or explicit expansion of datasets with all transformed features followed by feature selection. Such approaches incur high computational costs in runtime and/or memory. We present a novel technique, called Learning Feature Engineering (LFE), for automating feature engineering in classification tasks. LFE is based on learning the effectiveness of applying a transformation (e.g., arithmetic or aggregate operators) on numerical features, from past feature engineering experiences. Given a new dataset, LFE recommends a set of useful transformations to be applied on features without relying on model evaluation or explicit feature expansion and selection. Using a collection of datasets, we train a set of neural networks, which aim at predicting the transformation that impacts classification performance positively. Our empirical results show that LFE outperforms other feature engineering approaches for an overwhelming majority (89{\%}) of the datasets from various sources while incurring a substantially lower computational cost.},
author = {Nargesian, Fatemeh and Samulowitz, Horst and Khurana, Udayan and Khalil, Elias B. and Turaga, Deepak},
doi = {10.24963/ijcai.2017/352},
file = {::},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Classification,Feature Selection/Construction,Machine Learning},
number = {August},
pages = {2529--2535},
title = {{Learning feature engineering for classification}},
year = {2017}
}
@article{Dhakal2018,
author = {Dhakal, Vivek and Feit, Anna Maria and Kristensson, Per Ola and Oulasvirta, Antti},
doi = {10.1145/3173574.3174220},
file = {::},
isbn = {9781450356206},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Text entry,large-scale study,modern typing behavior},
pages = {1--12},
title = {{Observations on Typing from 136 Million Keystrokes}},
year = {2018}
}
@inproceedings{Rybicki2019,
abstract = {The goal of Data Science projects is to extract knowledge and insights from collected data. The focus is put on the novelty and usability of the obtained insights. However, the impact of a project can be seriously reduced if the results are not communicated well. In this paper, we describe a means of managing and describing the outcomes of the Data Science projects in such a way that they optimally convey the insights gained. We focus on the main artifact of 854the non-verbal communication, namely project structure. In particular, we surveyed three sources of information on how to structure projects: common management methodologies, community best practices, and data sharing platforms. The survey resulted in a list of recommendations on how to build the project artifacts to make them clear, intuitive, and logical. We also provide hints on tools that can be helpful for managing such structures in an efficient manner. The paper is intended to motivate and support an informed decision on how to structure a Data Science project to facilitate better communication of the outcomes.},
author = {Rybicki, Jedrzej},
booktitle = {Proceedings of 39th International Conference on Information Systems Architecture and Technology -- ISAT 2018},
doi = {10.1007/978-3-319-99993-7_31},
file = {::},
isbn = {978-3-319-99993-7},
keywords = {Data science,Management methodologies,Project management,Tools},
pages = {348--357},
publisher = {Springer International Publishing},
title = {{Best Practices in Structuring Data Science Projects}},
year = {2019}
}
@incollection{Herrera2018,
annote = {Keel is the main dataset
4 broad solutions
3 major problems, only 2 can be dealt with},
author = {Herrera, Francisco and Garc{\'{i}}a, Salvador and Fern{\'{a}}ndez, Alberto and Krawczyk, Bartosz and Galar, Mikel and Prati, Ronaldo C.},
booktitle = {Learning from Imbalanced Data Sets},
doi = {10.1007/978-3-319-98074-4_2},
title = {{Foundations on Imbalanced Classification}},
year = {2018}
}
@article{Raglio2019b,
abstract = {BACKGROUND Literature shows that music can reduce stress conditions. This pilot study investigated the effects of music listening on work-related stress and well-being in healthcare professionals. METHOD A total of 45 subjects were randomly assigned to three treatment groups: No Music, Individualized Music and Melomics-Health Listening. Music groups experienced a daily 30-min-playlist listening for 3 weeks at home. The Maugeri Stress Index-Revised (MASI-R) and the Psychological General Well-Being Index (PGWBI) were administered at baseline, after 3 weeks and after 7 weeks (follow-up). Longitudinal data were analyzed by means of a nested ANOVA model, testing the main effects of time and treatment and the interaction between them. RESULTS MASI-R scores showed a positive trend in music groups and a worsening in the control group. Only the interaction time/treatment emerged as supporting a trend toward statistical significance (P = 0.07). PGWBI showed a stability in music groups and a clear decline in controls, without significant effects. CONCLUSIONS Results from the study support the need for a larger clinical trial: it is suggested that daily music listening could be implemented to reduce work-related stress and that the effects may be related, not only to individual musical preferences and familiarity, but also to specific music structures and parameters.},
author = {Raglio, A and Bellandi, D and Gianotti, M and Zanacchi, E and Gnesi, M and Monti, M C and Montomoli, C and Vico, F and Imbriani, C and Giorgi, I and Imbriani, M},
doi = {10.1093/pubmed/fdz030},
issn = {1741-3842},
journal = {Journal of Public Health},
keywords = {listening,music,personal satisfaction,stress},
month = {apr},
publisher = {Oxford University Press (OUP)},
title = {{Daily music listening to reduce work-related stress: a randomized controlled pilot trial}},
url = {https://academic.oup.com/jpubhealth/advance-article/doi/10.1093/pubmed/fdz030/5426643},
year = {2019}
}
@article{Scaffidi2008,
abstract = {Inputs to spreadsheets and web forms often contain typos or other errors. However, existing tools require end-user programmers (EUPs) to write regular expressions ("regexps") or even scripts to validate data, which is slow and error-prone. We present a new technique enabling EUPs to describe data as a series of constrained parts. We incorporate our technique in a prototype tool called Toped, which generates validation code for Excel and web forms. Our technique enables EUPs to validate data more quickly and accurately than with existing techniques, finding 90{\%} of invalid inputs in a lab study.},
author = {Scaffidi, Christopher and Myers, Brad and Shaw, Mary},
doi = {10.1145/1358628.1358884},
file = {::},
isbn = {9781605580128},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {End-user programming,Spreadsheets,Web applications},
pages = {3519--3524},
title = {{Toped: Enabling end-user programmers to validate data}},
year = {2008}
}
@article{Barrenechea2011,
annote = {Only focuses on ensembles for the binary classification problem.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Barrenechea, E. and Herrera, F. and Fernandez, A. and Galar, M. and Bustince, H.},
doi = {10.1109/tsmcc.2011.2161285},
eprint = {1503.08895},
isbn = {1094-6977},
issn = {1094-6977},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
pmid = {9377276},
title = {{A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches}},
year = {2011}
}
@article{Haapakangas2018,
abstract = {The problems of open-plan offices are widely known. However, the factors explaining these effects have received less attention. The aim of this study was to investigate the role of office distractions in the emergence of other problems, and to examine the benefits of quiet workspaces in open-plan offices. Two organizations moved from private offices to open-plan offices that differed in the number and variety of quiet rooms. Survey data was gathered once before (N = 65 and 64) and once after the office relocation (N = 135 and 71). Perceived distractions increased in both organizations after the relocation. However, negative effects on environmental satisfaction, perceived collaboration and stress only emerged in the open-plan office where the number of quiet rooms was low. Increased distractions mediated the effects on collaboration and stress. Quiet workspaces, and the perceived ease of access to them, are associated with environmental perceptions, perceived collaboration and employee stress in open-plan offices.},
author = {Haapakangas, Annu and Hongisto, Valtteri and Varjo, Johanna and Lahtinen, Marjaana},
doi = {10.1016/j.jenvp.2018.03.003},
issn = {15229610},
journal = {Journal of Environmental Psychology},
keywords = {Distraction,Environmental satisfaction,Office design,Open-plan office,Privacy,Stress},
month = {apr},
pages = {63--75},
publisher = {Academic Press},
title = {{Benefits of quiet workspaces in open-plan offices – Evidence from two office relocations}},
volume = {56},
year = {2018}
}
@misc{Fraga-Lamas2016b,
abstract = {The Internet of Things (IoT) is undeniably transforming the way that organizations communicate and organize everyday businesses and industrial procedures. Its adoption has proven well suited for sectors that manage a large number of assets and coordinate complex and distributed processes. This survey analyzes the great potential for applying IoT technologies (i.e., data-driven applications or embedded automation and intelligent adaptive systems) to revolutionize modern warfare and provide beneﬁts similar to those in industry. It identiﬁes scenarios where Defense and Public Safety (PS) could leverage better commercial IoT capabilities to deliver greater survivability to the warﬁghter or ﬁrst responders, while reducing costs and increasing operation efﬁciency and effectiveness. This article reviews the main tactical requirements and the architecture, examining gaps and shortcomings in existing IoT systems across the military ﬁeld and mission-critical scenarios. The review characterizes the open challenges for a broad deployment and presents a research roadmap for enabling an affordable IoT for defense and PS.},
author = {Fraga-Lamas, Paula and Fern{\'{a}}ndez-Caram{\'{e}}s, Tiago M. and Su{\'{a}}rez-Albela, Manuel and Castedo, Luis and Gonz{\'{a}}lez-L{\'{o}}pez, Miguel},
booktitle = {Sensors (Basel, Switzerland)},
doi = {10.3390/s16101644},
file = {::},
issn = {14248220},
keywords = {Internet of Things,Machine-to-Machine communications,cloud computing,defense and public safety,heterogeneous networks,mission-critical networks,public safety responders,security,tactical environment,trust management,wireless sensor networks},
month = {oct},
number = {10},
pmid = {27782052},
publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
title = {{A Review on Internet of Things for Defense and Public Safety}},
volume = {16},
year = {2016}
}
@inproceedings{Elvitigala2018,
abstract = {Interactive displays are increasingly embedded into the architecture we inhabit. We designed Doodle Daydream, an LED grid display with a mobile interface, which acts similarly to a shared sketch pad. Users contribute their own custom drawings that immediately play back on the display, fostering moment-to-moment playful interactions. This project builds on related work by designing a collaborative display to support calming yet playful interactions in an office setting.},
author = {Elvitigala, Samitha and Chan, Samantha W.T. and Howell, Noura and Matthies, Denys J.C. and Nanayakkara, Suranga},
booktitle = {SUI 2018 - Proceedings of the Symposium on Spatial User Interaction},
doi = {10.1145/3267782.3274681},
isbn = {9781450357081},
month = {oct},
pages = {186},
publisher = {Association for Computing Machinery, Inc},
title = {{Doodle Daydream: An Interactive Display to Support Playful and Creative Interactions Between Co-workers}},
year = {2018}
}
@techreport{Kutin,
abstract = {We explore in some detail the notion of algorith­ mic stability as a viable framework for analyzing the generalization error of learning algorithms. We introduce the new notion of training stabil­ ity of a learning algorithm and show that, in a general setting, it is sufficient for good bounds on generalization error. In the PAC setting, train­ ing stability is both necessary and sufficient for learnability. The approach based on training stability makes no reference to VC dimension or VC entropy. There is no need to prove uniform convergence, and generalization error is bounded directly via an extended McDiarmid inequality. As a result it potentially allows us to deal with a broader class of learning algorithms than Empirical Risk Min­ imization. We also explore the relationships among VC di­ mension, generalization error, and various no­ tions of stability. Several examples of learning algorithms are considered.},
author = {Kutin', Samuel and Niyogi, Partha},
file = {::},
title = {{Almost-everywhere algorithmic stability and generalization error}}
}
@article{Dahlstrom2014,
abstract = {Although storytelling often has negative connotations within science, narrative formats of communication should not be disregarded when communicating science to nonexpert audiences. Narratives offer increased comprehension, interest, and engagement. Nonexperts get most of their science information from mass media content, which is itself already biased toward narrative formats. Narratives are also intrinsically persuasive, which offers science communicators tactics for persuading otherwise resistant audiences, although such use also raises ethical considerations. Future intersections of narrative research with ongoing discussions in science communication are introduced.},
author = {Dahlstrom, Michael F.},
doi = {10.1073/pnas.1320645111},
file = {::},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Ethics,Persuasion},
month = {sep},
number = {Suppl 4},
pages = {13614--13620},
publisher = {National Academy of Sciences},
title = {{Using narratives and storytelling to communicate science with nonexpert audiences}},
url = {https://pubmed.ncbi.nlm.nih.gov/25225368/},
volume = {111},
year = {2014}
}
@article{Yusop2020,
abstract = {Usability is one of the software qualities attributes that is subjective and often considered as a less critical defect to be fixed. One of the reasons was due to the vague defect descriptions that could not convince developers about the validity of usability issues. Producing a comprehensive usability defect description can be a challenging task, especially in reporting relevant and important information. Prior research in improving defect report comprehension has often focused on defects in general or studied various aspects of software quality improvement such as triaging defect reports, metrics and predictions, automatic defect detection and fixing. In this paper, we studied 2241 usability and non-usability defects from three open-source projects-Mozilla Thunderbird, Firefox for Android, and Eclipse Platform. We examined the presence of eight defect attributes-steps to reproduce, impact, software context, expected output, actual output, assume cause, solution proposal, and supplementary information, and used various statistical tests to answer the research questions. In general, we found that usability defects are resolved slower than non-usability defects, even for non-usability defect reports that have less information. In terms of defect report content, usability defects often contain output details and software context while non-usability defects are preferably explained using supplementary information, such as stack traces and error logs. Our research findings extend the body of knowledge of software defect reporting, especially in understanding the characteristics of usability defects. The promising results also may be valuable to improve software development practitioners' practice.},
author = {Yusop, Nor Shahida Mohamad and Grundy, John and Schneider, Jean Guy and Vasa, Rajesh},
doi = {10.18517/ijaseit.10.1.10225},
file = {::},
issn = {24606952},
journal = {International Journal on Advanced Science, Engineering and Information Technology},
keywords = {Defect report,Open-source,Software defect repository,Software repository mining,Usability defects},
number = {1},
pages = {98--105},
title = {{How usability defects defer from non-usability defects?: A case study on open source projects}},
volume = {10},
year = {2020}
}
@article{Guan2020,
abstract = {Background: Since December 2019, acute respiratory disease (ARD) due to 2019 novel coronavirus (2019-nCoV) emerged in Wuhan city and rapidly spread throughout China. We sought to delineate the clinical characteristics of these cases. Methods: We extracted the data on 1,099 patients with laboratory-confirmed 2019-nCoV ARD from 552 hospitals in 31 provinces/provincial municipalities through January 29th, 2020. Results: The median age was 47.0 years, and 41.90{\%} were females. Only 1.18{\%} of patients had a direct contact with wildlife, whereas 31.30{\%} had been to Wuhan and 71.80{\%} had contacted with people from Wuhan. Fever (87.9{\%}) and cough (67.7{\%}) were the most common symptoms. Diarrhea is uncommon. The median incubation period was 3.0 days (range, 0 to 24.0 days). On admission, ground-glass opacity was the typical radiological finding on chest computed tomography (50.00{\%}). Significantly more severe cases were diagnosed by symptoms plus reverse-transcriptase polymerase-chain-reaction without abnormal radiological findings than non-severe cases (23.87{\%} vs. 5.20{\%}, P{\textless}0.001). Lymphopenia was observed in 82.1{\%} of patients. 55 patients (5.00{\%}) were admitted to intensive care unit and 15 (1.36{\%}) succumbed. Severe pneumonia was independently associated with either the admission to intensive care unit, mechanical ventilation, or death in multivariate competing-risk model (sub-distribution hazards ratio, 9.80; 95{\%} confidence interval, 4.06 to 23.67). Conclusions: The 2019-nCoV epidemic spreads rapidly by human-to-human transmission. Normal radiologic findings are present among some patients with 2019-nCoV infection. The disease severity (including oxygen saturation, respiratory rate, blood leukocyte/lymphocyte count and chest X-ray/CT manifestations) predict poor clinical outcomes.

{\#}{\#}{\#} Competing Interest Statement

The authors have declared no competing interest.

{\#}{\#}{\#} Clinical Trial

NA

{\#}{\#}{\#} Funding Statement

Supported by Ministry of Science and Technology, National Health Commission, National Natural Science Foundation, Department of Science and Technology of Guangdong Province.

{\#}{\#}{\#} Author Declarations

All relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.

Yes

All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.

Yes

I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).

Yes

I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.

Yes

Data will be made available upon request to the corresponding author.},
author = {Guan, Wei-jie and Ni, Zheng-yi and Hu, Yu and Liang, Wen-hua and Ou, Chun-quan and He, Jian-xing and Liu, Lei and Shan, Hong and Lei, Chun-liang and Hui, David SC C and Du, Bin and Li, Lan-juan and Zeng, Guang and Yuen, Kowk-Yung and Chen, Ru-chong and Tang, Chun-li and Wang, Tao and Chen, Ping-yan and Xiang, Jie and Li, Shi-yue and Wang, Jin-lin and Liang, Zi-jing and Peng, Yi-xiang and Wei, Li and Liu, Yong and Peng, Peng and Liu, Ji-yang and Chen, Zhong and Li, Gang and Zheng, Zhi-jian and Qiu, Shao-qin and Luo, Jie and Ye, Chang-jiang and Zhu, Shao-yong and Zhong, Nan-shan},
doi = {10.1101/2020.02.06.20020974},
file = {::},
journal = {New England Journal of Medicine},
month = {feb},
pages = {2020.02.06.20020974},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{Clinical characteristics of 2019 novel coronavirus infection in China}},
year = {2020}
}
@article{Zhang2018,
author = {Zhang, Yan Ping and Wu, Zeng Bao and Zhao, Shu and Yan, Yuan Ting and Chen, Jie and Du, Xiu Quan},
doi = {10.1016/j.ijar.2018.12.011},
file = {::},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
pages = {1--16},
publisher = {Elsevier Inc.},
title = {{A three-way decision ensemble method for imbalanced data oversampling}},
url = {https://doi.org/10.1016/j.ijar.2018.12.011},
volume = {107},
year = {2018}
}
@article{OHara2015,
abstract = {Computational notebooks are documents that serve dual purposes: they serve as an archive format containing code, text, images and equations; but they can also be run like computer programs. This paper explores the use of these new computational notebooks to teach AI and introduces tools that we have developed - ICalico and Calysto - to facilitate that use. Not only do these new tools broaden the languages and contexts available to students exploring notebook-based AI computing, but they offer a new mode of teaching and learning for the AI classroom.},
author = {O'Hara, Keith J. and Blank, Douglas and Marshall, James},
file = {::},
isbn = {9781577357308},
journal = {Proceedings of the 28th International Florida Artificial Intelligence Research Society Conference, FLAIRS 2015},
keywords = {Special Track on Artificial Intelligence Education},
pages = {263--268},
title = {{Computational notebooks for AI education}},
year = {2015}
}
@article{.2015,
abstract = {People can access computer by the help of Human-Computer Interaction (HCI) assistance. In Human-Computer Interaction (HCI) field, usability is the important substance. If we assume usability substance delayed or at last for system development which will become so expensive to get usable system. So that we need to think usability fact from the beginning of the system development cycle. This research is about the way how to design and develop usable user interface system. Design rules and principles are the effective means to design usable system. These rules provide the designers assistance to improve the usability of a system while designing. Usability rules and principles need to apply during design time of the system to produce best usable system.},
author = {., Fourcan Karim Mazumder},
doi = {10.15623/ijret.2014.0309011},
file = {::},
issn = {23217308},
journal = {International Journal of Research in Engineering and Technology},
month = {jan},
number = {09},
pages = {79--82},
publisher = {eSAT Publishing House},
title = {{USABILITY GUIDELINES FOR USABLE USER INTERFACE}},
volume = {03},
year = {2015}
}
@article{Gomez-Uribe2015,
abstract = {This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention andmedium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.},
author = {Gomez-Uribe, Carlos A. and Hunt, Neil},
doi = {10.1145/2843948},
file = {::},
issn = {21586578},
journal = {ACM Transactions on Management Information Systems},
number = {4},
title = {{The netflix recommender system: Algorithms, business value, and innovation}},
volume = {6},
year = {2015}
}
@inproceedings{Barnett2015c,
abstract = {Quality attributes are essential in software architecture and they are determined by identifying the concerns of the stakeholders of a system. The concerns of constructing mobile applications (apps) are quite specific due to the characteristics of mobile devices. These concerns have not been adequately addressed in industry standards and practices. In this paper, we present a mobile app development conceptual model comprising six key concepts that impact quality. Using two case studies, we show that these interrelated concepts influence the architectural decisions of mobile apps and their tradeoffs need to be well considered. As such, we suggest that these concepts should be first class entities when designing mobile app architecture to ensure that the quality attributes are satisfied.},
author = {Barnett, Scott and Vasa, Rajesh and Tang, Antony},
booktitle = {2015 12th Working IEEE/IFIP Conference on Software Architecture},
doi = {10.1109/WICSA.2015.28},
isbn = {978-1-4799-1922-2},
month = {may},
pages = {105--114},
publisher = {IEEE},
title = {{A Conceptual Model for Architecting Mobile Applications}},
url = {http://ieeexplore.ieee.org/document/7158509/},
year = {2015}
}
@article{Reddy,
abstract = {As the efficacy of artificial intelligence (AI) in improving aspects of healthcare delivery is increasingly becoming evident, it becomes likely that AI will be incorporated in routine clinical care in the near future. This promise has led to growing focus and investment in AI medical applications both from governmental organizations and technological companies. However, concern has been expressed about the ethical and regulatory aspects of the application of AI in health care. These concerns include the possibility of biases, lack of transparency with certain AI algorithms, privacy concerns with the data used for training AI models, and safety and liability issues with AI application in clinical environments. While there has been extensive discussion about the ethics of AI in health care, there has been little dialogue or recommendations as to how to practically address these concerns in health care. In this article, we propose a governance model that aims to not only address the ethical and regulatory issues that arise out of the application of AI in health care, but also stimulate further discussion about gov-ernance of AI in health care.},
author = {Reddy, Sandeep and Allan, Sonia and Coghlan, Simon and Cooper, Paul},
doi = {10.1093/jamia/ocz192},
file = {::},
keywords = {artificial intelligence,ethics,governance framework,healthcare,regulation},
title = {{A governance model for the application of AI in health care}},
url = {https://academic.oup.com/jamia/article-abstract/27/3/491/5612169}
}
@techreport{Baniecki2020a,
abstract = {When analysing a complex system, very often an answer for one question raises new questions. The same law applies to the analysis of Machine Learning (ML) models. One method to explain the model is not enough because different questions and different stakeholders need different approaches. Most of the proposed methods for eXplainable Artificial Intelligence (XAI) focus on a single aspect of model behaviour. However, we cannot sufficiently explain a complex model using a single method that gives only one perspective. Isolated explanations are prone to misunderstanding, which inevitably leads to wrong reasoning. In this paper, we present the problem of model explainability as an interactive and sequential explanatory analysis of a model (IEMA). We introduce the grammar of such interactive explanations. We show how different XAI methods complement each other and why it is essential to juxtapose them together. We argue that without multi-faceted interactive explanation, there will be no understanding nor trust for models. The proposed process derives from the theoretical, algorithmic side of the model explanation and aims to embrace ideas learned through research in cognitive sciences. Its grammar is implemented in the modelStudio framework that adopts interactivity, automation and customisablity as its main traits. This thoughtful design addresses the needs of multiple diverse stakeholders, not only ML practitioners.},
archivePrefix = {arXiv},
arxivId = {2005.00497v1},
author = {Baniecki, Hubert and Biecek, Przemyslaw},
eprint = {2005.00497v1},
file = {::},
keywords = {Analysis {\textperiodcentered},Artificial,Black-,Box,Decision-making,Explanations {\textperiodcentered},Explanatory,Human-,Intelligence {\textperiodcentered},Interactive,Model,Models {\textperiodcentered},Oriented XAI {\textperiodcentered},Xplainable,e},
title = {{THE GRAMMAR OF INTERACTIVE EXPLANATORY MODEL ANALYSIS A PREPRINT}},
url = {https://orcid.org/0000-0001-8423-1823},
year = {2020}
}
@phdthesis{Veldkamp2017,
abstract = {Recent studies have highlighted that not all published findings in the scientific literature are trustworthy, suggesting that currently implemented control mechanisms such as high standards for the reporting of research methods and results, peer review, and replication, are not sufficient. In psychology in particular, solutions are sought to deal with poor reproducibility and replicability of research results. In this dissertation project I considered these problems from the perspective that the scien¬tific enterprise must better recognize the human fallibility of scientists, and I examined potential solutions aimed at dealing with human error and bias in psychological science. First, I studied whether the human fallibility of scientists is actually recognized (Chapter 2). I examined the degree to which scientists and lay people believe in the storybook image of the scientist: the image that scientists are more objective, rational, open-minded, intelligent, honest and communal than other human beings. The results suggested that belief in this storybook image is strong, particularly among scientists themselves. In addition, I found indications that scientists believe that scientists like themselves fit the storybook image better than other scientists. I consider scientist's lack of acknowledgement of their own fallibility problematic, because I believe that critical self-reflection is the first line of defense against potential human error aggravated by confirmation bias, hindsight bias, motivated reasoning, and other human cognitive biases that could affect any professional in their work. Then I zoomed in on psychological science and focused on human error in the use of null the most widely used statistical framework in psychology: hypothesis significance testing (NHST). In Chapters 3 and 4, I examined the prevalence of errors in the reporting of statistical results in published articles, and evaluated a potential best practice to reduce such errors: the so called ‘co-pilot model of statistical analysis'. This model entails a simple code of conduct prescribing that statistical analyses are always conducted independently by at least two persons (typically co-authors). Using statcheck, a software package that is able to quickly retrieve and check statistical results in large sets of published articles, I replicated the alarmingly high error rates found in earlier studies. Although I did not find support for the effectiveness of the co-pilot model in reducing these errors, I proposed several ways to deal with human error in (psychological) research and suggested how the effectiveness of the proposed practices might be studied in future research. Finally, I turned to the risk of bias in psychological science. Psychological data can often be analyzed in many different ways. The often arbitrary choices that researchers face in analyzing their data are called researcher degrees of freedom. Researchers might be tempted to use these researcher degrees of freedom in an opportunistic manner in their pursuit of statistical significance (often called p-hacking). This is problematic because it renders research results unreliable. In Chapter 5 I presented a list of researcher degrees of freedom in psychological studies, focusing on the use of NHST. This list can be used to assess the potential for bias in psychological studies, it can be used in research methods education, and it can be used to examine the effectiveness of a potential solution to restrict oppor¬tunistic use of RDFs: study pre-registration. Pre-registration requires researchers to stipulate in advance the research hypothesis, data collection plan, data analyses, and what will be reported in the paper. Different forms of pre-registration are currently emerging in psychology, mainly varying in terms of the level of detail with respect to the research plan they require researchers to provide. In Chapter 6, I assessed the extent to which current pre-registrations restricted opportunistic use of the researcher degrees of freedom on the list presented in Chapter 5. We found that most pre-registrations were not sufficiently restrictive, but that those that were written following better guidelines and requirements restricted opportunistic use of researcher degrees of freedom considerably better than basic pre-registrations that were written following a limited set of guidelines and requirements. We concluded that better instructions, specific questions, and stricter requirements are necessary in order for pre-registrations to do what they are supposed to do: to protect researchers from their own biases.},
author = {Veldkamp, Coosje L. S.},
doi = {10.17605/OSF.IO/G8CJQ},
file = {::},
isbn = {9789462337527},
school = {Tilburg University},
title = {{The Human Fallibility of Scientists}},
url = {https://psyarxiv.com/g8cjq/},
year = {2017}
}
@inproceedings{Maayan2020,
abstract = {Common secret = = Secret colours + + (assume that mixture separation is expensive) Public transport = = Secret colours + + Common paint Alice Bob Figure 1. Diagrams explain concepts visually in many domains, e.g.: (a) Diffie-Hellman key exchange with colors representing prime multiplication [82]. (b) Linking two views of the Klein 4-group [84]. (c) Unrolling a recurrent LSTM network [58]. (d) Natural numbers as 2D areas in a visual proof [28]. ABSTRACT Conceptual diagrams are used extensively to understand abstract relationships, explain complex ideas, and solve difficult problems. To illustrate concepts effectively, experts find appropriate visual representations and translate concepts into concrete shapes. This translation step is not supported explicitly by current diagramming tools. This paper investigates how domain experts create conceptual diagrams via semi-structured interviews with 18 participants from diverse backgrounds. Our participants create, adapt, and reuse visual representations using both sketches and digital tools. However, they had trouble using current diagramming tools to transition from sketches and reuse components from earlier diagrams. Our participants also expressed frustration with the slow feedback cycles and * Authors contributed equally and names are in alphabetical order.},
annote = {Tool Design with involvement of participants
Natural Diagramming Tools allow Exploration Support, Representation Salience, 
Live Engagement, Vocabulary Correspondance
Conceptual Diagram - Conceptual, Procedural, Meta cognitive
Local object placement, Global diagram layout
Drawing Tools - General Purpose (Illustrator, Figma), Dedicated Diagramming Tools (Lucidchart, Gliffy)
Programming Language (PL) based Tools, Direct Manipulation (DM) Tools
Interviews {\&} Semi structured interviews - abstract concepts and objects
Diagrammers organize reuse libraries by representation
Sketching
Exploration Support - Backtracking, Versioning and Reuse, Reduce unnecessary changes through
improved abstractions
Representation Salience - Track prior representations, Curate reuse libraries, Search for ones created by others
Climbing up (abstraction), Climbing down (Concretization)
Properties of abstraction - Manageable, Scalable, Consolable
Live Engagement - Programming by example, Programming by manipulation, bidirectional programming
Domain Specific Languages
Vocabulary Correspondance - Conceptual diagrams (abstract, topological, domain specific)},
author = {Ma'ayan, Dor and Ni, Wode and Ye, Katherine and Kulkarni, Chinmay and Sunshine, Joshua},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3313831.3376253},
file = {::},
isbn = {9781450367080},
keywords = {Conceptual Diagramming,Diagram Authoring,Information Visualization},
publisher = {ACM},
title = {{How Domain Experts Create Conceptual Diagrams and Implications for Tool Design}},
year = {2020}
}
@article{Tian2017,
abstract = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explores different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.},
archivePrefix = {arXiv},
arxivId = {1708.08559},
author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
eprint = {1708.08559},
file = {::},
isbn = {9781450356381},
keywords = {acm reference format,au-,deep learning,deep neural networks,neuron coverage,self-driving cars,testing,tonomous vehicle},
title = {{DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars}},
url = {http://arxiv.org/abs/1708.08559},
year = {2017}
}
@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
author = {Miller, George A.},
doi = {10.1145/219717.219748},
file = {::},
isbn = {1558602720},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
pmid = {17081734},
title = {{WordNet: a lexical database for English}},
url = {http://portal.acm.org/citation.cfm?doid=219717.219748},
volume = {38},
year = {1995}
}
@techreport{Smith1999,
abstract = {We assessed the frequency, causes and con-reported by 19/40, while unnecessary driving restrictions and employment difficulties were enco-sequences of erroneous diagnosis of epilepsy, and the outcome of patients referred with 'refractory untered by 12/33 and 5/33, respectively. Of those labelled 'refractory epilepsy', 12 did not have epi-epilepsy', by retrospective analysis of the case records of 324 patients. The sample was divided lepsy. Sixteen were rendered seizure-free and 25 significantly improved by the optimal use of anti-into those exposed to anti-epileptic drugs (n=184), of whom 92 were said to have refractory seizures, epileptic drugs or surgery. Diagnostic and management services for patients with suspected and estab-and those who had not received treatment (n= 140). The latter group is reported elsewhere. The lished epilepsy are suboptimal, with psychological and socioeconomic consequences for individual overall misdiagnosis rate was 26.1{\%} (46/184), with incomplete history-taking and misinterpretation of patients. The resulting economic burden on the health and welfare services is probably substantial. the EEG equally responsible. Side-effects were},
author = {Smith, D and Defalla1, B A and Chadwick, D W},
booktitle = {Q J Med},
file = {::},
pages = {15--23},
title = {{The misdiagnosis of epilepsy and the management of refractory epilepsy in a specialist clinic}},
url = {https://academic.oup.com/qjmed/article-abstract/92/1/15/1550396},
volume = {92},
year = {1999}
}
@techreport{Stevenson2018,
abstract = {The aim of this study was to develop methods for detecting the non-stationary periodic characteristics of neonatal electroencephalographic (EEG) seizures by adapting estimates of the correlation both in the time (spike correlation; SC) and time-frequency domain (time-frequency correlation; TFC). These measures were incorporated into a seizure detection algorithm (SDA) based on a support vector machine to detect periods of seizure and non-seizure. The performance of these non-stationary correlation measures was evaluated using EEG recordings from 79 term neonates annotated by three human experts. The proposed measures were highly discriminative for seizure detection (median AUC SC : 0.933 IQR: 0.821-0.975, median AUC TFC : 0.883 IQR: 0.707-0.931). The resultant SDA applied to multi-channel recordings had a median AUC of 0.988 (IQR: 0.931-0.998) when compared to consensus annotations, outperformed two state-of-the-art SDAs (p{\textless}0.001) and was non-inferior to the human expert for 73/79 of neonates.},
author = {Stevenson, Nathan J},
booktitle = {International Journal of Neural Systems},
file = {::},
keywords = {electroencephalography,neonatal seizure detection,nonstationary signal processing,support vector machines,time-frequency distributions},
number = {0},
pages = {1--15},
title = {{TIME-VARYING EEG CORRELATIONS IMPROVE AUTOMATED NEONATAL SEIZURE DETECTION}},
volume = {0},
year = {2018}
}
@article{Henry1986,
abstract = {Family practice literature indicates that survey research methods are frequently used, but that design and analysis considerations are often inadequate. Not surprisingly, then, many professionals believe this type of research is less rigorous and less useful than experimental research. Contrary to this belief, the planning and decision making involved in survey research is as technical as that of other types of research. Ten practical principles for survey research are presented. These principles describe the major issues researchers should address in planning and conducting a questionnaire survey study.},
author = {Henry, R. C. and Zivick, J. D.},
doi = {10.1145/571681.571686},
file = {::},
issn = {02702304},
journal = {The Family practice research journal},
number = {3},
pages = {145--157},
pmid = {3454525},
title = {{Principles of survey research.}},
volume = {5},
year = {1986}
}
@article{Massa2015,
abstract = {Cognitive problems following stroke are typically analysed using either short but relatively uninformative general tests or through detailed but time consuming tests of domain specific deficits (e.g., in language, memory, praxis). Here we present an analysis of neuropsychological deficits detected using a screen designed to fall between other screens by being 'broad' (testing multiple cognitive abilities) but 'shallow' (sampling the abilities briefly, to be time efficient) - the BCoS. Assessment using the Birmingham Cognitive Screen (BCoS) enables the relations between 'domain specific' and 'domain general' cognitive deficits to be evaluated as the test generates an overall cognitive profile for individual patients. We analysed data from 287 patients tested at a sub-acute stage of stroke ({\textless}3 months). Graphical modelling techniques were used to investigate the associative structure and conditional independence between deficits within and across the domains sampled by BCoS (attention and executive functions, language, memory, praxis and number processing). The patterns of deficit within each domain conformed to existing cognitive models. However, these within-domain patterns underwent substantial change when the whole dataset was modelled, indicating that domain-specific deficits can only be understood in relation to linked changes in domain-general processes. The data point to the importance of using over-arching cognitive screens, measuring domain-general as well as domain-specific processes, in order to account for neuropsychological deficits after stroke. The paper also highlights the utility of using graphical modelling to understand the relations between cognitive components in complex datasets.},
author = {Massa, M. Sofia and Wang, Naxian and Bickerton, Wa Ling and Demeyere, Nele and Riddoch, M. Jane and Humphreys, Glyn W.},
doi = {10.1016/j.cortex.2015.06.006},
file = {::},
issn = {19738102},
journal = {Cortex},
keywords = {Assessment,Cognitive impairment,Statistical analysis},
pages = {190--204},
publisher = {Elsevier Srl.},
title = {{On the importance of cognitive profiling: Agraphical modelling analysis of domain-specific and domain-general deficits after stroke}},
volume = {71},
year = {2015}
}
@article{Khurana2018,
author = {Khurana, Udayan and Samulowitz, Horst and Turaga, Deepak},
file = {::},
journal = {ICML 2018 AutoML Workshop},
keywords = {automl,ensembles,feature engineering,reinforcement learning},
title = {{Ensembles with Automated Feature Engineering}},
year = {2018}
}
@article{Schelter2018,
abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.},
annote = {From Duplicate 1 (Automating large-scale data quality verification - Schelter, Sebastian; Lange, Dustin; Schmidt, Philipp; Celikel, Meltem; Biessmann, Felix; Grafberger, Andreas)

Need for increased automation of data validation

Declarativity - Users to spend time on thinking, how their data should look like, Not to worry too much about how to implement the actual quality checks 

High flexibility on data validation

Data validation systems have to acknowledge - as data is being continuously produced

Scalability - Data validation systems should continuously scale to large datasets

Data quality dimensions - extension of the data (data values), intension of the data (the schema)

Completeness - The degree to which an entity includes data required to describe a real world object

Consistency - The degree to which a set of semantic rules are violated, Two types - Intra Relation Constraints (Tshirt size - S, M and L) and Inter Relation Constraints (Customer Id)

Accuracy - The correctness of data, Two types - syntactic and semantic 
Syntactic - Representation of a value with a corresponding definition domain
Semantic - Compares a value with its real world representation 

Unit tests for Data - Declarative definition of data quality constraints

Predictability

Data is seldomly static. Computing metrics for the growing dataset from all snapshots, result in incremental computations. For each feature column, summary statistics (minimum, maximum, mean, standard deviation, approximate quartiles) is computed.

Learning semantics of column and table names
Anomaly Detection - operates on historic time series of data, quality metrics (the ratio of missing values for different versions of a dataset)

Checks and constraints on dataframes
Metrics - size of the dataset, completeness of six columns, compliance for the three satisfies constraints

States Management
Logistic Regression -{\textgreater} SGD Classifier and hashing vectorizer
Experimental evaluation on 50 GB in parquet format, data sources are reddit.com and twitter

From Duplicate 2 (Automating large-scale data quality verification - Schelter, Sebastian; Lange, Dustin; Schmidt, Philipp; Celikel, Meltem; Biessmann, Felix; Grafberger, Andreas)

Need for increased automation of data validation

Declarativity - Users to spend time on thinking, how their data should look like, Not to worry too much about how to implement the actual quality checks 

High flexibility on data validation

Data validation systems have to acknowledge - as data is being continuously produced

Scalability - Data validation systems should continuously scale to large datasets

Data quality dimensions - extension of the data (data values), intension of the data (the schema)

Completeness - The degree to which an entity includes data required to describe a real world object

Consistency - The degree to which a set of semantic rules are violated, Two types - Intra Relation Constraints (Tshirt size - S, M and L) and Inter Relation Constraints (Customer Id)

Accuracy - The correctness of data, Two types - syntactic and semantic 
Syntactic - Representation of a value with a corresponding definition domain
Semantic - Compares a value with its real world representation 

Unit tests for Data - Declarative definition of data quality constraints

Predictability

Data is seldomly static. Computing metrics for the growing dataset from all snapshots, result in incremental computations. For each feature column, summary statistics (minimum, maximum, mean, standard deviation, approximate quartiles) is computed.

Learning semantics of column and table names
Anomaly Detection - operates on historic time series of data, quality metrics (the ratio of missing values for different versions of a dataset)

Checks and constraints on dataframes
Metrics - size of the dataset, completeness of six columns, compliance for the three satisfies constraints

States Management
Logistic Regression -{\textgreater}SGD Classifier and hashing vectorizer
Experimental evaluation on 50 GB in parquet format, data sources are reddit.com and twitter},
author = {Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas},
doi = {10.14778/3229863.3229867},
file = {::},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {12},
pages = {1781--1794},
title = {{Automating large-scale data quality verification}},
volume = {11},
year = {2018}
}
@article{Boogerd2008,
abstract = {In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. Not only can compliance with a set of rules having little impact on the number of faults be considered wasted effort, but it can actually result in an increase in faults, as any modification has a non-zero probability of introducing a fault or triggering a previously concealed one. Therefore, it is important to build a body of empirical knowledge, helping us understand which rules are worthwhile enforcing, and which ones should be ignored in the context of fault reduction. In this paper, we describe two approaches to quantify the relation between rule violations and actual faults, and present empirical data on this relation for the MISRA C 2004 standard on an industrial case study. {\textcopyright}2008 IEEE.},
author = {Boogerd, Cathal and Moonen, Leon},
doi = {10.1109/ICSM.2008.4658076},
file = {::},
isbn = {9781424426140},
journal = {IEEE International Conference on Software Maintenance, ICSM},
pages = {277--286},
title = {{Assessing the value of coding standards: An empirical study}},
year = {2008}
}
@inproceedings{Li2018,
abstract = {Human agents in technical customer support provide users with instructional answers to solve a task. Developing a technical support question answering (QA) system is challenging due to the broad variety of user intents. Moreover, user questions are noisy (for example, spelling mistakes), redundant and have various natural language expresses, which are challenges for QA system to match user queries to corresponding standard QA pair. In this work, we combine question intent categories classification and semantic matching model to filter and select correct answers from a back-end knowledge base. Using a real world user chat-log dataset with 60 intent categories, we observe that while supervised models, perform well on the individual classification tasks. For semantic matching, we add muti-info (answer and product information) into standard question and emphasize context information of user query (cap-tured by GRU) into our model. Experiment results indicate that neural multi-perspective sentence similarity networks outperform baseline models. The precision of semantic matching model is 85{\%}.},
author = {Li, Yang and Miao, Qingliang and Geng, Ji and Alt, Christoph and Schwarzenberg, Robert and Hennig, Leonhard and Hu, Changjian and Xu, Feiyu},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-99495-6_1},
file = {::},
isbn = {9783319994949},
issn = {16113349},
keywords = {Answer selection,Question and Answer,Semantic matching},
pages = {3--15},
publisher = {Springer Verlag},
title = {{Question Answering for Technical Customer Support}},
volume = {11108 LNAI},
year = {2018}
}
@article{AlbertoFernandez2018,
abstract = {The Synthetic Minority Oversampling Technique (SMOTE) preprocessing algorithm is considered "de facto" standard in the framework of learning from imbalanced data. This is due to its simplicity in the design of the procedure, as well as its robustness when applied to different type of problems. Since its publication in 2002, SMOTE has proven successful in a variety of applications from several different domains. SMOTE has also inspired several approaches to counter the issue of class imbalance, and has also significantly contributed to new supervised learning paradigms, including multilabel classification, incremental learning, semi-supervised learning, multi-instance learning, among others. It is standard benchmark for learning from imbalanced data. It is also featured in a number of different software packages - from open source to commercial. In this paper, marking the fifteen year anniversary of SMOTE, we reflect on the SMOTE journey, discuss the current state of affairs with SMOTE, its applications, and also identify the next set of challenges to extend SMOTE for Big Data problems.},
author = {{Alberto Fernandez} and {Salvador Garcia} and {Francisco Herrera} and {Nitesh V. Chawla}},
doi = {10.1613/jair.1.11192},
file = {::},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {863--905},
title = {{SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary}},
url = {https://www.jair.org/index.php/jair/article/view/11192},
volume = {61},
year = {2018}
}
@article{Selic2012,
abstract = {Model-based engineering (MBE) has been touted as a new and substantively different approach to software development, characterized by higher levels of abstraction and automation compared to traditional methods. Despite the availability of published verifiable evidence that it can significantly boost both developer productivity and product quality in industrial projects, adoption of this approach has been surprisingly slow. In this article, we review the causes behind this, both technical and non-technical, and outline what needs to happen for MBE to become a reliable mainstream approach to software development.},
author = {Selic, Bran},
doi = {10.1007/s10270-012-0261-0},
file = {::},
issn = {16191366},
journal = {Software and Systems Modeling},
keywords = {Model-based engineering},
number = {4},
pages = {513--526},
title = {{What will it take? A view on adoption of model-based methods in practice}},
volume = {11},
year = {2012}
}
@article{Kristiansen2016,
abstract = {The Elective Course Student Sectioning (ECSS) problem is a yearly recurrent planning problem at the Danish high schools. The problem is of assigning students to elective classes given their requests such that as many requests are fulfilled and the violations of the soft constraints are minimized. This paper presents an Adaptive Large Neighborhood Search heuristic for the ESCC. The algorithm is applied to 80 real-life instances from Danish high schools and compared with solutions found by using the state-of-the-art MIP solver Gurobi. The algorithm has been implemented in the commercial product Lectio, and is thereby available for approximately 200 high schools in Denmark.},
author = {Kristiansen, Simon and Stidsen, Thomas R.},
doi = {10.1007/s10479-014-1593-7},
file = {::},
issn = {15729338},
journal = {Annals of Operations Research},
keywords = {Adaptive large neighborhood search,Education timetabling,Elective course planning,High school timetabling,Integer programming,Student sectioning},
number = {1},
pages = {99--117},
publisher = {Springer US},
title = {{Elective course student sectioning at Danish high schools}},
url = {http://dx.doi.org/10.1007/s10479-014-1593-7},
volume = {239},
year = {2016}
}
@article{Vartak2018,
abstract = {Machine learning applications have become ubiquitous in a variety of domains. Powering each of these ML applications are one or more machine learning models that are used to make key decisions or compute key quantities. The life-cycle of an ML model starts with data processing, going on to feature engineering, model experimentation, deployment, and maintenance. We call the process of tracking a model across all phases of its life-cycle as model management. In this paper, we discuss the current need for model management and describe MODELDB, the first open-source model management system developed at MIT. We also discuss the changing landscape and growing challenges and opportunities in managing models.},
author = {Vartak, Manasi and Madden, Samuel},
file = {::},
journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
title = {{ModelDB : Opportunities and Challenges in Managing Machine Learning Models}},
url = {http://sites.computer.org/debull/A18dec/A18DEC-CD.pdf{\#}page=18},
year = {2018}
}
@article{Shochat2019,
abstract = {{\textless}p{\textgreater}: Organizational changes in shift scheduling provide rare opportunities for field studies aimed at investigating the effects of such changes on health and wellbeing. We studied the effects of a transition from 8-hour (8-h) to 12-hour (12-h) shift rosters in 39 airline ground crew managers on burnout, sleep quality, and sleepiness. Assessments were collected during the 8-h and were repeated three months after the transition to 12-h shift rosters. These assessments included the Shirom-Melamed Burnout Measure (SMBM), the Pittsburgh Sleep Quality Index (PSQI), actigraphy, the Karolinska Sleepiness Scale (KSS) completed hourly during one day and two night shifts, and caffeine intake. Findings demonstrated lower burnout, improved sleep quality, improved quality of naps, and increased afternoon sleepiness during the 12-h day shift. Napping was reported during 12-h night shifts by 36{\%} of the sample. In nappers, increased night shift sleepiness was associated with increased caffeine intake on 8- and 12-h shifts. In non-nappers, increased night shift sleepiness was associated with decreased caffeine intake on the 8-h shift only. Change in shift length affects other structural and behavioral parameters in the workplace, making it challenging to isolate distinct characteristics of the two rosters and their relative effects on study outcomes. Individual differences in adaptation to shiftwork may also play a role.{\textless}/p{\textgreater}},
author = {Shochat and Hadish-Shogan and {Banin Yosipof} and Recanati and Tzischinsky},
doi = {10.3390/clockssleep1020020},
file = {::},
issn = {2624-5175},
journal = {Clocks {\&} Sleep},
keywords = {burnout,caffeine,napping,shift length,shiftwork,sleep quality,sleepiness},
month = {apr},
number = {2},
pages = {226--239},
publisher = {MDPI AG},
title = {{Burnout, Sleep, and Sleepiness during Day and Night Shifts in Transition from 8- to 12-Hour Shift Rosters among Airline Ground Crew Managers}},
url = {https://www.mdpi.com/2624-5175/1/2/20},
volume = {1},
year = {2019}
}
@techreport{ZavidParvez2014,
author = {{Zavid Parvez}, Mohammad and Paul, Manoranjan},
file = {::},
title = {{Neurocomputing Epileptic Seizure Detection by Analyzing EEG Signals using Different Transformation Techniques}},
year = {2014}
}
@article{Myers2018,
abstract = {Our objective was to critically evaluate the literature surrounding heart rate variability (HRV) in people with epilepsy and to make recommendations as to how future research could be directed to facilitate and accelerate integration into clinical practice. We reviewed relevant HRV publications including those involving human subjects with seizures. HRV has been studied in patients with epilepsy for more than 30 years and, overall, patients with epilepsy display altered interictal HRV, suggesting a shift in autonomic balance toward sympathetic dominance. This derangement appears more severe in those with temporal lobe epilepsy and drug-resistant epilepsy. Normal diurnal variation in HRV is also disturbed in at least some people with epilepsy, but this aspect has received less study. Some therapeutic interventions, including vagus nerve stimulation and antiepileptic medications, may partially normalize altered HRV, but studies in this area are sometimes contradictory. During seizures, the changes in HRV may be complex, but the general trend is toward a further increase in sympathetic overactivity. Research in HRV in people with epilepsy has been limited by inconsistent experimental protocols and studies that are often underpowered. HRV measurement has the potential to aid clinical epilepsy management in several possible ways. HRV may be useful in predicting which patients are likely to benefit from surgical interventions such as vagus nerve stimulation and focal cerebral resection. As well, HRV could eventually have utility as a biomarker of risk for sudden unexpected death in epilepsy (SUDEP). However, at present, the inconsistent measurement protocols used in research are hindering translation into clinical practice. A minimum protocol for HRV evaluation, to be used in all studies involving epilepsy patients, is necessary to eventually allow HRV to become a useful tool for clinicians. We propose a straightforward protocol, involving 5-minute measurements of root mean square of successive differences in wakefulness and light sleep.},
author = {Myers, Kenneth A. and Sivathamboo, Shobi and Perucca, Piero},
doi = {10.1111/epi.14587},
file = {::;::},
issn = {00139580},
journal = {Epilepsia},
keywords = {autonomic,epilepsy,heart rate variability,sudden unexpected death in epilepsy,sympathetic},
month = {dec},
number = {12},
pages = {2169--2178},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Heart rate variability measurement in epilepsy: How can we move from research to clinical practice?}},
url = {http://doi.wiley.com/10.1111/epi.14587},
volume = {59},
year = {2018}
}
@article{Tunwattanapong2016,
abstract = {OBJECTIVE To determine the effectiveness of neck and shoulder stretching exercises for relief neck pain among office workers. DESIGN Randomized controlled trial. SETTING An outpatient setting. PARTICIPANTS A total of 96 subjects with moderate-to-severe neck pain (visual analogue score ⩾5/10) for ⩾3 months. INTERVENTIONS All participants received an informative brochure indicating the proper position and ergonomics to be applied during daily work. The treatment group received the additional instruction to perform neck and around shoulder stretching exercises two times/day, five days/week during four weeks. MAIN OUTCOMES Pain, neck functions, and quality of life were evaluated at baseline and week 4 using pain visual analogue scale, Northwick Park Neck Pain Questionnaire, and Short Form-36, respectively. RESULTS Both groups had comparable baseline data. All outcomes were improved significantly from baseline. When compared between groups, the magnitude of improvement was significantly greater in the treatment group than in the control group (-1.4; 95{\%} CI: -2.2, -0.7 for visual analogue scale; -4.8; 95{\%} CI: -9.3, -0.4 for Northwick Park Neck Pain Questionnaire; and 14.0; 95{\%} CI: 7.1, 20.9 for physical dimension of the Short Form-36). Compared with the patients who performed exercises {\textless}3 times/week, those who exercised ⩾3 times/week yielded significantly greater improvement in neck function and physical dimension of quality of life scores (p = 0.005 and p = 0.018, respectively). CONCLUSION A regular stretching exercise program performed for four weeks can decrease neck and shoulder pain and improve neck function and quality of life for office workers who have chronic moderate-to-severe neck or shoulder pain.},
author = {Tunwattanapong, Punjama and Kongkasuwan, Ratcharin and Kuptniratsaikul, Vilai},
doi = {10.1177/0269215515575747},
issn = {1477-0873},
journal = {Clinical rehabilitation},
keywords = {Neck and shoulder pain,exercise program,office workers,stretching},
month = {jan},
number = {1},
pages = {64--72},
pmid = {25780258},
publisher = {SAGE Publications Ltd},
title = {{The effectiveness of a neck and shoulder stretching exercise program among office workers with neck pain: a randomized controlled trial.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25780258},
volume = {30},
year = {2016}
}
@article{Kangasraasio2017,
abstract = {An important problem for HCI researchers is to estimate the parameter values of a cognitive model from behavioral data. This is a difficult problem, because of the substantial complexity and variety in human behavioral strategies. We report an investigation into a new approach using approximate Bayesian computation (ABC) to condition model parameters to data and prior knowledge. As the case study we examine menu interaction, where we have click time data only to infer a cognitive model that implements a search behaviour with parameters such as fixation duration and recall probability. Our results demonstrate that ABC (i) improves estimates of model parameter values, (ii) enables meaningful comparisons between model variants, and (iii) supports fitting models to individual users. ABC provides ample opportunities for theoretical HCI research by allowing principled inference of model parameter values and their uncertainty. Copyright is held by the owner/author(s).},
archivePrefix = {arXiv},
arxivId = {1612.00653},
author = {Kangasr{\"{a}}{\"{a}}si{\"{o}}, Antti and Athukorala, Kumaripaba and Howes, Andrew and Corander, Jukka and Kaski, Samuel and Oulasvirta, Antti},
doi = {10.1145/3025453.3025576},
eprint = {1612.00653},
file = {::},
isbn = {9781450346559},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Approximate Bayesian computation,Cognitive models in HCI,Computational rationality,Inverse modeling},
pages = {1295--1306},
title = {{Inferring cognitive models from data using approximate Bayesian computation}},
volume = {2017-May},
year = {2017}
}
@inproceedings{Oman,
author = {Oman, P. and Hagemeister, J.},
booktitle = {Proceedings Conference on Software Maintenance 1992},
doi = {10.1109/ICSM.1992.242525},
isbn = {0-8186-2980-0},
issn = {null},
keywords = {software maintenance,software met,software metrics},
month = {nov},
pages = {337--344},
publisher = {IEEE Comput. Soc. Press},
title = {{Metrics for assessing a software system's maintainability}},
url = {http://ieeexplore.ieee.org/document/242525/},
year = {1992}
}
@article{Mael2015,
abstract = {The purpose of this article is to develop a testable model of both the antecedents and consequences of workplace boredom. This model is needed because recent evidence suggests that boredom in the workplace is on the rise, despite the apparent reduction in boring, monotonous jobs, which have traditionally been seen as the primary causes of boredom. To develop this model, we first clarify the construct of boredom and distinguish it from other related constructs in the psychological literature. We put forth a typology of boredom that we view as more realistic than a single definition. We then present a model of workplace boredom that integrates past research with a more contemporary approach based on societal trends and individual differences in susceptibility to boredom. Based on this model, we offer a number of research propositions as well as suggestions for potential ways of decreasing workplace boredom.},
author = {Mael, Fred and Jex, Steve},
doi = {10.1177/1059601115575148},
file = {::},
issn = {1059-6011},
journal = {Group {\&} Organization Management},
keywords = {counterproductive work behavior,engagement,well-being},
month = {apr},
number = {2},
pages = {131--159},
publisher = {SAGE Publications Inc.},
title = {{Workplace Boredom}},
url = {http://journals.sagepub.com/doi/10.1177/1059601115575148},
volume = {40},
year = {2015}
}
@misc{Song2018,
abstract = {IEEE Context: Software defect prediction (SDP) is an important challenge in the field of software engineering, hence much research work has been conducted, most notably through the use of machine learning algorithms. However, class-imbalance typified by few defective components and many non-defective ones is a common occurrence causing difficulties for these methods. Imbalanced learning aims to deal with this problem and has recently been deployed by some researchers, unfortunately with inconsistent results.},
annote = {Application to software defects

Journal, 27 data sets, 7 classifiers, 7 types of input metrics and 17 imbalancedlearning methods 

Uses Weka as the programming framework

Structure:
- Intro
- Related work
- Method
- Measures
- Algorithm Evaluation
- Statistical Methods
- Software Metrics
- Data Sets
- Experimental Results and analysis
- RQs
- Threats to Validity


It's the effect size, stupid: What effect size is and why it is important,

Dominance statistics: Ordinal analyses to answer ordinal questions

Ordinal analysis of behavioral data

Social sciences and Psychology have a lot of stats based work.},
author = {Song, Qinbao and Guo, Yuchen and Shepperd, Martin},
booktitle = {IEEE Transactions on Software Engineering},
doi = {10.1109/TSE.2018.2836442},
file = {::},
issn = {00985589},
keywords = {Bagging,Boosting,Computer bugs,Machine learning algorithms,Measurement,Software,Software defect prediction,bug prediction,effect size,imbalance ratio,imbalanced learning},
title = {{A Comprehensive Investigation of the Role of Imbalanced Learning for Software Defect Prediction}},
year = {2018}
}
@article{Hofmeister2019,
abstract = {Developers spend the majority of their time reading code, a process in which identifier names play a key role. Although many identifier naming styles exist, they often lack an empirical basis and it is not clear whether short or long identifier names facilitate comprehension. In this paper, we investigate the effect of different identifier naming styles (single letters, abbreviations, and words) on program comprehension. We conducted an experimental study with 72 professional C{\#} developers who had to locate defects in source code snippets. We used a within-subjects design, such that each developer worked with all three versions of identifier naming styles, and we measured the time it took them to find a defect. We found that word identifiers led to a 19{\%} increase in speed to find defects compared to meaningless single letters and abbreviations, but we did not find a difference between letters and abbreviations. The results of our study suggest that code is more difficult to comprehend when it contains only letters and abbreviations as identifier names. Words as identifier names facilitate program comprehension and may help to save costs and improve software quality.},
author = {Hofmeister, Johannes C. and Siegmund, Janet and Holt, Daniel V.},
doi = {10.1007/s10664-018-9621-x},
file = {::},
isbn = {1066401896},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Defect detection,Identifier names,Professional C{\#} developers,Program comprehension,Psychology,Software quality},
number = {1},
pages = {417--443},
publisher = {Empirical Software Engineering},
title = {{Shorter identifier names take longer to comprehend}},
volume = {24},
year = {2019}
}
@inproceedings{Pannell2018,
author = {Pannell, Zachary and Ramachandran, Bhuvaneswari and Snider, Dallas},
booktitle = {2018 IEEE Texas Power and Energy Conference, TPEC 2018},
doi = {10.1109/TPEC.2018.8312089},
file = {::},
isbn = {9781538610060},
keywords = {data mining,load flow,power system analysis computing,power system faults,power system stability},
title = {{Machine learning approach to solving the transient stability assessment problem}},
year = {2018}
}
@article{Agarwal2018,
abstract = {—With the passage of recent federal legislation many medical institutions are now responsible for reaching target hospital readmission rates. Chronic diseases account for many hospital readmissions and Chronic Obstructive Pulmonary Disease has been recently added to the list of diseases for which the United States government penalizes hospitals incurring excessive readmissions. Though there have been efforts to statistically predict those most in danger of readmission, few have focused primarily on unstructured clinical notes. We have proposed a framework which uses Natural Language Processing to analyze clinical notes and predict readmission. Many algorithms within the field of data mining and machine learning exist, so a framework for component selection is created to select the best components. Na{\"{i}}ve Bayes using Chi-Squared feature selection offers an AUC of 0.690 while maintaining fast computational times. Keywords—Natural language processing, Medical information systems, Decision support systems, Data mining, Feature Extraction},
author = {Agarwal, Ankur and Baechle, Christopher and Behara, Ravi and Zhu, Xingquan},
doi = {10.1109/JBHI.2017.2684121},
file = {::},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Data mining,decision support systems,feature extraction,medical information systems,natural language processing},
month = {mar},
number = {2},
pages = {588--596},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Natural Language Processing Framework for Assessing Hospital Readmissions for Patients with COPD}},
volume = {22},
year = {2018}
}
@article{SUN2009,
abstract = {Classification of data with imbalanced class distribution has encountered a significant drawback of the performance attainable by most standard classifier learning algorithms which assume a relatively balanced class distribution and equal misclassification costs. This paper provides a review of the classification of imbalanced data regarding: the application domains; the nature of the problem; the learning difficulties with standard classifier learning algorithms; the learning objectives and evaluation measures; the reported research solutions; and the class imbalance problem in the presence of multiple classes.},
annote = {Overview of the classificaiton of imbalanced data, the emphasis is on binary classificaiton problems.},
author = {SUN, YANMIN and WONG, ANDREW K. C. and KAMEL, MOHAMED S.},
doi = {10.1142/s0218001409007326},
isbn = {0218-0014},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
title = {{CLASSIFICATION OF IMBALANCED DATA: A REVIEW}},
year = {2009}
}
@inproceedings{Biswas2019,
author = {Biswas, Sumon and Islam, Md Johirul and Huang, Yijia and Rajan, Hridesh},
booktitle = {2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2019.00086},
editor = {Storey, Margaret-Anne D and Adams, Bram and Haiduc, Sonia},
file = {::},
isbn = {978-1-7281-3412-3},
month = {may},
pages = {577--581},
publisher = {IEEE},
title = {{Boa Meets Python: A Boa Dataset of Data Science Software in Python Language}},
url = {https://ieeexplore.ieee.org/document/8816757/},
year = {2019}
}
@inproceedings{Papamichail2020,
author = {Papamichail, Aggelos and Zarras, Apostolos V. and Vassiliadis, Panos},
booktitle = {SOFSEM 2020: Theory and Practice of Computer Science},
doi = {10.1007/978-3-030-38919-2_35},
file = {::},
isbn = {9783030389185},
issn = {16113349},
keywords = {coding styles,naming conventions,sql programming},
pages = {429--440},
title = {{Do People Use Naming Conventions in SQL Programming?}},
url = {http://link.springer.com/10.1007/978-3-030-38919-2{\_}35},
year = {2020}
}
@misc{Kanagarajan2018,
abstract = {{\textcopyright}2018 Springer Science+Business Media, LLC, part of Springer Nature The question answering system is a major area in information retrieval field to get the appropriate answers to the user query instead of list of documents. Nowadays huge amount of documents uploaded in the Internet every day and the extraction of required information from those documents is a challenging and tedious task. In this paper, POS-tagger based question pattern analysis has applied for recognize the question type. Also introduce the semantic words based answer generator algorithm to extract semantic similar sentences for user query using Wordnet from the knowledgebase. The cuckoo search optimization algorithm has applied with the semantic words based answer generator algorithm to improve accuracy of retrieved sentences. The proposed algorithm is experimented with the benchmark datasets, obtained results are compared and found that outer performs than other optimization algorithms.},
author = {Kanagarajan, Karpagam and Arumugam, Saradha},
booktitle = {Cluster Computing},
doi = {10.1007/s10586-018-2054-x},
file = {::},
issn = {15737543},
keywords = {Cuckoo search optimization,Information retrieval,Optimization techniques,POS-tagger,Question answering system,Semantic similarity,Wordnet},
month = {feb},
pages = {1--11},
publisher = {Springer New York LLC},
title = {{Intelligent sentence retrieval using semantic word based answer generation algorithm with cuckoo search optimization}},
year = {2018}
}
@article{Ralph2019,
abstract = {Software engineering is increasingly concerned with theory because the foundational knowledge comprising theories provides a crucial counterpoint to the practical knowledge expressed through methods and techniques. Fortunately, much guidance is available for generating and evaluating theories for explaining why things happen (variance theories). Unfortunately, little guidance is available concerning theories for explaining how things happen (process theories), or theories for analyzing and understanding situations (taxonomies). This paper therefore attempts to clarify the nature and functions of process theories and taxonomies in software engineering research, and to synthesize methodological guidelines for their generation and evaluation. It further advances the key insight that most process theories are taxonomies with additional propositions, which helps inform their evaluation. The proposed methodological guidance has many benefits: it provides a concise summary of existing guidance from reference disciplines, it adapts techniques from reference disciplines to the software engineering context, and it promotes approaches that better facilitate scientific consensus.},
author = {Ralph, Paul},
doi = {10.1109/TSE.2018.2796554},
file = {::},
issn = {19393520},
journal = {IEEE Transactions on Software Engineering},
keywords = {Research methodology,action research,case study,experiment,framework,grounded theory,guidelines,model,process theory,questionnaire,taxonomy,theory for analysis,theory for understanding},
number = {7},
pages = {712--735},
publisher = {IEEE},
title = {{Toward Methodological Guidelines for Process Theories and Taxonomies in Software Engineering}},
volume = {45},
year = {2019}
}
@article{Wojciechowski2017,
abstract = {In this paper we describe results of an experimental study where we checked the impact of various difficulty factors in imbalanced data sets on the performance of selected classifiers applied alone or combined with several preprocessing methods. In the study we used artificial data sets in order to systematically check factors such as dimensionality, class imbalance ratio or distribution of specific types of examples (safe, borderline, rare and outliers) in the minority class. The results revealed that the latter factor was the most critical one and it exacerbated other factors (in particular class imbalance). The best classification performance was demonstrated by non-symbolic classifiers, particular by k-NN classifiers (with 1 or 3 neighbors - 1NN and 3NN, respectively) and by SVM. Moreover, they benefited from different preprocessing methods - SVM and 1NN worked best with undersampling, while oversampling was more beneficial for 3NN.},
annote = {Created a data generator to produce data that represents the different types of challenges. 

I wonder if we can create a simple text generator to handle class imbalance.},
author = {Wojciechowski, Szymon and Wilk, Szymon},
doi = {10.1515/fcds-2017-0007},
file = {::},
journal = {Foundations of Computing and Decision Sciences},
keywords = {difficulty factors,imbalanced data,learning,preprocessing methods},
number = {2},
pages = {149--176},
title = {{Difficulty Factors and Preprocessing in Imbalanced Data Sets: An Experimental Study on Artificial Data}},
volume = {42},
year = {2017}
}
@inproceedings{Guo2017,
abstract = {Confidence calibration-the problem of predicting probability estimates representative of the true correctness likelihood-is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datascts. Our analysis and experiments not only offer insights into neural net-work learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling-a single-parameter variant of Piatt Scaling-is surprisingly effective at calibrating predictions.},
archivePrefix = {arXiv},
arxivId = {1706.04599},
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
eprint = {1706.04599},
file = {::},
isbn = {9781510855144},
pages = {1321--1330},
title = {{On calibration of modern neural networks}},
volume = {70},
year = {2017}
}
@article{Abdi2016,
abstract = {Class imbalance problem is quite pervasive in our nowadays human practice. This problem basically refers to the skewness in the data underlying distribution which, in turn, imposes many difficulties on typical machine learning algorithms. To deal with the emerging issues arising from multi-class skewed distributions, existing efforts are mainly divided into two categories: model-oriented solutions and data-oriented techniques. Focusing on the latter, this paper presents a new over-sampling technique which is inspired by Mahalanobis distance. The presented over-sampling technique, called MDO (Mahalanobis Distance-based Over-sampling technique), generates synthetic samples which have the same Mahalanobis distance from the considered class mean as other minority class examples. By preserving the covariance structure of the minority class instances and intelligently generating synthetic samples along the probability contours, new minority class instances are modelled better for learning algorithms. Moreover, MDO can reduce the risk of overlapping between different class regions which are considered as a serious challenge in multi-class problems. Our theoretical analyses and empirical observations across wide spectrum multi-class imbalanced benchmarks indicate that MDO is the method of choice by offering statistical superior MAUC and precision compared to the popular over-sampling techniques},
annote = {Journal: 20 multi-clas benchmark data sets 

Also uses Weka.

Not good on high dimensions because it takes a long time to run. They recommend looking at feature reduction techniques i.e. PCA},
author = {Abdi, Lida and Hashemi, Sattar},
doi = {10.1109/TKDE.2015.2458858},
file = {::},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Mahalanobis distance,Multi-class imbalance problems,over-sampling techniques},
number = {1},
pages = {238--251},
publisher = {IEEE},
title = {{To Combat Multi-Class Imbalanced Problems by Means of Over-Sampling Techniques}},
volume = {28},
year = {2016}
}
@inproceedings{Abdelkader2020,
author = {Abdelkader, Hala},
booktitle = {35th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
doi = {10.1145/3324884.3415281},
file = {::},
isbn = {9781450367684},
pages = {1164--1166},
publisher = {ACM},
title = {{Towards Robust Production Machine Learning Systems: Managing Dataset Shift}},
year = {2020}
}
@article{Lombrozo2010,
abstract = {Both philosophers and psychologists have argued for the existence of distinct kinds of explanations, including teleological explanations that cite functions or goals, and mechanistic explanations that cite causal mechanisms. Theories of causation, in contrast, have generally been unitary, with dominant theories focusing either on coun-terfactual dependence or on physical connections. This paper argues that both approaches to causation are psychologically real, with different modes of explanation promoting judgments more or less consistent with each approach. Two sets of experiments isolate the contributions of counterfactual dependence and physical connections in causal ascriptions involving events with people, artifacts, or biological traits, and manipulate whether the events are construed teleologically or mechanistically. The findings suggest that when events are construed teleologically, causal ascriptions are sensitive to counterfactual dependence and relatively insensitive to the presence of physical connections, but when events are construed mechanistically, causal ascriptions are sensitive to both counterfac-tual dependence and physical connections. The conclusion introduces an account of causation, an ''exportable dependence theory," that provides a way to understand the contributions of physical connections and teleology in terms of the functions of causal ascriptions.},
author = {Lombrozo, Tania},
doi = {10.1016/j.cogpsych.2010.05.002},
file = {::},
journal = {Cognitive Psychology},
keywords = {Causal mechanism,Causation,Counterfactual dependence,Double prevention,Explanation,Functional explanation,Functions,Intentions,Late preemption,Teleological explanation},
pages = {303--332},
title = {{Causal-explanatory pluralism: How intentions, functions, and mechanisms influence causal ascriptions}},
url = {www.elsevier.com/locate/cogpsych},
volume = {61},
year = {2010}
}
@article{Sarkar2020,
author = {Sarkar, Advait and Borghouts, Judith W. and Iyer, Anusha and Khullar, Sneha and Canton, Christian and Hermans, Felienne and Gordon, Andrew D. and Williams, Jack},
doi = {10.1145/3334480.3382807},
file = {::},
isbn = {9781450368193},
pages = {1--9},
title = {{Spreadsheet Use and Programming Experience: An Exploratory Survey}},
year = {2020}
}
@article{VanGoethem2011,
abstract = {{\textless}p{\textgreater}Musical experiences are often reported to influence emotions ( Juslin {\&} V{\"{a}}stfj{\"{a}}ll, 2008 ; Sloboda, O'Neill, {\&} Ivaldi, 2001 ): people consciously and unconsciously use music to change, create, maintain or enhance their emotions and moods (affect) on a daily basis for their personal benefit ( DeNora, 1999 ; Schramm, 2005 ). This is known as affect regulation. However, existing research has not yet answered questions of how music regulates affect, especially beyond the expressive properties of music ( Meyer, 1956 ). The aims of the studies presented here were to investigate (a) how music functions to regulate affect, (b) which affects it regulates, and (c) whether music listening can be considered a successful affect regulation device. A one-week diary study with interviews and a three-week diary study were conducted. The main findings were: (1) music helps through broader affect regulation strategies like distraction, introspection, and active coping; music can for example distract someone from the affect or situation, or help to think about the affect or situation in a rational way; (2) music plays a major role in creating happiness and relaxation; (3) music overall is a successful regulation device with a range of underlying mechanisms helping different strategies. The current paper is a valuable addition to the existing literature and provides several new insights into the function of music for affect regulation in everyday life. The insight gained into which strategies and underlying mechanisms are involved when music is used for affect regulation might be used for the benefit of people's emotional wellbeing.{\textless}/p{\textgreater}},
author = {van Goethem, Annelies and Sloboda, John},
doi = {10.1177/1029864911401174},
issn = {1029-8649},
journal = {Musicae Scientiae},
keywords = {affect regulation,everyday life,music listening,strategies and mechanisms},
month = {jul},
number = {2},
pages = {208--228},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{The functions of music for affect regulation}},
url = {http://journals.sagepub.com/doi/10.1177/1029864911401174},
volume = {15},
year = {2011}
}
@book{Ju2008,
abstract = {An important challenge in designing ubiquitous computing experiences is negotiating transitions between explicit and implicit interaction, such as how and when to provide users with notifications. While the paradigm of implicit interaction has important benefits, it is also susceptible to difficulties with hidden modes, unexpected action, and misunderstood intent. To address these issues, this work presents a framework for implicit interaction and applies it to the design of an interactive whiteboard application called Range. Range is a public interactive whiteboard designed to support co-located, ad-hoc meetings. It employs proximity sensing capability to proactively transition between display and authoring modes, to clear space for writing, and to cluster ink strokes. We show how the implicit interaction techniques of user reflection (how systems indicate to users what they perceive or infer), system demonstration (how systems indicate what they are doing), and override (how users can interrupt or stop a proactive system action) can prevent, mitigate, and correct errors in the whiteboard's proactive behaviors. These techniques can be generalized to improve the designs of a wide array of ubiquitous computing experiences.},
author = {Ju, Wendy and Lee, Brian A and Klemmer, Scott R},
file = {::},
isbn = {9781605580074},
keywords = {ACM Classification Keywords H52 [Information Inter,Author Keywords Implicit interaction,foreground/background,interaction styles,proactive,proxemics,ubiquitous computing,whiteboards},
title = {{Range: Exploring Implicit Interaction through Electronic Whiteboard Design}},
url = {http://delivery.acm.org/10.1145/1470000/1460569/p17-ju.pdf?ip=139.132.188.30{\&}id=1460569{\&}acc=ACTIVE SERVICE{\&}key=65D80644F295BC0D.B242904781996EBA.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1550547969{\_}2325e4f9453662b789faa1044dccca65},
year = {2008}
}
@article{Khademian2013,
abstract = {BACKGROUND Interprofessional teamwork is considered as the key to improve the quality of patient management in critical settings such as trauma emergency departments, but it is not fully conceptualized in these areas to guide practice. The aim of this article is to explore interprofessional teamwork and its improvement strategies in trauma emergency departments. MATERIALS AND METHODS Participants of this qualitative study consisted of 11 nurses and 6 supervisors recruited from the emergency departments of a newly established trauma center using purposive sampling. Data were generated using two focus group and six in-depth individual interviews, and analyzed using qualitative content analysis. RESULTS Interprofessional teamwork attributes and improvement strategies were emerged in three main themes related to team, context, and goal. These were categorized as the effective presence of team members, role definition in team framework, managerial and physical context, effective patient management, and overcoming competing goals. CONCLUSIONS Interprofessional teamwork in trauma emergency departments is explained as interdependence of team, context, and goal; so, it may be improved by strengthening these themes. The findings also provide a basis to evaluate, teach, and do research on teamwork.},
author = {Khademian, Zahra and Sharif, Farkhondeh and Tabei, Seyed Ziaadin and Bolandparvaz, Shahram and Abbaszadeh, Abbas and Abbasi, Hamid Reza},
issn = {1735-9066},
journal = {Iranian journal of nursing and midwifery research},
month = {jul},
number = {4},
pages = {333--9},
pmid = {24403932},
publisher = {Wolters Kluwer -- Medknow Publications},
title = {{Teamwork improvement in emergency trauma departments.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24403932 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3872871},
volume = {18},
year = {2013}
}
@article{Strzelczyk2017,
abstract = {Objective: Super-refractory status epilepticus (SRSE) is a severe condition in which a patient in status epilepticus (SE) for ≥24 h does not respond to first-, second-, or third-line therapy. The economic impact of SRSE treatment remains unclear. A health insurance research database was used for a population-based estimation of SRSE-associated inpatient costs, length of stay, and mortality in Germany. Methods: An algorithm using International Classification of Diseases, 10th Edition coding and treatment parameters identified and classified patients in a German statutory health insurance database covering admissions from 2008 to 2013 as having refractory SE (RSE) or SRSE. Admissions data in our study refer to these classifications. Associated patient data included costs, procedures, and demographics. Results: The algorithm identified 2,585 (all type) SE admissions, classified as 1,655 nonrefractory SE (64{\%}), 592 (22.9{\%}) RSE, and 338 (13.1{\%}) SRSE, producing database incidence rates of 15.0 in 100,000, 5.2 in 100,000, and 3.0 in 100,000 per year, respectively. Median cost per admission was €4,063 for nonrefractory SE, €4,581 (p {\textless} 0.001) for RSE, and €32,706 (p {\textless} 0.001) for SRSE. Median length of stay varied significantly between 8 days (mean = 13.6) in nonrefractory SE, 14 days in RSE, and up to 37 days in SRSE. Discharge mortality increased from 9.6{\%} in nonrefractory SE to 15.0{\%} (p {\textless} 0.001) in RSE and 39.9{\%} (p {\textless} 0.001) in SRSE. Significance: This study evaluated the hospital treatment costs associated with admissions classified by the algorithm as SRSE in Germany. SRSE represented 13{\%} of all SE admissions, but resulted in 56{\%} of all SE-related costs. The lack of approved treatments and limited number of evidence-based treatment guidelines highlight the need for further evaluations of the SRSE burden of illness and the potential for further optimization of treatments for SRSE.},
author = {Strzelczyk, Adam and Ansorge, Sonja and Hapfelmeier, Jana and Bonthapally, Vijayveer and Erder, M. Haim and Rosenow, Felix},
doi = {10.1111/epi.13837},
file = {::},
issn = {15281167},
journal = {Epilepsia},
keywords = {Cost,Economic burden,Epilepsy,Intensive care,Mortality,Super-refractory status epilepticus},
month = {sep},
number = {9},
pages = {1533--1541},
publisher = {Blackwell Publishing Inc.},
title = {{Costs, length of stay, and mortality of super-refractory status epilepticus: A population-based study from Germany}},
volume = {58},
year = {2017}
}
@article{Tanjung2020,
author = {Tanjung, Handrizal and Ahmad, Noraziah and Abdalla, Ahmed and Alla},
month = {mar},
title = {{SPREAD SPECTRUM PROCESS USING DIRECT SEQUENCE SPREAD SPECTRUM (DSSS) AND FREQUENCY HOPPING SPREAD SPECTRUM (FHSS)}},
year = {2020}
}
@article{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {::},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Thomas2017,
abstract = {New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities (“crowds”) as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential—and limitations—of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine “technologies” are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing.},
author = {Thomas, James and Noel-Storr, Anna and Marshall, Iain and Wallace, Byron and McDonald, Steve and Mavergames, Chris and Glasziou, Paul and Shemilt, Ian and Synnot, Anneliese and Turner, Tari and Elliott, Julian and Agoritsas, Thomas and Hilton, John and Perron, Caroline and Akl, Elie and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and Kolakowsky-Hayner, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Salanti, Georgia and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch{\"{u}}nemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Shemilt, Ian and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and MacLehose, Harriet and Simmonds, Mark and Cossi, Marie Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Soares-Weiser, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Garc{\'{i}}a, Laura Mart{\'{i}}nez and Danko, Kristen and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Egan, Cathy and Tritton, Roger and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Meerpohl, Joerg and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Featherstone, Robin and Mondello, Stefania and Foxlee, Ruth and Morley, Richard and van Valkenhoef, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and {Yepes Nu{\~{n}}ez}, Juan Jos{\'{e}} and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
doi = {10.1016/j.jclinepi.2017.08.011},
file = {::},
issn = {18785921},
journal = {Journal of Clinical Epidemiology},
keywords = {Automation,Citizen science,Crowdsourcing,Machine learning,Systematic review,Text mining},
pages = {31--37},
pmid = {28912003},
title = {{Living systematic reviews: 2. Combining human and machine effort}},
volume = {91},
year = {2017}
}
@article{Kay2016,
abstract = {A core tradition of HCI lies in the experimental evaluation of the effects of techniques and interfaces to determine if they are useful for achieving their purpose. However, our individual analyses tend to stand alone, and study results rarely accrue in more precise estimates via meta-analysis: in a literature search, we found only 56 meta-analyses in HCI in the ACM Digital Library, 3 of which were published at CHI (often called the top HCI venue). Yet meta-analysis is the gold standard for demonstrating robust quantitative knowledge. We treat this as a user-centered design problem: the failure to accrue quantitative knowledge is not the users' (i.e. researchers') failure, but a failure to consider those users' needs when designing statistical practice. Using simulation, we compare hypothetical publication worlds following existing frequentist against Bayesian practice. We show that Bayesian analysis yields more precise effects with each new study, facilitating knowledge accrual without traditional meta-analyses. Bayesian practices also allow more principled conclusions from small-n studies of novel techniques. These advantages make Bayesian practices a likely better fit for the culture and incentives of the field. Instead of admonishing ourselves to spend resources on larger studies, we propose using tools that more appropriately analyze small studies and encourage knowledge accrual from one study to the next. We also believe Bayesian methods can be adopted from the bottom up without the need for new incentives for replication or meta-analysis. These techniques offer the potential for a more user-(i.e. researcher-) centered approach to statistical analysis in HCI.},
author = {Kay, Matthew and Nelson, Gregory L. and Hekler, Eric B.},
doi = {10.1145/2858036.2858465},
file = {::},
isbn = {9781450333627},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Bayesian statistics,Effect size,Estimation,Meta-analysis,Replication,Small studies},
pages = {4521--4532},
title = {{Researcher-centered design of statistics: Why Bayesian statistics better fit the culture and incentives of HCI}},
year = {2016}
}
@article{Carr2016,
abstract = {Introduction Office employees are exposed to hazardous levels of sedentary work. Interventions that integrate health promotion and health protection elements are needed to advance the health of sedentary workers. This study tested an integrated intervention on occupational sedentary/physical activity behaviors, cardiometabolic disease biomarkers, musculoskeletal discomfort, and work productivity. Design Two-group, RCT. Data were collected between January and August 2014. Setting/participants Overweight/obese adults working in sedentary desk jobs were randomized to: (1) a health protection-only group (HPO, n=27); or (2) an integrated health protection/health promotion group (HP/HP, n=27). Intervention HPO participants received an ergonomic workstation optimization intervention and three e-mails/week promoting rest breaks and posture variation. HP/HP participants received the HPO intervention plus access to a seated activity permissive workstation. Main outcome measures Occupational sedentary and physical activity behaviors (primary outcomes), cardiometabolic health outcomes, musculoskeletal discomfort, and work productivity (secondary outcomes) were measured at baseline and post-intervention (16 weeks). Results The HP/HP group increased occupational light intensity physical activity over the HPO group and used the activity permissive workstations 50 minutes/work day. Significant associations were observed between activity permissive workstation adherence and improvements in several cardiometabolic biomarkers (weight, total fat mass, resting heart rate, body fat percentage) and work productivity outcomes (concentration at work, days missed because of health problems). Conclusions The HP/HP group increased occupational physical activity and greater activity permissive workstation adherence was associated with improved health and work productivity outcomes. These findings are important for employers interested in advancing the well-being of sedentary office workers. Trial registration This study is registered at www.clinicaltrials.gov NCT02071420.},
author = {Carr, Lucas J. and Leonhard, Christoph and Tucker, Sharon and Fethke, Nathan and Benzo, Roberto and Gerr, Fred},
doi = {10.1016/j.amepre.2015.06.022},
file = {::},
issn = {18732607},
journal = {American Journal of Preventive Medicine},
month = {jan},
number = {1},
pages = {9--17},
publisher = {Elsevier Inc.},
title = {{Total Worker Health Intervention Increases Activity of Sedentary Workers}},
volume = {50},
year = {2016}
}
@inproceedings{Amershi2019,
abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components-models may be 'entangled' in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
doi = {10.1109/ICSE-SEIP.2019.00042},
file = {::},
isbn = {978-1-7281-1760-7},
keywords = {Artifical Intelligence,Data,Machine Learning,Process,Software Engineering},
month = {may},
pages = {291--300},
publisher = {IEEE},
title = {{Software Engineering for Machine Learning: A Case Study}},
url = {https://ieeexplore.ieee.org/document/8804457/},
year = {2019}
}
@article{Bird2020,
abstract = {In this work, we show the success of unsupervised transfer learning between Electroencephalographic (brainwave) classification and Electromyographic (muscular wave) domains with both MLP and CNN methods. To achieve this, signals are measured from both the brain and forearm muscles and EMG data is gathered from a 4-class gesture classification experiment via the Myo Armband, and a 3-class mental state EEG dataset is acquired via the Muse EEG Headband. A hyperheuristic multi-objective evolutionary search method is used to find the best network hyperparameters. We then use this optimised topology of deep neural network to classify both EMG and EEG signals, attaining results of 84.76{\%} and 62.37{\%} accuracy, respectively. Next, when pre-trained weights from the EMG classification model are used for initial distribution rather than random weight initialisation for EEG classification, 93.82{\%}(+29.95) accuracy is reached. When EEG pre-trained weights are used for initial weight distribution for EMG, 85.12{\%} (+0.36) accuracy is achieved. When the EMG network attempts to classify EEG, it outperforms the EEG network even without any training (+30.25{\%} to 82.39{\%} at epoch 0), and similarly the EEG network attempting to classify EMG data outperforms the EMG network (+2.38{\%} at epoch 0). All transfer networks achieve higher pre-training abilities, curves, and asymptotes, indicating that knowledge transfer is possible between the two signal domains. In a second experiment with CNN transfer learning, the same datasets are projected as 2D images and the same learning process is carried out. In the CNN experiment, EMG to EEG transfer learning is found to be successful but not vice-versa, although EEG to EMG transfer learning did exhibit a higher starting classification accuracy. The significance of this work is due to the successful transfer of ability between models trained on two different biological signal domains, reducing the need for building more computationally complex models in future research.},
author = {Bird, Jordan J. and Kobylarz, Jhonatan and Faria, Diego R. and Ekart, Aniko and Ribeiro, Eduardo P.},
doi = {10.1109/ACCESS.2020.2979074},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Applied machine learning,EEG,EMG,biological signal processing,knowledge adaptation,neural networks,transfer learning},
pages = {54789--54801},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Cross-Domain MLP and CNN Transfer Learning for Biological Signal Processing: EEG and EMG}},
volume = {8},
year = {2020}
}
@inproceedings{Best2007a,
author = {Best, Christopher and Eidman, Craig and Crane, Peter and Kam, Clinton and Skinner, Michael and Hasenbosch, Sam and Burchat, Eleanore and Finch, Melanie and Shanahan, Christopher and Zamba, Mitch},
booktitle = {Twelfth Australian Aeronautical Conference},
file = {::},
number = {7104},
title = {{Exercise Pacific Link 2 : Distributed Training for Air Battle Managers}},
year = {2007}
}
@article{Barnett2019,
abstract = {In this viewpoint we describe the architecture of, and design rationale for, a new software platform designed to support the conduct of digital phenotyping research studies. These studies seek to collect passive and active sensor signals from participants' smartphones for the purposes of modelling and predicting health outcomes, with a specific focus on mental health. We also highlight features of the current research landscape that recommend the coordinated development of such platforms, including the significant technical and resource costs of development, and we identify specific considerations relevant to the design of platforms for digital phenotyping. In addition, we describe trade-offs relating to data quality and completeness versus the experience for patients and public users who consent to their devices being used to collect data. We summarize distinctive features of the resulting platform, InSTIL (Intelligent Sensing to Inform and Learn), which includes universal (ie, cross-platform) support for both iOS and Android devices and privacy-preserving mechanisms which, by default, collect only anonymized participant data. We conclude with a discussion of recommendations for future work arising from learning during the development of the platform. The development of the InSTIL platform is a key step towards our research vision of a population-scale, international, digital phenotyping bank. With suitable adoption, the platform will aggregate signals from large numbers of participants and large numbers of research studies to support modelling and machine learning analyses focused on the prediction of mental illness onset and disease trajectories.},
author = {Barnett, Scott and Huckvale, Kit and Christensen, Helen and Venkatesh, Svetha and Mouzakis, Kon and Vasa, Rajesh},
doi = {10.2196/16399},
issn = {1438-8871},
journal = {Journal of Medical Internet Research},
month = {nov},
number = {11},
pages = {e16399},
title = {{Intelligent Sensing to Inform and Learn (InSTIL): A Scalable and Governance-Aware Platform for Universal, Smartphone-Based Digital Phenotyping for Research and Clinical Applications}},
url = {https://www.jmir.org/2019/11/e16399},
volume = {21},
year = {2019}
}
@misc{Trudel2018,
abstract = {Purpose of Review: Psychosocial stressors at work from the demand-latitude and effort-reward imbalance models are adverse exposures affecting about 20–25{\%} of workers in industrialized countries. This review aims to summarize evidence on the effect of these stressors on blood pressure (BP). Recent Findings: Three systematic reviews have recently documented the effect of these psychosocial stressors at work on BP. Among exposed workers, statistically significant BP increases ranging from 1.5 to 11 mmHg have been observed in prospective studies using ambulatory BP (ABP). Recent studies using ABP have shown a deleterious effect of these psychosocial stressors at work on masked hypertension as well as on blood pressure control in pharmacologically treated patients. Summary: Evidence on the effect of these psychosocial stressors on BP supports the relevance to tackle these upstream factors for primary prevention and to reduce the burden of poor BP control. There is a need for increased public health and clinical awareness of the occupational etiology of high BP, hypertension, and poor BP control.},
author = {Trudel, Xavier and Brisson, Chantal and Gilbert-Ouimet, Mah{\'{e}}e and Milot, Alain},
booktitle = {Current Cardiology Reports},
doi = {10.1007/s11886-018-1070-z},
file = {::},
issn = {15343170},
keywords = {Ambulatory blood pressure,Effort-reward imbalance,Hypertension control,Job strain,Masked hypertension,Psychosocial stressors at work},
month = {dec},
number = {12},
pages = {127},
publisher = {Current Medicine Group LLC 1},
title = {{Psychosocial Stressors at Work and Ambulatory Blood Pressure}},
volume = {20},
year = {2018}
}
@inproceedings{Ulinskas2018,
abstract = {Human daytime fatigue has many signs (tiredness, sleepiness, lack of vigilance). The on-set of fatigue during working hours can be dangerous for people of several professions such as lorry drivers or industry workers, however even for office workers it may lead to serious errors. Timely recognition of daytime fatigue using simple computer based tests can reduce fatigue related accidents or errors in workplace. In this paper, we analyze the use of keystroke data derived by typing on computer keyboard to recognize the state of an increased fatigue. Using specific key press and release timing information from text input tasks, we achieve an average daytime fatigue recognition accuracy of 98.11{\%} when only three qualitative classes of daytime fatigue (low, medium and high) are considered.},
author = {Ulinskas, Mindaugas and Dama{\v{s}}evi{\v{c}}ius, Robertas and Maskeliunas, Rytis and Wo{\'{z}}niak, Marcin},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2018.04.094},
file = {::},
issn = {18770509},
keywords = {assisted living,biometrics,fatigue recognition,office ergonomics},
month = {jan},
pages = {947--952},
publisher = {Elsevier B.V.},
title = {{Recognition of human daytime fatigue using keystroke data}},
volume = {130},
year = {2018}
}
@article{Mucci2016,
abstract = {Hypertension (HT) is a long-term medical condition characterized by persistently elevated blood pressure (BP) in the arterial vessels. Although HT initially is an asymptomatic condition, it chronically evolves into a major risk factor for cardiovascular, cerebrovascular, and renal diseases that, in turn, represent crucial causes of morbidity and mortality in industrialized countries. HT is a complex disorder that is estimated to affect more than a quarter of the world's adult population. It is classified on the basis of both its pathophysiology (primary and secondary HT) and on the resting BP values (elevated systolic, diastolic, and pulse pressure). It originates from a complicated interaction of genes and several environmental risk factors including aging, smoking, lack of exercise, overweight and obesity, elevated salt intake, stress, depression, and anxiety. Anxiety and depressive disorders are the most commonly diagnosed mental disorders, affecting millions of people each year and impairing every aspect of everyday life, both of them characterized by affective, cognitive, psychomotor, and neurovegetative symptoms. Moreover, work-related stress has been considered as an important risk factor for HT and cardiovascular diseases (CVDs). Although different authors have investigated and suggested possible relations between HT, stress, anxiety, and depression during the last decades, a full understanding of the underlying pathophysiological mechanisms has not been satisfactorily achieved, especially in young adults. The aim of this study was to investigate the impact of anxiety and workrelated stress in the development of HT amongst young health care profession students and the possible related consequences of early CVDs.},
author = {Mucci, Nicola and Giorgi, Gabriele and Ceratti, Stefano De Pasquale and Fiz-P{\'{e}}rez, Javier and Mucci, Federico and Arcangeli, Giulio},
doi = {10.3389/fpsyg.2016.01682},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Anxiety,Blood pressure,Health care professions,Health promotion,Occupational medicine,Students,Work-related stress,Workplace},
month = {oct},
pages = {1--10},
publisher = {Frontiers Media S.A.},
title = {{Anxiety, stress-related factors, and blood pressure in young adults}},
volume = {7},
year = {2016}
}
@article{Waddill-Goad2019,
abstract = {The purpose of raising awareness about the topics of stress, fatigue, and nurse burnout is twofold: (1) to recognize it exists and (2) to explore the options for mitigation strategies. The profession of nursing is prone to experience stress due to the intense nature of the work. Immense pressures from both internal and external sources add to the complexity of nearly every professional role in nursing. Recognizing and addressing the potentially negative impact of stress and the signs of overwork is imperative so stress, fatigue and burnout are not the result. A more intentional approach to nursing work relative to the thoughtful design of systems, forming efficient work processes, attaining more reasonable workloads, and establishing boundaries for work-life balance are key attributes for success. Care and compassion are foundational to nursing practice. However, caring for oneself to be at their best to care for others is rarely a priority in today's world. Nurses must especially be mindful of the consequences of their work, must strive to lead healthy lives, and be an example for those they may have the privilege to lead in health-care organizations and others.},
author = {Waddill-Goad, Suzanne M.},
doi = {10.1016/j.jradnu.2018.10.005},
file = {::},
issn = {15559912},
journal = {Journal of Radiology Nursing},
keywords = {Fatigue,Nurse burnout,Stress},
month = {mar},
number = {1},
pages = {44--46},
publisher = {Elsevier Inc.},
title = {{Stress, Fatigue, and Burnout in Nursing}},
volume = {38},
year = {2019}
}
@article{Tsiouris2018,
abstract = {The electroencephalogram (EEG) is the most prominent means to study epilepsy and capture changes in electrical brain activity that could declare an imminent seizure. In this work, Long Short-Term Memory (LSTM) networks are introduced in epileptic seizure prediction using EEG signals, expanding the use of deep learning algorithms with convolutional neural networks (CNN). A pre-analysis is initially performed to find the optimal architecture of the LSTM network by testing several modules and layers of memory units. Based on these results, a two-layer LSTM network is selected to evaluate seizure prediction performance using four different lengths of preictal windows, ranging from 15 min to 2 h. The LSTM model exploits a wide range of features extracted prior to classification, including time and frequency domain features, between EEG channels cross-correlation and graph theoretic features. The evaluation is performed using long-term EEG recordings from the open CHB-MIT Scalp EEG database, suggest that the proposed methodology is able to predict all 185 seizures, providing high rates of seizure prediction sensitivity and low false prediction rates (FPR) of 0.11–0.02 false alarms per hour, depending on the duration of the preictal window. The proposed LSTM-based methodology delivers a significant increase in seizure prediction performance compared to both traditional machine learning techniques and convolutional neural networks that have been previously evaluated in the literature.},
author = {Tsiouris, Κostas and Pezoulas, Vasileios C. and Zervakis, Michalis and Konitsiotis, Spiros and Koutsouris, Dimitrios D. and Fotiadis, Dimitrios I.},
doi = {10.1016/j.compbiomed.2018.05.019},
file = {::},
issn = {18790534},
journal = {Computers in Biology and Medicine},
keywords = {Deep learning,EEG,Epilepsy,LSTM model,Seizure prediction},
month = {aug},
pages = {24--37},
publisher = {Elsevier Ltd},
title = {{A Long Short-Term Memory deep learning network for the prediction of epileptic seizures using EEG signals}},
volume = {99},
year = {2018}
}
@article{Pradhan2018,
abstract = {From an accessibility perspective, voice-controlled, home-based intelligent personal assistants (IPAs) have the potential to greatly expand speech interaction beyond dictation and screen reader output. To examine the accessibility of off-the-shelf IPAs (e.g., Amazon Echo) and to understand how users with disabilities are making use of these devices, we conducted two exploratory studies. The first, broader study is a content analysis of 346 Amazon Echo reviews that include users with disabilities, while the second study more specifically focuses on users with visual impairments, through interviews with 16 current users of home-based IPAs. Findings show that, although some accessibility challenges exist, users with a range of disabilities are using the Amazon Echo, including for unexpected cases such as speech therapy and support for caregivers. Richer voice-based applications and solutions to support discoverability would be particularly useful to users with visual impairments. These findings should inform future work on accessible voice-based IPAs.},
author = {Pradhan, Alisha and Mehta, Kanika and Findlater, Leah},
doi = {10.1145/3173574.3174033},
isbn = {9781450356206},
keywords = {Author Keywords Intelligent personal assistants,HCI),accessibility,conversational interfaces,disability ACM Classification Keywords H5m Informa,speech},
title = {{"Accessibility Came by Accident": Use of Voice-Controlled Intelligent Personal Assistants by People with Disabilities}},
url = {https://doi.org/10.1145/3173574.3174033},
year = {2018}
}
@article{Gupta2016,
abstract = {A doodle is a simple drawing that is usually made to pass the time during a boring meeting, classroom lecture, or a prolonged telephonic conversation. Almost everyone has seen a doodle somewhere and many people have made such drawings. Doodling may not be of much interest to the general public as it is perceived to be a sign of disinterest, inattentiveness, or reverie. However, the act of doodling is of enduring interest to scientists as they believe that doodling research might actually reveal significant insights about the functioning of the subconscious mind. The widely held misconception about doodling as being just a way to ease one's boredom is all set to change since the findings of some recent researchers have shown that doodling might actually aid one's memory and recall performance. We hope that this review will instigate further research into this hitherto uncharted domain so that the real connotation of this seemingly mundane act can be decisively established.},
author = {Gupta, Sharat},
doi = {10.4103/0971-8990.182097},
issn = {0971-8990},
journal = {Journal of Mental Health and Human Behaviour},
number = {1},
pages = {16},
publisher = {Medknow},
title = {{Doodling: The artistry of the roving metaphysical mind}},
url = {http://www.jmhhb.org/text.asp?2016/21/1/16/182097},
volume = {21},
year = {2016}
}
@article{Stephanidis2019,
abstract = {This article aims to investigate the Grand Challenges which arise in the current and emerging landscape of rapid technological evolution towards more intelligent interactive technologies, coupled with increased and widened societal needs, as well as individual and collective expectations that HCI, as a discipline, is called upon to address. A perspective oriented to humane and social values is adopted, formulating the challenges in terms of the impact of emerging intelligent interactive technologies on human life both at the individual and societal levels. Seven Grand Challenges are identified and presented in this article: Human-Technology Symbiosis; Human-Environment Interactions; Ethics, Privacy and Security; Well-being, Health and Eudaimonia; Accessibility and Universal Access; Learning and Creativity; and Social Organization and Democracy. Although not exhaustive, they summarize the views and research priorities of an international interdisciplinary group of experts, reflecting different scientific perspectives, methodological approaches and application domains. Each identified Grand Challenge is analyzed in terms of: concept and problem definition; main research issues involved and state of the art; and associated emerging requirements. BACKGROUND This article presents the results of the collective effort of a group of 32 experts involved in the community of the Human Computer Interaction International (HCII) Conference series. The group's collaboration started in early 2018 with the collection of opinions from all group members, each asked to independently list and describe five HCI grand challenges. During a one-day meeting held on the 20th July 2018 in the context of the HCI International 2018 Conference in Las Vegas, USA, the identified topics were debated and challenges were formulated in terms of the impact of emerging intelligent interactive technologies on human life both at the individual and societal levels. Further analysis and consolidation led to a set of seven Grand Challenges presented herein. This activity was organized and supported by the HCII Conference series.},
author = {Stephanidis, Constantine and Salvendy, Gavriel and Antona, Margherita and Chen, Jessie Y.C. C and Dong, Jianming and Duffy, Vincent G. and Fang, Xiaowen and Fidopiastis, Cali and Fragomeni, Gino and Fu, Limin Paul and Guo, Yinni and Harris, Don and Ioannou, Andri and Jeong, Kyeong ah (Kate) and Konomi, Shin'ichi Shin'ichi and Kr{\"{o}}mker, Heidi and Kurosu, Masaaki and Lewis, James R. and Marcus, Aaron and Meiselwitz, Gabriele and Moallem, Abbas and Mori, Hirohiko and {Fui-Hoon Nah}, Fiona and Ntoa, Stavroula and Rau, Pei Luen Patrick and Schmorrow, Dylan and Siau, Keng and Streitz, Norbert and Wang, Wentao and Yamamoto, Sakae and Zaphiris, Panayiotis and Zhou, Jia},
doi = {10.1080/10447318.2019.1619259},
file = {::},
issn = {15327590},
journal = {International Journal of Human-Computer Interaction},
title = {{Seven HCI Grand Challenges}},
volume = {7318},
year = {2019}
}
@article{Bakke2011,
abstract = {A key feature of relational database applications is managing plural relationships - one-to-many and many-to-many - between entities. However, since it is often infeasible to adopt or develop a new database application for any given schema at hand, information workers instead turn to spreadsheets, which lend themselves poorly to schemas requiring multiple related entity sets. In this paper, we propose to reduce the cost-usability gap between spreadsheets and tailor-made relational database applications by extending the spreadsheet paradigm to let the user establish relationships between rows in related worksheets as well as view and navigate the hierarchical cell structure that arises as a result. We present Related Worksheets, a spreadsheet-like prototype application, and evaluate it with a screencast-based user study on 36 Mechanical Turk workers. First-time users of our software were able to solve lookup-type query tasks with the same or higher accuracy as subjects using Microsoft Excel, in one case 40{\%} faster on average. Copyright 2011 ACM.},
author = {Bakke, Eirik and Karger, David R. and Miller, Robert C.},
doi = {10.1145/1978942.1979313},
file = {::},
isbn = {9781450302289},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Design},
pages = {2541--2550},
title = {{A spreadsheet-based user interface for managing plural relationships in structured data}},
year = {2011}
}
@article{Arunkumar2017,
abstract = {Electroencephalogram (EEG) is the recording of the electrical activity of the brain which can be used to identify different disease conditions. In the case of a partial epilepsy, some portions of the brain is affected and the EEG measured from that portions are called as Focal EEG and the EEG measured from other regions is termed as Non Focal EEG. The identification of Focal EEG assists the doctors in finding the epileptogenic focus and thereby go for surgical removal of those portions of the brain for those who are having drug resistant epilepsy. In this work, we have proposed a classification methodology to classify Focal and Non Focal EEG. We used the Bern Barcelona database and used entropies such as Approximate entropy (ApEn), Sample entropy (SampEn) and Reyni's entropy as features. These features were fed into six different classifiers such as Na{\"{i}}ve Bayes (NBC), Radial Basis function (RBF), Support Vector Machines (SVM), KNN classifier, Non-Nested Generalized Exemplars classifier (NNge) and Best First Decision Tree (BFDT) classifier. It was found that NNge classifier gave the highest accuracy of 98{\%}, sensitivity of 100{\%} and specificity of 96{\%}, which is the highest comparing to other methods in the literature. In addition to the above, the maximum computation time of our features is 0.054 seconds which opens the window for real time processing. Thus our method can be written as a handy software tool towards assisting the physician.},
author = {Arunkumar, A. and Ramkumar, R. K. and Venkatraman, V. V. and Abdulhay, Enas and {Lawrence Fernandes}, Steven and Kadry, Seifedine and Segal, Sophia and N., Arunkumar and K., Ramkumar and V., Venkatraman and Abdulhay, Enas and {Lawrence Fernandes}, Steven and Kadry, Seifedine and Segal, Sophia and Arunkumar, A. and Ramkumar, R. K. and Venkatraman, V. V. and Abdulhay, Enas and {Lawrence Fernandes}, Steven and Kadry, Seifedine and Segal, Sophia and N., Arunkumar and K., Ramkumar and V., Venkatraman and Abdulhay, Enas and {Lawrence Fernandes}, Steven and Kadry, Seifedine and Segal, Sophia},
doi = {10.1016/j.patrec.2017.05.007},
file = {::},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classification,EEG signal,Entropy,Epilepsy},
month = {jul},
pages = {112--117},
publisher = {Elsevier B.V.},
title = {{Classification of focal and non focal EEG using entropies}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865517301472},
volume = {94},
year = {2017}
}
@article{Huang2017,
abstract = {The problem of probabilistic modeling and inference, at a high-level, can be viewed as constructing a (model, query, inference) tuple, where an inference algorithm implements a query on a model. Notably, the derivation of inference al-gorithms can be a difficult and error-prone task. Hence, re-searchers have explored how ideas from probabilistic pro-gramming can be applied. In the context of constructing these tuples, probabilistic programming can be seen as tak-ing a language-based approach to probabilistic modeling and inference. For instance, by using (1) appropriate languages for expressing models and queries and (2) devising infer-ence techniques that operate on encodings of models (and queries) as program expressions, the task of inference can be automated. In this paper, we describe a compiler that transforms a probabilistic model written in a restricted modeling language and a query for posterior samples given observed data into a Markov Chain Monte Carlo (MCMC) inference algorithm that implements the query. The compiler uses a sequence of intermediate languages (ILs) that guide it in gradually and successively refining a declarative specification of a proba-bilistic model and the query into an executable MCMC in-ference algorithm. The compilation strategy produces com-posable MCMC algorithms for execution on a CPU or GPU.},
author = {Huang, Daniel and Tristan, Jean-Baptiste and Morrisett, Greg},
doi = {10.1145/3140587.3062375},
file = {::},
isbn = {9781450349888},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {all or part of,classroom use is granted,copies are not made,guages,intermediate lan-,markov-chain monte carlo kernels,or,or distributed,or hard copies of,permission to make digital,probabilistic programming,this work for personal,without fee provided that},
number = {6},
pages = {111--125},
title = {{Compiling Markov chain Monte Carlo algorithms for probabilistic modeling}},
volume = {52},
year = {2017}
}
@inproceedings{Simmons2020,
abstract = {Background: Meeting the growing industry demand for Data Science requires cross-disciplinary teams that can translate machine learning research into production-ready code. Software engineering teams value adherence to coding standards as an indication of code readability, maintainability, and developer expertise. However, there are no large-scale empirical studies of coding standards focused specifically on Data Science projects. Aims: This study investigates the extent to which Data Science projects follow code standards. In particular, which standards are followed, which are ignored, and how does this differ to traditional software projects? Method: We compare a corpus of 1048 Open-Source Data Science projects to a reference group of 1099 non-Data Science projects with a similar level of quality and maturity. Results: Data Science projects suffer from a significantly higher rate of functions that use an excessive numbers of parameters and local variables. Data Science projects also follow different variable naming conventions to non-Data Science projects. Conclusions: The differences indicate that Data Science codebases are distinct from traditional software codebases and do not follow traditional software engineering conventions. Our conjecture is that this may be because traditional software engineering conventions are inappropriate in the context of Data Science projects.},
archivePrefix = {arXiv},
arxivId = {2007.08978},
author = {Simmons, Andrew J. and Barnett, Scott and Rivera-Villicana, Jessica and Bajaj, Akshat and Vasa, Rajesh},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1145/3382494.3410680},
eprint = {2007.08978},
file = {::},
isbn = {9781450375801},
keywords = {all or part of,code conventions,code quality,code smells,code style,data science,machine learning,open-source software,or,or hard copies of,permission to make digital,this work for personal},
title = {{A large-scale comparative analysis of Coding Standard conformance in Open-Source Data Science projects}},
year = {2020}
}
@article{WINKLER2018,
abstract = {Background According to cross-sectional and acute experimental evidence, reducing sitting time should improve cardiometabolic health risk biomarkers. Furthermore, the improvements obtained may depend on whether sitting is replaced with standing or ambulatory activities. Based on data from the Stand Up Victoria multicomponent workplace intervention, we examined this issue using compositional data analysis - a method that can examine and compare all activity changes simultaneously. Methods Participants receiving the intervention (n = 136 ≥ 0.6 full-time equivalent desk-based workers, 65{\%} women, mean ± SD age = 44.6 ± 9.1 yr from seven worksites) were asked to improve whole-of-day activity by standing up, sitting less, and moving more. Their changes in the composition of daily waking hours (activPAL-assessed sitting, standing, and stepping) were quantified then tested for associations with concurrent changes in cardiometabolic risk (CMR) scores and 14 biomarkers concerning body composition, glucose, insulin, and lipid metabolism. Analyses were by mixed models, accounting for clustering (3 months, n = 105-120; 12 months, n = 80-97). Results Sitting reduction was significantly (P {\textless} 0.05) associated only with lower systolic blood pressure at 3 months, and with CMR scores, weight, body fat, waist circumference, diastolic blood pressure, and fasting triglycerides, total/HDL cholesterol, and insulin at 12 months. Significant differences between standing and stepping were only observed for systolic blood pressure and insulin; both favored stepping. However, replacing sitting with standing was significantly associated only with improvements in CMR scores, whereas replacing sitting with stepping was significantly associated with CMR scores and six biomarkers. Conclusions Improvements in several cardiometabolic health risk biomarkers were significantly associated with sitting reductions that occurred in a workplace intervention. The greatest degree and/or widest range of cardiometabolic benefits appeared to occur with long-term changes, and when increasing ambulatory activities. Trial Registration: ACTRN1211000742976.},
author = {WINKLER, ELISABETH A. H. and CHASTIN, SEBASTIEN and EAKIN, ELIZABETH G. and OWEN, NEVILLE and LAMONTAGNE, ANTHONY D. and MOODIE, MARJ and DEMPSEY, PADDY C. and KINGWELL, BRONWYN A. and DUNSTAN, DAVID W. and HEALY, GENEVIEVE N.},
doi = {10.1249/MSS.0000000000001453},
issn = {0195-9131},
journal = {Medicine {\&} Science in Sports {\&} Exercise},
keywords = {AMBULATION,BIOMARKERS,COMPOSITIONAL DATA ANALYSIS (CODA),INTERVENTION,SEDENTARY},
month = {mar},
number = {3},
pages = {516--524},
publisher = {Lippincott Williams and Wilkins},
title = {{Cardiometabolic Impact of Changing Sitting, Standing, and Stepping in the Workplace}},
url = {http://insights.ovid.com/crossref?an=00005768-201803000-00015},
volume = {50},
year = {2018}
}
@article{Kuhne2006,
abstract = {Abstract With the recent trend to model driven engineering a common understanding of basic notions such as model and metamodel becomes a pivotal issue. Even though these notions have been in widespread use for quite a while, there is still little consensus about when exactly it is appropriate to use them. The aim of this article is to start establishing a consensus about generally acceptable terminology. Its main contributions are the distinction between two fundamentally different kinds of model roles, i.e. token model versus type model (The terms type and token have been introduced by C.S. Peirce, 18391914.), a formal notion of metaness, and the consideration of generalization as yet another basic relationship between models. In particular, the recognition of the fundamental difference between the above mentioned two kinds of model roles is crucial in order to enable communication among the model driven engineering community that is free of both unnoticed misunderstandings and unnecessary disagreement.},
annote = {From Duplicate 1 (Matters of (meta-) modeling - K{\"{u}}hne, Thomas)
And Duplicate 3 (Matters of (meta-) modeling - K{\"{u}}hne, Thomas)

"any relation between two entities, which is going to be used to build up a meta-entity must not be transitive.},
author = {K{\"{u}}hne, Thomas},
doi = {10.1007/s10270-006-0017-9},
file = {::},
issn = {16191366},
journal = {Software and Systems Modeling},
keywords = {Metamodeling,Model driven engineering,Modeling,Token model,Type model},
number = {4},
pages = {369--385},
title = {{Matters of (meta-) modeling}},
volume = {5},
year = {2006}
}
@inproceedings{Ohtake:2019vi,
abstract = {Intelligent APIs, such as Google Cloud Vision or Amazon Rekognition, are becoming evermore pervasive and easily accessible to developers to build applications. Because of the stochastic nature that machine learning entails and disparate datasets used in their training, the output from different APIs varies over time, with low reliability in some cases when compared against each other. Merging multiple unreliable API responses from multiple vendors may increase the reliability of the overall response, and thus the reliability of the intelligent end-product. We introduce a novel methodology – inspired by the proportional representation used in electoral systems – to merge outputs of different intelligent computer vision APIs provided by multiple vendors. Experiments show that our method outperforms both naive merge methods and traditional proportional representation methods by 0.015 F-measure.},
address = {Daejeon, Republic of Korea},
author = {Ohtake, Tomohiro and Cummaudo, Alex and Abdelrazek, Mohamed and Vasa, Rajesh and Grundy, John},
booktitle = {Proceedings of the 19th International Conference on Web Engineering},
doi = {10.1007/978-3-030-19274-7\_28},
isbn = {978-3-03-019273-0},
issn = {1611-3349},
keywords = {Application programming interfaces,Artificial intelligence,Data integration,Supervised learning,Web services},
month = {jun},
pages = {391--406},
publisher = {Springer},
title = {{Merging intelligent API responses using a proportional representation approach}},
year = {2019}
}
@article{Noble2009,
author = {Noble, William Stafford},
doi = {10.1371/journal.pcbi.1000424},
editor = {Lewitter, Fran},
file = {::},
issn = {1553-7358},
journal = {PLoS Computational Biology},
month = {jul},
number = {7},
pages = {e1000424},
title = {{A Quick Guide to Organizing Computational Biology Projects}},
volume = {5},
year = {2009}
}
@article{Sculley2014,
abstract = {Machine learning offers a fantastically powerful toolkit for building complex sys-tems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is re-markably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several ma-chine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
file = {::},
journal = {NIPS 2014 Workshop on Software Engineering for Machine Learning (SE4ML)},
title = {{Machine Learning : The High-Interest Credit Card of Technical Debt}},
year = {2014}
}
@article{Hong2016,
abstract = {We propose thresholding as an approach to deal with class imbalance. We define the concept of thresholding as a process of determining a decision boundary in the presence of a tunable parameter. The threshold is the maximum value of this tunable parameter where the conditions of a certain decision are satisfied. We show that thresholding is applicable not only for linear classifiers but also for non-linear classifiers. We show that this is the implicit assumption for many approaches to deal with class imbalance in linear classifiers. We then extend this paradigm beyond linear classification and show how non-linear classification can be dealt with under this umbrella framework of thresholding. The proposed method can be used for outlier detection in many real-life scenarios like in manufacturing. In advanced manufacturing units, where the manufacturing process has matured over time, the number of instances (or parts) of the product that need to be rejected (based on a strict regime of quality tests) becomes relatively rare and are defined as outliers. How to detect these rare parts or outliers beforehand? How to detect combination of conditions leading to these outliers? These are the questions motivating our research. This paper focuses on prediction of outliers and conditions leading to outliers using classification. We address the problem of outlier detection using classification. The classes are good parts (those passing the quality tests) and bad parts (those failing the quality tests and can be considered as outliers). The rarity of outliers transforms this problem into a class-imbalanced classification problem.},
annote = {From Duplicate 1 (Dealing with Class Imbalance using Thresholding - Hong, Charmgil; Ghosh, Rumi; Srinivasan, Soundar)
And Duplicate 2 (Dealing with Class Imbalance using Thresholding - Hong, Charmgil; Ghosh, Rumi; Srinivasan, Soundar)

Provides a good introduction for working on a paper that is part of an industry case study with experiments.},
archivePrefix = {arXiv},
arxivId = {1607.02705},
author = {Hong, Charmgil and Ghosh, Rumi and Srinivasan, Soundar},
doi = {10.475/123},
eprint = {1607.02705},
file = {::},
isbn = {1234567245},
keywords = {class imbalance,classification,decision trees,scrap detection},
title = {{Dealing with Class Imbalance using Thresholding}},
url = {http://arxiv.org/abs/1607.02705},
volume = {1},
year = {2016}
}
@article{Munaiah2017,
abstract = {Software forges like GitHub host millions of repositories. Software engineering researchers have been able to take advantage of such a large corpora of potential study subjects with the help of tools like GHTorrent and Boa. However, the simplicity in querying comes with a caveat: there are limited means of separating the signal (e.g. repositories containing engineered software projects) from the noise (e.g. repositories containing home work assignments). The proportion of noise in a random sample of repositories could skew the study and may lead to researchers reaching unrealistic, potentially inaccurate, conclusions. We argue that it is imperative to have the ability to sieve out the noise in such large repository forges. We propose a framework, and present a reference implementation of the framework as a tool called reaper, to enable researchers to select GitHub repositories that contain evidence of an engineered software project. We identify software engineering practices (called dimensions) and propose means for validating their existence in a GitHub repository. We used reaper to measure the dimensions of 1,857,423 GitHub repositories. We then used manually classified data sets of repositories to train classifiers capable of predicting if a given GitHub repository contains an engineered software project. The performance of the classifiers was evaluated using a set of 200 repositories with known ground truth classification. We also compared the performance of the classifiers to other approaches to classification (e.g. number of GitHub Stargazers) and found our classifiers to outperform existing approaches. We found stargazers-based classifier (with 10 as the threshold for number of stargazers) to exhibit high precision (97{\%}) but an inversely proportional recall (32{\%}). On the other hand, our best classifier exhibited a high precision (82{\%}) and a high recall (86{\%}). The stargazer-based criteria offers precision but fails to recall a significant portion of the population.},
annote = {Evaluating a project in Github whether it is software engineered or not in 7 dimensions.
7 dimensions are - community (collaboration), continuous integration (quality), documentation (maintainability), history (sustained evolution), issues (project management), license (accountability), unit testing (quality)

Community - core contributors 
CI - piecewise function to determine, if repository r uses a CI service
Github supports certain CI services and does not support CI services like Jenkins, Atlassian, Bamboo, Cloudship

Commit Frequency (History)
Comment Ratio (Documentation)
Test ratio - Ratio of test code lines for actual code lines

Implementation - Training the classifier (Evaluation Framework)

Organization repositories (Repositories with big organizations like Facebook, Google), Utility repositories.


Score based classifier - If score of a repository is greater than equal to reference score

Random Forest classifier - Tree based approach, multiple trees are found, each tree casts a vote and then aggregated to produce a result.

Validation Measures - False Positive Rate, False Negative Rate, Precision, Recall, F-Measure

100 projects that follow guidelines and 100 projects that do not follow guidelines},
author = {Munaiah, Nuthan and Kroh, Steven and Cabrey, Craig and Nagappan, Meiyappan},
doi = {10.1007/s10664-017-9512-6},
file = {::},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Curation tools,Data curation,GitHub,Mining software repositories},
number = {6},
pages = {3219--3253},
publisher = {Empirical Software Engineering},
title = {{Curating GitHub for engineered software projects}},
volume = {22},
year = {2017}
}
@article{Cashman,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.10782v2},
author = {Cashman, Dylan and Humayoun, Shah Rukh and Heimerl, Florian and Park, Kendall and Das, Subhajit and Thompson, John and Saket, Bahador and Mosca, Abigail and Stasko, John and Endert, Alex and Gleicher, Michael and Chang, Remco},
eprint = {arXiv:1809.10782v2},
file = {::},
title = {{Visual Analytics for Automated Model Discovery}}
}
@article{Trudel2016,
abstract = {Purpose: A number of prospective studies have documented the effect of adverse psychosocial work factors (work stress) on high blood pressure (BP). Weight gain could be an important pathway by which work stress exerts its effect on BP. No previous prospective study has examined this mediating effect. The aim of the present study was to examine the mediating effect of body mass index (BMI) in the association between psychosocial work factors from Siegrist's effort–reward imbalance model (ERI) and ambulatory BP (ABP). Methods: A prospective study was conducted among 1436 white-collar workers. Data were collected three times during a 5-year period. ERI was measured using validated scales, at each time. BMI was measured by a trained assistant. ABP was measured every 15 min during a working day. Results: ERI exposure onset over 3 years was indirectly associated with ABP changes (0.49 mmHg; 95 {\%} CI 0.05, 1.22), through BMI changes, in women with baseline BMI ≥25 kg/m2. An effect of similar magnitude and of borderline significance was observed for ERI chronic exposure. No mediating effect was observed among men, and using ERI exposure over 5 years. Conclusion: The mediating effect of BMI was of small magnitude and observed in certain subgroups and time frame only. Subgroup-specific mediating pathways might be involved to explain the effect of work stress on cardiovascular diseases risk.},
author = {Trudel, Xavier and Brisson, Chantal and Milot, Alain and Masse, Benoit and V{\'{e}}zina, Michel},
doi = {10.1007/s00420-016-1159-x},
file = {::},
issn = {03400131},
journal = {International Archives of Occupational and Environmental Health},
keywords = {Ambulatory blood pressure,Body mass index,Work stress},
month = {nov},
number = {8},
pages = {1229--1238},
publisher = {Springer Verlag},
title = {{Effort–reward imbalance at work and 5-year changes in blood pressure: the mediating effect of changes in body mass index among 1400 white-collar workers}},
volume = {89},
year = {2016}
}
@inproceedings{Petersen2013,
abstract = {Background - Validity threats should be considered and consistently reported to judge the value of an empirical software engineering research study. The relevance of specific threats for a particular research study depends on the worldview or philosophical worldview of the researchers of the study. Problem/Gap - In software engineering, different categorizations exist, which leads to inconsistent reporting and consideration of threats. Contribution - In this paper, we relate different worldviews to software engineering research methods, identify generic categories for validity threats, and provide a categorization of validity threats with respect to their relevance for different world views. Thereafter, we provide a checklist aiding researchers in identifying relevant threats. Method - Different threat categorizations and threats have been identified in literature, and are reflected on in relation to software engineering research. Results - Software engineering is dominated by the pragmatist worldviews, and therefore use multiple methods in research. Maxwell's categorization of validity threats has been chosen as very suitable for reporting validity threats in software engineering research. Conclusion - We recommend to follow a checklist approach, and reporting first the philosophical worldview of the researcher when doing the research, the research methods and all threats relevant, including open, reduced, and mitigated threats.},
annote = {Petersen and Gencel (2013) provided a reflection of validity threat categorizations for software engineering and proposed to discuss four types of validity threats: 1) descriptive validity (ability to describe what we observe objectively and truthfully), 2) theoretical validity (concerns controllability and whether the measures used capture what they intend to capture), 3) generalizability (the degree of generalizability internally (within groups, communities, or a company) and externally (across groups, communities, and companies)), and 4) interpretive validity (whether the conclusions/inferences are reasonably drawn from the data objectively).},
author = {Petersen, Kai and Gencel, Cigdem},
booktitle = {Proceedings - Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement, IWSM-MENSURA 2013},
doi = {10.1109/IWSM-Mensura.2013.22},
file = {::},
isbn = {9780769550787},
title = {{Worldviews, research methods, and their relationship to validity in empirical software engineering research}},
year = {2013}
}
@phdthesis{Patterson2020,
abstract = {As the twin movements of open science and open source bring an ever greater share of the scientific process into the digital realm, new opportunities arise for the meta-scientific study of science itself, including of data science and statistics. Future science will likely see machines play an active role in processing, organizing, and perhaps even creating scientific knowledge. To make this possible, large engineering efforts must be undertaken to transform scientific artifacts into useful computational resources, and conceptual advances must be made in the organization of scientific theories, models, experiments, and data. This dissertation takes steps toward digitizing and systematizing two major artifacts of data science, statistical models and data analyses. Using tools from algebra, particularly categorical logic, a precise analogy is drawn between models in statistics and logic, enabling statistical models to be seen as models of theories, in the logical sense. Statistical theories, being algebraic structures, are amenable to machine representation and are equipped with morphisms that formalize the relations between different statistical methods. Turning from mathematics to engineering, a software system for creating machine representations of data analyses, in the form of Python or R programs, is designed and implemented. The representations aim to capture the semantics of data analyses, independent of the programming language and libraries in which they are implemented.},
archivePrefix = {arXiv},
arxivId = {2006.08945},
author = {Patterson, Evan},
eprint = {2006.08945},
file = {::},
month = {jun},
school = {Stanford University},
title = {{The algebra and machine representation of statistical models}},
url = {http://arxiv.org/abs/2006.08945},
year = {2020}
}
@article{Elkan2001,
abstract = {This paper revisits the problem of optimal learning and decision-making when different misclassification errors incur different penalties. We characterize precisely but intuitively when a cost matrix is reasonable, and we show how to avoid the mistake of defining a cost matrix that is economically incoherent. For the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions using a classifier learned by a standard non-cost-sensitive learning method. However, we then argue that changing the balance of negative and positive training examples has little effect on the classifiers produced by standard Bayesian and decision tree learning methods. Accordingly, the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given, and then to compute optimal decisions explicitly using the probability estimates given by the classifier.},
author = {Elkan, Charles},
file = {::},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {973--978},
title = {{The foundations of cost-sensitive learning}},
year = {2001}
}
@inproceedings{Khalajzadeh2020d,
address = {New York, NY, USA},
author = {Khalajzadeh, Hourieh and Verma, Tarun and Simmons, Andrew J. and Grundy, John and Abdelrazek, Mohamed and Hosking, John},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
doi = {10.1145/3417990.3422004},
file = {::},
isbn = {9781450381352},
keywords = {BiDaML,bidaml,big data applications,centered approach to the,design of tooling for,in this paper we,modelling of big data,propose a user-,recommender,to address these shortcomings},
month = {oct},
pages = {1--5},
publisher = {ACM},
title = {{User-centred tooling for modelling of big data applications}},
url = {https://dl.acm.org/doi/10.1145/3417990.3422004},
year = {2020}
}
@article{Kasunic2005,
abstract = {A survey can characterize the knowledge, attitudes, and behaviors of a large group of people through the study of a subset of them. However, to protect the validity of conclusions drawn from a survey, certain procedures must be followed throughout the process of designing, developing, and distributing the survey questionnaire. Surveys are used extensively by software and systems engineering organizations to provide insight into complex issues, assist with problem solving, and support effective decision making. This document presents a seven-stage, end-to-end process for conducting a survey},
author = {Kasunic, Mark},
doi = {CMU/SEI-2005-HB-004},
file = {::},
isbn = {CMU/SEI-2005-HB-004},
journal = {Carnegie-Mellon Univ Pittsburgh Pa Software Engineering Inst},
pages = {142},
title = {{Designing an Effective Survey}},
year = {2005}
}
@article{Kim2017,
abstract = {Recovery literature has focused predominantly on recovery processes outside the workplace during nonwork times. Considering a lack of research on momentary recovery at work, we examined four categories of micro-break activities—relaxation, nutrition-intake, social, and cognitive activities—as possible recovery mechanisms in the workplace. Using effort recovery and conservation of resources theories, we hypothesized that micro-break activities attenuate the common stressor–strain relationship between work demands and negative affect. For 10 consecutive workdays, 86 South Korean office workers (842 data points) reported their specific daily work demands right after their lunch hour (Time 1) and then reported their engagement in micro-break activities during the afternoon and negative affective state at the end of the workday (Time 2). As expected, relaxation and social activities reduced the effects of work demands on end-of-workday negative affect. Nutrition intake of beverages and snacks did not have a significant moderating effect. Post hoc analyses, however, revealed that only caffeinated beverages reduced work demands effects on negative affect. Unexpectedly, cognitive activities aggravated the effects of work demands on negative affect. The findings indicate not only the importance of taking micro-breaks but also which types of break activities are beneficial for recovery. Implications, limitations, and future research directions are discussed. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
author = {Kim, Sooyeol and Park, YoungAh and Niu, Qikun},
doi = {10.1002/job.2109},
issn = {08943796},
journal = {Journal of Organizational Behavior},
keywords = {micro-break activities,momentary recovery,negative affect,work demands},
month = {jan},
number = {1},
pages = {28--44},
publisher = {John Wiley and Sons Ltd},
title = {{Micro-break activities at work to recover from daily work demands}},
url = {http://doi.wiley.com/10.1002/job.2109},
volume = {38},
year = {2017}
}
@article{Krantz2005,
abstract = {Background: The aim of this study was to analyse how paid work, unpaid household tasks, child care, work-child care interactions and perceived work stress are associated with reported symptoms in male and female white-collar employees. Methods: A questionnaire was mailed to 1300 men and 1300 women belonging to the white-collar sector, with at least 35 hours of regular employment a week and a participant age of between 32 and 58 years. It contained items relating to total workload (hours spent on paid work, unpaid household tasks and childcare), subjective indices for work stress and symptoms. The response rate was 65{\%} (743 women; 595 men). Gender difference in symptom prevalence was tested by analyses of variance. Odds ratios were used to estimate the bivariate associations between work-related variables and symptom prevalence. A multivariate analysis estimated the effect of paid and unpaid work interaction, work-childcare interplay and possible synergy. Results: The frequency and severity of symptoms was higher in women than in men (P {\textless} 0.0001). Employed women's health was determined by the interaction between conditions at work and household duties (OR 2.09; 1.06-4.14), whereas men responded more selectively to long working hours, i.e. {\textgreater}50h/week (OR 1.61; 1.02-2.54). However, childcare ({\textless}21 h/week) appeared to have a buffer effect on the risk of a high level of symptoms in men working long hours. Conclusion: Working life and private circumstances and the interplay between them need to be taken into account to curb stress-related ill health in both men and women. {\textcopyright} The Author 2005. Published by Oxford University Press on behalf of the European Public Health Association. All rights reserved.},
author = {Krantz, Gunilla and Berntsson, Leeni and Lundberg, Ulf},
doi = {10.1093/eurpub/cki079},
file = {::},
issn = {1464-360X},
journal = {European Journal of Public Health},
keywords = {Gender,Household work,Ill-health,Multiple roles,Paid work,Stress,Total workload},
month = {apr},
number = {2},
pages = {209--214},
publisher = {Narnia},
title = {{Total workload, work stress and perceived symptoms in Swedish male and female white-collar employees}},
url = {http://academic.oup.com/eurpub/article/15/2/209/567068/Total-workload-work-stress-and-perceived-symptoms},
volume = {15},
year = {2005}
}
@article{Holstein2019,
abstract = {The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners' needs.},
author = {Holstein, Kenneth and {Wortman Vaughan}, Jennifer and Daum{\'{e}}, Hal and Dudik, Miro and Wallach, Hanna},
doi = {10.1145/3290605.3300830},
file = {::},
isbn = {9781450359702},
keywords = {acm reference format,algorithmic bias,empirical study,fair machine learning,finding,need-,product t,product teams,ux of machine learning},
pages = {1--16},
title = {{Improving Fairness in Machine Learning Systems}},
year = {2019}
}
@article{Zacher2014,
abstract = {Organizational researchers and practitioners are increasingly interested in self-regulatory strategies employees can use at work to sustain or improve their occupational well-being. A recent cross-sectional study on energy management strategies suggested that many work-related strategies (e.g., setting a new goal) are positively related to occupational well-being, whereas many micro-breaks (e.g., listening to music) are negatively related to occupational well-being. We used a diary study design to take a closer look at the effects of these energy management strategies on fatigue and vitality. Based on conservation of resources theory, we hypothesized that both types of energy management strategies negatively predict fatigue and positively predict vitality. Employees (N= 124) responded to a baseline survey and to hourly surveys across one work day (6.7 times on average). Consistent with previous research, between-person differences in the use of work-related strategies were positively associated with between-person differences in vitality. However, results of multilevel analyses of the hourly diary data showed that only micro-breaks negatively predicted fatigue and positively predicted vitality. These findings suggest that taking micro-breaks during the work day may have short-term effects on occupational well-being, whereas using work-related strategies may have long-term effects.},
author = {Zacher, Hannes and Brailsford, Holly A. and Parker, Stacey L.},
doi = {10.1016/j.jvb.2014.08.005},
issn = {00018791},
journal = {Journal of Vocational Behavior},
keywords = {Diary study,Energy management,Fatigue,Micro-breaks,Vitality},
month = {dec},
number = {3},
pages = {287--297},
publisher = {Academic Press Inc.},
title = {{Micro-breaks matter: A diary study on the effects of energy management strategies on occupational well-being}},
volume = {85},
year = {2014}
}
@article{Truong2018,
abstract = {Seizure prediction has attracted growing attention as one of the most challenging predictive data analysis efforts to improve the life of patients with drug-resistant epilepsy and tonic seizures. Many outstanding studies have reported great results in providing sensible indirect (warning systems) or direct (interactive neural stimulation) control over refractory seizures, some of which achieved high performance. However, to achieve high sensitivity and a low false prediction rate, many of these studies relied on handcraft feature extraction and/or tailored feature extraction, which is performed for each patient independently. This approach, however, is not generalizable, and requires significant modifications for each new patient within a new dataset. In this article, we apply convolutional neural networks to different intracranial and scalp electroencephalogram (EEG) datasets and propose a generalized retrospective and patient-specific seizure prediction method. We use the short-time Fourier transform on 30-s EEG windows to extract information in both the frequency domain and the time domain. The algorithm automatically generates optimized features for each patient to best classify preictal and interictal segments. The method can be applied to any other patient from any dataset without the need for manual feature extraction. The proposed approach achieves sensitivity of 81.4{\%}, 81.2{\%}, and 75{\%} and a false prediction rate of 0.06/h, 0.16/h, and 0.21/h on the Freiburg Hospital intracranial EEG dataset, the Boston Children's Hospital-MIT scalp EEG dataset, and the American Epilepsy Society Seizure Prediction Challenge dataset, respectively. Our prediction method is also statistically better than an unspecific random predictor for most of the patients in all three datasets.},
author = {Truong, Nhan Duy and Nguyen, Anh Duy and Kuhlmann, Levin and Bonyadi, Mohammad Reza and Yang, Jiawei and Ippolito, Samuel and Kavehei, Omid},
doi = {10.1016/j.neunet.2018.04.018},
file = {::},
issn = {18792782},
journal = {Neural Networks},
keywords = {Convolutional neural network,Intracranial EEG,Machine learning,Scalp EEG,Seizure prediction},
month = {sep},
pages = {104--111},
publisher = {Elsevier Ltd},
title = {{Convolutional neural networks for seizure prediction using intracranial and scalp electroencephalogram}},
volume = {105},
year = {2018}
}
@article{Darejeh2013,
abstract = {This article presents a review on how software usability could be increased for users with less computer literacy. The literature was reviewed to extract user interface design principles by identifying the similar problems of this group of users. There are different groups of users with less computer literacy. However, based on the literature three groups of them need special attention from software designers. The first group is elderly users, as users with lack of computer background. The second group is children, as novice users and the third group is users with mental or physical disorders. Therefore, this study intends to focus on the mentioned groups, followed by a comparison between previous researches in the field, which reveals that some commonalities exist between the needs of these users. These commonalities were used to extract user interface design principles such as (a) reducing the number of features available at any given time, (b) avoiding using computer terms, (c) putting customization ability for font, color, size and (d) using appropriate graphical objects such as avatar or icon. Taking these principles into account can solve software usability problems and increase satisfaction of users with less computer literacy.},
author = {Darejeh, Ali and Singh, Dalbir},
doi = {10.3844/jcssp.2013.1443.1450},
file = {::},
issn = {15493636},
journal = {Journal of Computer Science},
keywords = {Amateur and novice users,Children software learnability,Elders software learnability,Software usability,User interface},
number = {11},
pages = {1443--1450},
title = {{A review on user interface design principles to increase software usability for users with less computer literacy}},
volume = {9},
year = {2013}
}
@article{Ndjaboue2017,
abstract = {Objectives Prospective studies which evaluated whether the effects of chronic exposure to psychosocial work factors on mental health persisted over time are scarce. For the first time, this study evaluated: 1) the effect of chronic exposure to effort-reward imbalance over 5 years on the prevalence of high psychological distress among men and women, and 2) the persistence of this effect over time. Methods Overall, 1747 white-collar workers from three public organizations participated in a prospective study. Psychological distress and effort-reward imbalance were measured using validated questionnaires at baseline, and at 3- and 5-year follow-ups. Prevalence ratios (PRs) of high psychological distress were estimated using log-binomial regression according to baseline and repeated exposure. Results Compared to unexposed workers, those with repeated exposure to effort-reward imbalance had a higher prevalence of high psychological distress. Workers exposed only at some time-points also had a higher prevalence. The deleterious effect of repeated exposure observed at the 3-year follow-up persisted at the 5-year follow-up among women (PR = 2.48 95{\%} confidence interval (CI) 1.97–3.11) and men (PR = 1.91 95{\%} CI 1.20–3.04). These effects were greater than those found using a single baseline measurement. Conclusion The current study supported a deleterious effect of repeated exposure to effort-reward imbalance on psychological distress, and a lack of adaptation to these effects over time among men and women. Since psychological distress may later lead to severe mental problems, current results highlight the need to consider exposure to these adverse work factors in primary and secondary preventions aimed at reducing mental health problems at work.},
author = {Ndjaboue, Ruth and Brisson, Chantal and Talbot, Denis and V{\'{e}}zina, Michel},
doi = {10.1016/j.jpsychores.2017.01.001},
issn = {18791360},
journal = {Journal of Psychosomatic Research},
month = {mar},
pages = {56--63},
publisher = {Elsevier Inc.},
title = {{Chronic exposure to adverse psychosocial work factors and high psychological distress among white-collar workers: A 5-year prospective study}},
volume = {94},
year = {2017}
}
@article{Gharehyazie2017,
abstract = {{\textcopyright}2017 IEEE. Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub.},
author = {Gharehyazie, Mohammad and Ray, Baishakhi and Filkov, Vladimir},
doi = {10.1109/MSR.2017.15},
file = {::},
isbn = {9781538615447},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Code reuse,Cross-project clones,GitHub},
pages = {291--301},
title = {{Some from Here, Some from There: Cross-Project Code Reuse in GitHub}},
year = {2017}
}
@misc{Beniczky2017,
abstract = {Standardized terminology for computer-based assessment and reporting of EEG has been previously developed in Europe. The International Federation of Clinical Neurophysiology established a taskforce in 2013 to develop this further, and to reach international consensus. This work resulted in the second, revised version of SCORE (Standardized Computer-based Organized Reporting of EEG), which is presented in this paper. The revised terminology was implemented in a software package (SCORE EEG), which was tested in clinical practice on 12,160 EEG recordings. Standardized terms implemented in SCORE are used to report the features of clinical relevance, extracted while assessing the EEGs. Selection of the terms is context sensitive: initial choices determine the subsequently presented sets of additional choices. This process automatically generates a report and feeds these features into a database. In the end, the diagnostic significance is scored, using a standardized list of terms. SCORE has specific modules for scoring seizures (including seizure semiology and ictal EEG patterns), neonatal recordings (including features specific for this age group), and for Critical Care EEG Terminology. SCORE is a useful clinical tool, with potential impact on clinical care, quality assurance, data-sharing, research and education.},
author = {Beniczky, S{\'{a}}ndor and Aurlien, Harald and Br{\o}gger, Jan C. and Hirsch, Lawrence J. and Schomer, Donald L. and Trinka, Eugen and Pressler, Ronit M. and Wennberg, Richard and Visser, Gerhard H. and Eisermann, Monika and Diehl, Beate and Lesser, Ronald P. and Kaplan, Peter W. and {Nguyen The Tich}, Sylvie and Lee, Jong Woo and Martins-da-Silva, Antonio and Stefan, Hermann and Neufeld, Miri and Rubboli, Guido and Fabricius, Martin and Gardella, Elena and Terney, Daniella and Meritam, Pirgit and Eichele, Tom and Asano, Eishi and Cox, Fieke and {van Emde Boas}, Walter and Mameniskiene, Ruta and Marusic, Petr and Z{\'{a}}rubov{\'{a}}, Jana and Schmitt, Friedhelm C. and Ros{\'{e}}n, Ingmar and Fuglsang-Frederiksen, Anders and Ikeda, Akio and MacDonald, David B. and Terada, Kiyohito and Ugawa, Yoshikazu and Zhou, Dong and Herman, Susan T.},
booktitle = {Clinical Neurophysiology},
doi = {10.1016/j.clinph.2017.07.418},
file = {::},
issn = {18728952},
keywords = {Clinical assessment,Database,EEG,Report,Standardized,Terminology},
month = {nov},
number = {11},
pages = {2334--2346},
pmid = {28838815},
publisher = {Elsevier Ireland Ltd},
title = {{Standardized computer-based organized reporting of EEG: SCORE – Second version}},
volume = {128},
year = {2017}
}
@article{Woolston,
author = {Woolston, H B},
editor = {Bristol, L M},
issn = {15503283},
journal = {The American Journal of Theology},
number = {2},
pages = {311--313},
publisher = {University of Chicago Press},
title = {{Social Adaptation}},
url = {http://www.jstor.org/stable/3155443},
volume = {21}
}
@misc{Kadhim2019b,
abstract = {{\textless}h3 class="a-plus-plus"{\textgreater}Abstract{\textless}/h3{\textgreater}
                  {\textless}p class="a-plus-plus"{\textgreater}Supervised machine learning studies are gaining more significant recently because of the availability of the increasing number of the electronic documents from different resources. Text classification can be defined that the task was automatically categorized a group documents into one or more predefined classes according to their subjects. Thereby, the major objective of text classification is to enable users for extracting information from textual resource and deals with process such as retrieval, classification, and machine learning techniques together in order to classify different pattern. In text classification technique, term weighting methods design suitable weights to the specific terms to enhance the text classification performance. This paper surveys of text classification, process of different term weighing methods and comparison between different classification techniques.{\textless}/p{\textgreater}},
annote = {K-NN with TFIDF was shown to perform well on a variety of text classification problems

Text classification:
Insufficient number of samples of testing text documents, Features for the dataset, Differences in techniques, Incompatibility between the techniques and the problems, Class ambiguity.},
author = {Kadhim, Ammar Ismael},
booktitle = {Artificial Intelligence Review},
doi = {10.1007/s10462-018-09677-1},
issn = {15737462},
keywords = {Classification techniques,Supervised machine learning,Term weighting,Text classification},
title = {{Survey on supervised machine learning techniques for automatic text classification}},
year = {2019}
}
@article{Marshall2018,
abstract = {Machine learning (ML) algorithms have proven highly accurate for identifying Randomized Controlled Trials (RCTs) but are not used much in practice, in part because the best way to make use of the technology in a typical workflow is unclear. In this work, we evaluate ML models for RCT classification (support vector machines, convolutional neural networks, and ensemble approaches). We trained and optimized support vector machine and convolutional neural network models on the titles and abstracts of the Cochrane Crowd RCT set. We evaluated the models on an external dataset (Clinical Hedges), allowing direct comparison with traditional database search filters. We estimated area under receiver operating characteristics (AUROC) using the Clinical Hedges dataset. We demonstrate that ML approaches better discriminate between RCTs and non-RCTs than widely used traditional database search filters at all sensitivity levels; our best-performing model also achieved the best results to date for ML in this task (AUROC 0.987, 95{\%} CI, 0.984-0.989). We provide practical guidance on the role of ML in (1) systematic reviews (high-sensitivity strategies) and (2) rapid reviews and clinical question answering (high-precision strategies) together with recommended probability cutoffs for each use case. Finally, we provide open-source software to enable these approaches to be used in practice.},
author = {Marshall, Iain J. and Noel-Storr, Anna and Kuiper, Jo{\"{e}}l and Thomas, James and Wallace, Byron C.},
doi = {10.1002/jrsm.1287},
file = {::},
issn = {17592887},
journal = {Research Synthesis Methods},
number = {4},
pages = {602--614},
pmid = {29314757},
title = {{Machine learning for identifying Randomized Controlled Trials: An evaluation and practitioner's guide}},
volume = {9},
year = {2018}
}
@article{Chen2018a,
abstract = {In the era of big data, recommender system (RS) has become an effective information filtering tool that alleviates information overload for Web users. Collaborative filtering (CF), as one of the most successful recommendation techniques, has been widely studied by various research institutions and industries and has been applied in practice. CF makes recommendations for the current active user using lots of users' historical rating information without analyzing the content of the information resource. However, in recent years, data sparsity and high dimensionality brought by big data have negatively affected the efficiency of the traditional CF-based recommendation approaches. In CF, the context information, such as time information and trust relationships among the friends, is introduced into RS to construct a training model to further improve the recommendation accuracy and user's satisfaction, and therefore, a variety of hybrid CF-based recommendation algorithms have emerged. In this paper, we mainly review and summarize the traditional CF-based approaches and techniques used in RS and study some recent hybrid CF-based recommendation approaches and techniques, including the latest hybrid memory-based and model-based CF recommendation algorithms. Finally, we discuss the potential impact that may improve the RS and future direction. In this paper, we aim at introducing the recent hybrid CF-based recommendation techniques fusing social networks to solve data sparsity and high dimensionality and provide a novel point of view to improve the performance of RS, thereby presenting a useful resource in the state-of-the-art research result for future researchers.},
author = {Chen, Rui and Hua, Qingyi and Chang, Yan Shuo and Wang, Bo and Zhang, Lei and Kong, Xiangjie},
doi = {10.1109/ACCESS.2018.2877208},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Collaborative filtering,Matrix factorization,Recommender systems,Singular value decomposition,Social networks,Trust-aware collaborative filtering},
pages = {64301--64320},
publisher = {IEEE},
title = {{A survey of collaborative filtering-based recommender systems: from traditional methods to hybrid methods based on social networks}},
volume = {6},
year = {2018}
}
@article{Yu2018,
abstract = {Artificial intelligence (AI) is gradually changing medical practice. With recent progress in digitized data acquisition, machine learning and computing infrastructure, AI applications are expanding into areas that were previously thought to be only the province of human experts. In this Review Article, we outline recent breakthroughs in AI technologies and their biomedical applications, identify the challenges for further progress in medical AI systems, and summarize the economic, legal and social implications of AI in healthcare.},
author = {Yu, Kun-Hsing and Beam, Andrew L and Kohane, Isaac S},
doi = {10.1038/s41551-018-0305-z},
issn = {2157-846X},
journal = {Nature Biomedical Engineering},
number = {10},
pages = {719--731},
title = {{Artificial intelligence in healthcare}},
url = {https://doi.org/10.1038/s41551-018-0305-z},
volume = {2},
year = {2018}
}
@article{Jobin2019,
abstract = {In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical', there is debate about both what constitutes ‘ethical AI' and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies. As AI technology develops rapidly, it is widely recognized that ethical guidelines are required for safe and fair implementation in society. But is it possible to agree on what is ‘ethical AI'? A detailed analysis of 84 AI ethics reports around the world, from national and international organizations, companies and institutes, explores this question, finding a convergence around core principles but substantial divergence on practical implementation.},
author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
doi = {10.1038/s42256-019-0088-2},
file = {::},
journal = {Nature Machine Intelligence},
keywords = {Ethics,Information systems and information technology,Information technology,Science,technology and society},
month = {sep},
number = {9},
pages = {389--399},
publisher = {Springer Science and Business Media LLC},
title = {{The global landscape of AI ethics guidelines}},
url = {https://doi.org/10.1038/s42256-019-0088-2},
volume = {1},
year = {2019}
}
@techreport{Brodie2002,
abstract = {Article Abstract-The natural history of treated epilepsy has substantial relevance to its pharmacologic and surgical management. In our center, 525 unselected, untreated patients were given a diagnosis of epilepsy, started on antiepileptic drug (AED) therapy, and followed for a median of 5 years. Sixty-three percent of patients had been seizure-free for at least the previous year. Forty-seven percent of 470 previously drug-naı¨venaı¨ve patients responded to their first AED. Thirteen percent were seizure-free on the second AED, and 1{\%} on the third monotherapy choice. Only 3{\%} were controlled with two AEDs and none with three. The prognosis for patients whose epilepsy did not respond to the first AED was strongly associated with the reason for failure. Only 11{\%} of patients with inadequate control on the first AED later became seizure-free. These results suggest that patients with newly diagnosed epilepsy comprise two distinct populations. Around 60{\%} will be controlled on monotherapy, usually with the first or second AED chosen. The remaining 30 to 40{\%} will be difficult to control from the outset. A management plan should be formulated for each patient when treatment is started. Strategies for combining drugs should involve individual assessment of patient-related factors, including seizure type and epilepsy syndrome classification, combined with an understanding of the mechanisms of action, side effects, and interactions of the AEDs. Epilepsy surgery should be considered after failure of two well-tolerated treatment regimens, whether as monotherapy or with one monotherapy and the first combination. Prevention of refractory epilepsy should be the goal of treatment when the first AED is prescribed. A staged approach to the pharmacologic management and, when appropriate, surgical work-up for each epilepsy syndrome will optimize the chance of perfect seizure control and help more patients achieve a fulfilling life. NEUROLOGY 2002;58(Suppl 5):S2-S8},
author = {Brodie, Martin J and Kwan, Patrick},
file = {::},
title = {{Staged approach to epilepsy management}},
year = {2002}
}
@article{Python.org2019,
author = {Python.org},
file = {::},
pages = {1--28},
title = {{PEP 8 -- Style Guide for Python Code | Python.org}},
url = {https://www.python.org/dev/peps/pep-0008/},
year = {2019}
}
@article{Dacrema2019,
abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difcult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifcally, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable efort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientifc practices in this area.},
author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
doi = {10.1145/3298689.3347058},
file = {::},
isbn = {9781450362436},
journal = {RecSys 2019 - 13th ACM Conference on Recommender Systems},
keywords = {Deep Learning,Evaluation,Recommender Systems,Reproducibility},
pages = {101--109},
title = {{Are we really making much progress? A worrying analysis of recent neural recommendation approaches}},
year = {2019}
}
@article{MinlongLin2013,
annote = {Journal: 20 datasets},
author = {{Minlong Lin} and {Ke Tang} and {Xin Yao}},
doi = {10.1109/tnnls.2012.2228231},
file = {::},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {4},
pages = {647--660},
publisher = {IEEE},
title = {{Dynamic Sampling Approach to Training Neural Networks for Multiclass Imbalance Classification}},
volume = {24},
year = {2013}
}
@inproceedings{Khalajzadeh2020c,
abstract = {Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.},
address = {New York, NY, USA},
author = {Khalajzadeh, Hourieh and Simmons, Andrew and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang and Ratnakanthan, Prasanna and Zia, Adil and Law, Meng},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
doi = {10.1145/3377812.3390811},
file = {::},
isbn = {9781450371223},
issn = {02705257},
month = {jun},
pages = {256--257},
publisher = {ACM},
title = {{A practical, collaborative approach for modeling big data analytics application requirements}},
url = {https://dl.acm.org/doi/10.1145/3377812.3390811},
year = {2020}
}
@article{Jacobs2019,
abstract = {Quantitative text analysis tools have become increasingly popular methods for the operationalization of various types of discourse analysis. However, their application usually remains fairly simple and superficial, and fails to exploit the resources which the digital era holds for discourse analysis to their full extent. This paper discusses the discourse-analytic potential of a more complex and advanced text analysis tool, which is already frequently employed in other approaches to textual analysis, notably topic modelling. We argue that topic modelling promises advances in areas where discourse analysis has traditionally struggled, such as scaling, repetition, and systematization, which go beyond the contributions of simpler frequency and collocation counts. At the same time, it does not violate the epistemological premises and methodological ethos of even the more radical theories of discourse, we will demonstrate. Finally, we present two small case studies to show how topic modelling–when used with appropriate parameters–can straightforwardly enhance our ability to systematically investigate and interpret discourses in large collections of text. Abbreviations: CDA: Critical Discourse Analysis; LDA: Latent Dirichlet Allocation.},
author = {Jacobs, Thomas and Tsch{\"{o}}tschel, Robin},
doi = {10.1080/13645579.2019.1576317},
issn = {1364-5579},
journal = {International Journal of Social Research Methodology},
keywords = {Discourse analysis,computational social science,digital humanities,hegemony,text analysis,topic modelling},
month = {sep},
number = {5},
pages = {469--485},
publisher = {Routledge},
title = {{Topic models meet discourse analysis: a quantitative tool for a qualitative approach}},
url = {https://www.tandfonline.com/doi/full/10.1080/13645579.2019.1576317},
volume = {22},
year = {2019}
}
@book{Brambilla2012,
abstract = {This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving p inciples, application scenarios and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The book is targeted to a diverse set of readers, spanning: professionals, CTOs, CIOs, and team managers that need to have a bird's eye vision on the matter, so as to take the appropriate decisions when it comes to choosing the best development techniques for their company or team; software analysts, developers, or designers that expect to use MDSE for improving everyday work productivity, either by applying the basic modeling techniques and notations or by defining new domain-specific modeling lang ages and applying end-to-end MDSE practices in the software factory; and academic teachers and students to address undergrad and postgrad courses on MDSE. In addition to the contents of the book, more resources are provided on the book's website, including the examples presented in the book. Table of Contents: Introduction / MDSE Principles / MDSE Use Cases / Model-Driven Architecture (MDA) / Integration of MDSE in your Development Process / Modeling Languages at a Glance / Developing your Own Modeling Language / Model-to-Model Transformations / Model-to-Text Transformations / Managing Models / Summary},
author = {Brambilla, Marco and Cabot, Jordi and Wimmer, Manuel},
booktitle = {Synthesis Lectures on Software Engineering},
doi = {10.2200/S00441ED1V01Y201208SWE001},
file = {::},
isbn = {9781608458837},
publisher = {Morgan {\&} Claypoo},
title = {{Model-Driven Software Engineering in Practice}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00441ED1V01Y201208SWE001},
year = {2012}
}
@article{Katz2017,
abstract = {—Feature generation is one of the challenging aspects of machine learning. We present ExploreKit, a framework for automated feature generation. ExploreKit generates a large set of candidate features by combining information in the original features, with the aim of maximizing predictive performance according to user-selected criteria. To overcome the exponential growth of the feature space, ExploreKit uses a novel ma-chine learning-based feature selection approach to predict the usefulness of new candidate features. This approach enables efficient identification of the new features and produces superior results compared to existing feature selection solutions. We demonstrate the effectiveness and robustness of our approach by conducting an extensive evaluation on 25 datasets and 3 different classification algorithms. We show that ExploreKit can achieve classification-error reduction of 20{\%} overall. Our code is available at https://github.com/giladkatz/ExploreKit.},
author = {Katz, Gilad and Shin, Eui Chul Richard and Song, Dawn},
doi = {10.1109/ICDM.2016.176},
file = {::},
isbn = {9781509054725},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
pages = {979--984},
publisher = {IEEE},
title = {{ExploreKit: Automatic feature generation and selection}},
year = {2017}
}
@article{wilson2014best,
author = {Wilson, Greg and Aruliah, Dhavide A and Brown, C Titus and Hong, Neil P Chue and Davis, Matt and Guy, Richard T and Haddock, Steven H D and Huff, Kathryn D and Mitchell, Ian M and Plumbley, Mark D and Others},
journal = {PLoS biology},
number = {1},
publisher = {Public Library of Science},
title = {{Best practices for scientific computing}},
volume = {12},
year = {2014}
}
@article{Kuhn2007,
abstract = {Many of the existing approaches in Software Comprehension focus on program structure or external documentation. However, by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked. To understand software as a whole, we need to enrich software analysis with the developer knowledge hidden in the code naming. This paper proposes the use of information retrieval to exploit linguistic information found in source code, such as identifier names and comments. We introduce Semantic Clustering, a technique based on Latent Semantic Indexing and clustering to group source artifacts that use similar vocabulary. We call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code. We compare the topics to each other, identify links between them, provide automatically retrieved labels, and use a visualization to illustrate how they are distributed over the system. Our approach is language independent as it works at the level of identifier names. To validate our approach we applied it on several case studies, two of which we present in this paper. Note: Some of the visualizations presented make heavy use of colors. Please obtain a color copy of the article for better understanding.},
author = {Kuhn, Adrian and Ducasse, St{\'{e}}phane and G{\^{i}}rba, Tudor},
doi = {10.1016/J.INFSOF.2006.10.017},
file = {::},
issn = {0950-5849},
journal = {Information and Software Technology},
month = {mar},
number = {3},
pages = {230--243},
publisher = {Elsevier},
title = {{Semantic clustering: Identifying topics in source code}},
url = {https://www.sciencedirect.com/science/article/pii/S0950584906001820},
volume = {49},
year = {2007}
}
@article{Petersen2007,
abstract = {Systematic configuration management is important for successful software product lines. We can use aspect-oriented software development to decompose software product lines based on features that can ease configuration management. In this paper, we present a military maintenance product line that employs such strategy. In particular, we applied a specific approach, feature based modeling (FBM), in the construction of the system. We have extended FBM to address properties specific to product line. We will discuss the advantages of FBM when applied to product lines. Such gains include the functional decomposition of the system along user requirements (features) as aspects. Moreover, those features exhibit unidirectional dependency (i.e. among any two features, at most one depend on another) that enables developers to analyze the effect of any modification they may make on any feature. In addition, any variations can be captured as aspects which can also be incorporated easily into the core asset if such variation is deemed to be important enough to be included in the product line for further evolution. [ABSTRACT FROM AUTHOR]},
author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
doi = {10.1142/S0218194007003112},
file = {::},
isbn = {0-7695-2555-5},
issn = {02181940},
journal = {International Journal of Software Engineering {\&} Knowledge Engineering},
keywords = {evidence based software engineering,systematic mapping studies,systematic reviews},
number = {1},
pages = {33--55},
title = {{Systematic Mapping Studies in Software Engineering}},
url = {http://content.ebscohost.com/ContentServer.asp?T=P{\&}P=AN{\&}K=22674743{\&}S=R{\&}D=bth{\&}EbscoContent=dGJyMNHX8kSeqK44zdnyOLCmr0qeprZSr6e4SrCWxWXS{\&}ContentCustomer=dGJyMPGosk+xq65QuePfgeyx44Dt6fIA{\%}5Cnhttp://search.ebscohost.com/login.aspx?direct=true{\&}db=bth{\&}AN=2447601},
volume = {17},
year = {2007}
}
@article{Elliott2017,
abstract = {Systematic reviews are difficult to keep up to date, but failure to do so leads to a decay in review currency, accuracy, and utility. We are developing a novel approach to systematic review updating termed “Living systematic review” (LSR): systematic reviews that are continually updated, incorporating relevant new evidence as it becomes available. LSRs may be particularly important in fields where research evidence is emerging rapidly, current evidence is uncertain, and new research may change policy or practice decisions. We hypothesize that a continual approach to updating will achieve greater currency and validity, and increase the benefits to end users, with feasible resource requirements over time.},
author = {Elliott, Julian H. and Synnot, Anneliese and Turner, Tari and Simmonds, Mark and Akl, Elie A. and McDonald, Steve and Salanti, Georgia and Meerpohl, Joerg and MacLehose, Harriet and Hilton, John and Tovey, David and Shemilt, Ian and Thomas, James and Agoritsas, Thomas and Perron, Caroline and Akl, Elie A. and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and Kolakowsky-Hayner, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch{\"{u}}nemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and Cossi, Marie Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Marshall, Iain and Soares-Weiser, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Garc{\'{i}}a, Laura Mart{\'{i}}nez and Danko, Kristen and Mavergames, Chris and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Egan, Cathy and Tritton, Roger and Elliott, Julian H. and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Turgeon, Alexis and Featherstone, Robin and Mondello, Stefania and Foxlee, Ruth and Morley, Richard and van Valkenhoef, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Wallace, Byron and Glasziou, Paul and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and Noel-Storr, Anna and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and {Yepes Nu{\~{n}}ez}, Juan Jos{\'{e}} and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
doi = {10.1016/j.jclinepi.2017.08.010},
file = {::},
issn = {18785921},
journal = {Journal of Clinical Epidemiology},
keywords = {Evidence synthesis,Guidelines,Living guidelines,Living systematic review,Systematic review},
pages = {23--30},
pmid = {28912002},
title = {{Living systematic review: 1. Introduction—the why, what, when, and how}},
volume = {91},
year = {2017}
}
@article{Sadowski2015,
annote = {Developers in Google compose 12 search queries on average for a weekday, how about data scientists' frequency of composing search queries ?

Code search covers the spectrum from searching for code reuse to searching for concept localization

Code search studies - Observing programmers directly, analyzing search logs in controlled settings using pre-defined tasks or surveying developers about their search practices

A browser extension plugin and plugin was installed in all computers, the logs are collected whenever developers use internally built search tool},
author = {Sadowski, Caitlin and Stolee, Kathryn T and Elbaum, Sebastian},
file = {::},
journal = {Proceedings of the 2015 10th Joint Meeting on Foundationsof Software Engineering},
keywords = {17,28,3,31,code search,code search appears to,developer tools,have ce-,mented its role in,software development,throughout this evolution,user evaluation},
pages = {191--201},
title = {{How Developers Search for Code: A Case Study}},
year = {2015}
}
@article{Grant2006,
abstract = {A neglected aspect of inventory management is stock loss caused by breakage, theft, deterioration or obsolescence. Such loss requires financial write-offs that affect financial indicators of a firm's overall inventory management effectiveness. Stock loss or damage related to finished goods is easier to identify than obsolete or dead stock, and firms often focus on finished goods due to their high and identifiable value and a higher risk of becoming unusable and thus obsolete. However, factors that affect finished goods also affect raw materials and work-in-progress. A change in operating environment can introduce factors of stock obsolescence, including a firm's external environment of technological change and demand, or internal procedures such as poor forecasting. Firms need to evaluate whether their current stock levels and write-off policies are reasonable for all inventory classifications to reduce the amount of write-offs or to keep the amount at a low and manageable level, and if not investigate the factors that produce these write-offs. This paper discusses dry goods stock obsolescence and write-offs and the impact of demand forecasting on them at a leading UK whisky producer.},
author = {Grant, David B. and Karagianni, Chariklia and Li, Mei},
doi = {10.1080/13675560600859615},
file = {::},
issn = {1367-5567},
journal = {International Journal of Logistics Research and Applications},
title = {{Forecasting and stock obsolescence in whisky production}},
year = {2006}
}
@book{gelman2013,
author = {Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
publisher = {Taylor $\backslash${\&} Francis},
title = {{Bayesian Data Analysis, Third Edition}},
year = {2013}
}
@techreport{Lange,
abstract = {Model selection is linked to model assessment, which is the problem of comparing different models, or model parameters, for a specific learning task. For supervised learning, the standard practical technique is cross-validation, which is not applicable for semi-supervised and unsupervised settings. In this paper, a new model assessment scheme is introduced which is based on a notion of stability. The stability measure yields an upper bound to cross-validation in the supervised case, but extends to semi-supervised and unsupervised problems. In the experimental part, the performance of the stability measure is studied for model order selection in comparison to standard techniques in this area.},
author = {Lange, Tilman and Braun, Mikio L and Roth, Volker and Buhmann, Joachim M},
file = {::},
title = {{Stability-Based Model Selection}}
}
@inproceedings{Hallsten2011,
abstract = {Objective: Contingent self-esteem has been assumed to be a risk for burnout-related disorders, and a contingent self-worth notion of job burnout was applied to study the prospective relationship between job burnout and registered episodes of sickness absence of ≥ 60 consecutive days. Methods: Job burnout was defined as being in the high quartiles on the Maslach Burnout Inventory - General Survey (MBI-GS) scales of exhaustion and cynicism and, in addition, as being above the median on a scale for performance-based self-esteem. Another high exhaustion-cynicism group, a "job wornout" group, was defined as being high on the same MBI-GS scales but having performance-based self-esteem scores below the median. Data were analyzed by a multivariate, logistic regression approach. Participants: 4,109 public employees in Sweden. Results: The job burnout group showed an over-risk of long-term sickness absence incidence, both compared with a low exhaustion-cynicism reference group and with the job wornout group after adjustment for several potential confounders. No association with incidence of long-term sickness absence was found for the job wornout group. Conclusions: The differential vulnerability to long-term sickness absence among high exhaustion-cynicism groups suggests that a self-worth perspective of job burnout can be advantageous for prevention of the costly long-term sickness absences. {\textcopyright} 2011 - IOS Press and the authors. All rights reserved.},
author = {Hallsten, Lennart and Voss, Margaretha and Stark, Stefan and Josephson, Malin and Ving{\aa}rd, Eva},
booktitle = {Work},
doi = {10.3233/WOR-2011-1120},
issn = {10519815},
keywords = {Job burnout,contingent self-esteem,long-term sickness absence,longitudinal study,prevention,public employees},
month = {jan},
number = {2},
pages = {181--192},
pmid = {21297288},
publisher = {IOS Press},
title = {{Job burnout and job wornout as risk factors for long-term sickness absence}},
volume = {38},
year = {2011}
}
@inproceedings{Garg2020,
abstract = {Label shift describes the setting where although the label distribution might change between the source and target domains, the class-conditional probabilities (of data given a label) do not. There are two dominant approaches for estimating the label marginal. BBSE, a moment-matching approach based on confusion matrices, is provably consistent and provides interpretable error bounds. However, a maximum likelihood estimation approach, which we call MLLS, dominates empirically. In this paper, we present a unified view of the two methods and the first theoretical characterization of the likelihoodbased estimator. Our contributions include (i) conditions for consistency of MLLS, which include calibration of the classifier and a confusion matrix invertibility condition that BBSE also requires; (ii) a unified view of the methods, casting the confusion matrix as roughly equivalent to MLLS for a particular choice of calibration method; and (iii) a decomposition of MLLS's finite-sample error into terms reflecting the impacts of miscalibration and estimation error. Our analysis attributes BBSE's statistical inefficiency to a loss of information due to coarse calibration. We support our findings with experiments on both synthetic data and the MNIST and CIFAR10 image recognition datasets.},
archivePrefix = {arXiv},
arxivId = {2003.07554},
author = {Garg, Saurabh and Wu, Yifan and Balakrishnan, Sivaraman and Lipton, Zachary C.},
booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
eprint = {2003.07554},
file = {::},
title = {{A unified view of label shift estimation}},
year = {2020}
}
@article{Lee2015,
abstract = {Adherence to coding conventions during the code production stage of software development is essential. Benefits include enabling programmers to quickly understand the context of shared code, communicate with one another in a consistent manner, and easily maintain the source code at low costs. In reality, however, programmers tend to doubt or ignore the degree to which the quality of their code is affected by adherence to these guidelines. This paper addresses research questions such as "Do violations of coding conventions affect the readability of the produced code?", "What kinds of coding violations reduce code readability?", and "How much do variable factors such as developer experience, project size, team size, and project maturity influence coding violations?" To respond to these research questions, we explored 210 open-source Java projects with 117 coding conventions from the Sun standard checklist. We believe our findings and the analysis approach used in the paper will encourage programmers and QA managers to develop their own customized and effective coding style guidelines.},
author = {Lee, Taek and Lee, Jung Been and In, Hoh Peter},
doi = {10.1587/transinf.2014EDP7327},
file = {::},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Code readability,Coding conventions,Coding style standard,Empirical study,Software quality},
number = {7},
pages = {1286--1296},
title = {{Effect analysis of coding convention violations on readability of post-delivered code}},
volume = {E98D},
year = {2015}
}
@misc{Acharya2019,
abstract = {Epilepsy is a common neurological condition that can occur in anyone at any age. Electroencephalogram (EEG) signals of non-focal (NF) and focal (F) types contain brain activity information that can be used to identify areas affected by seizures. Generally, F EEG signals are recorded from the epileptic part of the brain, while NF EEG signals are recorded from brain regions unaffected by epilepsy. It is essential to correctly detect F EEG signals, when and where they occur, as focal epilepsy can be successfully treated by surgical means. However, all EEG signals are complex and require highly trained personnel for right interpretation. To overcome the associated challenges, in this study a computer-aided detection (CAD) system to aid in the detection of F EEG signals has been developed, and the performance of nonlinear features for differentiating F and NF EEG signals is compared. Moreover, it is noted that nonlinear features can effectively capture concealed patterns and rhythms contained in the EEG signals. Overall, it was found that the CAD system will be useful to clinicians in providing an accurate and objective paradigm for localization of the epileptogenic area.},
author = {Acharya, U. Rajendra and Hagiwara, Yuki and Deshpande, Sunny Nitin and Suren, S. and Koh, Joel En Wei and Oh, Shu Lih and Arunkumar, N. and Ciaccio, Edward J. and Lim, Choo Min},
booktitle = {Future Generation Computer Systems},
doi = {10.1016/j.future.2018.08.044},
file = {::},
issn = {0167739X},
keywords = {Computer-aided detection system,Electroencephalogram signals,Epilepsy,Focal,Non-focal},
month = {feb},
pages = {290--299},
publisher = {Elsevier B.V.},
title = {{Characterization of focal EEG signals: A review}},
volume = {91},
year = {2019}
}
@article{Jansen2008,
abstract = {Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions. This paper presents ADDRA, an approach an architect can use for recovering architectural design decisions after the fact. ADDRA uses architectural deltas to provide the architect with clues about these design decisions. This allows the architect to systematically recover and document relevant architectural design decisions. The recovered architectural design decisions improve the documentation of the architecture, which increases traceability, communication, and general understanding of a system. {\textcopyright}2007 Elsevier Inc. All rights reserved.},
author = {Jansen, Anton and Bosch, Jan and Avgeriou, Paris},
doi = {10.1016/j.jss.2007.08.025},
file = {::},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Architectural design decisions,Software architecture recovery},
number = {4},
pages = {536--557},
title = {{Documenting after the fact: Recovering architectural design decisions}},
volume = {81},
year = {2008}
}
