\chapter[Better Documenting Computer Vision Services]
{Better Documenting Computer Vision Services\pubfootnote{Cummaudo:2020tse}}
\label{ch:tse2020}
\graphicspath{{mainmatter/publications/figures/tse2020/}}

\def\circlenotpresent{\faCircleO}
\def\circlepartialpresent{\faAdjust}
\def\circlepresent{\faCircle}
\newcommand{\dimcat}[1]{\textsc{\small[\textbf{#1}]}}

% Taxonomy Dimension Keys
\def \dima{Descriptions of \glsac{api} Usage}
\def \dimb{Descriptions of Design Rationale}
\def \dimc{Descriptions of Domain Concepts}
\def \dimd{Existence of Support Artefacts}
\def \dime{Overall Presentation of Documentation}

% Suggested improvements
\def\SuggestedImprovement{\noindent\itshape\small\textbf{\faHandORight{} Suggested improvement:~}}

% Participant count
\def\SurveyParticipantsTotal{104}
\def\SurveyParticipantsInternal{22}
\def\SurveyParticipantsInternalResponseRate{50.00\%}
\def\SurveyParticipantsExternalSnowball{38}
\def\SurveyParticipantsExternalMTurk{44}
\def\SurveyParticipantsExternalTotal{82}

\def\SurveyParticipantsExternalPartialResponses{13}
\def\SurveyParticipantsExternalFullResponses{69}
\def\SurveyParticipantsExternalPartialResponsesCompletionRate{43.23\%}
\def\SurveyParticipantsExternalResponseRate{51.49\%}
\def\SurveyParticipantsFullResponses{91}
\def\SurveyParticipantsAddThisShared{2}
\def\SurveyParticipantsSocialMediaPlatformLinkedIn{2}
\def\SurveyParticipantsSocialMediaPlatformReddit{1}
\def\SurveyParticipantsSocialMediaPlatformFacebook{5}
\def\SurveyParticipantsExternalMTurkRejected{12}
\def\SurveyParticipantsExternalResponseTooSmall{37}
\def\SurveyParticipantsTotalReached{153}
\def\SurveyParticipantsTotalResponseRate{67.97\%}
% IPS and ILS values
\def\dimcatIPSvalueAOne{0.85}
\def\dimcatIPSvalueATwo{0.56}
\def\dimcatIPSvalueAThree{0.75}
\def\dimcatIPSvalueAFour{0.53}
\def\dimcatIPSvalueAFive{0.88}
\def\dimcatIPSvalueASix{0.57}
\def\dimcatIPSvalueASeven{0.72}
\def\dimcatIPSvalueAEight{0.63}
\def\dimcatIPSvalueANine{0.69}
\def\dimcatIPSvalueATen{0.65}
\def\dimcatIPSvalueAEleven{0.84}
\def\dimcatIPSvalueATwelve{0.80}
\def\dimcatIPSvalueBOne{0.81}
\def\dimcatIPSvalueBTwo{0.63}
\def\dimcatIPSvalueBThree{0.53}
\def\dimcatIPSvalueBFour{0.42}
\def\dimcatIPSvalueBFive{0.61}
\def\dimcatIPSvalueBSix{0.54}
\def\dimcatIPSvalueBSeven{0.89}
\def\dimcatIPSvalueCOne{0.48}
\def\dimcatIPSvalueCTwo{0.77}
\def\dimcatIPSvalueCThree{0.48}
\def\dimcatIPSvalueDOne{0.81}
\def\dimcatIPSvalueDTwo{0.57}
\def\dimcatIPSvalueDThree{0.71}
\def\dimcatIPSvalueDFour{0.36}
\def\dimcatIPSvalueDFive{0.68}
\def\dimcatIPSvalueDSix{0.63}
\def\dimcatIPSvalueEOne{0.86}
\def\dimcatIPSvalueETwo{0.58}
\def\dimcatIPSvalueEThree{0.71}
\def\dimcatIPSvalueEFour{0.53}
\def\dimcatIPSvalueEFive{0.65}
\def\dimcatIPSvalueESix{0.64}
\def\dimcatILSvalueAOne{0.14}
\def\dimcatILSvalueATwo{0.52}
\def\dimcatILSvalueAThree{0.38}
\def\dimcatILSvalueAFour{0.33}
\def\dimcatILSvalueAFive{0.71}
\def\dimcatILSvalueASix{0.57}
\def\dimcatILSvalueASeven{0.24}
\def\dimcatILSvalueAEight{0.38}
\def\dimcatILSvalueANine{0.14}
\def\dimcatILSvalueATen{0.24}
\def\dimcatILSvalueAEleven{0.38}
\def\dimcatILSvalueATwelve{0.33}
\def\dimcatILSvalueBOne{0.48}
\def\dimcatILSvalueBTwo{0.29}
\def\dimcatILSvalueBThree{0.10}
\def\dimcatILSvalueBFour{0.05}
\def\dimcatILSvalueBFive{0.05}
\def\dimcatILSvalueBSix{0.19}
\def\dimcatILSvalueBSeven{0.29}
\def\dimcatILSvalueCOne{0.10}
\def\dimcatILSvalueCTwo{0.38}
\def\dimcatILSvalueCThree{0.14}
\def\dimcatILSvalueDOne{0.10}
\def\dimcatILSvalueDTwo{0.10}
\def\dimcatILSvalueDThree{0.14}
\def\dimcatILSvalueDFour{0.14}
\def\dimcatILSvalueDFive{0.24}
\def\dimcatILSvalueDSix{0.05}
\def\dimcatILSvalueEOne{0.33}
\def\dimcatILSvalueETwo{0.14}
\def\dimcatILSvalueEThree{0.14}
\def\dimcatILSvalueEFour{0.14}
\def\dimcatILSvalueEFive{0.14}
\def\dimcatILSvalueESix{0.43}


\glsresetall
\begin{abstract}
  Using cloud-based \glspl{cvs} is gaining traction, where developers access \glsac{ai}-powered components through familiar \glsac{rest}ful \glsacpl{api}, not needing to orchestrate large training and inference infrastructures or curate/label training datasets.
  However, while these \glsacpl{api} \textit{seem} familiar to use, their non-deterministic run-time behaviour and evolution is not adequately  communicated to developers.
  Therefore, improving these services' \glsac{api} documentation is paramount---more extensive documentation facilitates the development process of intelligent software.
  In a prior study, we extracted 34 \glsac{api} documentation artefacts from 21 seminal works, devising a taxonomy of five key requirements to produce quality \glsac{api} documentation. We extend this study in two ways. Firstly, by surveying \SurveyParticipantsTotal{} developers of varying experience to understand what \glsac{api} documentation artefacts are of \textit{most value} to practitioners. Secondly, identifying which of these highly-valued artefacts are or are not well-documented through a case study in the emerging \gls{cvs} domain.
  We identify: (i)~several gaps in the software engineering literature, where aspects of \glsac{api} documentation understanding is/is not extensively investigated; and (ii)~where industry vendors (in contrast) document artefacts to better serve their end-developers.
  We provide a set of recommendations to enhance intelligent software documentation for both vendors and the wider research community.
\end{abstract}
\glsresetall
\glsunset{api}

\section{Introduction}

Improving \glsac{api} documentation quality is a valuable task for any \glsac{api}. Succinct \glsac{api} documentation of good quality facilitates productivity \citep{Lethbridge:2005jv,myersstylos2016,7503516}, and therefore improved quality is better engineered into a system \citep{mcleod2011factors}. Where application developers integrate new services into their systems via \glsacpl{api}, their productivity is affected either by inadequate skills (\textit{``I've never used an \glsac{api} like this, so must learn from scratch''}) or, where their skills are adequate, an imbalanced cognitive load that causes excessive context switching (\textit{``I have the skills for this, but am confused or misunderstand''}).
As a real-world use case, consider intelligent \glspl{cvs}, in which an \glsac{ai}-based component produces a non-deterministic result based on a machine-learnt data-driven algorithm, rather than a predictable, rule-driven one \citep{Cummaudo:2019icsme}. These services use machine intelligence to make predictions on images such as object labelling or facial recognition \citepweb{GoogleCloud:Home,AWS:Home,Azure:Home,IBM:Home,Pixlab:Home,Clarifai:Home,Cloudsight:Home,DeepAI:Home,Imagaa:Home,Talkwaler:Home,Megvii:Home,TupuTech:Home,YiTuTech:Home,SenseTime:Home,DeepGlint:Home}. The impacts of poor and incomplete documentation results in developer complaints on online discussion forums such as Stack Overflow \citep{Cummaudo:2020icse}. Many comments show that developers do not think in the non-deterministic mental model of the designers who created the \glspl{cvs}. They ask many varied questions from their peers to try and clarify their understanding.

It is therefore important to ensure developers have access to high-quality \glsac{api} documentation artefacts when consuming these services. Vendors should cover all documentation artefacts that the wider developer community find valuable, and the research community should aide in this process by investigating with types of information that comprise these artefacts, or the aspects of information design to best present this information.
What causes a developer to be confused when using an \glsac{api}, and how to mitigate it via improved documentation, has been largely explored by researchers for \textit{conventional} \glsacpl{api} (an overview is provided in \cref{tse2020:sec:related-work}). Various studies provide a myriad of recommendations into the value of \glsac{api} documentation artefacts based on both qualitative and quantitative analyses, involving developer opinions (from surveys), observation of developers, event logging or content analysis (see \cref{tse2020:fig:sms}). Such guidelines propose ways for developers, managers, and solution architects can construct systems better with improved documentation.

However, there does not yet exist a consolidated \textit{systematic} review of this literature. Further, few studies offer a taxonomy to consolidate these guidelines together, and there still lacks a consolidated effort to capture guidelines on the requirements of good quality \glsac{api} documentation. Studies that produce these guidelines from literature are largely scattered across multiple sources. Investigating the ways by which these guidelines are produced can provide software engineering researchers with better insight into the research methods and data collection techniques used to produce these guidelines. Some studies, for example, use case studies, others use focus groups and brainstorming, or interviews and surveys. The extent to which researchers rely on developer opinion for \glsac{api} documentation guidelines is evident, and gaps in the methodological approaches that researchers use should be emphasised to shine light into new ways of conducting research in this important area. Furthermore, systematically capturing the information distilled from these guidelines into a readily accessible, consolidated taxonomy (designed to assist writing \glsac{api} documentation) must be validated in real-world circumstances to assess its efficacy with practitioners.

In our prior work, we proposed an \glsac{api} documentation taxonomy that was comprised of 21 key primary sources \citep{Cummaudo:2019esem}. This paper significantly extends our previous work by addressing limitations in the existing taxonomy, thus refining it. Previously, we developed a metric for each dimension (topmost-layers) and category (leaf nodes) within the taxonomy \citep{Cummaudo:2019esem}. This metric is an indication of the specific areas of \glsac{api} documentation software engineering researchers have focused their efforts, as measured by the ratio of papers that investigated or reported various issues concerning the documentation artefacts defined within our taxonomy. For the context of this paper, we refer to this metric as an `in-literature' score, or \glsac{ils}. Within this paper, we build upon this facet but \textit{in-practice} by assessing the efficacy of our taxonomy against developers using a survey instrument inspired by the \gls{sus} \citep{Brooke:1996ua}. Each artefact within the taxonomy is measured against this instrument for its utility, and a metric is produced to indicate how well developers \textit{value} each of these artefacts. We refer to this metric as an `in-practice' score, or \glsac{ips}. (Details for how the \glsac{ips} is calculated are in given in \cref{tse2020:sec:validation:survey:analysis}.) We then identify the artefacts that are highly researched, the ones that developers demand the most, and where gaps in these artefacts remain for future research exploration.


Lastly, while our prior work focused on \textit{generalised} \glsac{api} documentation, in this extension, we apply our taxonomy to a case study of interest: i.e., better documenting \glspl{cvs}. We empirically assess the taxonomy against three popular \glspl{cvs}, namely Google Cloud Vision \citepweb{GoogleCloud:Home}, Amazon Rekognition \citepweb{AWS:Home} and Azure Computer Vision \citepweb{Azure:Home}. For each category in our taxonomy, we assess whether the respective service's documentation contains, partially-contains or does not contain the documentation artefact from our taxonomy, thus determining the extent to which the requirements of good \glsac{api} documentation are met within the vendors' own documentation.
From this, we triangulate each \glsac{ils} and \glsac{ips} value against the service's level of inclusion of its respective documentation artefact, thereby making a judgement as to where the services can improve their documentation to make them more complete. Lastly, we present a ranking of each artefact for where research or vendors should be focus their documentation efforts that is of high value to both developers \textit{and} to industry vendors.

Thus, through this triangulation of the taxonomy with existing literature, utility to practitioners, and application via a case study (\glspl{cvs}), we summarise three aspects of \glsac{api} documentation by identifying:
\begin{enumerate}[label=(\roman*)]
    \item the documentation artefacts that been extensively studied by researchers, and those that warrant further attention by the software engineering research community (via high/low \glsac{ils} values);
    \item the documentation artefacts that are considered to be the most- and least-important from a practitioner's point of view (via high/low \glsac{ips} values);
    \item the documentation artefacts that have been well-established by vendors (via our case study on three prominent \glspl{cvs}).
\end{enumerate}

To demonstrate how our taxonomy was developed, we include an extended revision of the \gls{sms} from our existing work. The taxonomy we proposed consists of five key requirements: (1)~\dima{}; (2)~\dimb{}; (3)~\dimc{}; (4)~\dimd{}; and (5)~\dime{}. Following this, we developed a survey instrument to assess the overall utility of each of the artefacts that contribute towards these five requirements, which consisted of 43 questions of alternating positive and negative sentiment.
We then narrow our focus down to our case study by applying the prioritised documentation artefacts (as identified by the survey) to three \glspl{cvs}. Once our surveys were complete, we provide some general guidelines as to where cloud \glspl{cvs} can make improvements to their \glsac{api} documentation. Lastly, we compare and contrast the results from our \gls{sms} to the results of the survey and of our case study, thereby identifying where future research efforts into \glsac{api} documentation should focus to give the biggest value back to practitioners.

Our key contributions in this work are:

\begin{itemize}
  \item a score metric for each category that indicates where the highest research priorities have been in the existing literature;
  \item a score metric assessing the efficacy of the 34 categories that empirically reflects what artefacts are of the highest value from a \textit{practitioner} point of view;
  \item a heuristic validation of each artefact against \glspl{cvs}, assessing where existing \gls{cvs} \glsac{api} documentation needs improvement;
  \item a number of practical recommendations for \gls{cvs} vendors to better improve the quality of their \glsac{api} documentation; and
  \item an identification of the gaps for future research into \glsac{api} documentation based on the highest need by developers but, so far, has captured the least attention by researchers.

\end{itemize}
This paper is structured as follows: \cref{tse2020:sec:related-work} presents related work; \cref{tse2020:sec:method} is divided into two subsections, the first describing how primary sources were selected in the \gls{sms} with the second describing the development of our taxonomy from these sources; \cref{tse2020:sec:findings} presents the taxonomy; \cref{tse2020:sec:validation} describes how we developed a survey instrument of 43 questions to validate the taxonomy against developers, and assess its efficacy against the three popular \glspl{cvs} selected; \cref{tse2020:sec:tax-analysis} presents the findings from our validation analysis; \cref{tse2020:sec:limitations} describes the threats to validity of this work; and \cref{tse2020:sec:conclusions} provides concluding remarks and the future directions of this study. Additional materials are provided in \cref{ch:tse-supplementary-materials}.

\section{Related Work}
\label{tse2020:sec:related-work}

\subsection{Systematic Reviews in Software Documentation}

Systematic reviews into how developers produce and use software documentation gives researchers consolidated insights into the efforts of multiple, disparate \glsac{api} documentation studies. For example, a recent \citeyear{Nybom:2018ef} study explored 36 \glsac{api} documentation generation tools and approaches, and analysed the tools developed and their inputs and documentation outputs \citep{Nybom:2018ef}. The findings from this study emphasise that the largest effort in \glsac{api} documentation tooling is to assist developers to generate either example code snippets and/or templates or natural language descriptions of the \glsac{api} directly from the program's source code. These snippets or descriptions can then be placed in the \glsac{api} documentation, thereby increasing the efficiency at which \glsac{api} documentation can be written. Additionally, tools from 12 studies target the maintainability of existing \glsacpl{api} of existing \glsacpl{api}, while tools from 11 studies target the correctness and accuracy of the documentation by validating that what is written in the documentation is accurate to the technical structure of the \glsac{api}. From the end-developer's perspective, some tools (17 studies) help target improvements to the developer's understandability and learnability of new \glsacpl{api} by linking in examples directly with questions such as on Stack Overflow.
However, the results from this study regards the \textit{tooling} used to either assist in producing, validating or learning from \glsac{api} documentation. While this is a systematic study with key insights into the types of tooling produced, there is still a gap for an \gls{sms} in what \textit{guidelines} have been produced by the literature in developing natural language documentation itself---and how well developers \textit{agree} to those guidelines---which our work has addressed.

An extensive \gls{sms} into studies presented in the \textit{overall} software documentation domain was given in \citet{ZHI2015175}. This study reviewed a set of 69 papers from 1971 to 2011 to develop a systematic map on the various research aspects relating to documentation cost, benefit and quality, finding that 38\% of papers propose novel techniques while 29\% contribute empirical evidence (i.e., validation and evaluation papers---see \cref{tse2020:sec:data-extraction}). The authors find that a majority of papers discuss quality aspects of software documentation, namely the quality attributes of completeness, consistency and accessibility, and that the main usage of software documentation regards maintenance aid and program comprehension. Another key insight---relevant to our study---found that, on average, survey-based studies into documentation involved 106 participants and generally these participants were from the same (or only two)  organisations. However, unlike our study, this study formalises the documentation efforts of \textit{any} software document, and not exclusively into \glsac{api} documentation artefacts required to help developers produce software. Further, our study differs in that the results from our study are consolidated into a structured taxonomy, instead of a meta-model which \citeauthor{ZHI2015175} perform, which is then triangulated against a real-world use case (i.e., intelligent \glspl{cvs}) and software developers via a survey.

\subsection{API Usability and Documentation Knowledge}

\glsac{api} usability and its impact on documentation knowledge is an imperative area of study, since it provides useful links between \glsac{api} documentation and more technical issues related to \glsac{api} design or tools. Extensive discussions from \citet{myersstylos2016} and \mbox{\citet{7503516}} encapsulate a 30-year effort to evaluate and improve \glsac{api} usability through lenses adapted human-computer interaction research. Essentially, by treating a developer as the `end-user' of an \glsac{api} (i.e., interacting and programming with the \glsac{api} in their own systems), the authors discuss various case studies by which \glsac{api} usability was improved by various human-centred approaches, resulting in improved learnability of the \glsac{api} in addition to improved productivity and effectiveness in using the \glsac{api}. While the methods are primarily used for end-user usability testing, their observations highlight the importance of good aesthetic and interaction design of developer's tooling and the need for new tooling to augment what developers already do to reduce learning overhead. An extensive review of the usability methods used, and their benefits to \glsac{api} usability, demonstrates how various techniques---grounded through established usability guidelines and frameworks---can be used to assess how an \glsac{api}'s usability impacts its key stakeholders (i.e., \glsac{api} designers, developers, and end-users). The role of \glsac{api} \textit{documentation} in context to an \glsac{api}'s overall usability is imperative; for instance, limited documentation on a particular \glsac{api} (and limited code snippets) is often a key complaint to poor \glsac{api} usability \citep{myersstylos2016}. Exploring aspects on information design elements within \glsac{api} documentation is therefore critical to mitigate such complaints.

In \citet{Watson:2012uy}, the authors performed a heuristic assessment from 35 popular \glsacpl{api} against 11 high-level universal design elements of \glsac{api} documentation. Of these 35 \glsacpl{api}, 28 were open-source software repositories and seven came from commercial independent software vendors. Two coders manually inspected each \glsac{api}'s respective documentation sets, starting from the documentation's entry page and using the navigation features of the documentation to further explore the documentation. Both coders evaluated each of the 11 heuristics, noting whether they could be found. This study highlighted how many \glsacpl{api}, even popular ones, fail to grasp these basic design elements. For example, 25\% of the documentation sets did not provide any basic overview documentation to the \glsac{api}. Therefore, from a practitioner's perspective, the study describes a high-level overview of how certain documentation artefacts address their needs and whether they are typically found in documentation. However, while the methodological approach used in this study to assess the heuristics is similar to our approach, the heuristics themselves used within \citeauthor{Watson:2012uy}'s study is based on only three seminal works and only contains 11 design elements. Our study extends these heuristics and structures them into a consolidated, hierarchical taxonomy which we then validate against practitioners.

A taxonomy of distinct knowledge patterns within reference documentation by \citet{Maalej2013} classified 12 distinct knowledge types. Unlike our work, which uses an \gls{sms} of existing studies as the source of our taxonomy development, this study uses a grounded method via theoretical sampling of the \glsac{api} documentation of two mature (extensively documented) open source systems. This was performed by each author to elicit a list of knowledge types over an iterative six month process. The taxonomy was then evaluated against the JDK 6 and .NET 4.0 frameworks using a sample of 5574 documentation units and 17 trained coders to assign each knowledge type to the documentation unit. Results showed that the functionality and structure of these \glsacpl{api} are well-communicated, although core concepts and rationale about the \glsac{api} are quite rarer to see. The authors also identified low-value `non-information'---described as documentation that provides uninformative boilerplate text with no insight into the \glsac{api} at all---which was  substantially present in the documentation of methods and fields in the two frameworks. They recommend that developers factor their 12 distinct knowledge types into the process of code documentation, thereby preventing low-value non-information, and thus developers can use the patterns of knowledge to evaluate the content, organisation, and utility of their own documentation. The development of their taxonomy consisted of questions to model knowledge and information, thereby capturing the reason about disparate information units independent to context; a key difference to this paper is the \textit{systematic} taxonomy approach utilised and the source of information of our taxonomy (i.e., existing literature).

\subsection{Computer Vision Services}

Recent studies into cloud-based \glspl{cvs} have demonstrated that poor reliability and robustness in computer vision can `leak' into end-applications if such aspects are not sufficiently appreciated by developers. A study by \citet{Hosseini:2018jr} showed that Google Cloud Vision's labelling fails when as little as 10\% noise is added to the image. Facial recognition classifiers are easily confused by modifying pixels of a face and using transfer learning to adapt one person's face into another \citep{Wang:2018vl}. Our own prior work found that the non-deterministic evolution of these types of services is not adequately communicated to developers \citep{Cummaudo:2019icsme}, resulting in lost developer productivity whereby developers ask fundamental questions about the concepts behind these services, how they work, and where better documentation can be found \citep{Cummaudo:2020icse}. This paper continues this line of research by providing a means for service providers to better document their services using a taxonomy and suggested improvements.

\section{Taxonomy Development}
\label{tse2020:sec:method}


We developed our taxonomy under two primary phases. First, we conducted an \gls{sms} identifying \glsac{api} documentation studies, following guidelines by \citet{Kitchenham:2007dd} and \citet{Petersen:2008td} (\cref{tse2020:sec:method:lit-review}). A high level overview of this first phase is given in \cref{tse2020:fig:filtering}. Second, we followed a software engineering taxonomy development method by \citet{Usman:2017hn} (\cref{tse2020:sec:method:taxonomy-development}) based on the findings of our \gls{sms}, which involved an extensive validation involving real-world developers and contextualised with computer vision \glsacpl{api} (\cref{tse2020:sec:validation}).

\subsection{Systematic Mapping Study}
\label{tse2020:sec:method:lit-review}

\afterpage{\begin{landscape}
\begin{figure*}[t]
  \includegraphics[width=\linewidth]{slr-years.pdf}
  \caption[Systematic mapping study search results, by years]{Search results by year and venue type.}
  \label{tse2020:fig:slr-years}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{filtering.pdf}
\caption[Filtering steps used in the systematic mapping study]{A high level overview of the filtering steps from defining and executing our search query to the data extraction of our primary studies. Number of accepted papers resulting from each filtering step is shown.}
\label{tse2020:fig:filtering}
\end{figure*}
\end{landscape}}

\subsubsection{Research Questions (RQs)}

The first step in producing our \gls{sms} was to pose two RQs:
\begin{itemize}[leftmargin=\parindent]
  \item \textbf{RQ1:} What documentation `knowledge' do \glsac{api} documentation studies contribute?
  \item \textbf{RQ2:} How is \glsac{api} documentation studied?
\end{itemize}
Our intent behind RQ1 was to collect as many studies provided by literature on how \glsac{api} documentation should be written using natural language, i.e., not using assistive tooling. In this regard, documentation `knowledge' encompasses any natural language \glsac{api} documentation artefact associated with the implementation of an application using a third-party \glsac{api}. As the goals of this study are to arrive at a taxonomy encapsulating the requirements of good \glsac{api} documentation (\cref{tse2020:sec:findings}), we sought to arrive at studies that provide useful information to developers that informs the relevance and value of which aspects of \glsac{api} documentation are more useful than others. This captures the knowledge that developers need to know about what aspects of their \glsacpl{api} should be documented and the artefacts by which they do this. This helped us shape and form the taxonomy provided in \cref{tse2020:sec:findings}. Secondly, RQ2's intent was to understand how the studies derive at their conclusions, thereby helping us identify gaps in literature where future studies can potentially focus.


\subsubsection{Automatic Filtering}

As done in similar software engineering studies \citep{Glass:2002wa,Usman:2017hn,GAROUSI2019101}, we explored  automatic filtering of online databases. We defined which \glsac{swebok} knowledge areas \citep{IEEE:1990wp} were relevant to devise a search query. Our search query was built using related knowledge areas, relevant synonyms, and the term `software engineering' (for comprehensiveness) all joined with the OR operator. Due to the lack of a standard definition of an \glsac{api}, we include the terms: `\glsac{api}' and its expanded term; software library, component and framework; and lastly \glsac{sdk} and its expanded term. These too were joined with the OR operator, appended with an AND. Lastly, the term `documentation' was appended with an AND.
Our final search string was:

\begin{framed}
\noindent
\parbox{\linewidth}{
\scriptsize
( ``software design'' \textbf{OR} ``software architecture" \textbf{OR} ``software construction" \textbf{OR} ``software development" \textbf{OR} ``software maintenance" \textbf{OR} ``software engineering process" \textbf{OR} ``software process" \textbf{OR} ``software lifecycle" \textbf{OR} ``software methods" \textbf{OR} ``software quality" \textbf{OR} ``software engineering professional practice" \textbf{OR} ``software engineering" ) \textbf{AND} ( \glsac{api} \textbf{OR} ``application programming interface" \textbf{OR} ``software library" \textbf{OR} ``software component" \textbf{OR} ``software framework" \textbf{OR} sdk \textbf{OR} ``software development kit" ) \textbf{AND} (~documentation )
}
\end{framed}

We executed the query on all available metadata (title, abstract and keywords) in May 2019 against Web of Science\footnoteurl{http://apps.webofknowledge.com}{23 May 2019}  (WoS), Compendex/Inspec\footnoteurl{http://www.engineeringvillage.com}{23 May 2019} (C/I), and Scopus\footnoteurl{http://www.scopus.com}{23 May 2019}. We selected three particular primary sources given their relevance in software engineering literature (containing the IEEE, ACM, Springer and Elsevier databases) and their ability to support advanced queries \citep{Brereton:2007by,Kitchenham:2007dd}. A total 4,501 results\footnote{Raw results can be located at \url{http://bit.ly/2KxBLs4}.} were found, with 549 being duplicates. \Cref{tse2020:tab:search-results} displays our results in further detail (duplicates not omitted); \cref{tse2020:fig:slr-years} shows an exponential trend of \glsac{api} documentation publications produced within the last two decades. (As this search was conducted in May 2019, results taper in 2019.)

\begin{table}[tb]
  \caption[Summary of search results in API documentation]{Search results and publication types}
  \label{tse2020:tab:search-results}
  \centering
  \begin{tabular}{l|lll|l}
    \toprule
    \textbf{Publication type} &
    \textbf{WoS} &
    \textbf{C/I} &
    \textbf{Scopus} &
    \textbf{Total} \\
    \midrule
    Conference Paper & 27 & 442 & 2353 & 2822 \\
    Journal Article & 41 & 127 & 1236 & 1404\\
    Book & 23 & 17 & 224 & 264\\
    Other & 0 & 5 & 6 & 11\\
    \midrule
    \textbf{Total} & 91 & 591 & 3819 & 4501\\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Manual Filtering}

A follow-up manual filtering stage followed the 4,501 results obtained by automatic filtering. As described below, we applied the following inclusion criteria (IC) and exclusion criteria (EC) to each result:

\begin{enumerate}[leftmargin=2\parindent,label=\textbf{IC\arabic*}]
  \item Studies must be relevant to \glsac{api} documentation: specifically, we exclude studies that deal with improving the technical \glsac{api} usability (e.g., improved usage patterns);
  \item Studies must discuss artefacts that document \glsacpl{api};
  \item Studies must be relevant to software engineering as defined in \glsac{swebok};
\end{enumerate}
\begin{enumerate}[leftmargin=2\parindent,label=\textbf{EC\arabic*}]
  \item Studies where full-text is not accessible through standard institutional databases;
  \item Studies that do not propose or extend how to improve the official, natural language documentation of an \glsac{api};
  \item Studies proposing a third-party tool to enhance existing documentation or generate new documentation using data mining (i.e., not proposing strategies to improve official documentation);
  \item Studies not written in English;
  \item Studies not peer-reviewed.
\end{enumerate}
\smallskip

Each of these ICs and ECs were applied to every paper  after exporting all  metadata of our results to a spreadsheet. The first author then curated the publications using the following revision process.

Firstly, we read the publication source---to rapidly omit non-software engineering papers---as well as the author keywords, title, and abstract of all 4,501 studies. As some studies were duplicated between our three primary sources, we needed to remove any repetitions. We sorted and reviewed any duplicate DOIs and fuzzy-matched all very similar titles (i.e., changes due to punctuation between primary sources), thereby retaining only one copy of the paper from a single database. Similarly, as there was no limit do our date ranges, some studies were republished in various venues (i.e., same title but different DOIs). These were also removed using fuzzy-matching on the title, and the first instance of the paper's publication was retained. This second phase resulted in 3,987 papers.

Secondly, we applied our inclusion and exclusion criteria to each of the 3,987 papers by reading the abstract. Where there was any doubt in applying the criteria to the abstract alone, we automatically shortlisted the study. We rejected 427 studies that were unrelated to software engineering, 3,235 were not directly related to documenting \glsacpl{api} (e.g., to enhance coding techniques that improve the overall developer usability of the \glsac{api}), 182 proposed new tools to enhance \glsac{api} documentation or used machine learning to mine developer's discussion of \glsacpl{api}, and 10 were not in English. This resulted in 133 studies being shortlisted to the final phase.

Thirdly, we re-evaluated each shortlisted paper by re-reading the abstract, the introduction and conclusion. We removed a further 64 studies that were on \glsac{api} usability or non \glsac{api}-related  documentation (i.e., code commenting). At this stage, we decided to refine our exclusion criteria to better match the research goals of this study by including the word `natural language' documentation in EC2. This removed studies where the focus was to improve technical documentation of \glsacpl{api} such as data types and communication schemas. Additionally, we removed 26 studies as they were related to introducing new tools (EC3), 3 were focused on tools to mine \glsac{api} documentation, 7 studies where no guidelines were provided, 2 further duplicate studies, and a further 10 studies where the full text was not available, not peer reviewed or in English. Books are commonly not peer-reviewed (EC5), however no books were shortlisted within these results. This final stage resulted in 21 primary studies for further analysis, and the mapping of primary study identifiers to references S1--21 can be found in \cref{tse2020:sec:primary-sources}.

As a final phase, we conducted reliability analysis of our shortlisting method. We conducted intra-rater reliability of our 133 shortlisted papers using the test-retest approach suggested by \citet{Kitchenham:2007dd}. We re-evaluated a random sample of 10\% of the 133 shortlisted papers a week after initial studies were shortlisted. This resulted in \textit{substantial agreement}~\citep{Landis:1977kv}, measured using Cohen's kappa ($\kappa=0.7547$).


\subsubsection{Data Extraction \& Systematic Mapping}
\label{tse2020:sec:data-extraction}

Of the 21 primary studies, we conducted abstract key-wording adhering to \citeauthor{Petersen:2008td}'s guidelines \citep{Petersen:2008td} to develop a classification scheme.
An initial set of keywords were applied for each paper in terms of their methodologies and research approaches (RQ2), based on an existing classification schema used in the requirements engineering field by \citet{Wieringa:2006vd}. These are: \textit{evaluation papers}, which evaluates existing techniques currently used in-practice; \textit{validation papers}, which investigates proposed techniques not yet implemented in-practice; \textit{experience papers}, which are written by practitioners in the field and provide insight into their experiences of adopting existing techniques; and \textit{philosophical papers}, which presents new conceptual frameworks that describes a language by which we can describes our observations of existing or new techniques, thereby implying a new viewpoint for understanding phenomena. For example, documenting \glsacpl{api} using code snippets is a commonly used practice by developers (see the primary sources listed in \cref{tse2020:tab:taxonomy}), and conducting an experiment exploring how quickly practitioners achieve this would be an evaluation paper. In contrast, a validation paper explores novel techniques that are proposed but not yet implemented in practice; for example, a paper proposing that \glsacpl{api} should document success stories so that developers know where, why, and how the \glsac{api} was successfully implemented may test this novel technique via field study experiments (e.g., interviewing developers on the new technique) without reference to real-world examples. A paper written by a group of developers sharing their insights into the improvements of their documentation before and after providing extensive tutorials would be an experience paper. Philosophical papers may propose entirely new vocabulary to explore \glsac{api} documentation, devising new frameworks from which other researchers can explore the field from a new viewpoint.

After all primary studies had been assigned keywords, we noticed that all papers used field study techniques, and thus we consolidated these keywords using \citeauthor{Singer:2007tu}'s framework of software engineering field study techniques \citep{Singer:2007tu}. \citeauthor{Singer:2007tu} captures both study techniques \textit{and} methods to collect data within the one framework, namely: \textit{direct techniques}, including brainstorming and focus groups, interviews and questionnaires, conceptual modelling, work diaries, think-aloud sessions, shadowing and observation, participant observation; \textit{indirect techniques}, including instrumenting systems, fly-on-the-wall; and \textit{independent techniques}, including analysis of work databases, tool use logs, documentation analysis, and static and dynamic analysis.

\Cref{tse2020:tab:extraction} describes our data extraction form, which was used to collect relevant data from each paper. \Cref{tse2020:fig:sms} presents our systematic mapping, where each study is mapped to one (or more, if applicable) of methodologies plotted against \citeauthor{Wieringa:2006vd}'s research approaches. We find that a majority of these studies survey developers using direct techniques (i.e., interviews and questionnaires) and some performing structured documentation analysis. Few studies report recent experiences; literature reports the artefacts that document \glsacpl{api} from evaluation research, in addition to some validation studies. There are few experience papers describing anecdotal evidence, and almost no philosophical papers that describe new conceptual ways at approaching \glsac{api} documentation as a large majority of existing work either evaluates existing (in-practice) strategies or validates the effectiveness of new strategies.

\begin{table}[tb]
  \caption[Data extraction form used for the systematic mapping study]{Data extraction form}
  \label{tse2020:tab:extraction}
  \centering
  \tablefit{\begin{tabular}{l|p{0.8\linewidth}}
    \toprule
    \textbf{Data item(s)} &
    \textbf{Description}
    \\
    \midrule
    Citation metadata & Title, author(s), years, publication venue, publication type \\
    Artefact(s) discussed & As per IC2, the study must identify at least one \glsac{api} documentation artefact \\
    Evaluation method & Did the authors evaluate their proposed artefacts? If so, how? \\
    Primary technique & The primary technique used to devise the artefact(s) \\
    Secondary technique & As above, if a second study was conducted \\
    Tertiary technique & As above, if a third study was conducted \\
    Research type & The research type employed in the study as defined by \citeauthor{Wieringa:2006vd}'s taxonomy \\
    \bottomrule
  \end{tabular}}
\end{table}

\subsection{Development of the Taxonomy}
\label{tse2020:sec:method:taxonomy-development}


A majority of taxonomies produced in software engineering studies are often made extemporaneously \citep{Usman:2017hn}. For this reason, we decided to proceed with a systematic approach to develop our taxonomy using the guidelines provided by \citet{Usman:2017hn}, which are extended from lessons learned in more mature domains. In this subsection, we outline the 4 phases and 13 steps taken to develop our taxonomy based on \citeauthor{Usman:2017hn}'s technique. \citeauthor{Usman:2017hn}'s final \textit{validation} phase is largely detailed within \cref{tse2020:sec:validation} after we present our taxonomy in \cref{tse2020:sec:findings}.


Formally, \citeauthor{Usman:2017hn} provides guidelines to define these units under the first six stages under the planning phase. In our study, our preliminary phase involves answering the following:

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{sms}
  \caption[A systematic map of API documentation studies]{Systematic map: field study technique vs research type}
  \label{tse2020:fig:sms}
\end{figure}

\begin{enumerate}[label=\textbf{(\arabic*)}]
  \item \textit{define the software engineering knowledge area}: The software engineering knowledge area, as defined by the \glsac{swebok}, is software construction;
  \item \textit{define the objective}: The main objective of the proposed taxonomy is to define a set of categories that enables to classify different facets of natural language \glsac{api} \textit{documentation} artefacts (not \glsac{api} \textit{usability}) as reported in existing literature;
  \item \textit{define the subject matter}: The subject matter of our proposed taxonomy is  documentation artefacts of \glsacpl{api};
  \item \textit{define the classification structure}: The classification structure of our  proposed taxonomy is \textit{hierarchical};
  \item \textit{define the classification procedure}: The procedure used to classify the documentation artefacts is qualitative;
  \item \textit{define the data sources}: The basis of the taxonomy is derived from field study techniques (see \cref{tse2020:sec:data-extraction}).
\end{enumerate}

\subsubsection{Identification and extraction phase} The second phase of the taxonomy development involves \textbf{(7)}~\textit{extracting all terms and concepts} from relevant literature, which we have achieved from our \gls{sms}. These terms are then consolidated by \textbf{(8)}~\textit{performing terminology control}, as some terms may refer to different concepts and vice-versa. For example, \citeauthor{Watson:2012uy} defines one of the heuristics used in the study's experiment as ``sample apps to understand how to use the elements of an \glsac{api} in context and as another source from which to copy program code... a sample app is a complete application that includes examples of the \glsac{api} as well as the other functions that comprise a complete program''~\citep{Watson:2012uy}. In this case, the term `sample app', `program code', and `complete application' were extracted as a term of interest and noted. Similarly, in \citet{Robillard:2009uk}, the phrase `applications' is used to define a category of example code snippets which ``consists of code segments from complete applications'' and is generally some form of ``demonstration samples sometimes distributed with an \glsac{api}... that developers can download from various source code repositories''~\citep{Robillard:2009uk}. Again, the phrase `complete applications', `demonstration samples', `download', and `source code' was identified as a terms of interest and noted. Once all papers were read, we consolidated a list of all of these noted highlights to help consolidate the terms and perform terminology control. In this example, the phrase `Downloadable source code demonstrating complete sample applications' was consolidated from both \citeauthor{Watson:2012uy} and \citeauthor{Robillard:2009uk}'s studies, which---in addition to the other primary studies that iteratively changed wording slightly due to steps (9--10)---formed the basis of the taxonomy dimension \dimcat{A7}.

\subsubsection{Design phase} \label{tse2020:sec:method:taxonomy-development:design-phase} The design phase identified the core dimensions and categories within the extracted data items. The first step is to \textbf{(9)}~\textit{identify and define taxonomy dimensions}; for this study we utilised a bottom-up approach to identify each dimension, i.e., extracting the categories first and then nominating which dimensions these categories fit into using an iterative approach. As we used a bottom-up approach, step (9) also encompassed the second stage of the design phase, which is to \textbf{(10)}~\textit{identify and describe the categories} of each dimension. Thirdly, we \textbf{(11)}~\textit{identify and describe relationships} between dimensions and categories, which can be skipped if the relationships are too close together, as is the case of our grouping technique which allows for new dimensions and categories to be added. The last step in this phase is to \textbf{(12)}~\textit{define guidelines for using and updating the taxonomy}. The taxonomy is as simple as a checklist that can be heuristically applied to \glsac{api} documentation, and each dimension is malleable and covers a broad spectrum of artefacts; while we do not anticipate any further dimensions to be added, new categories can easily be fitted into one of the dimensions (see \cref{tse2020:sec:conclusions}). We provide guidelines for use in our application of the taxonomy against \glspl{cvs} within \cref{tse2020:sec:findings,tse2020:sec:tax-analysis}.

\subsubsection{Validation phase} In the final phase of taxonomy development, taxonomy designers must \textbf{(13)}~\textit{validate the taxonomy} to assess its usefulness. \citet{Usman:2017hn} describe three approaches to validate taxonomies: (i)~orthogonal demonstration, in which the taxonomy's orthogonality is demonstrated against the dimensions and categories, (ii)~benchmarking the taxonomy against similar classification schemes, or (iii)~utility demonstration by applying the taxonomy heuristically against subject-matter examples. In our study, we adopt utility demonstration by use of a survey and heuristic application of the taxonomy against real-world case-studies (i.e., within the domain of \glspl{cvs}). This is is discussed in greater detail within \cref{tse2020:sec:validation}.


\section{A Taxonomy for API Documentation}
\label{tse2020:sec:findings}

\begin{figure}[p]
\centering
\includegraphics[width=.7\linewidth]{taxonomy.pdf}
\caption[Our proposed API documentation taxonomy]{Our proposed taxonomy: The requirements of good-quality \glsac{api} documentation (dimensions) represented through individual documentation artefacts (categories).}
\label{tse2020:fig:taxonomy}
\end{figure}

Our taxonomy consists of five dimensions (labelled A--E). These five dimensions are made of 34 categories, which represent  \glsac{api} documentation artefacts that contribute towards these dimensions. In the context of our taxonomy, a category can represent (i)~discrete and self-contained documentation artefacts (e.g., quick start guides \dimcat{A1}), (ii)~additional information used to describe the \glsac{api} (e.g., licensing information about the \glsac{api} \dimcat{D6}), or (iii)~aspects regarding the information design of this documentation (e.g., consistent look and feel \dimcat{E6}). {Collectively, the categories form the \textit{requirements} of good quality \glsac{api} documentation, as expressed through the five dimensions.} When worded as questions, each dimension respectively covers the following:

\begin{itemize}
  \item \dimcat{A}~\textbf{\dima{}}: \textit{how} does the developer use this \glsac{api} for their intended use case?
  \item \dimcat{B}~\textbf{\dimb{}}: \textit{when} should the developer choose this particular \glsac{api} for their intended use case?
  \item \dimcat{C}~\textbf{\dimc{}}: \textit{why} does the developer select this particular \glsac{api} for their application's domain and does the \glsac{api}'s domain align with the application's domain?
  \item \dimcat{D}~\textbf{\dimd{}}: \textit{what} additional \glsac{api} documentation can the developer find to aid their productivity?
  \item \dimcat{E}~\textbf{\dime{}}: is the \textit{visualisation} of the above information well organised and easy for the developer to digest?
\end{itemize}
Further descriptions of the categories encompassing each dimension are given within \cref{tse2020:fig:taxonomy} and \cref{tse2020:tab:taxonomy}, coded as [$Xi$], where $i$ is the category identifier within a dimension, $X$, where $X~\in~\{ A, B, C, D, E \}$.

\Cref{tse2020:tab:taxonomy} shows which of the primary sources (S1--21) reports aspects of the artefacts described as an `in-literature score' (\glsac{ils}). This score is calculated as a percentage of the number of primary studies that investigated or reported various issues regarding the specific artefact divided by the total of primary studies (see \cref{tse2020:sec:tax-analysis:ils}). This score is contrasted to the `in-practice score' (\glsac{ips}) which indicates the overall level of agreement that \textit{practitioners} think such documentation artefacts are needed (see \cref{tse2020:sec:tax-analysis:ips}).
For comparative purposes, we illustrate a colour scale (from red to green) to indicate the relevancy weight between \glsac{ils} and \glsac{ips} values in \cref{tse2020:tab:taxonomy} as per their assigned, discretised intervals (see \cref{tse2020:tab:ils-ips-intervals}). We also show illustrative interpretations of these generalised artefacts through italicised examples within \cref{tse2020:tab:taxonomy}.
We then provide three columns that assesses the presence of these documentation artefacts against three popular \glspl{cvs}: Google Cloud Vision, AWS's Rekognition, and Azure Cloud Vision (abbreviated to GCV, AWS and ACV). A fully shaded circle~(\circlepresent{}) indicates that the documentation artefact was clearly found in the service, while a half-shaded circle~(\circlepartialpresent{}) indicates that the artefact was only partially present. An outlined circle~(\circlenotpresent{}) indicates that the service lacks the indicated documentation artefact within our taxonomy. This empirical assessment is further detailed in \cref{tse2020:sec:tax-analysis:cvs-improvement}, which outlines concrete areas in the respective services' documentation where improvements could be made, as well as hyperlinks to the documentation where relevant.

\Cref{tse2020:fig:taxonomy} illustrates a condensed version of taxonomy.
We provide iconography for the presence~(\faCheckCircle) or non-presence~(\faTimesCircle) of these   artefacts in \textit{all three} \glspl{cvs} assessed, per \cref{tse2020:sec:tax-analysis:ips}.



\section{Validating the Taxonomy}
\label{tse2020:sec:validation}

\subsection{Survey Study}
\label{tse2020:sec:validation:survey}

\subsubsection{Designing the Survey}

We followed the guidelines by \citet{Kitchenham:2007ux} on conducting personal opinion surveys in software engineering to validate our survey. In developing our survey instrument, we  shaped questions around each of our 5 dimensions and 34 categories. To achieve this, we used \citeauthor{Brooke:1996ua}'s \gls{sus} \citep{Brooke:1996ua} as a loose inspiration and re-shaped the 34 categories around a question that imitates the style of wording of questions used in the \gls{sus}. Each dimension was marked a numeric question (Q\#3--7), and alphabetic sub-questions were marked for each sub-dimension or category.

We used closed questioning where respondents could choose an answer on a 5-point Likert-scale (1=\textit{strongly disagree}, 2=\textit{somewhat disagree}, 3=\textit{neither agree nor disagree}, 4=\textit{slightly agree} and 5=\textit{strongly agree}).  Like \citeauthor{Brooke:1996ua}'s study, each question alternated in positive and negative sentiment. Half of our questions were written where a likely common response would be in strong agreement and vice-versa for the other half, such that participants would have to ``read each statement and make an effort to think whether they would agree or disagree with it'' \citep{Brooke:1996ua}. For example, the question regarding \dimcat{B7} on \glsac{api} limitations was framed as: ``\textit{I believe it is important to know about what the limitations are on what the \glsac{api} can and cannot provide}'' (Q4g), whereas the question regarding \dimcat{C1} on domain concepts of the \glsac{api} was framed as: ``\textit{I wouldn't read through theory about the \glsac{api}'s domain that relates theoretical concepts to \glsac{api} components and how both work together}'' (Q5a).

In addition, the remaining eight questions asked demographical information. An extra open question asked for further comments. The full survey is provided in \cref{tse2020:sec:survey} and anonymised survey data is available at \mbox{\url{https://bit.ly/33siqll}}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{roles-and-seniority.pdf}
\caption[Roles and seniority from survey participants]{A wide variety of roles and seniority were observed in our respondents.}
\label{tse2020:fig:roles-and-seniority}
\end{figure*}

\subsubsection{Evaluating the Survey}
\label{tse2020:sec:validation:survey:eval}

After the first pass at designing questions was completed, we evaluated our survey on three researchers within our research group for general feedback. This resulted in minor changes, such as slight re-wording of questions and providing specific questions with examples (some with images). For example, the question regarding \dimcat{A9} on an exhaustive list of all major components in the \glsac{api} was framed as ``\textit{I believe an exhaustive list of all major components in the \glsac{api} without excessive detail would be useful when learning an \glsac{api}}'' (Q3i) with the example ``\textit{e.g., a computer vision web \glsac{api} might list object detection, object localisation, facial recognition, and facial comparison as its 4 components}''.

After this, we conducted reliability analysis using a test-retest approach on three developers within our group seven weeks apart. Using the R statistical computation environment \citep{RCoreTeam}, we conducted our analysis using the \texttt{irr} package~\citep{Gamer:tj} (as suggested in~\citep{Hallgren:2012kt}) and  resulted in an average intra-class correlation (ICC) of 0.63 which indicates a good overall index of agreement \citep{cicchetti1994guidelines}.

\subsubsection{Recruiting Participants}

Our target population for the study was application software developers with varying degrees of experience (including those who and who have not used \glspl{cvs} or related tools before) and varying understanding of fundamental machine learning concepts. We began by recruiting software developers within our research group using a group-wide message sent on our internal messaging system. Of the 44 developers in our group's engineering cohort,\footnote{Our research group's engineering cohort consists of fully-qualified software engineers, with on average 5+ years industry experience.} \SurveyParticipantsInternal{} responses were returned, indicating an internal response rate of \SurveyParticipantsInternalResponseRate{}. Based on the \SurveyParticipantsInternal{} results from this internal trial, we calculated the median time to our complete survey was just over 20 minutes.

For external participant recruiting, we shared the survey on social media platforms and online-discussion forums relevant to software development. We adopted a non-probabilistic snowballing sampling where the participants, at the end of the survey, were encouraged to share the survey link to others using \textit{AddThis}.\footnoteurl{https://www.addthis.com/}{7 January 2020} Additionally, snowballing sampling was encouraged within members of our research group who were asked to share the survey. This sampling approach resulted in \SurveyParticipantsExternalSnowball{} external responses. A further \SurveyParticipantsExternalMTurk{} participants were recruited via Amazon Mechanical Turk\footnoteurl{https://www.mturk.com/}{9 July 2020}---often referred to as MTurk---which has been a successful approach adopted in previous software engineering surveys (e.g., \citep{Jiarpakdee2020}). To ensure our target demographic was selected, we applied the participant filter option `Employment Industry - Software \& IT Services'.
An additional \SurveyParticipantsExternalPartialResponses{} responses were partially filled (on average at a completion rate of \SurveyParticipantsExternalPartialResponsesCompletionRate{}). These partially completed responses were included in our analysis since they did yield some insight (see \cref{tse2020:sec:limitations:internal}).
As participants recruited via MTurk have a financial incentive to complete surveys,\footnote{A total budget of AUD\$600 was allocated for recruitment via MTurk, with each participant receiving between AUD\$3.50--\$10.00.} we ensured strict quality control was applied to each survey response we received. For example, \SurveyParticipantsExternalResponseTooSmall{} participants opened the survey but did not answer any questions; for this reason, all survey responses by these participants were discarded. We identified that \SurveyParticipantsExternalMTurkRejected{} MTurk responses were filled out too quickly (where the median response time was under five minutes; well below the internal average of 20 minutes), and further analysis of these \SurveyParticipantsExternalMTurkRejected{} responses indicated poor reading of the question, and thus poor responses; this was identified via our use of alternating positively- and negatively-worded questions. Thus, \SurveyParticipantsExternalMTurkRejected{} MTurk responses were removed from the final analysis. Therefore, our final response rate yielded \SurveyParticipantsTotal{} responses of the total \SurveyParticipantsTotalReached{} participants reached; an overall response rate of \SurveyParticipantsTotalResponseRate{}.

\subsubsection{Analysing Response Data}
\label{tse2020:sec:validation:survey:analysis}

To analyse our response data, we produced a single score for each question's 5-point response. In line with with \citeauthor{Brooke:1996ua}'s \gls{sus} methodology \citep{Brooke:1996ua}, we subtracted one from the raw value of positive items, and subtracted the raw value from five for the negative items. This resulted in values on an ordinal scale of 0--4. We then averaged each response for every question and divide by four (i.e, now a 4-point scale) to obtain scores for each category. For example, two responses of \textit{strongly agree}=5 and one of \textit{neither agree nor disagree}=3 were given to \dimcat{A1} (positively worded); these values are mapped to 4 and 2, respectively, and are averaged (to 3.33) which is then divided by a maximum possible score of four, giving 0.84. We then discretise these calculated values into five intervals (as per \cref{tse2020:tab:ils-ips-intervals}, see \cref{tse2020:sec:tax-analysis:ips}) to interpret the findings; this is presented in \cref{tse2020:tab:taxonomy} under the `in-practice score' (\glsac{ips}) for each category.

Demographics for our survey were consistent in terms of the experience levels of developers who responded. 78\% of respondents indicated they were professional programmers. Years of programming experience were: \textless 1~year (3.30\%); 1--5~years (41.76\%); 6--10~years (35.16\%); 11--15~years (9.89\%); 16--20~years (5.49\%); 21--30~years (3.30\%); 31--40~years (1.10\%); 41+ years~(0.00\%). A wide range of roles and seniority were listed by developers as presented in \cref{tse2020:fig:roles-and-seniority}, thereby indicating that our results include the different expectations of \glsac{api} documentation from a variety of sources. The highest role was a full-stack developer at either a mid-tier or senior role, followed by mid-tier or senior back-end developers and graduate and junior business analysts. Various managerial roles were also listed. Only five students (5.00\%) responded in our study, two listing themselves as interns with one as an embedded applications developer. Most respondents were Australian (40.00\%), Indian (26.70\%) or from the United States (20.00\%). Besides information technology services (30.77\%), consulting and other software development (both at 9.89\%) were the most predominant industries listed by participants.

\subsection{Empirical Application on Computer Vision Services}
\label{tse2020:sec:validation:empirical-app}

Once our taxonomy had been developed and assessed with developers, we performed an empirical application against three \glspl{cvs}:  Google Cloud Vision \citepweb{GoogleCloud:Home}, Amazon Rekognition \citepweb{AWS:Home} and Azure Computer Vision \citepweb{Azure:Home}. Our selection criteria in choosing these particular services to analyse is based on the prominence of the service providers in industry and the ubiquity of their cloud platforms (Google Cloud, Amazon Web Services, and Microsoft Azure) in addition to being the top three adopted vendors used for cloud-based enterprise applications \citep{RightScaleInc:2018kJ}. In addition, we had conducted extensive investigation into the services' non-deterministic runtime behaviour and evolution profile in prior work \citep{Cummaudo:2019icsme} and have also identified developers' complaints about their incomplete documentation in a prior mining study on Stack Overflow \citep{Cummaudo:2020icse}.

We began with an exploratory analysis of the presence of each dimension and its categories. \Cref{tse2020:tab:docsources} displays all sources of documentation used; although we initially started on the respective services homepages \citepweb{GoogleCloud:Home,AWS:Home,Azure:Home}, this search was expanded to other webpages hyperlinked. For each category, we listed the documentation's presence as either fully present, partially present or not present at all. This is shown in \cref{tse2020:tab:taxonomy} with the indication of \mbox{(half-)filled} circles or circle outlines for Google Cloud Vision (abbreviated to GCV), Amazon Rekognition (abbreviated to AWS), and Azure Computer Vision (abbreviated to ACV). Notes were taken for each webpage justifying the presence, and exact sources of documentation were listed when (partially) present. PDFs of each webpage were downloaded between 14--18 March 2019 for analysis. Analysis was performed manually by the lead author by manual inspection of the downloaded web pages (as PDFs) and presence of each item was noted by the lead author using an approach similar to \citet{Watson:2012uy}.

\section{Taxonomy Analysis}
\label{tse2020:sec:tax-analysis}

In this section, we analyse investigating the taxonomy from two perspectives. Firstly, we contrast the \glsac{ils} values, being an interpretation of the relevancy researchers have emphasised, against the \glsac{ips} values found from the results of our survey (being an interpretation of what documentation artefacts developers value more). We are therefore able to identify the \glsac{api} documentation artefacts that are of high value to practitioners, but are yet to be deeply explored by researchers. Secondly, we contrast the \glsac{ips} values against our assessment of \glspl{cvs}, and whether important \glsac{api} documentation artefacts have been included in popular services. We are therefore able to identify whether vendors have or have not already included these highly-valued documentation artefacts within their own \glsacpl{api}, and where existing areas of improvement lie.

\subsection{Exploring IPS and ILS Values}
\label{tse2020:sec:tax-analysis:ils-vs-ips}

\subsubsection{IPS Results}
\label{tse2020:sec:tax-analysis:ips}


\begin{table*}
  \centering
  \caption[Intervals assigned to ILS and IPS values]{Intervals of \glsac{ils} (top) and \glsac{ips} (bottom) values and frequencies.}
  \label{tse2020:tab:ils-ips-intervals}
  \tablefit{\begin{tabular}{c|ccp{0.35\linewidth}}
    \toprule
    \textbf{Research Attention} & \textbf{Range} & \textbf{Frequency} & \textbf{Categories}\\
    \midrule
Very Low & $0.00 \leq \textrm{\glsac{ils}}(\,[Xi]\,) < 0.14$ & 7 & B4, B5, D6, B3, C1, D1, D2\\
Low & $0.14 \leq \textrm{\glsac{ils}}(\,[Xi]\,) < 0.29$ & 13 & A1, A9, C3, D3, D4, E2, E3,\newline E4, E5, B6, A7, A10, D5\\
Medium & $0.29 \leq \textrm{\glsac{ils}}(\,[Xi]\,) < 0.43$ & 9 & B2, B7, A4, A12, E1, A3, A8,\newline A11, C2\\
High & $0.43 \leq \textrm{\glsac{ils}}(\,[Xi]\,) < 0.57$ & 3 & E6, B1, A2\\
Very High & $0.57 \leq \textrm{\glsac{ils}}(\,[Xi]\,) \leq 0.71$ & 2 & A6, A5\\
    \midrule
    \midrule
    \textbf{Value to Developers} & \textbf{Range} & \textbf{Frequency} & \textbf{Categories}\\
    \midrule
Very Low & $0.00 \leq \textrm{\glsac{ips}}(\,[Xi]\,) < 0.18$ & 0 & --\\
Low & $0.18 \leq \textrm{\glsac{ips}}(\,[Xi]\,) < 0.36$ & 0 & -- \\
Medium & $0.36 \leq \textrm{\glsac{ips}}(\,[Xi]\,) < 0.53$ & 6 & D4, B4, C3, C1, E4, B3\\
High & $0.53 \leq \textrm{\glsac{ips}}(\,[Xi]\,) < 0.71$ & 16 & A4, B6, A2, D2, A6, E2, B5, D6,\newline A8, B2, E6, A10, E5, D5, A9, D3\\
Very High & $0.71 \leq \textrm{\glsac{ips}}(\,[Xi]\,) \leq 0.89$ & 12 & E3, A7, A3, C2, A12, B1, D1,\newline A11, A1, E1, A5, B7\\
    \bottomrule
  \end{tabular}}
\end{table*}

\glsac{ips} values indicate the extent to which developers agree with the statements made in our survey, as calculated by the method described in \cref{tse2020:sec:validation:survey:analysis}. The interpretation of these values are the documentation artefacts (categories) that developers \textit{value} the most. Thus collectively, these artefacts indicate the overall level of importance towards specific \glsac{api} documentation requirements (dimensions).

To interpret these values, we group the data from each of our survey's 34 statements (for each category) into an ordinal scale of five intervals. These intervals indicate relative value to developers; a documentation artefact has \textit{very low} value to developers, \textit{low} value, \textit{medium} value, \textit{high} value, or \textit{very high} value. \Cref{tse2020:tab:ils-ips-intervals} presents these intervals and frequencies of each, with the order of the categories shown in the last column indicating raw \glsac{ips} values (least useful to most useful) before discretisation in ascending order.

Practitioners tend to agree that each documentation artefact is important to have, and thus \glsac{ips} values likely fall into the \textit{High} or \textit{Very High} intervals. Only six categories fall into the \textit{Medium} interval and none fall into lower intervals. Developers find technical support contact information \dimcat{D4} to be of the lowest value (see \cref{tse2020:tab:ils-ips-intervals}), likely since developers tend to rely on crowd-sourced peer support through mediums such as Stack Overflow. They also see little value in: descriptions of the types of end-users the \glsac{api} is intended for \dimcat{B4}; documentation for non-technical audiences \dimcat{C3}; conceptual information relating the \glsac{api} back to its application domain \dimcat{C1}; structured navigation of the presented \glsac{api} documentation \dimcat{E4}; and descriptions of the intended developers who should be using the \glsac{api} \dimcat{B3}.

\subsubsection{ILS Results}
\label{tse2020:sec:tax-analysis:ils}
\glsac{ils} values indicate overall research attention of categories of our taxonomy through the proportion of papers in our \gls{sms} that investigated or reported various issues regarding a specific \glsac{api} documentation artefact. Collectively, each of these categories combined form a dimension (labelled A--E) in a bottom-up approach (see \cref{tse2020:sec:method:taxonomy-development:design-phase}). Each dimension (top-node) describes the requirements of good quality \glsac{api} documentation, while the category (leaf-node) is the specific \glsac{api} documentation artefact that, collectively, form the requirement. A category with a high \glsac{ils} value indicates that existing studies that there is substantial attention by researchers on this specific documentation artefact (or, collectively, requirement of good quality \glsac{api} documentation). Conversely, a lower \glsac{ils} value indicates less attention reported on these categories (artefact) or dimensions (requirement) by the software engineering research community.

To demonstrate the attention of these documentation artefacts within literature, we interpret the \glsac{ils} values in a similar fashion to the \glsac{ips} values. It is represented as a discretised value of intervals within a five-dimensional ordinal scale, where the attention on these artefacts in literature are one of: \textit{very low} attention, \textit{low} attention, \textit{medium} attention, \textit{high} attention, \textit{very high} attention. \Cref{tse2020:tab:ils-ips-intervals} indicates the boundaries for each interval (as calculated by the highest \glsac{ils} value of \dimcatILSvalueAFive{} divided by the five intervals) in addition to the frequency of categories appearing in each interval. The order of the categories shown in the last column indicate the ascending order (least research attention to most) of raw \glsac{ils} values before discretisation. As shown, most of the artefacts (20) found in the taxonomy are discussed in literature disproportionately more than others (i.e., those that fall into the `low' (13) or `very low' (7) intervals), though the underlying reasons behind this should be considered on a case-by-case basis (see \cref{tse2020:sec:threats:construct}).

There are only five categories that fall into the `high' or `very high' intervals, three of which fall under dimension \dimcat{A}, \dima{}. Research attention on a particular documentation artefact that is considered \textit{Very High} gravitates towards code snippets \dimcat{A5} and tutorials \dimcat{A6}. Code snippets are the readiest form of \glsac{api} documentation for developers, representing exemplary nuggets of information for developers to rapidly digest singular components of the \glsac{api}'s functionality. While code snippets generally only reflect small portions of \glsac{api} functionality (generally limited to 15--30 LoC), this is complimented by step-by-step tutorials. These may tie in multiple (disparate) components of \glsac{api} functionality to demonstrate development of more non-trivial applications. Therefore, unsurprisingly, research has substantially explored how best \glsac{api} developers can extract code snippets or write tutorials for these purposes in mind. This is followed by low-level reference documentation \dimcat{A2}---under the `high' interval---whereby developers should document all client-facing implementation or usage aspects of their \glsac{api} (e.g., class, method, parameter descriptions etc.).
Lastly, the entry-level purpose/overview of an \glsac{api} \dimcat{B1} and consistency in the look and feel of the documentation throughout all of the \glsac{api}'s official documentation \dimcat{E6} are fall under the `high' interval. \glsac{api} vendors must give motivation as to why a developer should choose a particular \glsac{api} over another, articulating the \textit{need} of their \glsac{api}, presenting this and other documentation aspects in the easiest way for developers to consume.

\subsubsection{Research Opportunities for High-Value Artefacts}
\label{tse2020:sec:tax-analysis:ips-vs-ils}

\begin{figure}[h]
	\centering
  \includegraphics[width=.8\linewidth]{ips-vs-ils.png}
  \caption[Comparing value of API documentation artefacts to developers vs research attention]{Value of \glsac{api} documentation artefacts to developers (\glsac{ips}) vs their research attention (\glsac{ils}). Colour intensity represents greater number of categories in each intersection}
  \label{tse2020:fig:ips-vs-ils}
\end{figure}

In this section, we explore the \glsac{ils} and \glsac{ips} values as two distinct indicators of research exploration that would provide the most value to practitioners. We then provide a qualitative discussion by inspecting the intersection of categories at each respective interval identified by our \gls{sms} and survey study. Thus, we are able to determine documentation artefacts (categories) and requirements (dimensions) that provide the \textit{greatest value} to developers but have not gained proportional attention in the software engineering literature when compared to other artefacts, and vice-versa. Graphically, we represent these intersections within a five-by-five matrix with intervals of the \glsac{ips} ($x$ axis) plotted against intervals of the \glsac{ils} ($y$ axis). Intersections between the two are listed for each category within the taxonomy. This is presented in \cref{tse2020:fig:ips-vs-ils}.

There is a distinction between \mbox{(very-)highly} valued documentation artefacts whose research attention is \mbox{(very-)low}, as presented in the bottom-right of \cref{tse2020:fig:ips-vs-ils}. Most notably, we find that developers find \dimd{} \dimcat{D} a highly valued \glsac{api} documentation requirement, but there still exists a substantial gap in existing literature into this requirement. For example, besides category \dimcat{D4} (which is of only \textit{Medium} value to developers), less research has explored all other dimension \dimcat{D} categories (though there may be understandable reasons as to why, as detailed in \cref{tse2020:sec:threats:construct}). Furthermore, developers highly value detailed \dima{} \dimcat{A} through many documentation artefacts, notably quick-start guides \dimcat{A1}, downloadable sample applications \dimcat{A7}, exhaustive list of major components \dimcat{A9}, and system requirements to use the \glsac{api} \dimcat{A10}. Such artefacts emphasise the need for developers to rapidly pick-up a new \glsac{api}; however, the best ways to provide such information is still open to further investigation in literature.

Conversely, the top-right of \cref{tse2020:fig:ips-vs-ils} emphasises (very)-highly researched artefacts that are of (very)-high value to developers. Here we see that \dima{} \dimcat{A} is the most-researched requirement, with code snippets \dimcat{A5} being an \glsac{api} usage artefact that is both most-researched and of highest value. Hence, this demonstrates how many existing studies have an empirical basis on software developers (e.g., via surveys or interviews; see \cref{tse2020:fig:sms})---code snippets is a well-researched artefact since most developers agree to its need in the documentation of \glsacpl{api}. Therefore, it is clear to see how the correlation between the respective \glsac{ils} and \glsac{ips} values for \dimcat{A5} are high. However, if we look at other areas of our taxonomy, such as \dimcat{A12},  \dimcat{B7}, \dimcat{D3}, \dimcat{E3} or \dimcat{E5}, we find that developers do indeed desire these aspects of \glsac{api} documentation, and, consequently, demand usage descriptions, design rationale descriptions, support artefacts, or good presentation of the documentation to be a necessary requirement of good quality \glsac{api} documentation. Thus, these aspects have not gained proportional attention in literature, thereby highlighting future research potential.

\subsection{Triangulating IPS, ILS and Computer Vision}

\begin{figure}[h]
	\centering
  \includegraphics[width=.8\linewidth]{ips-vs-cvs2.png}
  \caption[Comparing value of API documentation artefacts to presence in Computer Vision Services]{Value of \glsac{api} documentation artefacts to developers (\glsac{ips}) vs their presence in \glspl{cvs}. Colour intensity represents greater number of categories in each intersection.}
  \label{tse2020:fig:ips-vs-cvs}
\end{figure}

To interpret our comparison of \glsac{ips} values with \glspl{cvs}, we introduce a calculated `presence score' for each category. As discussed in \cref{tse2020:sec:validation:empirical-app}, we empirically evaluate each category of our taxonomy with three \glspl{cvs}: Azure Computer Vision (ACV), Amazon Rekognition (AWS) and Google Cloud Vision (GCV). We indicate whether the respective \glsac{api} documentation artefact is present, partially present, or nor present (as listed in \cref{tse2020:tab:taxonomy}). To interpret this data, we assign a full circle (\circlepresent{}) for present, half-circle (\circlepartialpresent{}) for partially present and an empty circle (\circlenotpresent{}) for not present. Combinations of presence for each category per service are indicated with the three circles of varying shade. For example, \dimcat{A1} has a presence score of {\small \circlepresent{}~\circlepresent{}~\circlepartialpresent{}} because it was found to be present in both GCV and ACV but only partially present in AWS; \dimcat{B3} has a presence score of {\small \circlepartialpresent{}~\circlenotpresent{}~\circlenotpresent{}} because it was only found to be partially present in GCV, etc. For a list of full presence values, see \cref{tse2020:tab:taxonomy}.

We illustrate which artefacts industry vendors provide developers with and the artefact's respective developer value using this combination of three circles. Using a similar approach to the previous section, these results are presented in a ten-by-five matrix as illustrated in \cref{tse2020:fig:ips-vs-cvs}. If only one service fully implements a documentation artefact of \mbox{(very-)high} value to developers ({\small \circlepresent{}~\circlenotpresent{}~\circlenotpresent{}}), if one or two services partially implement the artefact ({\small \circlepartialpresent{}~\circlenotpresent{}~\circlenotpresent{}} and {\small \circlepartialpresent{}~\circlepartialpresent{}{}~\circlenotpresent{}{}}) or if none do ({\small \circlenotpresent{}~\circlenotpresent{}~\circlenotpresent{}}), then we believe there is room for improvement for service vendors to improve their documentation and include these artefacts.

In this instance, we can see 10 categories listed in \cref{tse2020:fig:ips-vs-cvs} that developers feel are important but are not fully implemented across all three \gls{cvs} vendors. This is especially the case for dimensions  \dimcat{A} (\dima{}) and \dimcat{D} (\dimd{}), corroborating our findings with existing gaps in literature under \cref{tse2020:sec:tax-analysis:ips-vs-ils}. In other words, while both the goals of existing studies and \gls{cvs} vendors have emphasised the need for artefacts such as code-snippets \dimcat{A5}, tutorials \dimcat{A6}, and entry-points to the \glsac{api} \dimcat{B1}, less attention is given to by \textit{both} literature and vendors on the same, \mbox{(very-)highly} valued aspects to developers (e.g., troubleshooting hints \dimcat{D2}, licensing information \dimcat{D6} or links to related components \dimcat{E3}).

Furthermore, from our analysis, we can see areas with which the research community has and has \textit{not} paid extensive attention to. We still see that vendors have paid attention to artefacts even where there has been less research attention, namely \dimcat{D1} (FAQs), \dimcat{B5} (success stories), \dimcat{A7} (downloadable sample applications), \dimcat{A1} (quick-start guides), \dimcat{E2} (forums), \dimcat{D5} (printable guides), and \dimcat{A9} (\glsac{api} component lists). These seven categories are of (very) high value to developers but research attention on these topics are (very) low; however, their presence score within \glspl{cvs} are {\small\circlepresent{}{}{} \circlepartialpresent{}~\circlepartialpresent{}{}} or greater. Hence, we can see that vendors address developer's concerns despite the lack of attention by software engineering researchers in these areas, and thus future research potential to better serve developers and ensure vendors' implementation of these documentation artefacts is evident.

From the above, we can therefore conclude that the vendors' documentation largely covers a majority of \glsac{api} documentation requirements. However, there still remains opportunity for improvement to \glsac{api} documentation by either vendors and/or the research community: that is, low research attention on documentation artefacts that present high value to developers which are \textit{also} generally missing from vendor documentation. To explore this aspect, we triangulate the documentation artefacts (categories) that have a low or very low research attention and that are only present in one service, partially present in one or two, or not present at all. This results in three documentation requirements that warrant further exploration by industry vendors or the research community (see \cref{tse2020:tab:high-ips-low-ils-low-cvs}).

\afterpage{\begin{landscape}
\begin{table*}
  \centering
  \caption[Documentation artefacts of high value to developers that are under-researched and under-documented]{Documentation artefacts of high value to developers that have less attention in software engineering literature and are under-documented in \glspl{cvs}. Documentation requirements (i.e., dimensions) separated by rules.}
  \label{tse2020:tab:high-ips-low-ils-low-cvs}
  \tablefitlandscape{0.9}{\begin{tabular}{p{0.275\linewidth}|lp{0.2\linewidth}p{0.35\linewidth}}
    \toprule
    \textbf{Artefact} & \textbf{Value} & \textbf{Research Attention} & \textbf{Presence in Computer Vision Services}\\
    \midrule
    \dimcat{A10} Documenting \glsac{api}'s minimum system requirements and/or dependencies &
    High &
    \textbf{Low:} 5 studies (23\%) &
    \textbf{Score=1.0:} No dedicated web pages found for this artefact in any service. Dependencies for client libraries embedded within GCV and ACV quick-start guides \citepweb{Quicksta92:online,CalltheC0:online}. Other system requirements not listed.\\
    \midrule
    \dimcat{D2} Troubleshooting hints &
    High &
    \textbf{Very Low:} 2 studies (10\%) &
    \textbf{Score=0.5:} Only found in AWS's video recognition service \citepweb{Troubles2:online}, but no troubleshooting tips found for non-video image recognition.\\
    \dimcat{D3} Diagrammatic representation of \glsac{api} &
    Very High &
    \textbf{Low:} 3 studies (14\%) &
    \textbf{Score=0.0:} Not found for any service.\\
    \dimcat{D6} Licensing Information &
    Very High &
    \textbf{Very Low:} 1 study (5\%) &
    \textbf{Score=0.5:} Partially present only in ACV \citepweb{LegalTermsMS:online}; information is non-specific to the licensing terms of ACV exclusively.\\
    \midrule
    \dimcat{E3} Quick-links to other relevant components &
    Very High &
    \textbf{Low:} 3 studies (14\%) &
    \textbf{Score=0:} Not found for any service.\\
    \dimcat{E5} Visualised map of navigational paths &
    Very High &
    \textbf{Low:} 3 studies (14\%) &
    \textbf{Score=0:} Not found for any service.\\
    \bottomrule
  \end{tabular}}
\end{table*}
\end{landscape}}

\subsection{Recommendations Resulting from Analysis}
\label{tse2020:sec:tax-analysis:cvs-improvement}

In this section, we triangulate the taxonomy developed from literary sources, the developer survey on this taxonomy to understand its efficacy in-practice, and the application of the taxonomy to \glspl{cvs} to provide several recommendations for both service providers and researchers. Our recommendations are based both on extrapolations of our findings, our prior work, and existing experience with such work.

\subsubsection{Recommendations for vendors}

\Cref{tse2020:tab:high-ips-low-ils-low-cvs} emphasises how service vendors still lack key documentation requirements of critical importance to developers that are still widely under-researched in software engineering literature. The largest of these requirements are the need for vendors to provide additional support artefacts \dimcat{D} and the need for vendors to present this in a way that's most digestible for developers to understand \dimcat{E}. A list of detailed suggestions for vendors are provided in \cref{tse2020:sec:suggested-improvements}; here we discuss generalised findings on a sample of key artefacts.

For example, no services assessed had any form of diagrammatic overview of their \glsacpl{api} at a high-level \dimcat{D3}, thereby indicating how various components of their \glsacpl{api} work together, such as how specific endpoints work or an overview of the lifecyle of the technical domain behind these endpoints (i.e., label/train/infer/re-train), thereby incorporating conceptual relationships behind the \glsac{api} \dimcat{C1}. For instance, an interactive overview of the developer's need to pre-process their data, send it to the service, and post-process the response data would help developers understand how the service better fits into the `flow' of their application. Moreover, we failed to find lower-level diagrammatic overviews of the client \glsacpl{sdk}---such as a UML diagram---that developers find very useful. We strongly advise vendors to provide diagrams illustrating the service within context to help support existing written documentation.

Troubleshooting hints \dimcat{D2} are also a valuable support artefact, but were only found for AWS's video processing endpoints. As our prior work shows, developers are likely to question what aspects of the service can and cannot do, such as the types of labels it can find, or how to make it focus on specific ontologies when an input image is provided; e.g., time of day (day vs night) location (indoors vs outdoors) or the subject of the image (dog vs cat) \citep{Cummaudo:2020icse}. Troubleshooting in identifying service evolution \citep{Cummaudo:2020icse} would also be important, since developers are likely to overlook subtle (but application-breaking) changes to response data, such as labels introduced/removed or confidence changes. Therefore, vendors must document detailed troubleshooting suggestions on their websites on how best to resolve discrepancies in the results found from these services. This could easily be tied in with \dimcat{A12} to incorporate usage description requirements when errors are presented to users and how to deal with them; also largely missing from existing documentation.

Another important aspect is the need to make documentation of one component more easily relatable to other parts of the documentation \dimcat{E3}. Again, no service provided quick-links to related documentation; an example here could be links to definitions of domain-specific terminology \dimcat{C2} to help developers with the learning process of adopting these new generation of \glsacpl{api} (e.g., the `score' field could be linked back to a video explaining the concept of probability within the services' guesses).

\subsubsection{Recommendations for researchers}
As shown in \cref{tse2020:tab:high-ips-low-ils-low-cvs}, we see that there are cases of (very) high-value documentation artefacts (to practitioners) in which literature has not paid great attention to. For example, for the requirement of \glsac{api} usage description \dimcat{A}, practitioners agree that both code snippets \dimcat{A5} and documenting system requirements to use the \glsac{api} \dimcat{A10} are of, at least, high value. However, while code snippets has had \textit{consistent} attention within the software engineering research community (i.e., 15 papers spanning 1998--2019), we see that system requirements documentation only gained fluctuating interest by researchers (i.e., predominantly in the 2000s, with two further papers in the last three years). Thus, five papers investigating \textit{some} aspects on this artefact may not cover \textit{all} its aspects; for example, we may have identified a \textit{need} to document these requirements and dependencies, but does this mean we know \textit{all} aspects on how to produce them, the best way to \textit{communicate} them, and the most efficient means for developers to \textit{consume} that information? Contrasting this artefact against the 15 papers on code snippets, we see two documentation artefacts of at least high value to practitioners, yet, evidently, researchers have paid attention to one over the other.

As \cref{tse2020:fig:ips-vs-ils} shows, the need for additional support \dimcat{D} within documentation is the largest requirement that \textit{may} be an indicator for further research in this domain (see \cref{tse2020:sec:threats:construct}). Notably, RQ2 of our \gls{sms} identified the methodologies and data collection techniques by which our existing understanding of \glsac{api} documentation requirements were gathered; as demonstrated through \cref{tse2020:fig:sms}, a majority of our understanding is grounded through the opinions of developers, namely evaluation research using direct techniques. Too many studies are shown to rely on a handful of data collection techniques (interviews and questionnaires, shadowing and observation, think-aloud sessions) and a stronger emphasis for indirect and independent techniques is needed moving forward; there is therefore a gap in literature on \textit{other} types of data collection techniques that may provide different insights into satisfying the documentation requirements within our taxonomy.

For example, we see \dimcat{A9} (exhaustive list of major \glsac{api} components) as a high-value documentation artefact that satisfies the requirement of the \glsac{api} usage description \dimcat{A}. However research attention is lower. A validation research paper could propose a method to generate a baseline list of these components through an independent technique, such mining the \glsac{api} codebase for its major components through class usage (static analysis) or analysing an existing work database or tool use logs to see which components developers have accessed the most. This would satisfy the need for the documentation artefact, bolstering the \glsac{api} usage requirement and exploring new techniques to do so.

Few philosophical papers result in a lack of insight into completely new ways of exploring \glsac{api} documentation. Further exploration into this type of research may help us devise a whole new framework of producing \glsac{api} documentation. For example, as shown by developers and vendors, quick-start guides \dimcat{A1} are highly valued, and well-documented in \glspl{cvs}. But literature does not provide any vocabulary or frameworks into how best to develop such guides. Involving both software engineering researchers and developers through a brainstorming or focus group to conceptualise, devise, and refine such a framework may be a worthwhile study to better improve our understanding of quick-start guides whilst also exploring new approaches to research new guidelines.

Beyond requirement \dimcat{A}, another insight identified is the need for developers to have visualised maps of navigational paths \dimcat{E5} which is not yet provided by any of the \gls{cvs} providers investigated. With the low \glsac{ils} value in this category (14\% or 3 studies), we see a potential research topic for future exploration. For example, if research can demonstrate that such visualised maps are not just something developers desire, but can make them \textit{more effective} in their day-to-day work, then this could be a strong case made to vendors to improve the presentation of their documentation.

Thus, as we have shown in these sample recommendations, many potential studies and research directions can stem by exploring the discrepancies of \glsac{api} documentation in literature, in practice, and their presence in \glspl{cvs} (i.e., as a sample case study) when assessed on a case-by-case basis. The method researchers decide upon depends the research questions they wish to address; thus, observations we present in \cref{tse2020:fig:sms} may trigger fruitful reasoning about approaches future research could take, however inferring methodological gaps will need to be compatible with research goals. Thus, mapping these discrepancies to gaps in the techniques used in studies to devise of novel ways to improve \glsac{api} documentation whilst also exploring new methodologies should be balanced carefully by researchers.

\section{Threats to Validity}
\label{tse2020:sec:limitations}

\subsection{Internal Validity}

Threats to \textit{internal validity} represent internal factors of our study which affect concluded results. \citeauthor{Kitchenham:2007dd}' guidelines on producing systematic reviews \citep{Kitchenham:2007dd} suggest that researchers conducting reviews should discuss the review protocol, inclusion decisions, data extraction with a third party. Within this study, we discussed our protocols with other researchers within our research group and utilised test-retest reliability. Further assessments into reliability would involve an assessment of the review and extraction processes, which can be investigated using inter-rater reliability measures. Guidelines suggested by \citet{Garousi:2017:EGE:3084226.3084238} describe methods for independent analysis and conflict resolution could help resolve this.

As stated in \cref{tse2020:sec:method:taxonomy-development}, we utilised a systematic software engineering taxonomy development method by \citet{Usman:2017hn}. Two additional taxonomy validation approaches proposed by \citeauthor{Usman:2017hn} were not considered in our work: benchmarking and orthogonality demonstration. To our knowledge, there are no other studies that classify existing \glsac{api} documentation studies into a structured taxonomy, and therefore we are unable to benchmark our taxonomy against others. We would encourage the research community to conduct a replication of our work and investigate whether our taxonomy classification approaches are replicable to ensure that categories are reliable and the dimensions fit the objectives of the taxonomy. Moreover, we did not investigate orthogonality demonstration as our primary goals for this work were to investigate the efficacy of the taxonomy by practitioners and in-practice, with reference to our wider research area of intelligent \glspl{cvs}. Therefore, we solely adopted the utility demonstration approach in two detailed experiments (\cref{tse2020:sec:validation,tse2020:sec:tax-analysis}) to analyse the efficacy of our taxonomy and identify potential improvements for these services' \glsac{api} documentation.

\subsection{External Validity}\label{tse2020:sec:limitations:internal}

Threats to \textit{external validity} concern the generalisation of our observations. Our \gls{sms} has used a broad range of sources however not all papers contributing to \glsac{api} documentation may have been found or captured within the taxonomy. While we attempted to include as many papers as we could find in our study, some papers may have been filtered out due to our exclusion criteria. For example, there are studies we found that were excluded as they were not written in English, and these excluding factors may alter our conclusions, introducing conflicting recommendations. However, given the consistency of these trends within the studies that were sourced, we consider this a low likelihood.

Online documentation of \glsacpl{api} are non-static, and may evolve using contributions from both official sources and the developer community (e.g., via GitHub). We downloaded the three service's \glsac{api} documentation in March of 2019---it is highly likely that new documentation may have been added since or modified since publication. A recommendation to mitigate this would be to re-evaluate this study once intelligent \glspl{cvs} have matured and become even more mainstream in developer communities.

Unless significant inducements are offered, \citet{Singer:2007tu} report that a consistent response rate of 5\% has been found in software engineering questionnaires distributed and in information systems the median response rates for surveys are 60\% \citep{Baruch:1999vf}. We observe that low response rates may adversely effect the findings of our survey, typically as software engineers find little time to do them \citep{Singer:2007tu}. When compared to typical software engineering studies, our response rate of \SurveyParticipantsTotalResponseRate{} was likely successful due to designing and carefully testing succinct, unambiguous and well-worded questions with  researchers within our research group. All adjustments made from the pilot study due to unexpected poor quality of the questionnaire have been reported and explained in \cref{tse2020:sec:validation:survey:eval}. However, further improvements could be made to increase this response rate.

The survey reached \SurveyParticipantsExternalTotal{} external and \SurveyParticipantsInternal{} internal participants. This yielded a total of \SurveyParticipantsTotal{} participants. However, only \SurveyParticipantsFullResponses{} participants fully completed the survey and, on average, those who only partially completed the survey completed \SurveyParticipantsExternalPartialResponsesCompletionRate{} of all questions. Therefore, demographic data for these participants is largely missing.
To verify the reliability of partially submitted responses, we calculated the average response of each item in our survey (i.e., question) for all fully completed results and all partially completed results. All partially completed questions, except \dimcat{B7}, were within 1 standard deviation from the mean, and therefore we believe the  \SurveyParticipantsExternalPartialResponses{} partial results to be valid when excluding B7. Even if these partial results are excluded, our full-response participant count of \SurveyParticipantsFullResponses{} is still comparable to existing studies, such as \citet{Nykaza:2002td} (57 participants), \citet{Robillard:2011uv} (80 participants), or \citep{Robillard:2009uk} (83 participants). Therefore, given these comparable numbers, we believe this does not compromise validity of our results.

We also adopt research conducted in the field of questionnaire design, such as ensuring all scales are worded with labels \citep{Krosnick:1999wt} and have used a summating rating scale \citep{Spector:1992uj} to address a specific topic of interest if people are to make mistakes in their response or answer in different ways at different times. This approach was also extended using alternating positive and negative sentiment for each question---as multiple studies have shown \citep{Sauro:2011aj,Brooke:2013vt}, this approach helps reduce poor-quality responses by minimising extreme responses and acquiescence biases.

\subsection{Construct Validity}\label{tse2020:sec:threats:construct}

Threats to \textit{construct validity} relates to the degree by which the data extrapolated in this study sufficiently measures its intended goals. Our interpretation of the \glsac{ils} (as given in \cref{tse2020:sec:findings,tse2020:sec:tax-analysis:ils}) is reported as the proportion of papers whose research investigates or explores issues regarding the aspects of specific \glsac{api} documentation artefacts (i.e., categories in the taxonomy) that, collectively, comprise the requirements of good \glsac{api} documentation (i.e., dimensions in the taxonomy). Every effort has been made in this work to provide a constructive analysis on the \glsac{api} documentation landscape, however, the studies that comprise the \glsac{ils} may differ in their intent toward a specific documentation artefact. For example, some studies may have distinct goals to extensively study \textit{how} code snippets \dimcat{A5} specifically improve developer productivity (e.g., through interviews or by observational studies), while others may just reflect that code snippets are a commonly-used artefact self-reported by developers (e.g., through a survey). Thus, the interpretation of the \glsac{ils} may range between deep exploration of an artefact or whether a study mentions the artefact without any attempts to thoroughly investigate it. For this reason, we suggest that a high \glsac{ils} value for a category within the taxonomy suggests that the documentation artefact is within the attention of the research community, and that subsequent attention \textbf{may} be required for those artefacts with low \glsac{ils} values as a \textit{potential indicator} for future research (i.e., it also may \textit{not}). However, each artefact with a low \glsac{ils} (but high \glsac{ips}) would need to be carefully examined in isolation to evaluate whether future research is indeed warranted, and how that research can be conducted with the ultimate goal to assist practitioners.

Automatic searching was conducted in the \gls{sms} by choice of three popular databases (see \cref{tse2020:sec:method:lit-review}). As a consequence of selecting multiple databases, duplicates were returned. This was mitigated by manually curating out all duplicate results from the set of studies returned. Additionally, we acknowledge that the lack manual searching of papers within particular venues may be an additional threat due to the misalignment of search query keywords to intended papers of inclusion. Thus, our conclusions are only applicable to the information we were able to extract and summarise, given the primary sources selected.

While we have investigated the application of this taxonomy using a user study (\cref{tse2020:sec:validation:survey}), we would like to explore a controlled study of developers to assess how improved and non-improved \glsac{api} documentation impacts developer productivity. The outcome of this work can help design a follow-up experiment, consisting of a comparative controlled study \citep{Seaman:2007wa} that capture firsthand behaviours and interactions toward how software engineers approach using a \gls{cvs} with and without our taxonomy applied. This can be achieved by providing `mock' improved documentation with the suggested improvements included in this work. Such an experiment could recruit a sample of developers of varying experience (from beginner programmer to principal engineer) to complete a certain number of tasks under a comparative controlled study, half of which will (a) develop using the improved `mock' documentation, and the other half will (b) develop with the \textit{as-is/existing} documentation. From this, we can compare if the taxonomy makes improvements by capturing metrics and recording the sessions for qualitative analysis. Visual modelling can be adopted to analyse the qualitative data using matrices \citep{Dey:2003ty}, maps and networks \citep{Miles:1994ty} as these help illustrate any causal, temporal or contextual relationships that may exist to map out the developer's mindset and difference in approaching the two sets of designs of the same tasks.

\section{Conclusions \& Future Work}
\label{tse2020:sec:conclusions}

The emergence of \glsac{ai}-based intelligent components present significant challenges to our existing understanding of traditional \glsac{api} documentation. The inherent probabilistic and non-deterministic nature of these components means that developers must shift their mindset of conventional \glsacpl{api}, and vendors of these services must similarly shift the mindset of documenting their \glsacpl{api} using traditional means. Without adapting to the new mental model (of the vendors designing these services) and by vendors presenting poor or incomplete (traditional) documentation that is not compatible with these next-generation components, developers face many struggles. They fail to grasp how to properly understand how these services work, seeking further documentation or support from their peers on forums on such as Stack Overflow \citep{Cummaudo:2020icse}. This ultimately hinders developers' productivity and thus adversely affects the internal quality of the applications that they build.

This study has explored the artefacts and means by which traditional \glsac{api} documentation is studied through the use of an \gls{sms} of 4,501 studies, identifying 21 key works. From this, we synthesised a taxonomy of the various documentation artefacts that improves \glsac{api} documentation quality, and thus collectively synthesising the requirements of good \glsac{api} documentation. Furthermore, we also capture the most commonly used analysis techniques used in the academic literature to understand the means by which the goals of these studies resulted in their findings. We then validate our taxonomy against developers to assess its efficacy with practitioners, and conduct a heuristic evaluation against three popular \glspl{cvs}. We determine that developers demand certain documentation artefacts more than others, since not all documentation artefacts are equally valued. We map the value (to developers) of these artefacts against their exposure within the software engineering literature, thereby highlighting the gaps by which future research could expand upon. Furthermore, we present a similar mapping against how well the coverage \glspl{cvs} have incorporated such artefacts into their own \glsac{api} documentation, thus highlighting that while industry vendors cover most documentation artefacts that may not be in the interest to researchers, some artefacts with low research interest are still largely missing (see \cref{tse2020:tab:high-ips-low-ils-low-cvs}). We therefore provide several generalised recommendations to vendors and the wider research community to explore how best these artefacts can be better addressed and incorporated into further research, thus improving our understanding of the requirements of good \glsac{api} documentation.

Future extensions of our work may involve a restricted systematic literature review in \glsac{api} documentation artefacts, and many suggestions are further detailed in \cref{tse2020:sec:limitations}. Further, a review into the techniques of these primary studies may extend the mapping we conducted in this work, by evaluating the the effectiveness of the various approaches used in each study and assessing these against the proposed conclusions of each study.

The findings of our work provides a solid baseline for improving the documentation of non-deterministic software, such as \glspl{cvs}. While our aim is to eventually improve the quality of \glsac{api} documentation, the ultimate goal is to improve the software engineer's experience of non-deterministic and abstracted \glsac{ai}-based components, such as \glspl{iws}. We hope the guidelines from this extensive study help both software developers and \glsac{api} providers alike by using our taxonomy as a go-to checklist for what should be considered in documenting any \glsac{api}.
