\chapter[Interpreting Pain-Points in Computer Vision Services]
{Interpreting Pain-Points in Computer Vision Services\pubfootnote{Cummaudo:2020icse}}
\label{ch:icse2020}
\graphicspath{{mainmatter/publications/figures/icse2020/}}
\def\tablepath{mainmatter/publications/tables/icse2020}

\def\NumPostsFromSO{1,425}
\def\NumPostsCategorised{1,825}
\def\NumPostsFromFiftyClassified{380}
\def\NumPostsFromFiftyNoise{70}
\def\NumPostsNoise{238}	\def\PctPostsNoise{13.04\%}

\def\NumTaxACategorised{188}	\def\PctTaxACategorised{10.30\%}
\def\NumTaxBCategorised{1,579}	\def\PctTaxBCategorised{86.52\%}
\def\NumTaxAUnCategorised{1,410}	\def\PctTaxAUnCategorised{77.26\%}
\def\NumTaxBUnCategorised{19}	\def\PctTaxBUnCategorised{1.04\%}

\def\PctTaxACorrectness{22.87\%}
\def\PctTaxACompleteness{47.87\%}
\def\PctTaxADocumentation{23.94\%}
\def\PctTaxBAPIUsage{22.29\%}
\def\PctTaxBDiscrepancy{16.34\%}
\def\PctTaxBErrors{32.05\%}
\def\PctTaxBReview{15.14\%}
\def\PctTaxBConceptual{11.02\%}
\def\PctTaxBAPIChange{1.08\%}
\def\PctTaxBLearning{2.09\%}

\newcommand{\solink}[1]{~\citepweb{SOLink:#1}}

\glsresetall
\begin{abstract}
\Glspl{iws} are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as \gls{cv} to provide new services and products to users, and large technology firms enable this via \glsac{rest}ful \glsacpl{api}.
While such \glsacpl{api} promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such \glsacpl{api} look and feel like conventional \glsacpl{api} but abstract away data-driven probabilistic behaviour---the implications of a developer treating these \glsacpl{api} in the same way as other, traditional cloud services, such as cloud storage, is of concern.
The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these \glslongpl{iws}, specifically those that provide \gls{cv}.
We use \glslong{so} to mine indications of the frustrations that developers appear to face when using \glslongpl{cvs}, classifying their questions against two recent classification taxonomies (documentation-related and general questions).
We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems.
We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.
\end{abstract}
\glsresetall

\section{Introduction}

The availability of recent advances in \gls{ai} over simple \glsac{rest}ful end-points offers application developers new opportunities. These new \glspl{iws} are \gls{ai} components that abstract complex \gls{ml} and \gls{ai} techniques behind simpler \glsac{api} calls. In particular, they hide (either explicitly or implicitly) any data-driven and non-deterministic properties inherent to the process of their construction. The promise is that software engineers can incorporate complex machine learnt capabilities, such as \gls{cv}, by simply calling an \glsac{api} end-point.

The expectation is that application developers can use these \gls{ai}-powered services like they use other conventional software components and cloud services (e.g., object storage like AWS S3). Furthermore, the documentation of these \gls{ai} components is still anchored to the traditional approach of briefly explaining the end-points with some information about the expected inputs and responses. The presupposition is that developers can reason and work with this high level information. These services are also marketed to suggest that application developers do not need to fully understand how these components were created (i.e., assumptions in training data and training algorithms), the ways in which the components can fail, and when such components should and should not be used.

The nuances of \gls{ml} and \gls{ai} powering \glspl{iws} have to be appreciated, as there are real-world consequences to software quality for applications that depend on them if they are ignored~\citep{Cummaudo:2019icsme}. This is especially true when \gls{ml} and \gls{ai} are abstracted and masked behind a conventional-looking \glsac{api} call, yet the mechanisms behind the \glsac{api} are data-dependent, probabilistic and potentially non-deterministic~\citep{Ohtake:2019vi}. We are yet to discover what long-term impacts exist during development and production due to poor documentation that do not capture these traits, nor do we know the depth of understanding application developers have for these components. Given the way \gls{ai}-powered services are currently presented, developers are also likely to reason about these new services much like a string library or a cloud data storage service. That is, they may not fully consider the implications of the underlying statistical nature of these new abstractions or the consequent impacts on productivity and quality.

Typically, when developers are unable to correctly align to the mindset of the \glsac{api} designer, they attempt to resolve issues by \mbox{(re-)reading} the \glsac{api} documentation. If they are still unable to resolve these issues on their own after some internet searching, they consider online discussion platforms (e.g., Stack Overflow, GitHub Issues, Mailing Lists) where they seek technological advice from their peers~\citep{Aghajani:2019bo}.
Capturing what developers discuss on these platforms offers an insight into the frustrations developers face when using different software components as shown by recent works~\citep{Rosen:2016uk,Beyer:2014ec,Kavaler:2013uh,Wang:2013ub,Stevens:2013vf}.
However, to our knowledge, no studies have yet analysed what developers struggle with when using the new generation of \textit{intelligent} services. Given the re-emergent interest in \gls{ai} and the anticipated value from this technology~\citep{LoGiudice:2016wf}, a better understanding of issues faced by developers will help us improve the quality of services. Our hypothesis is that application developers do not fully appreciate the probabilistic nature of these services, nor do they have sufficient appreciation of necessary background knowledge---however, we do not know the specific areas of concern. The motivation for our study is to inform \glsac{api} designers on which aspects to focus in their documentation, education, and potentially refine the design of the end-points.

This study involves an investigation of \NumPostsCategorised{} \gls{so} posts regarding one of the most mature types of \glspl{iws}---\glspl{cvs}---dating from November 2012 to June 2019. We adapt existing methodologies of prior \gls{so} analyses~\citep{Tahir:2018ks, Beyer:2014ec} to extract posts related to \glspl{cvs}. We then apply two existing \gls{so} question classification schemes presented at ICPC and ICSE in 2018 and 2019~\citep{Aghajani:2019bo,Beyer:2018fm}. These previous studies focused on mobile apps and web applications. Although not a direct motivation, our work also serves as a validation of the applicability of these two issue classification taxonomies~\citep{Aghajani:2019bo,Beyer:2018fm} in the context of \glspl{iws} (hence potential for generalisation). Additionally our work is the first---to our knowledge---to \textit{test} the applicability of these taxonomies in a new study.

The taxonomies in previous works focus on the specific aspects from the domain (e.g. \glsac{api} usage, specificity within the documentation etc.) and as such do not deeply consider the learning gap of an application developer.
To explore the \glsac{api} learning implications raised by our \gls{so} analysis, we applied an additional lens of two taxonomies from the field of pedagogy. This was motivated by the need to offer an insight into the work needed to help developers learn how to use these relatively new services.

The key findings of our study are:
\begin{itemize}
\item The primary areas that developers raise as issues reflect a relatively primitive understanding of the underlying concepts of data-driven \gls{ml} approaches used. We note this via the issues raised due to conceptual misunderstanding and confusion in interpreting errors,
\item Developers predominantly encounter a different distribution of issue types than were reported in previous studies, indicating the complexity of the technical domain has a non-trivial influence on intelligent \glsac{api} usage; and
\item Most of these issues can be resolved with better documentation, based on our analysis.
\end{itemize}

The paper also offers a data-set as an additional contribution to the research community and to permit replication~\citepweb{SupplementaryMaterials}. The paper structure is as follows: \cref{icse2020:sec:motivation} provides motivational examples to highlight the core focus of our study; \cref{icse2020:sec:related-work} provides a background on prior studies that have mined \gls{so} to gather insight into the \gls{se} community; \cref{icse2020:sec:method} describes our study design in detail; \cref{icse2020:sec:findings} presents the findings from the \gls{so} extraction; \cref{icse2020:sec:discussion} offers an interpretation of the results in addition to potential implications that arise from our work; \cref{icse2020:sec:limitations} outlines the limitations of our study; concluding remarks are given in \cref{icse2020:sec:conclusions}.

\section{Motivation}
\label{icse2020:sec:motivation}

``Intelligent'' services are often available as a cloud end-point and provide developers a friendly approach to access recent \gls{ai}/\gls{ml} advances without being experts in the underlying processes. \Cref{icse2020:fig:traits} highlights how these services abstract away much of the technical know-how needed to create and operationalise these \glspl{iws}~\citep{Ortiz:2017wg}. In particular, they hide information about the training algorithm and data-sets used in training, the evaluation procedures, the optimisations undertaken, and---surprisingly---they often do not offer a properly versioned end-point~\citep{Cummaudo:2019icsme, Ohtake:2019vi}. That is, the cloud vendors may change the behaviour of the services without sufficient transparency.

The trade-off towards ease of use for application developers, coupled with the current state of documentation (and assumed developer background) has a cost as reflected in the increasing discussions on developer communities such as \gls{so} (see \cref{icse2020:fig:posts-trend}). To illustrate the key concerns, we list below a few up-voted questions:

\begin{itemize}
  \item \textbf{unsure of \gls{ml} specific vocabulary:} ``\textit{Though it's now not \gls{so} clear to me what `score' actually means.}''\solink{51273104}; ``\textit{I'm trying out the [\gls{iws}], and there's a score field that returns that I'm not sure how to interpret [it].}''\solink{43249555}
  \item \textbf{frustrated about non-deterministic results:} ``\textit{Often the \glsac{api} has troubles in recognizing single digits... At other times Vision confuses digits with letters.}''\solink{49386572}; ``\textit{Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?}''\solink{39540741}
  \item \textbf{unaware of the limitations behind the services:} ``\textit{Is there any \glsac{api} available where we can recognize human other body parts (Chest, hand, legs and other parts of the body), because as per the Google vision \glsac{api} it's only able to detect face of the human not other parts.}''\solink{39071341}
  \item \textbf{seeking further documentation:} ``\textit{Does anybody know if Google has published their full list of labels (\texttt{[`produce', `meal', ...]}) and where I could find that? Are those labels structured in any way? - e.g. is it known that `\texttt{food}' is a superset of `\texttt{produce}', for example.}''\solink{38363182}
\end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{traits}
  \caption[Traits of intelligent web services compared to DIY ML]{Some traits of Intelligent Services vs. `Do-It-Yourself' \gls{ml}. Green-to-red arrows indicate the presence of these traits. \textit{Adapted from~\citet{Ortiz:2017wg}}.}
  \label{icse2020:fig:traits}
\end{figure}

\begin{figure*}[tb]
  \includegraphics[width=\linewidth]{posts-trend}
  \caption[Trend of Stack Overflow posts discussing computer vision services]{Trend of posts, where IBM = IBM Watson Visual Recognition, MS = Azure Computer Vision, AWS = AWS Rekognition and GCV = Google Cloud Vision. Three MS posts from Q4 2012, Q3 2013 and Q4 2013 have been removed for graph clarity.}
  \label{icse2020:fig:posts-trend}
\end{figure*}

The objective of our study is to better understand the nature of the questions
that developers raise when using \glspl{iws}, in order to inform the service designers and documenters. In particular, the knowledge we identify can be used to improve the documentation, educational material and (potentially) the information contained in the services' response objects---these are the main avenues developers have to learn and reason about when using these services. There is previous work that has investigated issues raised by developers~\citep{Tahir:2018ks, Beyer:2018fm, Aghajani:2019bo}. We build on top of this work by adapting the study methodology and apply the taxonomies offered to identify the nature of the issues and this results in the following research questions in this paper:

\begin{enumerate}[label=\textbf{RQ\arabic*.},leftmargin=0.1125\linewidth]
  \item \textbf{How do developers mis-comprehend \glspl{iws} as presented within \gls{so} pain-points?} While the \gls{ai} community is well aware in the the nuances that empower \glspl{iws}, such services are being released for application developers who may not be aware of their limitations or how they work. This is especially the case when machine intelligence is accessed via web-based \glsacpl{api} where such details are not fully exposed.
  \item \textbf{Are the distribution of issues similar to prior studies?}
  We compare how the distributions of previous studies' of posts about conventional, deterministic \glsac{api} services differ from those of \glspl{iws}. By assessing the distribution of \glspl{iws}' issues against similar studies that focus on mobile and web development, we identify whether a new taxonomy is needed specific to \gls{ai}-based services, and if gaps specific to \gls{ai} knowledge exist that need to be captured in these taxonomies.
\end{enumerate}


\section{Background}
\label{icse2020:sec:related-work}

The primary goal of analysing issues is to better understand the root causes. Hence, a good issue classification taxonomy should ideally capture the underlying causal aspects (instead of pure functional groupings)~\citep{Chillarege:1992tm}. Although this idea (of cause related classification) is not new (Chillarege advocated for it in this TSE paper in 1992), this is not a universally followed approach when studying online discussions and some recent works have largely classified issues into the \textit{``what is''} and not \textit{``how to fix it''}~\citep{Barua:2012gz,Beyer:2014ec,Uddin:2019cz}. They typically (manually) classify discussion into either \textit{functional areas} (e.g.,~Website Design/CSS, Mobile App Development, .NET Framework, Java~\citep{Barua:2012gz}) or \textit{descriptive areas} (e.g.,~Coding Style/Practice, Problem/Solution, Design, QA~\citep{Barua:2012gz,Uddin:2019cz}). As a result, many of these studies do not give us a prioritised means of targeted attack on how to \textit{resolve} these issues with, for example, improved documentation. Interestingly, recent taxonomies that studied \gls{so} data (\citet{Aghajani:2019bo} and~\citet{Beyer:2018fm}) were causal in nature and developed to understand discussions related to mobile and web applications.  However, issues that arise when developers use \glspl{iws} have not been studied, nor do we know if existing issue classification taxonomies are sufficient in this domain.

Researchers studying \glsacpl{api} have also attempted to understand developer's opinions towards \glsacpl{api}~\citep{Uddin:2019cz}, categorise the questions they ask about these \glsacpl{api}~\citep{Rosen:2016uk,Barzilay:2013cn,Barua:2012gz,Beyer:2018fm}, and understand \glsac{api} related documentation and usage issues~\citep{Tahir:2018ks,Ahasanuzzaman:2018kv,Hou:2013jf,Aghajani:2019bo,Barua:2012gz,Allamanis:2013is}. These studies often employ automation to assist in the data analysis stages of their research. Latent Dirichlet Allocation~\citep{Uddin:2019cz,Barua:2012gz,Rosen:2016uk,Allamanis:2013is} is applied for topic modelling and other \gls{ml} techniques such as Random Forests~\citep{Beyer:2018fm}, Conditional Random Fields~\citep{Ahasanuzzaman:2018kv} or Support Vector Machines~\citep{Hou:2013jf,Beyer:2018fm} are also used.

However, automatic techniques are tuned to classify into \textit{descriptive} categories, that is, they help paint a landscape of \textit{what is}, but generally do not address the causal factors to address the issues in great detail. For example, functional areas such as `Website Design'~\citep{Barua:2012gz}, `User Interface'~\citep{Beyer:2014ec} or `Design'~\citep{Uddin:2015hn} result from such analyses. These automatic approaches are generally non-causal, making it hard to address reasons for \textit{why} developers are asking such questions. However, not all studies in the space use automatic techniques; other studies employ manual thematic analysis~\citep{Tahir:2018ks,Aghajani:2019bo,Barzilay:2013cn} (e.g., card sorting) or a combination of both~\citep{Beyer:2018fm,Beyer:2014ec,Rosen:2016uk,Treude:2011fh}. Our work uses a manual approach for classification, and we use taxonomies that are more causally aligned allowing our findings to be directly useful in terms of addressing the issues.

Evidence-based \gls{se}~\citep{Kitchenham:2004vj} has helped shape the last 15 years worth of research, but the reliability of such evidence has been questioned~\citep{rgensen:2016gl,Juristo:2012bp,Shepperd:2018hr}.
Replication studies, especially in empirical works, can give us the confidence that existing results are adaptable to new domains; in this context, we extend (to \glspl{iws}) and work with study methods developed in previous works.

\section{Method}
\label{icse2020:sec:method}

\subsection{Data Extraction}

This study initially attempted to capture \gls{so} posts on a broad range of many \glspl{iws} by identifying issues related to four popular \gls{iws} cloud providers: Google Cloud~\citepweb{GoogleCloud:Home}, AWS~\citepweb{AWS:Home}, Azure~\citepweb{Azure:Home} and IBM Cloud~\citepweb{IBM:Home}.
We based our selection criteria on the prominence of the providers in industry (Google, Amazon, Microsoft, IBM) and their ubiquity in cloud platform services. Additionally, in 2018, these services were considered the most adopted cloud vendors for enterprise applications~\citep{RightScaleInc:2018kJ}.

However, during the filtering stage (see \cref{icse2020:ssec:method:filtering}), we decided to focus on a subset of these services, \gls{cv}, as these are one of the more mature and stable \gls{ml}/\gls{ai}-based services with widespread and increasing adoption in the developer community (see \cref{icse2020:fig:posts-trend}). We acknowledge other services beyond the four analysed provide similar capabilities~\citepweb{Pixlab:Home,Clarifai:Home,Cloudsight:Home,DeepAI:Home,Imagaa:Home,Talkwaler:Home} and only English-speaking services have been selected, excluding popular services from Asia (e.g.,~\citepweb{Megvii:Home,TupuTech:Home,YiTuTech:Home,SenseTime:Home,DeepGlint:Home})---see \cref{icse2020:sec:limitations}. For comprehensiveness, we explain below our initial attempts to extract \textit{all} \glspl{iws}. %

\subsubsection{Defining a list of \glspl{iws}}
As there exists no global `list' of \glspl{iws} to search on, we needed to derive a \textit{corpus of initial terms} to allow us to know \textit{what} to search for on the Stack Exchange Data Explorer\footnote{\url{http://data.stackexchange.com/stackoverflow}} (SEDE). We began by looking at different brand names of cloud services and their permutations (e.g., \uline{G}oogle \uline{C}loud \uline{S}ervices and GCS) as well as various \gls{ml}-related products (e.g., Google Cloud \gls{ml}). To do this, we performed extensive Google searches\footnote{This search was conducted on 17 January 2019}\def\footnotesearchdate{2} in addition to manually reviewing six `overview' pages of the relevant cloud platforms. We identified 91 initial \glspl{iws} to incorporate into our search terms\footnote{For reproducibility, this is available at \url{http://bit.ly/2ZcwNJO}.}.\def\footnotereproducability{3}

\subsubsection{Manual search for relevant, related terms}
We then ran a manual search\footnotemark[\footnotesearchdate{}] %
on each term to determine if these terms were relevant. We did this by querying each term within SO's search feature, reviewing the titles and body post previews of the first three pages of results (we did not review the answers, only the questions). We also noted down the user-defined \textit{Tags} of each post (up to five per question); by clicking into each tag, we could review similar tags (e.g., `project-oxford' for `azure-cognitive-services') and check if the tag had synonyms (e.g., `aws-lex' and `amazon-lex'). We then compiled a \textit{corpus of tags} consisting of 31 terms.

\subsubsection{Developing a search query}
We recognise that searching SEDE via \textit{Tags} exclusively can be ineffective (see~\citep{Tahir:2018ks,Barua:2012gz}). To mitigate this, we produced a \textit{corpus of title and body terms}. Such terms are those that exist within the title and body of the posts to reflect the ways in which individual developers commonly use to refer to different \glspl{iws}. To derive at such a list, we performed a search\footnotemark[\footnotesearchdate{}]\textsuperscript{,}\footnotemark[\footnotereproducability{}] of the 31 tags above in SEDE, filtering out posts that were not answers (i.e., questions only) as we wanted to see how developers \textit{phrase} their questions. For each search, we extracted a random sample of 100 questions (400 total for each service) and reviewed each question. We noted many patterns in the permutations of how developers refer to these services, such as: common misspellings (`bind' vs. `bing'); brand misunderstanding (`Microsoft \gls{cv}' vs. `Azure \gls{cv}'); hyphenation (`Auto-\gls{ml}' vs. `Auto \gls{ml}'); UK and US English (`Watson Analyser' vs. `Watson Analy\uline{\textbf{z}}er'); and, the use of apostrophes, plurals, and abbreviations (`Microsoft\uline{\textbf{'s}} Computer Vision \glsac{api}', `Microsoft Computer Vision Service\uline{\textbf{s}}, `GCV' vs. `\uline{G}oogle \uline{C}loud \uline{V}ision'). We arrived at a final list of 229 terms compromising all of the \glspl{iws} provided by Google, Amazon, Microsoft and IBM as of January 2019\footnotemark[3].

\subsubsection{Executing our search query}

Our next step was to perform a case-insensitive search of all 229 terms within the body or title of posts. We used Google BigQuery's public data-set of \gls{so} posts\footnote{http://bit.ly/2LrN7OA} to overcome SEDE's 50,000 row limit and to conduct a case-insensitive search. This search was conducted on 10 May 2019, where we extracted 21,226 results. We then performed several filtering steps to cleanse our extracted data, as explained below.

\subsection{Data Filtering}
\label{icse2020:ssec:method:filtering}


\subsubsection{Refining our inclusion/exclusion criteria}
\label{icse2020:ssec:method:filtering:refining}

We performed an initial manual filtering of the 50 most recent posts (sorted by descending \textit{CreationDate} values) of the 21,226 posts above, assessing the suitability of the results and to help further refine our inclusion and exclusion criteria. We did note that some abbreviations used in the search terms (e.g., `GCV', `WCS'\footnote{Watson Cognitive Services}), resulting in irrelevant questions in our result set. We therefore removed abbreviations from our search query and consolidated all overlapping terms (e.g., `Google Vision \uline{\textbf{\glsac{api}}}' was collapsed into `Google Vision').

We also recognised that 21,226 results would be non-trivial to analyse without automated techniques. As we wanted to do manual qualitative analysis, we reduced our search space to 27 search terms of just the \textit{\glspl{cvs}} within the original corpus of 229 terms. These were Google Cloud Vision~\citepweb{GoogleCloud:Home}, AWS Rekognition~\citepweb{AWS:Home}, Azure Computer Vision~\citepweb{Azure:Home}, and IBM Watson Visual Recognition~\citepweb{IBM:Home}. This resulted in \NumPostsFromSO{} results that were extracted on 21 June 2019. The query used and raw results are available online in our supplementary materials~\citepweb{SupplementaryMaterials}.

\subsubsection{Duplicates} Within \NumPostsFromSO{} results, no duplicate questions were noted, as determined by unique post ID, title or timestamp.

\subsubsection{Automated and manual filtering}
\label{icse2020:ssec:method:filtering:automated-manual-filtering}

To assess the suitability and nature of the \NumPostsFromSO{} questions extracted, the first author began with a manual check on a randomised sample of 50 questions. As the questions were exported in a raw CSV format (with HTML tags included in the post's body), we parsed the questions through an ERB templating engine script\footnote{We make this available for future use at: \url{http://bit.ly/2NqBB70}} in which the ID, title, body, tags, created date, and view, answer and comment counts were rendered for each post in an easily-readable format. %
Additionally, SQL matches in the extraction process were also highlighted in yellow (i.e., in the body of the post) and listed at the top of each post. These visual cues helped to identify 3 false positive matches where library imports or stack traces included terms within our corpus of 26 \gls{cvs} terms. For example, \texttt{aws-java-sdk-\uline{rekognition}:jar} is falsely matched as a dependency within an unrelated question. As such exact matches would be hard to remove without the use of regular expressions, and due to the low likelihood (6\%) of their appearance, we did not perform any followup automatic filtering.

\subsubsection{Classification}
\label{icse2020:ssec:method:filtering:classification}

Our \NumPostsFromSO{} posts were then split into 4 additional random samples (in addition to the random sample of 50 above). 475 posts were classified by the first author and three other research assistants, software engineers with at least 2 years industry experience, assisted to classify the remaining 900. This left a total of 1,375 classifications made by four people plus an additional 450 classifications made from reliability analysis, in which the remaining 50 posts were classified nine times (as detailed in \cref{icse2020:ssec:method:filtering:reliability}). Thus, a total of \NumPostsCategorised{} classifications were made from the original \NumPostsFromSO{} posts extracted.

Whilst we could have chosen to employ topic modelling, these are too descriptive in nature (as discussed in \cref{icse2020:sec:related-work}). Moreover, we wanted to see if prior taxonomies can be applied to \glspl{iws} (as opposed to creating a new one) and compare if their distributions are similar.
Therefore, we applied the two existing taxonomies described in \cref{icse2020:sec:related-work} to each post; (i)~a documentation-specific taxonomy that addresses issues directly resulting from documentation, and (ii)~a generalised taxonomy that covers a broad range of \gls{so} issues in a well-defined \gls{se} area (specifically mobile app development).
\uline{A}ghajani et al.'s documentation-specific taxonomy (Taxonomy \uline{A}) is multi-layered consisting of four dimensions and 16 sub-categories \citep{Aghajani:2019bo}. Similarly, \uline{B}eyer's \gls{so} generalised post classification taxonomy (Taxonomy \uline{B}) consists of seven dimensions~\citep{Beyer:2018fm}. We code each dimension with a number, $X$, and each sub-category with a letter $y$: $(Xy)$. We describe both taxonomies in detail within \cref{icse2020:tab:taxonomies}. Where a post was included in our results but not applicable to \glspl{iws} (see \cref{icse2020:ssec:method:filtering:automated-manual-filtering}) or not applicable to a taxonomy dimension/category, then the post was flagged for removal in further analysis.
\Cref{icse2020:tab:taxonomies} presents \textit{our understanding} of the respective taxonomies; our intent is not to methodologically replicate \citeauthor{Aghajani:2019bo} or \citeauthor{Beyer:2018fm}'s studies in the \gls{iws} domain, rather to acknowledge related work in the area of \gls{so} classification and reduce the need to synthesise a new taxonomy. We baseline all coding against \textit{our interpretation only}. Our classifications are therefore independent of the previous authors' findings.


\afterpage{\begin{landscape}
\input{\tablepath/taxonomy}
\end{landscape}}

\subsection{Data Analysis}

\subsubsection{Reliability of Classification}
\label{icse2020:ssec:method:filtering:reliability}

To measure consistency of the categories assigned by each rater to each post, we utilised both intra- and inter-rater reliability~\citep{McHugh:2012up}. As verbatim descriptions from dimensions and sub-categories were considered quite lengthy from their original sources, all raters met to agree on a shared interpretation of the descriptions, which were then paraphrased as discussed in the previous subsection and tabulated in \cref{icse2020:tab:taxonomies}. To perform statistical calculations of reliability, each category was assigned a nominal value and a random sample of 50 posts were extracted. Two-phase reliability analysis followed.

Firstly, intra-rater agreement by the first author was conducted twice on 28 June 2019 and 9 August 2019. Secondly, inter-rater agreement was conducted with the remaining four co-authors in addition to three research assistants within our research group in mid-August 2019. Thus, the 50 posts were classified an additional nine times, resulting in 450 classifications for reliability analysis. We include these classifications in our overall analysis.

At first, we followed methods of reliability analysis similar to previous \gls{so} studies (e.g.,~\citep{Tahir:2018ks}) using the percentage agreement metric that divides the number of agreed categories assigned per post by the total number of raters~\citep{McHugh:2012up}. However, percentage agreement is generally rejected as an inadequate measure of reliability analysis~\citep{Cohen:1960tf,Krippendorff:2018tda,Hallgren:2012kt} in statistical communities. As we used more than 2 coders and our reliability analysis was conducted under the same random sample of 50 posts, we applied \textit{Light's Kappa}~\citep{Light:1971vz} to our ratings, which indicates an overall index of agreement. This was done using the \texttt{irr} computational R package~\citep{Gamer:tj} as suggested in~\citep{Hallgren:2012kt}.

\subsubsection{Distribution Analysis}

In order to compare the distribution of categories from our study with previous studies we carried out a \(\chi^2\) test. We selected a \(\chi^2\) test as the following assumptions~\citep{Sheskin:2003tx} are satisfied: (i) the data is categorical, (ii) all counts are greater than 5, and (iii) we can assume simple random sampling. The null hypothesis describes the case where each population has the same proportion of observations and the alternative hypothesis is where at least one of the null hypothesis statements is false. We chose a significance value, \(\alpha\), of 0.05 following a standard rule of thumb. As to the best of our knowledge this is the first statistical comparison using Taxonomy A and B on \gls{so} posts. To report the effect size we selected Cramer's Phi, \(\phi_c\) which is well suited for use on nominal data~\citep{Sheskin:2003tx}.


\section{Findings}
\label{icse2020:sec:findings}

\begin{figure*}[t]
  \centering
  \begin{subfigure}[c]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{a-compare.pdf}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{b-compare.pdf}
  \end{subfigure}
  \caption[Comparing documentation-specific and generalised classifications of Stack Overflow posts]{
    \textit{Left:} Documentation-specific classification taxonomy results highlights a mostly similar distribution to that of~\citeauthor{Aghajani:2019bo}'s findings~\citep{Aghajani:2019bo}.
    \textit{Right:} Generalised classification taxonomy results highlight differences from more mature fields (i.e., Android \glsacpl{api} in~\citet{Beyer:2018fm}) to less mature fields (i.e., \glspl{iws}).
  }  \label{icse2020:fig:ab-compare}
\end{figure*}

We present our findings from classifying a total of \NumPostsCategorised{} \gls{so} posts aimed at answering RQs 1 and 2. 450 posts were classified using Taxonomies A and B for reliability analysis as described in \cref{icse2020:ssec:method:filtering:reliability} and the remaining 1,375 posts were classified as per \cref{icse2020:ssec:method:filtering:classification}. A summary of our classification using Taxonomies A and B is shown in \cref{icse2020:fig:ab-compare}.

\subsection{Post classification and reliability analysis}
%
When undertaking the classification, we found that \NumPostsNoise{} issues (\PctPostsNoise{}) did not relate to \glspl{iws} directly. For example, library dependencies were still included in a number of results (see \cref{icse2020:ssec:method:filtering:automated-manual-filtering}), and we found there to be many posts  discussing Android's Mobile Vision \glsac{api} as \uline{Google} (Cloud) \uline{Vision}. These issues were flagged and ignored for further analysis (see \cref{icse2020:ssec:method:filtering:classification}).
%

For our reliability analysis, we classified a total of 450 posts of which \NumPostsFromFiftyNoise{} posts were flagged as irrelevant.
%
~\citet{Landis:1977kv} provide guidelines to interpret kappa reliability statistics, where $0.00\,\leq \kappa\,\leq\,0.20$ indicates \textit{slight} agreement and $0.21\,\leq \kappa\,\leq\,0.40$ indicates \textit{fair} agreement.
Despite all raters meeting to agree on a shared interpretation of the taxonomies (see \cref{icse2020:ssec:method:filtering:reliability}) our inter-rater measures aligned \textit{slightly} (0.148) for Taxonomy A and \textit{fairly} (0.295) for Taxonomy B. We report further in \cref{icse2020:sec:limitations}.

\subsection{Developer Frustrations}

We found~\citeauthor{Beyer:2018fm}'s high-level abstraction taxonomy (Taxonomy B) was able to classify \PctTaxBCategorised{} of posts.  \PctTaxACategorised{} posts were assigned exclusively under~\citeauthor{Aghajani:2019bo}'s documentation-specific taxonomy (Taxonomy A).
We found that developers do not generally ask questions exclusive to documentation, and typically either pair documentation-related issues to their own code or context. The following two subsections further explain results from both Taxonomy A and B's perspective.

\subsubsection{Results from~\citeauthor{Aghajani:2019bo}'s taxonomy}

%
Results for~\citeauthor{Aghajani:2019bo}'s low-level documentation taxonomy (Taxonomy A), indicates that most discussion on \gls{so} does not directly relate to documentation about an \gls{iws}. We did not find any process-related (A-3) or tool-related (A-4) questions as, understandably, the developers who write the documentation of the \glspl{iws} would not be posting questions of such nature on \gls{so}.
%
One can \textit{infer} documentation-related issues from posts (i.e., parts of the documentation \textit{lacking} that may cause the issue posted). However, there are few questions that \textit{directly} relate to documentation of \glspl{iws}.

%

Few developers question or ask questions directly about the \glsac{api} documentation, but some (\PctTaxACompleteness{}) posts ask for additional information to understand the \glsac{api} (\textbf{completeness (A-1b)}), for example: ``\textit{Is there a full list of potential labels that Google's Vision \glsac{api} will return?}''\solink{38363182}; ``\textit{There seems to be very little to no documentation for AWS iOS text recognition inside an image}''\solink{51561234}.

\PctTaxACorrectness{} of posts question the \textbf{accuracy (A-1a)} of certain parts of the cloud documentation, especially in relation to incorrect quotas and limitations: ``\textit{Are the Cloud Vision \glsac{api} limits in documentation correct?}''\solink{36655630}, ``\textit{According to the Google Vision documentation, the maximum number of image files per request is 16. Elsewhere, however, I'm finding that the maximum number of requests per minute is as high as 1800.}''\solink{53585778}.

There are also many references (\PctTaxADocumentation{}) addressing the confusing nature of some documentation, indicating that the \textbf{readability, usability and usefulness of the documentation (A-2b, A-2c and A-2d)} could be improved. For example, ``\textit{Am I encoding it correctly? The docs are quite vague.}''\solink{41166264}, ``\textit{The aws docs for this are really confusing.}''\solink{52046473}.

\subsubsection{Results from~\citeauthor{Beyer:2018fm}'s taxonomy}
\label{icse2020:sub:sub:resultsB}

%
We found that a majority (\PctTaxBErrors{}) of posts are primarily \textbf{error-related questions (B-3)}, including a dump of the stack trace or exception message from the service's programming-language SDK (usually Java, Python or C\#) that relates to a specific error. For example: ``\textit{I can't fix an error that's causing us to fall behind.}''\solink{48846121}; ``\textit{I'm using the Java Google Vision \glsac{api} to run through a batch of images... I'm now getting a \texttt{channel closed} and \texttt{ClosedChannelException} error on the request.}''\solink{51142340}.

%
\textbf{\glsac{api} usage questions (B-1)} were the second highest category at \PctTaxBAPIUsage{} of posts. Reading the questions revealed that many developers present an insufficient understanding of the behaviour, functional capability and limitation of these services and the need for further data processing. For example, while Azure provides an image captioning service, this is not universal to all \glspl{cvs}: ``\textit{In Amazon Rekognition for image processing how do I get the caption for an image?}''\solink{52746720}. Similarly, OCR-related and label-related questions often indicate interest in cross-language translation, where a separate translation service would be required: ``\textit{Can Google Cloud Vision generate labels in Spanish via its \glsac{api}?}''\solink{45260779}; ``\textit{[How can I] specify language for response in Google Cloud Vision \glsac{api}}'' \solink{50764331}; ``\textit{When I request a text detection of an image, it gives only English Alphabet characters (characters without accents) which is not enough for me. How can I get the UTF-32 characters?}''\solink{38860919}.

%
It was commonplace to see questions that demonstrate a lack of depth in understanding and appreciating how these services work, instead posting simple debugging questions. For instance, in the \PctTaxBConceptual{} of \textbf{conceptual-related questions (B-5)} that we categorised, we noticed causal links to a misunderstanding (or lack of awareness) of the vocabulary used within \gls{cv}. For example: ``\textit{The problem is that I need to know not only \uline{what} is on the image but also the \uline{position} of that object. Some of those \glsacpl{api} have such feature but only for face detection.}''\solink{38634409}; ``\textit{I want to know if the new image has a face \uline{similar} to the original image.... [the service] can \uline{identify} faces, but can I use it to get similar faces to the identified face in other images?}''\solink{47563346}. It is evident that some application developers are not aware of conceptual differences in \gls{cv} such as object/face \textit{detection} versus \textit{localisation} versus \textit{recognition}.

%
In the \PctTaxBDiscrepancy{} of \textbf{discrepancy-related questions (B-2)}, we see further unawareness from developers in how the underlying systems work. In OCR-related questions, developers do not understand the pre-processing steps required before an OCR is performed. In instances where text is separated into multiple columns, for example, text is read top-down rather than left-to-right and segmentation would be required to achieve the expected results. For example, ``\textit{it appears that the \glsac{api} is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan.}''\solink{42391009}; ``\textit{this method returns scanned text in wrong sequence... please tell me how to get text in proper sequence.}''\solink{53591219}.

%
A number of \textbf{review-related questions (B-4)} (\PctTaxBReview{}) seem to provide some further depth in understanding the context to which these systems work, where training data (or training stages) are needed to understand how inferences are made: ``\textit{How can we find an exhaustive list (or graph) of all logos which are effectively recognized using Google Vision logo detection feature?}''\solink{45484524}; ``\textit{when object \texttt{banana} is detected with accuracy greater than certain value, then next action will be dispatched... how can I confidently define and validate the threshold value for each item?}''\solink{47614963}.

%
\textbf{\glsac{api} change (B-6)} was shown in \PctTaxBAPIChange{} of posts, with evolution of the services occurring (e.g., due to new training data) but not necessarily documented ``\textit{Recently something about the Google Vision \glsac{api} changed... Suddenly, the \glsac{api} started to respond differently to my requests. I sent the same picture to the \glsac{api} today, and I got a different response (from the past).}''\solink{44740363}.

%


%


%

\subsection{Statistical Distribution Analysis}

We obtained the following results $\chi^2 = 131.86$, $\alpha = 0.05$, $p\ value = 2.2 \times 10^{-16}$ and $\phi_c=0.362$ from our distribution analysis with Taxonomy A to compare our study with that of~\citet{Aghajani:2019bo}. Comparing our study to~\citet{Beyer:2018fm} produced the following results $\chi^2~=~145.58$, $\alpha~=~0.05$, $p\ value~=~2.2 \times 10^{-16}$ and $\phi_c~=~0.252$. These results show that we are able to reject the null hypothesis that the distribution of posts using each taxonomy was the same as the comparison study. While there are limited guidelines for interpreting \(\phi_c\) when there is no prior information for effect size~\citep{Sun:2010ut},~\citeauthor{Sun:2010ut} suggests the following: $0.07\,\leq \phi_c\,\leq\,0.20$  indicates a \textit{small} effect, $0.21\,\leq \phi_c\,\leq\,0.35$ indicates a \textit{medium} effect, and $0.35 > \phi_c$ indicates a \textit{large} effect. Based on this criteria we obtained a \textit{large} effect size for the documentation-specific classification (Taxonomy A) and a \textit{medium} effect size for the generalised classification (Taxonomy B).

\section{Discussion}
\label{icse2020:sec:discussion}

%

%

\subsection{Answers to Research Questions}


\subsubsection{How do developers mis-comprehend \glspl{iws} as presented within \gls{so} pain-points? (RQ1)}
%
Upon meeting to discuss the discrepancies between our categorisation of \gls{iws} usage \gls{so} posts, we found that our interpretations of the \textit{posts themselves} were largely subjective. For example, many posts presented multi-faceted dimensions for Taxonomy B;~\citet{Beyer:2018fm} argue that a post can have more than one question category and therefore multi-label classification is appropriate at times. We highlight this further in the threats to validity (\cref{icse2020:sec:limitations}).

We have to define the context of \glspl{iws} to address RQ1. We use the concept of a ``technical domain''~\citep{Barnett:2018Kx} to define this context. A technical domain captures the domain-specific concerns that influence the non-functional requirements of a system~\citep{Barnett:2018Kx}. In the context of \glspl{iws}, the technical domain includes exploration, data engineering, distributed infrastructure, training data, and model characteristics as first class citizens~\citep{Barnett:2018Kx}. We would then expect to see posts on \gls{so} related to these core concerns.

In \cref{icse2020:fig:ab-compare}, for the documentation-specific classification, the majority of posts were classified as \textbf{Completeness (A1-b)} related (\PctTaxACompleteness{}). An interpretation for this is that the documentation does not adequately cover the technical domain concerns. Comments by developers such as ``\textit{I'm searching for a list of all the possible image labels that the Google Cloud Vision \glsac{api} can return?}''\solink{45313874} indicates the documentation does not adequately describe the training data for the \glsac{api}---developers do not know the required usage assumptions. Another quote from a developer, ``\textit{Can Google Cloud Vision generate labels in Spanish via its \glsac{api}? ... [Does the \glsac{api}] allow to select which language to return the labels in?}''\solink{45260779} points to a lack of details relating to the characteristics of the models used by the \glsac{api}. It would seem that developers are unaware of aspects of the technical domain concerns.%

The next most frequent category is \textbf{Correctness (A-1a)} with \PctTaxACorrectness{} of posts. In the context of the technical domain there are many limits that developers need to be aware of: range and increments of a model score~\citep{Cummaudo:2019icsme}; required data pre-processing steps for optimal performance; and features provided by the models (as explained in \cref{icse2020:sub:sub:resultsB}). Considering the relation between technical concerns and software quality, developers are right to question providers on correctness; ``\textit{Are the Cloud Vision \glsac{api} limits in documentation correct?}''\solink{36655630}.

\subsubsection{Are the distribution of issues similar to prior studies? (RQ2)}
Visual inspection of \cref{icse2020:fig:ab-compare} shows that the distributions for the documentation-specific classification and the generalised classification are different (compared to prior studies). As a sanity check we conducted a $\chi^2$ test and calculated the effect size $\phi_c$. We were able to reject the null hypothesis for both classification schemes, that the distribution of issues were the same as the previous studies (see \cref{icse2020:sec:findings}). We now discuss the most prominent differences between our study and the previous studies.

%
%

%
%

%

In the context of \gls{iws} \gls{so} posts, Taxonomy B suggests that Errors (B-3) are discussed most amongst developers. These results are in contrast to similar studies made in more \textit{mature} \glsac{api} domains, such as Mobile Development~\citep{Beyer:2018fm,Beyer:2014ec,Rosen:2016uk, Barnett:2015ec, Barnett:2015ut} and Web Development~\citep{Treude:2011fh}. Here, \glsac{api} Usage (B-1) is much more frequently discussed, followed by Conceptual (B-5), Discrepancy (B-2) and Errors (B-3). We argue in the following section that an improved developer understanding can be achieved by educating them about the \gls{iws} lifecycle and the `whole' system that wraps such services.

In the Android study \glsac{api} usage questions (B-1) were the highest category (28.93\% compared to \PctTaxBAPIUsage{} in our study). As stated in the analysis of the Error questions this discrepancy could be due to the maturity of the domain. However, another explanation could be the scope of the two individual studies.~\citet{Beyer:2018fm} used a broad search strategy consisting of posts tagged Android. This search term fetches issues related to the entire Android platform which is significantly larger than searching for \gls{cv} \glsacpl{api} using 229 search terms. As a consequence of more posts and more \glsacpl{api} there would be use cases resulting in additional posts related to \glsac{api} Usage (B-1).

Applying existing \gls{so} taxonomies allowed us to better understand the distribution of the issues across different domains. In particular, the issues raised around \glspl{iws} appear to be primarily due to poor documentation, or insufficient explanation around errors and limitations. Hence, many of the concerns could be addressed by adding more details to the end-point descriptions, and by providing additional information around how these services are designed to work.

\subsection{The Developer's Learning Approach}
\label{icse2020:ssec:bloomsolo}

In this subsection, we offer an explanation as to why developers are complaining about certain things when trying to use \glspl{iws} on \gls{so} (RQ1), as characterised through the use of prior \gls{so} classification frameworks (RQ2). This is described through the theoretical lenses of two learning taxonomies: Bloom's context complexity and intellectual ability taxonomy, and the \gls{solo} taxonomy (i.e., the nature by which developer's learn). We argue that the issues with using \glspl{iws} relating to the lower-levels of these learning taxonomies are easily solvable by slight fixes and improvements to the documentation of these services. However, the higher dimensions of these taxonomies demand far more rigorous mitigation strategies than documentation alone (potentially more structured education). Thus, many of the questions posted are from developers who are \textit{learning to understand} the domain of \glspl{iws} and \gls{ai}, and (hence) both \gls{solo} and Bloom's taxonomies are applicable for this discussion---as described below within the context of our domain---as pedagogical aides.

\subsubsection{Bloom's Taxonomy}

The cognitive domain under Bloom's taxonomy~\citep{Krathwohl:2001wr} consists of six objectives. Within the context of \glspl{iws}, developers are likely to ask questions due to causal links that exist in the following layers of Bloom's taxonomy:
%
(i)~\textit{knowledge}, where the developer does not remember or know of the basic concepts of \gls{cv} and \gls{ai} (in essence, they may think that \gls{ai} is as smart as a human);
%
(ii)~\textit{comprehension}, where the developer does not understand how to interpret basic concepts, or they are mis-understanding how they are used in context;
%
(iii)~\textit{application}, where the developer is struggling to apply existing concepts within the context of their own situation;
%
(iv)~\textit{analysis}, where the developer is unable to analyse the results from \glspl{iws} (i.e., understand response objects);
%
(v)~\textit{evaluation}, where the developer is unable to evaluate issues and make use of best-practices when using \glspl{iws}; and
%
(vi)~\textit{synthesise}, where the developer is posing creative questions to ask if new concepts are possible with \glspl{cvs}.

\subsubsection{\gls{solo} Taxonomy}

The \gls{solo} taxonomy~\citep{Biggs:2014ur} consists of five levels of understanding. The causal links behind the \gls{so} questions we have found relate to the following layers of the \gls{solo} taxonomy:
%
(i)~\textit{pre-structural}, where the developer has a question indicating incompetence or has little understanding of \gls{cv};
%
(ii)~\textit{uni-structural}, where the developer is struggling with one key aspect (i.e., a simple question about \gls{cv});
%
(iii)~\textit{multi-structural}, where the developer is questioning multiple concepts (independently) to understand how to build their system (e.g., system integration with the \gls{iws});
%
(iv)~\textit{relational}, where the developer is comparing and contrasting the best ways to achieve something with \glspl{iws}; and
%
(v)~\textit{extended abstract}, where the developer poses a question theorising, formulating or postulating a new concept within \glspl{iws}.

\begin{table*}[tbh]
\centering
  \caption[Example Stack Overflow posts aligning to Bloom's and SOLO taxonomies]{Example Alignments of \gls{so} posts to Bloom's and the \gls{solo} taxonomy.}
  \label{icse2020:tab:bloom-solo-examples}
  \tablefit{\begin{tabular}{p{0.7\linewidth}|p{0.15\linewidth}p{0.2\linewidth}}
    \toprule
    \textbf{Issue Quote} & \textbf{Bloom} & \textbf{\gls{solo}}\\
    \midrule

    ``\textit{I'm using Microsoft Face \glsac{api} for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directory \texttt{D:\textbackslash{}Image\textbackslash{}abc.jpg})... but it does not work.}''\solink{40714481} &
    Knowledge &
    Pre-Structural\\

    ``\textit{The problem is that the response JSON is rather big and confusing. It says a lot about the picture but doesn't say what the whole picture is of (food or something like that).}''\solink{56224197} &
    Comprehension &
    Uni-Structural\\

    ``\textit{The bounding box around individual characters is sometimes accurate and sometimes not, often within the same image. Is this a normal side-effect of a probabilistic nature of the vision algorithm, a bug in the Vision \glsac{api}, or of course an issue with how I'm interpreting the response?}''\solink{46244980} &
    Comprehension &
    Multi-Structural\\

    ``\textit{I'm working on image processing. \gls{so} far Google Cloud Vision and Clarifai are the best \glsac{api}'s to detect objects from images and videos, but both \glsac{api}'s doesn't support object detection from 360 degree images and videos. Is there any solution for this problem?}''\solink{47671289}&
    Application &
    Uni-Structural\\

    ``\textit{Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of: Multiple dogs, A dog with another animal, A dog with a person, A partially obscured dog, A dog wearing glasses, Also, would dogs on a white background make for better training samples? Watson also takes negative examples. Would cats and other small animals be good negative examples?}''\solink{40346408}&
    Analysis &
    Relational \\
    \bottomrule
  \end{tabular}}
\end{table*}

\subsubsection{Aligning \gls{so} taxonomies to Bloom's and \gls{solo} taxonomies}

To understand our findings with the lenses of pedagogical aids, we aligned Taxonomies A and B to Bloom's and the \gls{solo} taxonomies for a random sample of 50 issues described in \cref{icse2020:ssec:method:filtering:reliability}.  To do this, we reviewed all 50 of these \gls{so} posted questions and applied both the Bloom and \gls{solo} taxonomies. The primary author assigned each of the 50 questions a level within the Bloom and \gls{solo} taxonomies, removed out noise (i.e., false positive posts of no relevance to \glspl{iws}) and unassigned dimensions from reliability agreement, and then compared the relevant dimensions of Taxonomy A and B dimensions (not sub-categories). The comparison of alignments of posts to the five \gls{solo} dimensions and six Bloom dimensions are shown in \cref{icse2020:fig:alignment-of-blooms-solo}.
We acknowledge that this is only an approximation of the current state of the developer's understanding of \glspl{iws}. This early model will require further studies to perform a more thorough analysis, but we offer this interpretation for early discussion. %

As shown in \cref{icse2020:fig:alignment-of-blooms-solo}, the bulk of the posts fall in the lower constructs of Bloom's and the \gls{solo} taxonomy. This indicates that modification to certain documentation aspects can address many of these issues. For example, many issues can be ratified with better descriptions of response data and error messages: ``\textit{I was exploring google vision and in the specific function `detectCrops', gives me the crop hints. what does this means exactly?}''\solink{44304400}; ``\textit{I am a making a very simple \glsac{api} call to the Google Vision \glsac{api}, but all the time it's giving me error that `google.oauth2' module not found.}''\solink{55037756}

However, and more importantly, the higher-construct questions ranging from the middle of the third dimensions on are not as easily solvable through improved documentation (i.e., apply and multi-structural) which leaves 34.74\% (Bloom's) and 11.84\% (\gls{solo}) unaccounted for, resolvable only through improved education practices. %

\begin{figure}[t]
  \centering
  \includegraphics[width=.29\linewidth]{bloom}\includegraphics[width=.29\linewidth]{solo}
  \caption[Alignment of Bloom and SOLO taxonomies against computer vision issues]{
    Alignment of Bloom (Orange) and \gls{solo} (Blue) taxonomies against Taxonomy A and B dimensions against all 213 classifications made in the random sample of 50 posts.
  }
  \label{icse2020:fig:alignment-of-blooms-solo}
\end{figure}

\subsection{Implications}
\label{icse2020:ssec:findings:documentation-vs-education}

\subsubsection{For Researchers}

\paragraph{Investigate the evolution of post classification} Analysing how the distribution of the reported issues changes over time would be an important study. This study could answer questions such as `\textit{Does the evolution of \glspl{iws} follow the same pattern as previous software engineering trends such as mobile app or web development?}' As with any new emerging field, it is key to analyse how developers perceive such issues over time. For instance, early issues with web or mobile app development matured as their respective domain matured, and we would expect similar results to occur in the \glspl{iws} space. Future researchers could plan for a longitudinal study, such as a long-term survey with developers to gather their insights in this evolving domain, reviewing case studies of projects that use intelligent web services from now into the future, or re-mining \gls{so} at a later date and comparing the results to this study. This will help assess evolving trends and characteristics, and determine how and if the nature of the developer's experience with \glspl{iws} (and \gls{ai} in general) changes with time.

\paragraph{Investigate the impact of technical challenges on \glsac{api} usage} As discussed above, \glspl{iws} have characteristics that may influence \glsac{api} usage patterns and should be investigated as a further avenue of research. Further mining of open source software repositories that make use of \glspl{iws} could be assessed, thereby investigating if \glsac{api} patterns evolve with the rise of \gls{ai}-based applications.

%

\subsubsection{For Educators}
\label{icse2020:sssec:educators}

\paragraph{Education on high-level aspects of \glspl{iws}} As demonstrated in our analysis of their \gls{so} posts, many developers appear to be unaware of the higher-level concepts that exist within the \gls{ai} and \gls{ml} realm. This includes the need to pre- and post-process data, the data dependency and instability that exists in these services, and the specific algorithms that empower the underlying intelligence and hence their limitations and characteristics. However, most developers don't seem to complain about these factors due to the lack of documentation (i.e., via Taxonomy A). Rather, they are unaware that such information should be documentation and instead ask generalised and open questions (i.e., via Taxonomy B). Thus, documentation improvements alone may not be enough to solve these issues. This results in uncertainty during the preparation and operation (usage) of such services.  Such high-level conceptual information is currently largely missing in developer documentation for \glspl{iws}. Furthermore, many of the background \gls{ml} and \gls{ai} algorithm information needed to understand and use intelligent systems in context are built within data science (not \gls{se}) communities. A possible road-map to mitigate this issue would be the development of a software engineer's `crash-course' in \gls{ml} and \gls{ai}. The aim of such a course would encourage software engineers to develop an appreciation of the nuances and the inherent risks and implications that comes with using \glspl{iws}. This could be taught at an undergraduate level to prepare the next generation of developers of a `programming 2.0' era. However, the key aspects and implications that are presented with \gls{ai} would need to be well-understood before such a course is developed, and determining the best strategy to curate the content to developers would be best left to the \gls{se} education domain. Further investigation in applying educational taxonomies in the area (such as our attempts to interpret our findings using Bloom's and the \gls{solo} taxonomies) would need to be thoroughly explored beforehand.

\subsubsection{For Software Engineers}

\paragraph{Better understanding of intelligent \glsac{api} contextual usage} Our results show that developers are still learning to use these \glsacpl{api}. We applied two learning perspectives to interpret our results. In applying the two pedagogical taxonomies to our findings, we see that most issues seem to fall into the pre-structural and knowledge-based categories; little is asked of higher level concepts and a majority of issues do not offer complex analysis from developers. This suggests that developers are struggling as they are unaware of the vocabulary needed to actually use such \glsacpl{api}, further reinforcing the need for \glsac{api} providers to write overview documentation (as noted in prior work \citep{Cummaudo:2019esem}) and not just simple endpoint documentation. This said, improved documentation isn't always enough---as suggested by our discussion in \cref{icse2020:ssec:bloomsolo}, software engineers should explore further education to attain a greater appreciation of the nuances of \gls{ml} when attempting to use these services.

\subsubsection{For Intelligent Service Providers}

\paragraph{Clarify use cases for \glspl{iws}} Inspecting \gls{so} posts revealed that there is a level of confusion around the capabilities of different \glspl{iws}. This needs to be clarified in associated \glsac{api} documentation.
The complication with this comes with targeting the documentation such that software developers (who are untrained in the nuances of \gls{ai} and \gls{ml} as per \cref{icse2020:sssec:educators}) can to digest it and apply it in-context to application development.

\paragraph{Technical domain matters} More needs to be provided than a simple endpoint description as conventional \glsacpl{api} offer by describing the whole framework by which the endpoint sits, giving further context. This said, compared to traditional \glsacpl{api}, we find that developers complain less about the documentation and more about shallower issues. All expected pre-processing and post-processing needs to be clearly explained.
A possible mitigation to this could be an interactive tutorial that helps developers fully understand the technical domain using a hands-on approach. For example, websites offer interactive Git tutorials\footnote{For example, \url{https://learngitbranching.js.org}.} to help developers understand and explore the technical domain matters under version control in their own pace.

\paragraph{Clarify limitations} \glsac{api} developers need to add clear limitations of the existing \glsacpl{api}. Limitations include list of objects that can be returned from an endpoint.  We found that the cognitive anchors of how existing, conventional \glsac{api} documentation is written has become `ported' to the \gls{cv} realm, however a lot more overview documentation than what is given at present (i.e., better descriptions of errors, improved context of how these systems work in etc.) needs to be given.
Such documentation could be provided using interactive tutorials.

%


%
%
%
%
%




\section{Threats to Validity}
\label{icse2020:sec:limitations}

%
\subsection{Internal Validity}
%
As detailed in \cref{icse2020:ssec:method:filtering:reliability}, Taxonomies A and B present slight and fair agreement, respectively, when inter-rater reliability was applied. The nature of our disagreements largely fell due to the subjectivity in applying either taxonomies to posts. Despite all coders agreeing to the shared interpretation of both taxonomies, both taxonomies are subjective in their application, which was not reported by either \citeauthor{Aghajani:2019bo} or \citeauthor{Beyer:2018fm}. In many cases, multi-label classification seemed appropriate, however both taxonomies use single-label mapping which we find results in too much subjectivity. This subjectivity, therefore, ultimately adversely affects \gls{irr} analysis. Thus, a future mitigation strategy for similar work should explore multi-label classification to avoid this issue; \citeauthor{Beyer:2018fm}, for example, plan for multi-label classification as future work. However, these studies would need to consider the statistical challenges in calculating multi-rater, multi-label \gls{irr} for thorough reliability analysis in addressing subjectivity.
%
The selection of \gls{so} posts used for our labelling, chiefly in the subjectivity of our classifications, is of concern. We mitigate this by an extensive review process assessing the reliability of our results as per \cref{icse2020:ssec:method:filtering:reliability}.
%
The classification of our posts into the \gls{solo} and Bloom's taxonomies was performed by the primary author only, and therefore no inter-rater reliability statistics were performed. However, we used these pedagogy related taxonomies as a lens to gain an additional perspective to interpret our results. Future studies should attempt a more rigorous analysis of \gls{so} posts using Bloom's and \gls{solo} taxonomies.
%
We only aligned posts to one category for each taxonomy and did not align these using multi-label classification. This brings more complexity to the analysis, and our attempts to repeat prior studies' methodologies (see \cref{icse2020:sec:related-work}). Multi-label classification for \glspl{iws} \gls{so} posts is an avenue for future research.

%
\subsection{External Validity}

While every effort was made to select posts from \gls{so} relevant to \glspl{cvs}, there are some cases where we may have missed some posts. This is especially due to the case where some developers mis-reference certain \glspl{iws} under different names (see \cref{icse2020:ssec:method:filtering:refining}).

%
Our \gls{solo} and Bloom's taxonomy analysis has only been investigated through the lenses of \glspl{iws}, and not in terms of conventional \glsacpl{api} (e.g., Andriod \glsacpl{api}). Therefore, we are not fully certain how these results found would compare to other types of \glsacpl{api}. %
%
Two \textit{existing} \gls{so} classification taxonomies were used rather than developing our own. We wanted to see if previous \gls{so} taxonomies could be applied to \glspl{iws} before developing a new, specific taxonomy, and these taxonomies were applied based on our interpretation  (see \cref{icse2020:ssec:method:filtering:classification}) and may not necessarily reflect the interpretation of the original authors. Moreover, automated techniques such as topic modelling were not utilised as we found these produce descriptive classifications only (see \cref{icse2020:sec:related-work}). Hence, manual analysis was performed by humans to ensure categories could be aligned back to causal factors.
%
Only English-speaking \glspl{iws} were selected; the applicability of our analysis to other, non-English speaking services may affect results.
%
Use of \gls{cv} in this study is an illustrative example to focus on one area of the \glspl{iws} spectrum. While our narrow scope helps us obtain more concrete findings, we suggest that wider issues exist in other \gls{iws} domains may affect the generalisability of this study, and suggest future work be explored in this space.

\subsection{Construct Validity}

Some questions extracted from \gls{so} produced false positives, as mentioned in \cref{icse2020:ssec:method:filtering:refining,icse2020:ssec:method:filtering:automated-manual-filtering,icse2020:sec:findings}. However, all non-relevant posts were marked as noise for our study, and thus did not affect our findings.
%
Moreover, \gls{so} is known to have issues where developers simply ask basic questions without looking at the actual documentation where the answer exists. Such questions, although down-voted, were still included in our data-set analysis, but as these were \gls{so} few, it does not have a substantial impact on categorised posts.

\section{Conclusions}
\label{icse2020:sec:conclusions}

\Glspl{cvs} offer powerful capabilities that can be added into the developer's toolkit via simple \glsac{rest}ful \glsacpl{api}. However, certain technical nuances of \gls{cv} become abstracted away. We note that this abstraction comes at the expense of a full appreciation of the technical domain, context and proper usage of these systems. We applied two recent existing \gls{so} classification taxonomies (from 2018 and 2019) to see if existing taxonomies are able to fully categorise the types of complaints developers have. \Glspl{iws} have a diverging distribution of the types of issues developers ask when compared to more mature domains (i.e., mobile app development and web development). Developers are more likely to complain about shallower, simple debugging issues without a distinct understanding of the \gls{ai} algorithms that actually empower the \glsacpl{api} they use. Moreover, developers are more likely to complain about the completeness and correctness of existing \gls{iws} documentation, thereby suggesting that the documentation approach for these services should be reconsidered. Greater attention to education in the use of \gls{ai}-powered \glsacpl{api} and their limitations is needed, and our discussion offered in \cref{icse2020:ssec:bloomsolo} motivates future work in resolving these issues in the \gls{se} education space.
