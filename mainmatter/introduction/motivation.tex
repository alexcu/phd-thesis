\todo{Describe deterministic mindset clashing against probabilistic behaviour}

\section{Motivation: The Nondeterministic Impact to Quality}
\label{sec:introduction:motivation}

\glspl{iws} are accessible through \glspl{api} consisting of `black box' intelligence (\cref{fig:introduction:cloud-intelliegnce-service}).\footnote{The `black box' refers to a system that transforms input (or stimulus)  to outputs (or response) without any understanding of the internal architecture by which this transformation occurs. This arises from a theory in the electronic sciences and adapted to wider applications since the 1950s--60s \citep{Ashby:1957db,Bunge:1963jm} to describe ``systems whose internal mechanisms are not fully open to inspection'' \citep{Ashby:1957db}. }
Data scientists produce \gls{ml} algorithms to make predictions in our datasets and discover patterns within them. These black boxes are inherently probabilistic and stochastic; there is little room for certainty in these results, as the insight is purely statistical and associational \citep{Pearl:2018uv} against its training dataset. 
A \gls{cvs}, for example, may return the \textit{probability} that a particular object exists in an input images' pixels, and thus for a more certain (though not fully certain) distribution of overall confidence returned from the service, a developer must treat the problem stochastically by testing this case hundreds if not thousands of times to find a richer interpretation of the inference made. 
Developers (at present) do not need to treat their programs in any such stochastic way given their rule-driven mindset that computers make certain outcomes.

Unless a change in \gls{api} version occurs, educators teach application developers a deterministic mindset that an \gls{api} call returns the same outputs for the same inputs. As an example, consider simple arithmetic representations (e.g., $2+2=4$). The deterministic (rule-driven) mindset suggests that the result will \textit{always} be 4. However, the non-deterministic (data-driven) mindset suggests that results are probable: target output (\textit{exactly} 4) and the output inferred (\textit{a likelihood of} 4) matches as a probable percentage (or as an error where it does not match)\footnote{\citet{Blake:1998vd} produces a multi-layer perceptron \glslong{nn} performing arithmetic representation.}. Instead of an exact output, there is a \textit{probabilistic} result: $2+2$ \textit{may} equal 4 to a confidence of $n$. 

Developers must appreciate such a critical principle of \gls{ml} software when building their \gls{ai}-first applications. The \textit{external quality} of such software needs to consider reliability in the case of thresholding confidence values, that is whether the inference has an appropriate level of confidence to justify a predicted result to end-users. Similarly, developers should consider the \textit{internal quality} of building \gls{ai}-first software. Evaluations of \gls{api} usability advocate for the accuracy, consistency and completeness of \glspl{api} and their documentation \citep{Piccioni:2013em,Robillard:2009uk} and providers should consider mismatches between a developer's conceptual knowledge of the \gls{api} its implementation \citep{Ko:2011fb}. Poor \gls{api} usability hinders developer performance and thus productivity to produce their software.
%
%% TODO: Copied from ml inconsistency paper
%\subsection{The Impact on Software Quality}
%\label{ssec:introduction:motivation:impact}
%
%Do traditional techniques for documenting deterministic \glspl{api} apply to non-deterministic systems? As \glspl{api} reflect a set of design choices made by their providers intended for use by the developer, does the mindset between the machine learning architect and the novice programmer match? 
%
%Moreover, \gls{iws} \glspl{api} are inherently non-deterministic in nature, but 
%In order to fully understand this problem, there are multiple dimensions one must consider: the impact of software quality; the fact that these systems underneath are probabilistic and are stochastic; the cognitive biases of determinism in developers; the issue of consistency in \gls{api} usage. While existing literature does extensively explore software quality and \gls{api} usability, these studies have only had emphasis on deterministic systems and thus little work to date has investigated such factors on probabilistic systems that make up the core of \glspl{cvs}. We explore more of these facets in the motivating scenarios below.

\subsection{Motivating Scenarios}
\label{ssec:introduction:motivation:scenario}

The market for \glspl{iws} is increasing (\cref{fig:introduction:ai-products}) and as is developer uptake and enthusiasm in the software engineering community (\cref{fig:introduction:stackoverflow-trends}). However, the impact to software quality (internal and external) due to a mismatch of the application developer's deterministic  mindset and the service provider's nondeterministic mindset is of concern.

To illustrate the context of use, we present the two scenarios of varying risk: (i)~a fictional software developer, named Tom, who wishes to develop an inherently low-risk photo detection application for his friends and family; and (ii) a high-risk cancer \gls{cdss} that uses patient scans to recommend if surgeons should send their patients to surgery. Both describe scenarios where \gls{ai}-first components has substantiative impact to end-users when the software engineers developing with them misunderstand the nuances of \gls{ml}, ultimately adversely affecting external quality. Moreover, due to lack of compression, this hinders developer experience, productivity, and understanding/appreciation of \gls{ai}-based components.
  
\subsubsection{Motivating Scenario I: Tom's \textit{PhotoSharer} App}
\label{ssec:introduction:motivation:scenario:pam}

Tom wants to develop a social media photo-sharing app on iOS and Android, \textit{PhotoSharer}, that analyses photos taken on smartphones. Tom wants the app to categorise photos into scenes (e.g., day vs. night, landscape vs. indoors), generate brief descriptions of each photo, and catalogue photos of his friends and common objects (e.g., photos with his Border Collie dog, photos taken on a beach on a sunny day with his partner). His app will shares this analysed photo intelligence with his friends on a social-media platform, where his friends can search and view the photos.

Instead of building a computer vision engine from scratch, which takes too much time and effort, Tom thinks he can achieve this using one of the common \glspl{cvs}. Tom comes from a typical software engineering background and has insufficient knowledge of key computer vision terminology and no understanding of its underlying techniques. However, inspired by easily accessible cloud \glspl{api} that offer computer vision analysis, he chooses to use these. Built upon his experience of using other similar cloud services, he decides on one of the \gls{cvs} \glspl{api}, and expects a static result always and consistency between similar \glspl{api}. Analogously, when Tom invokes the iOS Swift substring method \texttt{"doggy".prefix(3)}, he expects it to be consistent with the Android Java equivalent \texttt{"doggy".substring(0, 2)}. Consistent, here, means two things: (i) that calling \texttt{substring} or \texttt{prefix} on \texttt{`dog'} will \textit{always} return in the same way every time he invokes the method; and (ii) that the result is \textit{always} \texttt{`dog'} regardless of the programming language or string library used, given the deterministic nature of the `substring' construct (i.e., results for substring are \gls{api}-agnostic). 

\input{mainmatter/introduction/tables/changing-confidence}

More concretely, in \cref{tab:introduction:motivation:scenario:pam:confchanges}, we illustrate how three (anonymised) \gls{cvs} providers fail to provide similar consistency to that of the substring example above. If Tom uploads a photo of a border collie\footnote{The image used for these results is \url{https://www.akc.org/dog-breeds/border-collie/}.} to three different providers in August 2018 and January 2019, he would find that each provider is different in both the vocabulary used between. The confidence values and labels within the \textit{same} provider varies within a matter of five months. The evolution of the confidence changes is not explicitly documented by the providers (i.e., when the models change) nor do they document what confidence means. Service providers use a tautological nature when defining what the confidence confidence values are (as presented in the \gls{api} documentation) provides no insight for Tom to understand why there was a change in confidence, which we show in \cref{tab:introduction:motivation:scenario:pam:tautological}, unless he \textit{knows} that the underlying models change with them. Furthemore, they do not provide detailed understanding on how to select a threshold cut-off for a confidence value. When Tom reads Google's machine learning course, he finds that \textit{``it is tempting to assume that [a] classification threshold should always be 0.5, but \textbf{thresholds are problem-dependent, and are therefore values that you must tune}.''}~\citepweb{GoogleClassifcation:Thresholding}. Therefore, he's left with no understanding on how best to tune for image classification in this instance. The deterministic problem of a substring compared to the nondeterministic nature of the \gls{iws} is, therefore, non-trivial.

\input{mainmatter/introduction/tables/tautological-definitions}

To make an assessment of these \glspl{api}, he tries his best to read through the documentation of different \gls{cvs} \glspl{api}, but he has no guiding framework to help him choose the right one. A number of questions come to mind:

\begin{itemize}
  \item What does `confidence' mean?%ICSME(?)
  \item Which confidence is acceptable in this scenario?%ICSE-Demo
  \item Are these \glspl{api} consistent in how they respond?%ICSME
  \item Are the responses in \glspl{api} static and deterministic?%ICSME
  \item Would a combination of multiple \gls{cvs} \glspl{api} improve the response?%ICWE
  \item How does he know when there is a defect in the response? How can he report it?%ICSE
  \item How does he know what labels the \gls{api} knows, and what labels it doesn't?%ICSE
  \item How does it describe his photos and detect the faces?%ICSE
  \item Does he understand that the \gls{api} uses a machine learnt model? Does he know what a \gls{ml} model is?%ICSE
  \item Does he know when models update? What is the release cycle?%ASE
\end{itemize}

Although Tom generally anticipates these \glspl{cvs} to not be perfect, he has no prior benchmark to guide him on what to expect. The imperfections appear to be low-risk, but may become socially awkward when in use; for instance, if Tom's friends have low self-esteem and use the app, they may be sensitive to the app not identifying them or mislabelling them. Privacy issues come into play especially if certain friends have access to certain photos that they are (supposedly) in; e.g., photos from a holiday with Tom and his partner, however if the \gls{api} identifies Tom's partner as a work colleague, Tom's partner's privacy is at risk.

Therefore, the level of risk and the determination of what constitutes an `error' is dependent on the situation. In the following example, an error caused by the service may be more dangerous.

% Motiviating Example 2: Google Cancer Detect API...

\subsubsection{Motivating Scenario II: Cancer Detection \gls{cdss}}
\label{ssec:introduction:motivation:scenario:cancer}

Recent studies in the oncology domain have used deep-learning \glspl{cnn} to detect \glspl{roi} in image scans of tissue (e.g., \citep{Liu:2018fa,Haenssle:2018bz,EhteshamiBejnordi:2017kq}), flagging these regions for doctors to review. Trials of such algorithms have been able to accurately detect cancer at higher rates than humans, and thus incorporating such capabilities into a \gls{cdss} is closer within reach.  Studies have suggested these systems may erode a practitioner's independent decision-making \citep{Jaspers:2011hy,Chambers:1991uh} due to over-reliance; therefore the risks in developing \glspl{cdss} powered by \glspl{iws} become paramount.

In \cref{fig:introduction:motivation:scenario:cancer} we present a context diagram for a fictional \gls{cdss} named \textit{CancerAssist}. A team of busy pathologists utilise CancerAssist to review patient lymph node scans and discuss and recommend, on consensus, if the patient requires an operation. When the team makes a consensus, the lead pathologist enters the verdict into CancerAssist---running passively in the background---to ensure there is no oversight in the team's discussions. When a conflict exists between the team's verdict and CancerAssist's verdict, the system produces the scan with \glspl{roi} it thinks the team should review. Where the team overrides the output of CancerAssist, this reinforces CancerAssist's internal model as a \gls{hitl} learning process.

\begin{figure}[th]
\centering
  \includegraphics[width=0.85\linewidth]{cancer-assist}
  \caption[CancerAssist Context Diagram]{CancerAssist Context Diagram. \textit{\textbf{Key:} Red Arrows~=~Scan Input; Yellow Arrows~=~Decision Output; Blue Arrows~=~\gls{hitl} Feedback Input.}}
  \label{fig:introduction:motivation:scenario:cancer}
\end{figure}

Powering CancerAssist is Google AI's Lymph Node Assistant (LYNA) \citep{Liu:2018fa}, a \gls{cnn} based on the Inception-v3 model \citep{Szegedy:2016ws,Krizhevsky:2012wl}. To provide intelligence to CancerAssist, the development team decide to host LYNA as an \gls{iws} using a cloud-based \gls{paas} solution. Thus, CancerAssist provides \gls{api} endpoints integrated with patient data and medical history, which produces the verdict. In the case of a positive verdict, CancerAssist highlights the relevant \glspl{roi} found are with their respective bounding boxes and their respective cancer detection accuracies.

The developer of CancerAssist has no interaction with the Data Science team maintaining the LYNA \gls{iws}. As a result, they are unaware when updates to the model occur, nor do they know what training data they provide to test their system. The default assumptions are that the training data used to power the intelligence is near-perfect for universal situations; i.e., the algorithm chosen is the correct one for every assessable ontology tests in the given use case of CancerAssist. Thus, unlike deterministic systems---where the developer can manually test and validate the outcomes of the \glspl{api}---this is impossible for non-deterministic systems such as CancerAssist and its underlying \gls{iws}. The ramifications of not being able to test such a system and putting it out into production may prove fatal to patients.

Certain questions in the production of CancerAssist and its use of an \gls{iws} may come into mind:

\begin{itemize}%ASE
  \item When is the model updated and how do the \gls{iws} team communicate these updates?
  \item What benchmark test set of data ensures that the changed model doesn't affect other results?
  \item Are assumptions made by the \gls{iws} team who train the model correct?
\end{itemize}

Thus, to improve communication between developers and \gls{iws} providers, developers require enhanced documentation, additional metadata, and guidance tooling. 

%While developers are expected to use these services as they would any other traditional software component, the marketed promise is that such services are forever-learning and, therefore, `improve' in their accuracies and judgements. But what are the real-world consequences if this `black box' evolves with time and has materially substantiate impacts on both the software that is built on top of them, and (more importantly) the \textit{people} using that software? If the impact is substantial, the robustness of applications dependent on these services can have drastic effects, and---depending of the context of the application---the real-world damage it may do could be catastrophic. Should we consider this an `error' of the service?
%
%% Real world implications -- what is an error? (i.e., it depends on the application context)
%Whether or not it is an error is dependent on the context of the consuming application. For example, an educational mobile app designed to teach different dog breeds to children using a fine-grade visual categorisation service wouldn't be substantially erroneous should a Boston Terrier be misclassified as a French Bulldog---the \textit{ultimate effects} are not substantiate enough. However, consider a safety-critical application where an intelligent service performs facial recognition to identify a potential suspect in CCTV footage; evolution within the empowering \gls{iws} cannot be afforded, especially given the ethical and legal considerations if the wrong person is sentenced. If such risk is not mitigated, the accountability of these services must be considered~\citep{DoshiVelez:2017vm}.
%
%% Why is it happening? (DS land vs Eng. Land)
%Traditional software engineering principles advocate for software systems to be versioned upon substantial change, but unfortunately we find that the most prominent cloud vendors providing these intelligent services (Microsoft Azure, Google Cloud and Amazon Web Services) do not release new versioned endpoints of the APIs when the \textit{internal model} changes~\citep{Cummaudo:2019va}. In the context of computer vision, new labels may be introduced or dropped, entire ontologoies or specific training parameters may change, but we hypothesise that is not effectively communicated to developers. Broadly speaking, this can be attributed to a dichotomy of release cycles from the data science and software engineering communities: the data science iterations and work by which new models are trained and released runs at a faster cycle than the maintenance cycle of traditional software engineering. Thus we see cloud vendors integrating model changes without the \textit{need} to update the \gls{api} version unless substantial code or schema changes are also introduced---the nuance changes in the internal model does not warrant a shift in the \gls{api} itself, and therefore the version shift in a new model does not always propagate to a version shift in the \gls{api} endpoint.
%
%% Versioning alone is not enough!!
%Currently, it is impossible to invoke requests specific to a particular model that was trained at a particular date in time, and therefore developers need to consider how evolutionary changes of the services may impact their solutions \textit{in production}. Again, whether there is any noticeable behavioural changes from these changes is dependent on the context of the problem domain---unless developers benchmark these changes against their own domain-specific dataset and frequently check their selected service against such a dataset, there is no way of knowing if substantive errors have been introduced. 
%
%Given that such substantiative \gls{se} principles on versioning, quality and robustness are under-investigated within the context of \glspl{iws}, we aim to explore guidance from the \gls{se} literature to investigate what aspects in the development lifecyle could aide in mitigating these issues when developing using \gls{ai}-based components. The following section explains the research outcomes resolving from this thesis.