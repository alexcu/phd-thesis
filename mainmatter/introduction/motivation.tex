\section{Motivation: Current Developer Mindsets'}
\label{sec:introduction:motivation}

\cref{fig:introduction:stackoverflow-trends} shows an increasing trend to the adoption and discussion of \glspl{cis} with developers. As aforementioned, these services are accessible through \glspl{api} and consist of an `intelligence' black box (\cref{fig:introduction:cloud-intelliegnce-service}). When a term `black box' is used, the input (or stimulus) is transformed to its to outputs (or response) without any understanding of the internal architecture by which this transformation occurs; indeed, this well-understood theory arose from the electronic sciences and since adapted to wider applications since the 1950s--60s \citep{Ashby:1957db,Bunge:1963jm} to describe ``systems whose internal mechanisms are not fully open to inspection'' \citep{Ashby:1957db}. 

In the world of machine learning and data mining, where we develop algorithms to make predictions in our datasets or discover patterns within them, these black boxes are inherently probabilistic and stochastic; there is little room for certainty in these results as such insight is purely statistical and associational \citep{Pearl:2018uv} against its training dataset. As an example, a computer vision \gls{cis} returns the \textit{probability} that a particular object (the response) exists in the raw pixels (the stimulus), and thus for a more certain (though not fully certain) distribution of overall confidence returned from the service, a developer must treat the problem stochastically by testing this case hundreds if not thousands of times to find a richer interpretation of the inference made. Developers (at present) do not need to treat their programs in any such stochastic way as traditionally their mindset is that computers will always make certain outcomes. But in the day and age of stochastic and probabilistic systems, this mindset needs to shift.

There are thus therefore three key factors to consider when implementing, testing and developing with a \gls{cis}: (i) the \gls{api} usability, (ii) the nature of stochastic and probabilistic systems, and (iii) how both impact on software quality.

% TODO: Copied from ml inconsistency paper
\subsection{The Impact on Software Quality}
\label{ssec:introduction:motivation:impact}

Do traditional techniques for documenting deterministic \glspl{api} also apply to non-deterministic systems? As \glspl{api} reflect a set of design choices made by their providers intended for use by the developer, does the mindset between the machine learning architect and the novice programmer match? Evaluations of \gls{api} usability advocate for the accuracy, consistency and completeness of \glspl{api} and their documentation \citep{Piccioni:2013em,Robillard:2009uk} written by providers, while providers should consider mismatches between the developer's conceptual knowledge of the \gls{api} its implementation \citep{Ko:2011fb}. However, consistency cannot be guaranteed in probabilistic systems, and the conceptual knowledge of such systems are still treated like black boxes. It is therefore imperative that \gls{cis} providers consider the impact of their \gls{api} usability; if not, poor \gls{api} usability hinders on the internal quality of development practices, slowing developers down to produce the software they need to create.

Moreover, \gls{cis} \glspl{api} are inherently non-deterministic in nature, but developers are still taught with the deterministic mindset that all \gls{api} calls are the same. Simple arithmetic representations (e.g., $2+2=4$) will \textit{always} result in 4; but a multi-layer perceptron neural network performing similar arithmetic representation \citep{Blake:1998vd} gives the probability where the target output (\textit{exactly} 4) and the output inferred (\textit{possibly} 4) matches as a percentage (or as an error where it does not match). That is, instead of an exact output, there is instead a \textit{probabilistic} result: $2+2$ \textit{may} equal 4 with a confidence of $n$. External quality must therefore be considered in the outcome of these systems, such as in the case of thresholding values, to consider whether or not the inference has a high enough confidence to justify its result to end-users.

In order to fully understand this problem, there are multiple dimensions one must consider: the impact of software quality; the fact that these systems underneath are probabilistic and are stochastic; the cognitive biases of determinism in developers; the issue of consistency in \gls{api} usage. While existing literature does extensively explore software quality and \gls{api} usability, these studies have only had emphasis on deterministic systems and thus little work to date has investigated such factors on probabilistic systems that make up the core of computer vision \glspl{cis}. We explore more of these facets in the motivating scenarios below.

% TODO: Copied from ml inconsistency paper
\subsection{Motivating Scenarios}
\label{ssec:introduction:motivation:scenario}

\todo{AC: ***Rewrote all of \cref{ssec:introduction:motivation:scenario} for review; \today.***}\\
Before introducing our research questions in \cref{sec:introduction:hypohtesis}, let us summarise what has been claimed so far: intelligence-based services are increasing rapidly (\cref{fig:introduction:ai-products}), and as is developer uptake and discussion in the software engineering community (\cref{fig:introduction:stackoverflow-trends}). As introduced in the previous section, the impact on our claimed developer mindset mismatch on software quality is a gap in literature to be investigated. How do developers work with a \gls{cis}, how usable are these \glspl{api}, and how well do developers understand the non-deterministic and stochastic nature of a deep-learning cloud-based \gls{api}? 

To motivate these questions, let us contextualise the usage of a \gls{cis} with two scenarios of varying risk: (i) a fictional software developer named Pam who wishes to develop an inherently low-risk photo detection application for her friends and family; and (ii) a high-risk cancer \gls{cdss} that uses patient scans to recommend to surgeons if the patient should be sent to surgery.
  
\subsubsection{Motivating Scenario I: Pam's \textit{PhotoSharer} App}
\label{ssec:introduction:motivation:scenario:pam}

Pam wants to develop a social media photo-sharing app on iOS and Android, \textit{PhotoSharer}, that analyses photos taken on smartphones as they are taken. Pam wants the app to categorise photos into scenes (e.g., day vs. night, landscape vs. indoors), generate brief descriptions of each photo, and catalogue photos of her friends as well as common objects (e.g., all photos with her Border Collie dog, all photos taken on a beach on a sunny day). Her app will then share all of this analysed intelligence of her photos with her friends on a social-media-like platform, where her friends can search the photos using intelligent-like queries.

Rather than building a computer vision engine from scratch, which would take far too much time and effort, Pam thinks she can achieve this using one of the common computer vision \glspl{cis}. Pam comes from a typical software engineering background and has insufficient knowledge of key computer vision terminology and no understanding of the processes behind deep-learning. Ultimately, and understandably so, she believes all of the computer vision \glspl{api} are more-or-less alike and internalises a deterministic mindset of them; when she decides on one of the three \glspl{api}, she expects a static result always and consistency between similar \glspl{api}. Analogously, when Pam invokes the iOS Swift substring method \texttt{"doggy".prefix(3)}, she rightfully expects it to be consistent with the Android Java equivalent \texttt{"doggy".substring(0, 2)}. Consistent, here, means two things: (i) that \texttt{`dog'} will \textit{always} be returned every time she invokes the method in either language (i.e., a static response); and (ii) that \texttt{`dog'} will \textit{always} be returned regardless of what programming language or string library is used, given the deterministic nature of the `substring' construct (i.e., results for substring are \gls{api}-agnostic).

To make an assessment of these \glspl{api}, she tries her best to read through the documentation of some computer vision \glspl{api}, but she has no guiding framework to help her choose the right one. Some of the questions that come to mind include:

\begin{itemize}
  \item What does confidence mean? 
  \item Are these APIs consistent in the intelligence they respond?
  \item Will she need a combination of many computer vision \glspl{api} to solve this task?
  \item How does she know when there is a defect in the response? How can she report it?
  \item How does she know what labels the \gls{api} can pick up, and what labels it can't?
  \item How does she know when the models update? What is the release cycle?
  \item How does it describe her photos and detect the faces?
  \item How can she interpret the results if she disagrees with it to help improve her app?
\end{itemize}

Dazzled by this, she does some brief reading on Wikipedia but is confused by the immense technical detail to take in. She would like some form of guiding framework to assist her and in software engineering terms she can understand.

She understands that the app is not always going to be perfect: perhaps a few photos of her dog may be missed because the dog is in the background and not the foreground, or her friends can't find the photos of their recent trip to the beach because it wasn't sunny enough for the beach to be recognised. These imperfections are low-risk. But the consequences of high-risk processing may be far greater, as we discuss in the following motivating scenario.

% Motiviating Example 2: Google Cancer Detect API...

\subsubsection{Motivating Scenario II: Cancer Detection \gls{cdss}}
\label{ssec:introduction:motivation:scenario:cancer}

Recent works in the oncology domain have used deep-learning \glspl{cnn} to detect \glspl{roi} in image scans of tissue (e.g., \citep{Liu:2018fa,Haenssle:2018bz,EhteshamiBejnordi:2017kq}), flagging these regions for doctors to review. Trials of such algorithms have been able to accurately detect cancer at higher rates than humans, and thus incorporating such capabilities into a \gls{cdss} is closer within reach. Some studies have suggested that practitioner over-reliance may erode independent decision-making \citep{Jaspers:2011hy,Chambers:1991uh}; therefore the risks in developing \glspl{cdss} powered by intelligent services become paramount.

In \cref{fig:introduction:motivation:scenario:cancer} we present a context diagram for a fictional \gls{cdss} named \textit{CancerAssist}. CancerAssist is used by a team of busy pathologists who review patient lymph node scans and discuss and recommend, on consensus, if the patient should or should not be sent to surgery. When consensus is made, the lead pathologist enters the verdict into CancerAssist---running passively in the background---to ensure no oversight has been made in the team's discussions. When a conflict exists between the team's verdict and CancerAssist's verdict, the system produces the scan with regions of interest it thinks the team should review. Where the team override the output of CancerAssist, this helps to reinforce CancerAssist's internal model as a \gls{hitl} learning process.

\begin{figure}[th]
  \includegraphics[width=\linewidth]{cancer-assist}
  \caption{CancerAssist Context Diagram}
  \label{fig:introduction:motivation:scenario:cancer}
\end{figure}

Powering CancerAssist is Google AI's Lymph Node Assistant (LYNA) \citep{Liu:2018fa}, a \gls{cnn} based on the Inception-v3 model \citep{Szegedy:2016ws,Krizhevsky:2012wl}. To provide intelligence to CancerAssist, LYNA is hosted on a \gls{cis}, and thus the developers of CancerAssist call the relevant \gls{cis} \gls{api} endpoints, in conjunction with extra information such as patient data and medical history, to produce the verdict. In the case of a positive verdict, the relevant \glspl{roi} CancerAssist has found are highlighted with their respective bounding boxes and their respective cancer detection accuracies.

The developer of CancerAssist has no interaction with the Data Science team maintaining the LYNA \gls{cis}. As a result, they are unaware when updates to the model occur, nor do they know what training data they could provide to test their own system. The default assumptions are that the training data used to power the intelligence is near-perfect for universal situations; i.e., the algorithm chosen is the correct one for all the ontology tests that need to be assessed in the given use case of CancerAssist. Thus, unlike deterministic systems---where the developer can manually test and validate the outcomes of the \glspl{api}---this is impossible for non-deterministic systems such as CancerAssist and its underlying \gls{cis}. The ramifications of not being able to test such a system and putting it out into production may prove fatal to patients.

Several questions in the production of CancerAssist and its use of a \gls{cis} may come into mind:

\begin{itemize}
  \item When is the model updated and how do the Data Science team communicate that the model is updated?
  \item What benchmark test set of data do I use to ensure that the changed model doesn't affect other results?
  \item How do we know that the assumptions made by the Data Science team in training the model are correct?
\end{itemize}

Thus, it may be the case the improved documentation to communicate with the \gls{cis} and additional metadata is needed in the response object may prove useful. Such claims are further expanded upon in the following section.

%Context diagram