\section{Motivation: Current Developer Mindsets'}
\label{sec:introduction:motivation}

\cref{fig:introduction:stackoverflow-trends} shows an increasing trend to the adoption and discussion of \glspl{iws} (particularly, \glspl{cvs}) with developers. These services are accessible through \glspl{api} and consist of an `intelligence' black box (\cref{fig:introduction:cloud-intelliegnce-service}). When a term `black box' is used, the input (or stimulus) is transformed to its outputs (or response) without any understanding of the internal architecture by which this transformation occurs, a theory arising from the electronic sciences and adapted to wider applications since the 1950s--60s \citep{Ashby:1957db,Bunge:1963jm} to describe ``systems whose internal mechanisms are not fully open to inspection'' \citep{Ashby:1957db}. 

In the world of machine learning and data mining, where we develop algorithms to make predictions in our datasets or discover patterns within them, these black boxes are inherently probabilistic and stochastic; there is little room for certainty in these results as such insight is purely statistical and associational \citep{Pearl:2018uv} against its training dataset. As an example, a \gls{cvs} returns the \textit{probability} that a particular object (the response) exists in the raw pixels (the stimulus), and thus for a more certain (though not fully certain) distribution of overall confidence returned from the service, a developer must treat the problem stochastically by testing this case hundreds if not thousands of times to find a richer interpretation of the inference made. Developers (at present) do not need to treat their programs in any such stochastic way given their rule-driven mindset that computers will always make certain outcomes.

There are thus therefore three key factors to consider when implementing, testing and developing with an \gls{iws}: (i) the \gls{api} usability, (ii) the nature of nondeterministic and probabilistic systems, and (iii) how both impact on software quality.

% TODO: Copied from ml inconsistency paper
\subsection{The Impact on Software Quality}
\label{ssec:introduction:motivation:impact}

Do traditional techniques for documenting deterministic \glspl{api} also apply to non-deterministic systems? As \glspl{api} reflect a set of design choices made by their providers intended for use by the developer, does the mindset between the machine learning architect and the novice programmer match? Evaluations of \gls{api} usability advocate for the accuracy, consistency and completeness of \glspl{api} and their documentation \citep{Piccioni:2013em,Robillard:2009uk} written by providers, while providers should consider mismatches between the developer's conceptual knowledge of the \gls{api} its implementation \citep{Ko:2011fb}. However, consistency cannot be guaranteed in probabilistic systems, and the conceptual knowledge of such systems are still treated like black boxes. It is therefore imperative that \gls{iws} providers consider the impact of their \gls{api} usability; if not, poor \gls{api} usability hinders on the internal quality of development practices, slowing developers down to produce the software they need to create.

Moreover, \gls{iws} \glspl{api} are inherently non-deterministic in nature, but developers are still taught with the deterministic mindset that all \gls{api} calls are the same. Simple arithmetic representations (e.g., $2+2=4$) will \textit{always} result in 4; but a multi-layer perceptron \glslong{nn} performing similar arithmetic representation \citep{Blake:1998vd} gives the probability where the target output (\textit{exactly} 4) and the output inferred (\textit{possibly} 4) matches as a percentage (or as an error where it does not match). That is, instead of an exact output, there is instead a \textit{probabilistic} result: $2+2$ \textit{may} equal 4 with a confidence of $n$. External quality must therefore be considered in the outcome of these systems, such as in the case of thresholding values, to consider whether or not the inference has a high enough confidence to justify its result to end-users.

In order to fully understand this problem, there are multiple dimensions one must consider: the impact of software quality; the fact that these systems underneath are probabilistic and are stochastic; the cognitive biases of determinism in developers; the issue of consistency in \gls{api} usage. While existing literature does extensively explore software quality and \gls{api} usability, these studies have only had emphasis on deterministic systems and thus little work to date has investigated such factors on probabilistic systems that make up the core of \glspl{cvs}. We explore more of these facets in the motivating scenarios below.

% TODO: Copied from ml inconsistency paper
\subsection{Motivating Scenarios}
\label{ssec:introduction:motivation:scenario}

The market for \glspl{iws} is increasing (\cref{fig:introduction:ai-products}) and as is developer uptake and enthusiasm in the software engineering community (\cref{fig:introduction:stackoverflow-trends}). We investigate the impact of the mismatch between the developer's mindset and the service provider's mindset as little work has been presented in literature. How do developers work with an \gls{iws}, how usable are these cloud \glspl{api}, and how well do developers understand the non-deterministic and stochastic nature of a services backed by machine-learnt models? 

To illustrate the context of use, we present the two scenarios of varying risk: (i)~a fictional software developer, named Tom, who wishes to develop an inherently low-risk photo detection application for his friends and family; and (ii) a high-risk cancer \gls{cdss} that uses patient scans to recommend to surgeons if the patient should be sent to surgery.
  
\subsubsection{Motivating Scenario I: Tom's \textit{PhotoSharer} App}
\label{ssec:introduction:motivation:scenario:pam}

Tom wants to develop a social media photo-sharing app on iOS and Android, \textit{PhotoSharer}, that analyses photos taken on smartphones as they are taken. Tom wants the app to categorise photos into scenes (e.g., day vs. night, landscape vs. indoors), generate brief descriptions of each photo, and catalogue photos of his friends as well as common objects (e.g., all photos with his Border Collie dog, all photos taken on a beach on a sunny day). His app will then share all of this analysed intelligence of his photos with his friends on a social-media-like platform, where his friends can search and view the photos.

Rather than building a computer vision engine from scratch, which would take far too much time and effort, Tom thinks he can achieve this using one of the common \glspl{cvs}. Tom comes from a typical software engineering background and has insufficient knowledge of key computer vision terminology and no understanding of its underlying techniques. However, inspired by easily accessible cloud \glspl{api} that offer computer vision analysis, he chooses to use these. Built upon his experience of using other similar cloud services, he decides on one of the \gls{cvs} \glspl{api}, and expects a static result always and consistency between similar \glspl{api}. Analogously, when Tom invokes the iOS Swift substring method \texttt{"doggy".prefix(3)}, he rightfully expects it to be consistent with the Android Java equivalent \texttt{"doggy".substring(0, 2)}. Consistent, here, means two things: (i) that \texttt{`dog'} will \textit{always} be returned every time he invokes the method in either language (i.e., a static response); and (ii) that \texttt{`dog'} will \textit{always} be returned regardless of what programming language or string library is used, given the deterministic nature of the `substring' construct (i.e., results for substring are \gls{api}-agnostic). 

\input{mainmatter/introduction/tables/changing-confidence}

More concretely, in \cref{tab:introduction:motivation:scenario:pam:confchanges}, we illustrate how three (anonymised) \gls{cvs} providers fail to provide similar consistency to that of the substring example above. If Tom uploads a photo of a border collie\footnote{The image used for these results can be found at \url{https://www.akc.org/dog-breeds/border-collie/}.} to three different providers in August 2018 and January 2019, he would find that each provider is different in both the vocabulary used between. The confidence values and labels within the \textit{same} provider also vary within a matter of five months. The evolution of the confidence changes is not explicitly documented by the providers (i.e., when the models change) nor do they document what confidence even means. Their current tautological nature of the definition of these changing confidence values (as presented in the \gls{api} documentation) provides no insight for Tom to understand why there was a change in confidence, which we show in \cref{tab:introduction:motivation:scenario:pam:tautological}, unless he \textit{knows} that the underlying models change with them. Thus, the deterministic problem of a substring compared to the nondeterministic nature of the \gls{iws} is not so simple to comprehend unless he is explicitly told.


\input{mainmatter/introduction/tables/tautological-definitions}

To make an assessment of these \glspl{api}, he tries his best to read through the documentation of some \gls{cvs} \glspl{api}, but he has no guiding framework to help him choose the right one. Some of the questions that come to mind include:

\begin{itemize}
  \item What does `confidence' mean? How does he use it?
  \item Are these \glspl{api} consistent in how they respond?
  \item Are the responses in \glspl{api} static and deterministic?
  \item Will he need a combination of multiple \gls{cvs} \glspl{api} to solve this task?
  \item How does he know when there is a defect in the response? How can he report it?
  \item How does he know what labels the \gls{api} can pick up, and what labels it can't?
  \item How does it describe his photos and detect the faces?
  \item How can he interpret the results if he disagrees with it to help improve his app?
  \item Does he understand that the \gls{api} uses a machine learnt model? Does he know what a \gls{ml} model even is?
  \item If so, does he know when the models update? What is the release cycle?
\end{itemize}

Dazzled by this, he does some brief reading on Wikipedia but is confused by the immense technical detail to take in. He would like some form of guiding communication framework to assist him and in software engineering terms aligned to his background.

Although Tom generally anticipates some imperfections, he has no prior benchmark to guide him on what to expect. He understands that the app is not always going to be perfect: perhaps some photos of his dog may be missed because the dog is in the background and not the foreground, or his friends can't find the photos of their recent trip to the beach because it wasn't sunny enough for the beach to be recognised. These imperfections appear to be low-risk, but may become socially awkward when in use; for instance, if some of Tom's friends have low self-esteem and use the app, they may be sensitive to being misidentified or even mislabelled. Privacy issues also come into play especially if certain friends have only access to certain photos that they are (supposedly) in; e.g., photos from a holiday with Tom and his partner, however if the \gls{api} identifies Tom's partner as a work colleague, then Tom's partner's privacy is at risk.

% Motiviating Example 2: Google Cancer Detect API...

\subsubsection{Motivating Scenario II: Cancer Detection \gls{cdss}}
\label{ssec:introduction:motivation:scenario:cancer}

Recent works in the oncology domain have used deep-learning \glspl{cnn} to detect \glspl{roi} in image scans of tissue (e.g., \citep{Liu:2018fa,Haenssle:2018bz,EhteshamiBejnordi:2017kq}), flagging these regions for doctors to review. Trials of such algorithms have been able to accurately detect cancer at higher rates than humans, and thus incorporating such capabilities into a \gls{cdss} is closer within reach. Some studies have suggested that practitioner over-reliance may erode independent decision-making \citep{Jaspers:2011hy,Chambers:1991uh}; therefore the risks in developing \glspl{cdss} powered by \glspl{iws} become paramount.

In \cref{fig:introduction:motivation:scenario:cancer} we present a context diagram for a fictional \gls{cdss} named \textit{CancerAssist}. CancerAssist is used by a team of busy pathologists who review patient lymph node scans and discuss and recommend, on consensus, if the patient should or should not be sent to surgery. When consensus is made, the lead pathologist enters the verdict into CancerAssist---running passively in the background---to ensure no oversight has been made in the team's discussions. When a conflict exists between the team's verdict and CancerAssist's verdict, the system produces the scan with \glspl{roi} it thinks the team should review. Where the team override the output of CancerAssist, this helps to reinforce CancerAssist's internal model as a \gls{hitl} learning process.

\begin{figure}[th]
\centering
  \includegraphics[width=0.85\linewidth]{cancer-assist}
  \caption[CancerAssist Context Diagram]{CancerAssist Context Diagram. \textit{\textbf{Key:} Red Arrows~=~Scan Input; Yellow Arrows~=~Decision Output; Blue Arrows~=~\gls{hitl} Feedback Input.}}
  \label{fig:introduction:motivation:scenario:cancer}
\end{figure}

Powering CancerAssist is Google AI's Lymph Node Assistant (LYNA) \citep{Liu:2018fa}, a \gls{cnn} based on the Inception-v3 model \citep{Szegedy:2016ws,Krizhevsky:2012wl}. To provide intelligence to CancerAssist, LYNA is hosted on an \gls{iws}, and thus the developers of CancerAssist call the relevant \gls{iws} \gls{api} endpoints, in conjunction with extra information such as patient data and medical history, to produce the verdict. In the case of a positive verdict, the relevant \glspl{roi} CancerAssist has found are highlighted with their respective bounding boxes and their respective cancer detection accuracies.

The developer of CancerAssist has no interaction with the Data Science team maintaining the LYNan \gls{iws}. As a result, they are unaware when updates to the model occur, nor do they know what training data they could provide to test their own system. The default assumptions are that the training data used to power the intelligence is near-perfect for universal situations; i.e., the algorithm chosen is the correct one for all the ontology tests that need to be assessed in the given use case of CancerAssist. Thus, unlike deterministic systems---where the developer can manually test and validate the outcomes of the \glspl{api}---this is impossible for non-deterministic systems such as CancerAssist and its underlying \gls{iws}. The ramifications of not being able to test such a system and putting it out into production may prove fatal to patients.

Certain questions in the production of CancerAssist and its use of an \gls{iws} may come into mind:

\begin{itemize}
  \item When is the model updated and how do the \gls{iws} team communicate that the model is updated?
  \item What benchmark test set of data do I use to ensure that the changed model doesn't affect other results?
  \item How do we know that the assumptions made by the \gls{iws} team who train model are correct?
\end{itemize}

Thus, improved documentation, additional metadata, and guidance tooling may be needed to better improve communication between developers and \gls{iws} providers. Such claims are further expanded upon in the following section.

\subsection{Motivation Summary}

\Cref{ssec:introduction:motivation:scenario} describes two instances where an \gls{iws} can have either minor or substantiative impact when the nuances of \gls{ml} are misinterpreted (or unappreciated) by the software engineers who use them. The impact (as discussed in \cref{ssec:introduction:motivation:impact}) fundamentally falls on the developer's mindset---this hinders the developer experience, productivity, and understanding/appreciation of \gls{ai}-based components. Thus, both the internal and external quality of applications using these components may have real-world consequences.

While developers are expected to use these services as they would any other traditional software component, the marketed promise is that such services are forever-learning and, therefore, `improve' in their accuracies and judgements. But what are the real-world consequences if this `black box' evolves with time and has materially substantiate impacts on both the software that is built on top of them, and (more importantly) the \textit{people} using that software? If the impact is substantial, the robustness of applications dependent on these services can have drastic effects, and---depending of the context of the application---the real-world damage it may do could be catastrophic. Should we consider this an `error' of the service?

% Real world implications -- what is an error? (i.e., it depends on the application context)
Whether or not it is an error is dependent on the context of the consuming application. For example, an educational mobile app designed to teach different dog breeds to children using a fine-grade visual categorisation service wouldn't be substantially erroneous should a Boston Terrier be misclassified as a French Bulldog---the \textit{ultimate effects} are not substantiate enough. However, consider a safety-critical application where an intelligent service performs facial recognition to identify a potential suspect in CCTV footage; evolution within the empowering \gls{iws} cannot be afforded, especially given the ethical and legal considerations if the wrong person is sentenced. If such risk is not mitigated, the accountability of these services must be considered~\citep{DoshiVelez:2017vm}.

% Why is it happening? (DS land vs Eng. Land)
Traditional software engineering principles advocate for software systems to be versioned upon substantial change, but unfortunately we find that the most prominent cloud vendors providing these intelligent services (Microsoft Azure, Google Cloud and Amazon Web Services) do not release new versioned endpoints of the APIs when the \textit{internal model} changes~\citep{Cummaudo:2019va}. In the context of computer vision, new labels may be introduced or dropped, entire ontologoies or specific training parameters may change, but we hypothesise that is not effectively communicated to developers. Broadly speaking, this can be attributed to a dichotomy of release cycles from the data science and software engineering communities: the data science iterations and work by which new models are trained and released runs at a faster cycle than the maintenance cycle of traditional software engineering. Thus we see cloud vendors integrating model changes without the \textit{need} to update the \gls{api} version unless substantial code or schema changes are also introduced---the nuance changes in the internal model does not warrant a shift in the \gls{api} itself, and therefore the version shift in a new model does not always propagate to a version shift in the \gls{api} endpoint.

% Versioning alone is not enough!!
Currently, it is impossible to invoke requests specific to a particular model that was trained at a particular date in time, and therefore developers need to consider how evolutionary changes of the services may impact their solutions \textit{in production}. Again, whether there is any noticeable behavioural changes from these changes is dependent on the context of the problem domain---unless developers benchmark these changes against their own domain-specific dataset and frequently check their selected service against such a dataset, there is no way of knowing if substantive errors have been introduced. 

Given that such substantiative \gls{se} principles on versioning, quality and robustness are under-investigated within the context of \glspl{iws}, we aim to explore guidance from the \gls{se} literature to investigate what aspects in the development lifecyle could aide in mitigating these issues when developing using \gls{ai}-based components. The following section explains the research outcomes resolving from this thesis.